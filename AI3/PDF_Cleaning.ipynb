{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraphs extracted and saved to C:\\Users\\eanam\\OneDrive\\Desktop\\AI3\\extracted_paragraphs.csv\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def clean_paragraph(text):\n",
    "    \"\"\"\n",
    "    Clean the paragraph text by removing extra spaces and unwanted characters.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'[^\\w\\s.!?;:\\'\\-]', '', text)  # Remove unwanted characters\n",
    "    return ' '.join(text.split())  # Normalize spaces\n",
    "\n",
    "def remove_header_footer(text):\n",
    "    \"\"\"\n",
    "    Remove potential header and footer content such as chapter names, page numbers,\n",
    "    section numbering, bullet points, repeated subheader-like lines, and references to tables, figures, and theorems.\n",
    "    \"\"\"\n",
    "    lines = text.splitlines()\n",
    "    filtered_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Skip lines with page numbers (e.g., \"Page 1\", \"1\", \"1/10\", etc.)\n",
    "        if re.match(r'^(Page\\s*\\d+|\\d+(/\\d+)?\\s*)$', line, re.IGNORECASE):\n",
    "            continue\n",
    "\n",
    "        # Skip lines with chapter titles\n",
    "        if re.match(r'^(Chapter\\s+\\d+|CHAPTER\\s+\\w+)', line, re.IGNORECASE):\n",
    "            continue\n",
    "\n",
    "        # Skip decorative lines (e.g., \"-----\", \"====\", etc.)\n",
    "        if re.match(r'^[-=_]{3,}$', line.strip()):\n",
    "            continue\n",
    "\n",
    "        # Skip lines with section numbering like \"13.8 Topic Name\"\n",
    "        if re.match(r'^\\d+\\.\\d+\\s+.+$', line.strip()):\n",
    "            continue\n",
    "\n",
    "        # Skip subsection numbering like \"1.2.2 Subsection Name\" or deeper levels like \"1.2.4.1.2\"\n",
    "        if re.match(r'^\\d+(\\.\\d+)+\\s+.+$', line.strip()):\n",
    "            continue\n",
    "\n",
    "        # Skip lines starting with '0 ' (zero followed by space)\n",
    "        if re.match(r'^0\\s+', line):\n",
    "            continue\n",
    "\n",
    "        # Skip specific repeated subheaders or bullet-like lines\n",
    "        if re.match(r'^(learning to\\s+.+)', line, re.IGNORECASE):\n",
    "            continue\n",
    "\n",
    "        # Skip references to tables, figures, and theorems\n",
    "        if re.match(r'^(Table|Figure|Theorem)\\s+\\d+\\.\\d+', line, re.IGNORECASE):\n",
    "            continue\n",
    "\n",
    "        filtered_lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(filtered_lines)\n",
    "\n",
    "def extract_paragraphs_with_continuation(pdf_path, output_csv_path):\n",
    "    \"\"\"\n",
    "    Extract paragraphs from a PDF, considering continuation across pages, and save to a CSV file.\n",
    "\n",
    "    :param pdf_path: Path to the PDF file.\n",
    "    :param output_csv_path: Path to the CSV file to save the extracted paragraphs.\n",
    "    \"\"\"\n",
    "    paragraphs = []\n",
    "    current_paragraph = \"\"\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                # Extract text from the current page\n",
    "                text = page.extract_text()\n",
    "\n",
    "                if text:\n",
    "                    # Remove headers/footers and unwanted lines\n",
    "                    filtered_text = remove_header_footer(text)\n",
    "                    lines = filtered_text.splitlines()\n",
    "\n",
    "                    for line in lines:\n",
    "                        line = line.strip()\n",
    "                        if not line:  # Skip empty lines\n",
    "                            continue\n",
    "                        \n",
    "                        # Add line to the current paragraph\n",
    "                        if current_paragraph:\n",
    "                            current_paragraph += \" \" + line\n",
    "                        else:\n",
    "                            current_paragraph = line\n",
    "\n",
    "                        # Check if the paragraph ends\n",
    "                        if line.endswith(('.', '!', '?')):\n",
    "                            paragraphs.append(clean_paragraph(current_paragraph))\n",
    "                            current_paragraph = \"\"  # Reset for the next paragraph\n",
    "            \n",
    "            # Add any remaining paragraph\n",
    "            if current_paragraph:\n",
    "                paragraphs.append(clean_paragraph(current_paragraph))\n",
    "\n",
    "        # Write paragraphs to a CSV file\n",
    "        with open(output_csv_path, mode='w', encoding='utf-8', newline='') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            writer.writerow([\"Paragraph\"])  # Add header\n",
    "            for paragraph in paragraphs:\n",
    "                writer.writerow([paragraph])\n",
    "\n",
    "        print(f\"Paragraphs extracted and saved to {output_csv_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Usage\n",
    "pdf_file = r\"C:\\Users\\eanam\\OneDrive\\Desktop\\AI3\\AI_Russell_Norvig_removed.pdf\"\n",
    "output_csv = r\"C:\\Users\\eanam\\OneDrive\\Desktop\\AI3\\extracted_paragraphs.csv\"\n",
    "\n",
    "extract_paragraphs_with_continuation(pdf_file, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraphs extracted and saved to C:\\Users\\eanam\\OneDrive\\Desktop\\AI3\\extracted_paragraphs.csv\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def clean_paragraph(text):\n",
    "    \"\"\"\n",
    "    Clean the paragraph text by removing extra spaces and unwanted characters.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'[^\\w\\s.!?;:\\'\\-]', '', text)  # Remove unwanted characters\n",
    "    return ' '.join(text.split())  # Normalize spaces\n",
    "\n",
    "def remove_header_footer(text):\n",
    "    \"\"\"\n",
    "    Remove potential header and footer content such as chapter names, page numbers,\n",
    "    section numbering, bullet points, repeated subheader-like lines, and references to tables, figures, and theorems.\n",
    "    \"\"\"\n",
    "    lines = text.splitlines()\n",
    "    filtered_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Skip lines like \"1 Why Machine Learning Strategy\"\n",
    "        if re.match(r'^\\d+\\s+.+$', line):\n",
    "            continue\n",
    "\n",
    "        # Skip lines like \"Page 6 Machine Learning Yearning-Draft Andrew Ng\"\n",
    "        if re.match(r'^Page\\s+\\d+.+$', line, re.IGNORECASE):\n",
    "            continue\n",
    "\n",
    "        # Skip lines with chapter titles\n",
    "        if re.match(r'^(Chapter\\s+\\d+|CHAPTER\\s+\\w+)', line, re.IGNORECASE):\n",
    "            continue\n",
    "\n",
    "        # Skip decorative lines (e.g., \"-----\", \"====\", etc.)\n",
    "        if re.match(r'^[-=_]{3,}$', line.strip()):\n",
    "            continue\n",
    "\n",
    "        # Skip lines with section numbering like \"13.8 Topic Name\"\n",
    "        if re.match(r'^\\d+\\.\\d+\\s+.+$', line.strip()):\n",
    "            continue\n",
    "\n",
    "        # Skip subsection numbering like \"1.2.2 Subsection Name\" or deeper levels like \"1.2.4.1.2\"\n",
    "        if re.match(r'^\\d+(\\.\\d+)+\\s+.+$', line.strip()):\n",
    "            continue\n",
    "\n",
    "        # Skip lines starting with '0 ' (zero followed by space)\n",
    "        if re.match(r'^0\\s+', line):\n",
    "            continue\n",
    "\n",
    "        # Skip lines starting with \"Topic Name\" followed by any integer\n",
    "        if re.match(r'^Topic Name\\s+\\d+', line, re.IGNORECASE):\n",
    "            continue\n",
    "\n",
    "        # Skip references to tables, figures, and theorems\n",
    "        if re.match(r'^(Table|Figure|Theorem)\\s+\\d+\\.\\d+', line, re.IGNORECASE):\n",
    "            continue\n",
    "\n",
    "        # Skip lines that appear as author name, footer notes, etc.\n",
    "        if re.match(r'^(Author Name|Book Title|.*\\s*\\d{4})$', line, re.IGNORECASE):\n",
    "            continue\n",
    "\n",
    "        # Skip lines with patterns like \"Page X Title-Subtitle Author\"\n",
    "        if re.match(r'^Page\\s+\\d+\\s+.*$', line, re.IGNORECASE):\n",
    "            continue\n",
    "\n",
    "        filtered_lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(filtered_lines)\n",
    "\n",
    "def extract_paragraphs_with_continuation(pdf_path, output_csv_path):\n",
    "    \"\"\"\n",
    "    Extract paragraphs from a PDF, considering continuation across pages, and save to a CSV file.\n",
    "\n",
    "    :param pdf_path: Path to the PDF file.\n",
    "    :param output_csv_path: Path to the CSV file to save the extracted paragraphs.\n",
    "    \"\"\"\n",
    "    paragraphs = []\n",
    "    current_paragraph = \"\"\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                # Extract text from the current page\n",
    "                text = page.extract_text()\n",
    "\n",
    "                if text:\n",
    "                    # Remove headers/footers and unwanted lines\n",
    "                    filtered_text = remove_header_footer(text)\n",
    "                    lines = filtered_text.splitlines()\n",
    "\n",
    "                    for line in lines:\n",
    "                        line = line.strip()\n",
    "                        if not line:  # Skip empty lines\n",
    "                            continue\n",
    "                        \n",
    "                        # Add line to the current paragraph\n",
    "                        if current_paragraph:\n",
    "                            current_paragraph += \" \" + line\n",
    "                        else:\n",
    "                            current_paragraph = line\n",
    "\n",
    "                        # Check if the paragraph ends\n",
    "                        if line.endswith(('.', '!', '?')):\n",
    "                            paragraphs.append(clean_paragraph(current_paragraph))\n",
    "                            current_paragraph = \"\"  # Reset for the next paragraph\n",
    "            \n",
    "            # Add any remaining paragraph\n",
    "            if current_paragraph:\n",
    "                paragraphs.append(clean_paragraph(current_paragraph))\n",
    "\n",
    "        # Write paragraphs to a CSV file\n",
    "        with open(output_csv_path, mode='w', encoding='utf-8', newline='') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            writer.writerow([\"Paragraph\"])  # Add header\n",
    "            for paragraph in paragraphs:\n",
    "                writer.writerow([paragraph])\n",
    "\n",
    "        print(f\"Paragraphs extracted and saved to {output_csv_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Usage\n",
    "pdf_file = r\"C:\\Users\\eanam\\OneDrive\\Desktop\\AI3\\AI_Russell_Norvig_removed.pdf\"\n",
    "output_csv = r\"C:\\Users\\eanam\\OneDrive\\Desktop\\AI3\\extracted_paragraphs.csv\"\n",
    "\n",
    "extract_paragraphs_with_continuation(pdf_file, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text extracted and saved to C:\\Users\\eanam\\OneDrive\\Desktop\\AI3\\extracted_raw_text.csv\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def restore_spaces(text):\n",
    "    \"\"\"\n",
    "    Restore spaces in the extracted text by identifying concatenated words.\n",
    "    \"\"\"\n",
    "    # Regex to insert spaces where necessary:\n",
    "    # This pattern ensures spaces between lowercase and uppercase letters that are likely\n",
    "    # part of separate words.\n",
    "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)  # e.g., \"wordAnd\" -> \"word And\"\n",
    "    \n",
    "    # Add space where a lowercase letter is followed by another lowercase letter and a word boundary.\n",
    "    text = re.sub(r'([a-z])([a-z])', r'\\1 \\2', text)  # e.g., \"wordword\" -> \"word word\"\n",
    "    \n",
    "    # Ensure proper spacing between words that have a number and alphabet or vice versa\n",
    "    text = re.sub(r'([a-zA-Z])([0-9])', r'\\1 \\2', text)  # e.g., \"word123\" -> \"word 123\"\n",
    "    text = re.sub(r'([0-9])([a-zA-Z])', r'\\1 \\2', text)  # e.g., \"123word\" -> \"123 word\"\n",
    "\n",
    "    # Also, add a space between words and punctuation marks where necessary\n",
    "    text = re.sub(r'([a-zA-Z])([.,;!?])', r'\\1 \\2', text)  # e.g., \"word,word\" -> \"word, word\"\n",
    "\n",
    "    # Normalize multiple spaces to a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def extract_raw_text_to_csv(pdf_path, output_csv_path):\n",
    "    \"\"\"\n",
    "    Extract raw text from a PDF and save it to a CSV file,\n",
    "    ensuring that words are properly spaced.\n",
    "    \n",
    "    :param pdf_path: Path to the PDF file.\n",
    "    :param output_csv_path: Path to the CSV file to save the extracted text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        paragraphs = []\n",
    "\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                # Extract raw text from the current page\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    # Restore spaces in text and split into lines\n",
    "                    lines = [restore_spaces(line.strip()) for line in text.splitlines() if line.strip()]\n",
    "                    paragraphs.extend(lines)\n",
    "\n",
    "        # Write processed text to a CSV file\n",
    "        with open(output_csv_path, mode='w', encoding='utf-8', newline='') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            writer.writerow([\"Line\"])  # Add header\n",
    "            for line in paragraphs:\n",
    "                writer.writerow([line])\n",
    "\n",
    "        print(f\"Raw text extracted and saved to {output_csv_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Usage\n",
    "pdf_file = r\"C:\\Users\\eanam\\OneDrive\\Desktop\\AI3\\AI_Russell_Norvig_removed.pdf\"\n",
    "output_csv = r\"C:\\Users\\eanam\\OneDrive\\Desktop\\AI3\\extracted_raw_text.csv\"\n",
    "\n",
    "extract_raw_text_to_csv(pdf_file, output_csv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
