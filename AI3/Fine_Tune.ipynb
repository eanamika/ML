{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity scores calculated using SBERT and saved to Updated_File_With_Similarity_Scores_SBERT.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load your CSV file\n",
    "file_path = 'ML Interview Questions.csv'  \n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1')  \n",
    "\n",
    "# Load the SBERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "\n",
    "# Compute SBERT embeddings for Questions and Answers\n",
    "question_embeddings = model.encode(data['Question'].tolist(), convert_to_tensor=True)\n",
    "answer_embeddings = model.encode(data['GPT4 Answer'].tolist(), convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine similarity for each Question-Answer pair\n",
    "similarity_scores = util.cos_sim(question_embeddings, answer_embeddings).diagonal()\n",
    "\n",
    "# Add the similarity scores as a new column in the dataframe\n",
    "data['Similarity Score'] = similarity_scores.tolist()\n",
    "\n",
    "# Save the updated file to your device\n",
    "output_file_path = 'Updated_File_With_Similarity_Scores_SBERT.csv' \n",
    "data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Similarity scores calculated using SBERT and saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to Filtered_Data.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Updated_File_With_Similarity_Scores_SBERT.csv' \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Filter the data to include only rows with a similarity score above 0.8\n",
    "filtered_data = data[data['Similarity Score'] > 0.7]\n",
    "\n",
    "# Save the filtered data to a new CSV file\n",
    "filtered_file_path = 'Filtered_Data.csv'  \n",
    "filtered_data.to_csv(filtered_file_path, index=False)\n",
    "\n",
    "print(f\"Filtered data saved to {filtered_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in dataset: 162\n",
      "Rows with similarity score < 0.7: 39\n",
      "Rows with Label == 1: 39\n",
      "Labeled data saved to Labeled_Data.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Updated_File_With_Similarity_Scores_SBERT.csv'  \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Verify data loaded\n",
    "print(f\"Total rows in dataset: {len(data)}\")\n",
    "\n",
    "# Step 1: Filter rows where similarity score is less than 0.8\n",
    "filtered_data = data[data['Similarity Score'] < 0.7].copy()\n",
    "\n",
    "# Debug: Check how many rows are filtered\n",
    "print(f\"Rows with similarity score < 0.7: {len(filtered_data)}\")\n",
    "\n",
    "# Step 2: Add a label column: Label = 1 if Similarity Score < 0.7 (your updated request)\n",
    "filtered_data['Label'] = 1  # Assigning label 1 to all rows with similarity score < 0.8\n",
    "\n",
    "# Step 3: Verify if labeling works correctly\n",
    "print(f\"Rows with Label == 1: {len(filtered_data)}\")\n",
    "\n",
    "# Step 4: Save the filtered and labeled data to a new CSV file\n",
    "labeled_file_path = 'Labeled_Data.csv'  # Specify the desired output file name\n",
    "filtered_data.to_csv(labeled_file_path, index=False)\n",
    "\n",
    "print(f\"Labeled data saved to {labeled_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been created at: C:/Users/eanam/OneDrive/Desktop/AI3/Contradictory.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create questions and contradictory answers\n",
    "questions = [\n",
    "    \"What is Artificial Intelligence?\",\n",
    "    \"How does Machine Learning work?\",\n",
    "    \"What is the purpose of Deep Learning?\",\n",
    "    \"Why are neural networks important?\",\n",
    "    \"What is the role of data in AI?\",\n",
    "    \"How does supervised learning work?\",\n",
    "    \"What is unsupervised learning?\",\n",
    "    \"What does reinforcement learning focus on?\",\n",
    "    \"How does AI help in automation?\",\n",
    "    \"What is the goal of Machine Learning?\",\n",
    "    \"What are convolutional neural networks used for?\",\n",
    "    \"Why is data preprocessing important in ML?\",\n",
    "    \"What is a decision tree?\",\n",
    "    \"How does a support vector machine work?\",\n",
    "    \"What is overfitting in ML?\",\n",
    "    \"Why do we use gradient descent?\",\n",
    "    \"How does AI impact healthcare?\",\n",
    "    \"What is transfer learning?\",\n",
    "    \"How do generative models work?\",\n",
    "    \"What is the purpose of clustering algorithms?\",\n",
    "    \"Why are GPUs important for AI?\",\n",
    "    \"What does natural language processing do?\",\n",
    "    \"What is the goal of AI ethics?\",\n",
    "    \"How do AI models learn from data?\",\n",
    "    \"What is the significance of activation functions?\",\n",
    "    \"How do recurrent neural networks work?\",\n",
    "    \"What is a hyperparameter in Machine Learning?\",\n",
    "    \"How does AI improve customer experiences?\",\n",
    "    \"What is backpropagation in neural networks?\",\n",
    "    \"How does AI affect cybersecurity?\",\n",
    "    \"What is feature selection?\",\n",
    "    \"Why do we use regularization in ML?\",\n",
    "    \"How does a random forest algorithm work?\",\n",
    "    \"What is the purpose of an optimizer in ML?\",\n",
    "    \"How does AI handle big data?\",\n",
    "    \"What are autoencoders?\",\n",
    "    \"What is bias in Machine Learning?\",\n",
    "    \"Why do we need deep learning?\",\n",
    "    \"How does AI impact climate change?\",\n",
    "    \"What are the layers in a neural network?\",\n",
    "    \"What is the importance of training data?\",\n",
    "    \"Why is AI important in education?\",\n",
    "    \"How do robots learn tasks?\",\n",
    "    \"What is the purpose of a chatbot?\",\n",
    "    \"Why do we use loss functions in ML?\",\n",
    "    \"How does unsupervised learning differ from supervised learning?\",\n",
    "    \"What are the benefits of ensemble learning?\",\n",
    "    \"Why are activation functions important?\",\n",
    "    \"What is transfer learning in NLP?\",\n",
    "    \"Why is image recognition challenging?\",\n",
    "    \"How does overfitting occur?\",\n",
    "    \"What is the role of AI in agriculture?\",\n",
    "    \"How does reinforcement learning differ from supervised learning?\",\n",
    "    \"What is a GAN (Generative Adversarial Network)?\",\n",
    "    \"Why is natural language understanding important?\",\n",
    "    \"What is over-sampling in ML?\",\n",
    "    \"What does an ML pipeline consist of?\"\n",
    "]\n",
    "\n",
    "# Generate more variations by repeating and modifying\n",
    "questions = questions * 2  \n",
    "questions = questions[:100]  \n",
    "\n",
    "contradictory_answers = [\n",
    "    \"Artificial Intelligence is a concept focused on reducing the intelligence of machines.\",\n",
    "    \"Machine Learning is designed to avoid finding patterns in data.\",\n",
    "    \"Deep Learning aims to stay away from complex decision-making processes.\",\n",
    "    \"Neural networks exist to ignore any input data provided.\",\n",
    "    \"Data plays no role in AI; the models work without it.\",\n",
    "    \"Supervised learning ensures that no labels are ever used during training.\",\n",
    "    \"Unsupervised learning is all about using explicit labels for predictions.\",\n",
    "    \"Reinforcement learning focuses on discouraging any actions that lead to rewards.\",\n",
    "    \"AI deliberately avoids automating any repetitive tasks.\",\n",
    "    \"The goal of Machine Learning is to ensure that predictions are always incorrect.\",\n",
    "    \"CNNs are used to destroy image features rather than detect them.\",\n",
    "    \"Data preprocessing is unnecessary and has no impact on the results.\",\n",
    "    \"A decision tree is used to avoid making any decisions.\",\n",
    "    \"Support Vector Machines are meant to make boundaries between data indistinguishable.\",\n",
    "    \"Overfitting occurs when a model generalizes perfectly to all unseen data.\",\n",
    "    \"Gradient descent ensures the model never learns from its errors.\",\n",
    "    \"AI makes healthcare less efficient by ignoring all patterns in medical data.\",\n",
    "    \"Transfer learning is used to completely restart learning for every new task.\",\n",
    "    \"Generative models are designed to never generate any useful data.\",\n",
    "    \"Clustering algorithms scatter data points randomly rather than grouping them.\",\n",
    "    \"GPUs slow down computations and make AI processes less efficient.\",\n",
    "    \"NLP ensures that machines cannot understand human language.\",\n",
    "    \"AI ethics ensures machines always make unethical decisions.\",\n",
    "    \"AI models are incapable of learning from data and function randomly.\",\n",
    "    \"Activation functions deactivate the flow of information in neural networks.\",\n",
    "    \"RNNs forget everything from previous steps instead of remembering sequences.\",\n",
    "    \"Hyperparameters are adjusted to reduce the performance of ML models.\",\n",
    "    \"AI disrupts customer experiences by making services unpredictable.\",\n",
    "    \"Backpropagation ensures no learning occurs during model training.\",\n",
    "    \"AI weakens cybersecurity by making systems more vulnerable.\",\n",
    "    \"Feature selection involves removing the most informative features from the dataset.\",\n",
    "    \"Regularization ensures models overfit to training data.\",\n",
    "    \"Random forests prevent any consensus by growing uncorrelated decision trees.\",\n",
    "    \"Optimizers are used to make models converge to the worst solutions possible.\",\n",
    "    \"AI cannot process large datasets and breaks down when data volume increases.\",\n",
    "    \"Autoencoders are used to lose information during the encoding and decoding process.\",\n",
    "    \"Bias ensures that models remain inaccurate across all datasets.\",\n",
    "    \"Deep learning avoids solving any real-world problems.\",\n",
    "    \"AI actively contributes to increasing environmental harm.\",\n",
    "    \"The layers of a neural network are designed to block data flow between neurons.\",\n",
    "    \"Training data is unnecessary as models work without any input.\",\n",
    "    \"AI ensures that educational methods become less effective.\",\n",
    "    \"Robots are designed to never learn or execute any tasks.\",\n",
    "    \"Chatbots are developed to confuse users and avoid giving relevant answers.\",\n",
    "    \"Loss functions ensure that models never improve their predictions.\",\n",
    "    \"Unsupervised learning uses more supervision than supervised learning.\",\n",
    "    \"Ensemble learning ensures individual models cancel each other out.\",\n",
    "    \"Activation functions remove any non-linearity in the model.\",\n",
    "    \"Transfer learning ensures text models lose prior knowledge during fine-tuning.\",\n",
    "    \"Image recognition is easy because models avoid looking at pixel patterns.\",\n",
    "    \"Overfitting occurs when models perform equally well on training and test data.\",\n",
    "    \"AI ensures agricultural techniques become less productive.\",\n",
    "    \"Reinforcement learning avoids learning through rewards, unlike supervised learning.\",\n",
    "    \"GANs are models that prevent generating any realistic images.\",\n",
    "    \"NLU ensures that text remains incomprehensible to machines.\",\n",
    "    \"Over-sampling involves removing minority class samples to worsen model balance.\",\n",
    "    \"ML pipelines are designed to disrupt the workflow of data preparation and modeling.\"\n",
    "] * 2\n",
    "\n",
    "contradictory_answers = contradictory_answers[:100]\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\"Question\": questions, \"Answer\": contradictory_answers})\n",
    "\n",
    "# Save to CSV\n",
    "output_path = \"C:/Users/eanam/OneDrive/Desktop/AI3/Contradictory.csv\"  \n",
    "data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"CSV file has been created at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity scores calculated using SBERT and saved to Contradictory_Similarity_Scores_SBERT.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load your CSV file\n",
    "file_path = 'C:/Users/eanam/OneDrive/Desktop/AI3/Contradictory.csv' \n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1')  \n",
    "\n",
    "# Load the SBERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  \n",
    "\n",
    "# Compute SBERT embeddings for Questions and Answers\n",
    "question_embeddings = model.encode(data['Question'].tolist(), convert_to_tensor=True)\n",
    "answer_embeddings = model.encode(data['Answer'].tolist(), convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine similarity for each Question-Answer pair\n",
    "similarity_scores = util.cos_sim(question_embeddings, answer_embeddings).diagonal()\n",
    "\n",
    "# Add the similarity scores as a new column in the dataframe\n",
    "data['Similarity Score'] = similarity_scores.tolist()\n",
    "\n",
    "# Save the updated file to your device\n",
    "output_file_path = 'Contradictory_Similarity_Scores_SBERT.csv'  \n",
    "data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Similarity scores calculated using SBERT and saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to Contradictory_filter_data.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Contradictory_Similarity_Scores_SBERT.csv'  \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Filter the data to include only rows with a similarity score above 0.8\n",
    "filtered_data = data[data['Similarity Score'] > 0.7]\n",
    "\n",
    "# Save the filtered data to a new CSV file\n",
    "filtered_file_path = 'Contradictory_filter_data.csv'  \n",
    "filtered_data.to_csv(filtered_file_path, index=False)\n",
    "\n",
    "print(f\"Filtered data saved to {filtered_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in dataset: 35\n",
      "Rows with similarity score < 0.7: 35\n",
      "Rows with Label == 1: 35\n",
      "Labeled data saved to Contradictory_Labeled_Data.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Contradictory_filter_data.csv'  \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Verify data loaded\n",
    "print(f\"Total rows in dataset: {len(data)}\")\n",
    "\n",
    "# Step 1: Filter rows where similarity score is less than 0.8\n",
    "filtered_data = data[data['Similarity Score'] > 0.7].copy()\n",
    "\n",
    "# Debug: Check how many rows are filtered\n",
    "print(f\"Rows with similarity score < 0.7: {len(filtered_data)}\")\n",
    "\n",
    "# Step 2: Add a label column: Label = 1 if Similarity Score < 0.8 (your updated request)\n",
    "filtered_data['Label'] = 0  # Assigning label 1 to all rows with similarity score < 0.8\n",
    "\n",
    "# Step 3: Verify if labeling works correctly\n",
    "print(f\"Rows with Label == 1: {len(filtered_data)}\")\n",
    "\n",
    "# Step 4: Save the filtered and labeled data to a new CSV file\n",
    "labeled_file_path = 'Contradictory_Labeled_Data.csv'  \n",
    "filtered_data.to_csv(labeled_file_path, index=False)\n",
    "\n",
    "print(f\"Labeled data saved to {labeled_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b83fdb5b5946f2acea0c00af6e3622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 31.7407, 'train_samples_per_second': 5.576, 'train_steps_per_second': 0.756, 'train_loss': 0.4546387990315755, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f62a7a6719645bba3ee489c34ac693c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.67\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Load the datasets\n",
    "related_df = pd.read_csv('Labeled_Data.csv')  # This contains columns: Question, GPT4 Answer, Similarity Score, Label\n",
    "contradictory_df = pd.read_csv('Contradictory_Labeled_Data.csv')  # This contains columns: Question, Answer, Similarity Score, Label\n",
    "\n",
    "# Step 2: Add a label column (1 for related, 0 for contradictory)\n",
    "related_df = related_df[['Question', 'GPT4 Answer', 'Label']]\n",
    "contradictory_df = contradictory_df[['Question', 'Answer', 'Label']]\n",
    "\n",
    "# Step 3: Combine the datasets and create the 'text' column for input\n",
    "related_df['text'] = related_df['Question'] + \" \" + related_df['GPT4 Answer']\n",
    "contradictory_df['text'] = contradictory_df['Question'] + \" \" + contradictory_df['Answer']\n",
    "\n",
    "# Combine both datasets\n",
    "df = pd.concat([related_df[['text', 'Label']], contradictory_df[['text', 'Label']]])\n",
    "\n",
    "# Step 4: Split the dataset into train and test\n",
    "train_data, test_data = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Step 5: Convert the data into SBERT InputExamples\n",
    "train_examples = [InputExample(texts=[row['text'], row['text']], label=row['Label']) for _, row in train_data.iterrows()]\n",
    "test_examples = [InputExample(texts=[row['text'], row['text']], label=row['Label']) for _, row in test_data.iterrows()]\n",
    "\n",
    "# Step 6: Load pre-trained SBERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  \n",
    "\n",
    "# Step 7: Prepare DataLoader for training\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=8)\n",
    "\n",
    "# Step 8: Use CosineSimilarityLoss for binary classification\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "# Step 9: Fine-tune the model\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=3,\n",
    "    warmup_steps=500,  \n",
    "    output_path='./sbert_finetuned_model'\n",
    ")\n",
    "\n",
    "# Step 10: Evaluate the model\n",
    "def evaluate_model(model, test_data):\n",
    "    # Get embeddings for the test data\n",
    "    test_embeddings = []\n",
    "    for example in test_data:\n",
    "        embedding = model.encode(example.texts[0])  # Get embedding for the first sentence\n",
    "        test_embeddings.append(embedding)\n",
    "    \n",
    "    # Calculate the cosine similarities and compare with the true labels\n",
    "    predictions = []\n",
    "    for i, embedding in enumerate(test_embeddings):\n",
    "        similarity = np.dot(embedding, embedding) / (np.linalg.norm(embedding) * np.linalg.norm(embedding))  # Cosine similarity\n",
    "        predicted_label = 1 if similarity > 0.5 else 0  # Threshold similarity for binary classification \n",
    "        predictions.append(predicted_label)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    true_labels = [example.label for example in test_data]\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "evaluate_model(model, test_examples)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
