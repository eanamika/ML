Paragraph
1
I NT RO DU CT IO N
In which we try to explain why we consider artificial intelligence to be a subject
mostworthyofstudyandinwhichwetrytodecidewhatexactlyitisthisbeinga
goodthingtodecidebeforeembarking.
We call ourselves Homo sapiensman the wisebecause our intelligence is so important
I NT EL LI GE NC E
tous. Forthousands ofyears wehavetried tounderstand how wethink; that is how amere
handful of matter can perceive understand predict and manipulate a world far larger and
A RT IF IC IA L more complicated than itself. The field of artificial intelligence or A I goes further still: it
I NT EL LI GE NC E
attemptsnotjusttounderstand butalsotobuildintelligent entities.
A Iisoneofthe newestfieldsinscience and engineering. Work started inearnest soon
after World War I I and the name itself was coined in 1956. Along with molecular biology
A Iisregularlycitedasthefield Iwouldmostliketobeinbyscientistsinotherdisciplines.
Astudentinphysicsmightreasonably feelthatallthegoodideashavealreadybeentakenby
Galileo Newton Einstein and the rest. A I on the other hand still has openings forseveral
full-time Einsteinsand Edisons.
A Icurrentlyencompassesahugevarietyofsubfieldsrangingfromthegenerallearning
andperceptiontothespecificsuchasplayingchessprovingmathematicaltheoremswriting
poetry driving a car on a crowded street and diagnosing diseases. A I is relevant to any
intellectual task;itistrulyauniversal field.
We have claimed that A I is exciting but we have not said what it is. In Figure 1.1 we see
eight definitions of A I laid out along twodimensions. The definitions on top are concerned
withthoughtprocessesandreasoningwhereastheonesonthebottomaddressbehavior. The
definitions on the left measure success in terms of fidelity to human performance whereas
the ones on the right measure against an ideal performance measure called rationality. A
R AT IO NA LI TY
systemisrationalifitdoestherightthinggivenwhatitknows.
Historically all four approaches to A I have been followed each by different people
withdifferentmethods. Ahuman-centered approachmustbeinpartanempiricalsciencein-
1
Thinking Humanly Thinking Rationally
Theexcitingnewefforttomakecomput- Thestudyofmentalfacultiesthroughthe
ers think ... machines with minds in the useofcomputational models.
fullandliteralsense. Haugeland 1985 Charniak and Mc Dermott1985
The automation of activities that we Thestudyofthecomputationsthatmake
associate with human thinking activities itpossibletoperceive reasonandact.
such as decision-making problem solv- Winston 1992
inglearning ...Bellman1978
Acting Humanly Acting Rationally
The art of creating machines that per- Computational Intelligence is the study
form functions that require intelligence ofthedesignofintelligentagents. Poole
when performed by people. Kurzweil etal.1998
1990
Thestudy ofhowtomakecomputers do A I ...is concerned with intelligent be-
thingsatwhichatthemomentpeopleare haviorinartifacts. Nilsson1998
better. Richand Knight1991
Figure1.1 Somedefinitionsofartificialintelligenceorganizedintofourcategories.
volvingobservationsandhypothesesabouthumanbehavior. Arationalist1 approachinvolves
acombination ofmathematicsandengineering. Thevariousgrouphavebothdisparaged and
helpedeachother. Letuslookatthefourapproaches inmoredetail.
The Turing Test proposed by Alan Turing 1950 was designed to provide a satisfactory
T UR IN GT ES T
operationaldefinitionofintelligence. Acomputerpassesthetestifahumaninterrogatorafter
posingsomewrittenquestions cannottellwhetherthewrittenresponsescomefromaperson
orfromacomputer. Chapter26discussesthedetailsofthetestandwhetheracomputerwould
really be intelligent if it passed. For now we note that programming a computer to pass a
rigorously applied test provides plenty towork on. Thecomputerwould need to possess the
followingcapabilities:
N AT UR AL LA NG UA GE naturallanguageprocessing toenableittocommunicatesuccessfully in English;
P RO CE SS IN G
K NO WL ED GE knowledgerepresentation tostorewhatitknowsorhears;
R EP RE SE NT AT IO N
A UT OM AT ED automated reasoning to use the stored information to answer questions and to draw
R EA SO NI NG
newconclusions;
machinelearningtoadapttonewcircumstancesandtodetectandextrapolatepatterns.
M AC HI NE LE AR NI NG
irrationalinthesenseof emotionallyunstable orinsane. Onemerelyneednotethat wearenot perfect:
notallchessplayersaregrandmasters;andunfortunatelynoteveryonegetsan Aontheexam.Somesystematic
errorsinhumanreasoningarecatalogedby Kahnemanetal.1982.
Section1.1. What Is A I? 3
Turingstestdeliberately avoideddirectphysicalinteraction betweentheinterrogatorandthe
computer because physical simulation ofaperson isunnecessary forintelligence. However
the so-called total Turing Test includes a video signal so that the interrogator can test the
T OT AL TU RI NG TE ST
subjects perceptual abilities as well as the opportunity for the interrogator to pass physical
objectsthrough thehatch. Topassthetotal Turing Testthecomputerwillneed
computervisiontoperceiveobjects and
C OM PU TE RV IS IO N
roboticstomanipulate objectsandmoveabout.
R OB OT IC S
These six disciplines compose most of A I and Turing deserves credit for designing a test
that remains relevant 60 years later. Yet A I researchers have devoted little effort to passing
the Turing Test believing that it is more important to study the underlying principles of in-
telligence thantoduplicate anexemplar. Thequest forartificial flightsucceeded whenthe
Wrightbrothers andothers stopped imitating birdsandstarted using windtunnels andlearn-
ing about aerodynamics. Aeronautical engineering texts do not define the goal of their field
asmakingmachinesthatflysoexactlylikepigeonsthattheycanfoolevenotherpigeons.
Ifwearegoing to saythat agiven program thinks like ahuman wemusthave some wayof
determining how humans think. Weneed toget inside the actual workings of human minds.
There are three ways to do this: through introspectiontrying to catch our own thoughts as
they go by; through psychological experimentsobserving a person in action; and through
brain imagingobserving the brain in action. Once wehave a sufficiently precise theory of
theminditbecomespossible toexpress thetheory asacomputer program. Iftheprograms
inputoutput behavior matchescorresponding human behavior thatisevidence thatsomeof
the programs mechanisms could also be operating in humans. For example Allen Newell
and Herbert Simonwhodeveloped G PSthe General Problem Solver Newelland Simon
1961 were not content merely to have their program solve problems correctly. They were
more concerned with comparing the trace of its reasoning steps to traces of human subjects
solving the same problems. The interdisciplinary field of cognitive science brings together
C OG NI TI VE SC IE NC E
computermodelsfrom A Iandexperimentaltechniques frompsychologytoconstructprecise
andtestabletheories ofthehumanmind.
Cognitivescienceisafascinating fieldinitselfworthyofseveraltextbooksandatleast
one encyclopedia Wilson and Keil 1999. Wewilloccasionally comment onsimilarities or
differences between A Itechniques andhumancognition. Realcognitive science howeveris
necessarily based on experimental investigation of actual humans or animals. We will leave
thatforotherbooks asweassumethereaderhasonlyacomputerforexperimentation.
In the early days of A I there was often confusion between the approaches: an author
would argue that an algorithm performs well on a task and that it is therefore a good model
of human performance or vice versa. Modern authors separate the two kinds of claims;
this distinction has allowed both A I and cognitive science to develop more rapidly. Thetwo
fields continue to fertilize each other most notably in computer vision which incorporates
neurophysiological evidence intocomputational models.
The Greekphilosopher Aristotlewasoneofthefirsttoattempttocodifyrightthinkingthat
is irrefutable reasoning processes. His syllogisms provided patterns forargument structures
S YL LO GI SM
thatalwaysyieldedcorrectconclusionswhengivencorrectpremisesforexample Socrates
is a man; all men are mortal; therefore Socrates is mortal. These laws of thought were
supposed togoverntheoperation ofthemind;theirstudyinitiated thefieldcalledlogic.
L OG IC
Logiciansinthe19thcenturydevelopedaprecisenotationforstatementsaboutallkinds
ofobjects intheworldandtherelations amongthem. Contrastthiswithordinaryarithmetic
notation which provides only for statements about numbers. By 1965 programs existed
that could inprinciple solve anysolvable problem described inlogical notation. Although
ifnosolution exists theprogram mightloop forever. Theso-called logicist tradition within
L OG IC IS T
artificialintelligence hopestobuildonsuchprogramstocreateintelligent systems.
There are two main obstacles to this approach. First it is not easy to take informal
knowledge and state it in the formal terms required by logical notation particularly when
the knowledge is less than 100 certain. Second there is a big difference between solving
a problem in principle and solving it in practice. Even problems with just a few hundred
facts can exhaust the computational resources of any computer unless it has some guidance
astowhichreasoningstepstotryfirst. Althoughbothoftheseobstaclesapplytoanyattempt
tobuildcomputational reasoning systems theyappearedfirstinthelogicisttradition.
An agent is just something that acts agent comes from the Latin agere to do. Of course
A GE NT
all computer programs do something but computer agents are expected to do more: operate
autonomously perceive their environment persist over a prolonged time period adapt to
change and create and pursue goals. A rational agent is one that acts so as to achieve the
R AT IO NA LA GE NT
bestoutcomeorwhenthereisuncertainty thebestexpectedoutcome.
Inthelawsofthought approach to A Itheemphasiswasoncorrectinferences. Mak-
ing correct inferences is sometimes part of being a rational agent because one way to act
rationally istoreason logically totheconclusion thatagivenaction willachieve ones goals
and then to act on that conclusion. On the other hand correct inference is not all of ration-
ality; insomesituations thereisnoprovably correct thingtodo butsomething muststillbe
done. There are also ways of acting rationally that cannot be said to involve inference. For
example recoiling from a hot stove is a reflex action that is usually more successful than a
sloweractiontakenaftercarefuldeliberation.
Alltheskillsneededforthe Turing Testalsoallowanagenttoactrationally. Knowledge
representation and reasoning enable agents to reach good decisions. We need to be able to
generate comprehensible sentences in natural language to get by in a complex society. We
need learning not only for erudition but also because it improves our ability to generate
effectivebehavior.
The rational-agent approach has two advantages over the other approaches. First it
is more general than the laws of thought approach because correct inference is just one
of several possible mechanisms for achieving rationality. Second it is more amenable to
Section1.2. The Foundations of Artificial Intelligence 5
scientificdevelopmentthanareapproaches basedonhumanbehaviororhumanthought. The
standard of rationality is mathematically well defined and completely general and can be
unpacked togenerateagentdesignsthatprovablyachieve it. Humanbehavior ontheother
hand is well adapted for one specific environment and is defined by well the sum total
of all the things that humans do. This book therefore concentrates on general principles
of rational agents and on components for constructing them. We will see that despite the
apparent simplicity with which the problem can be stated an enormous variety of issues
comeupwhenwetrytosolveit. Chapter2outlines someoftheseissuesinmoredetail.
Oneimportantpointtokeepinmind: Wewillseebeforetoolongthatachievingperfect
rationalityalways doing the right thingis notfeasible in complicated environments. The
computational demands are just too high. Formost of the book however we will adopt the
working hypothesis that perfect rationality isa good starting point foranalysis. It simplifies
the problem and provides the appropriate setting for most of the foundational material in
L IM IT ED the field. Chapters 5 and 17 deal explicitly with the issue of limited rationalityacting
R AT IO NA LI TY
appropriately whenthereisnotenoughtimetodoallthecomputations onemightlike.
Inthissectionweprovideabriefhistoryofthedisciplinesthatcontributedideasviewpoints
and techniques to A I. Like any history this one is forced to concentrate on a small number
of people events and ideas and to ignore others that also were important. We organize the
history aroundaseriesofquestions. Wecertainly wouldnot wishtogivetheimpression that
these questions arethe only ones the disciplines address or that the disciplines have allbeen
workingtoward A Iastheirultimatefruition.
Canformalrulesbeusedtodrawvalidconclusions?
Howdoesthemindarisefromaphysical brain?
Wheredoesknowledgecomefrom?
Howdoesknowledgeleadtoaction?
Aristotle 384322 B.C. whose bust appears on the front cover of this book was the first
to formulate a precise set of laws governing the rational part of the mind. He developed an
informalsystemofsyllogismsforproperreasoning whichinprincipleallowedonetogener-
ate conclusions mechanically given initial premises. Much later Ramon Lull d. 1315 had
theideathatuseful reasoning couldactually becarried out byamechanical artifact. Thomas
Hobbes 15881679 proposed thatreasoning waslike numerical computation thatweadd
and subtract in our silent thoughts. The automation of computation itself was already well
under way. Around 1500 Leonardo da Vinci 14521519 designed but did not build a me-
chanical calculator; recent reconstructions have shown the design to be functional. Thefirst
known calculating machine was constructed around 1623 by the German scientist Wilhelm
Schickard 15921635 although the Pascaline builtin1642by Blaise Pascal16231662
is more famous. Pascal wrote that the arithmetical machine produces effects which appear
nearer to thought than all the actions of animals. Gottfried Wilhelm Leibniz 16461716
built amechanical device intended to carry out operations on concepts rather than numbers
but its scope was rather limited. Leibniz did surpass Pascal by building a calculator that
could add subtract multiply and take roots whereas the Pascaline could only add and sub-
tract. Some speculated that machines might not just do calculations but actually be able to
think and act on their own. In his 1651 book Leviathan Thomas Hobbes suggested the idea
of an artificial animal arguing For what is the heart but a spring; and the nerves but so
manystrings; andthejointsbutsomanywheels.
Itsonethingtosaythatthemindoperatesatleastinpartaccordingtologicalrulesand
to build physical systems that emulate some of those rules; its another to say that the mind
itself is such a physical system. Rene Descartes 15961650 gave the first clear discussion
ofthedistinction betweenmindandmatterandoftheproblemsthatarise. Oneproblemwith
a purely physical conception of the mind is that it seems to leave little room for free will:
if the mind is governed entirely by physical laws then it has no more free will than a rock
decidingtofalltowardthecenteroftheearth. Descarteswasastrongadvocateofthepower
of reasoning in understanding theworld aphilosophy now called rationalism and one that
R AT IO NA LI SM
counts Aristotle and Leibnitz as members. But Descartes was also a proponent of dualism.
D UA LI SM
He held that there is a part of the human mind or soul or spirit that is outside of nature
exempt from physical laws. Animals on the other hand did not possess this dual quality;
they could be treated as machines. An alternative to dualism is materialism which holds
M AT ER IA LI SM
that the brains operation according to the laws of physics constitutes the mind. Free will is
simplythewaythattheperception ofavailable choicesappears tothechoosing entity.
Given a physical mind that manipulates knowledge the next problem is to establish
the source of knowledge. The empiricism movement starting with Francis Bacons 1561
E MP IR IC IS M
1626 Novum Organum2 ischaracterizedbyadictumof John Locke16321704: Nothing
is in the understanding which was not first in the senses. David Humes 17111776 A
Treatise of Human Nature Hume 1739 proposed what is now known as the principle of
induction: thatgeneralrulesareacquiredbyexposuretorepeatedassociations betweentheir
I ND UC TI ON
elements. Building on the work of Ludwig Wittgenstein 18891951 and Bertrand Russell
18721970 the famous Vienna Circle led by Rudolf Carnap 18911970 developed the
doctrineoflogicalpositivism. Thisdoctrineholdsthatallknowledgecanbecharacterizedby
L OG IC AL PO SI TI VI SM
O BS ER VA TI ON logical theories connected ultimately to observation sentences that correspond to sensory
S EN TE NC ES
inputs;thuslogicalpositivismcombinesrationalismandempiricism.3 Theconfirmationthe-
C ON FI RM AT IO N oryof Carnapand Carl Hempel19051997 attempted toanalyze theacquisition ofknowl-
T HE OR Y
edge from experience. Carnaps book The Logical Structure of the World 1928 defined an
explicit computational procedure for extracting knowledge from elementary experiences. It
wasprobably thefirsttheoryofmindasacomputational process.
seenasbothanempiricistandarationalist.
ofthemeaningofthewords.Becausethisrulesoutmostofmetaphysicsaswastheintentionlogicalpositivism
wasunpopularinsomecircles.
Section1.2. The Foundations of Artificial Intelligence 7
The final element in the philosophical picture of the mind is the connection between
knowledgeandaction. Thisquestionisvitalto A Ibecauseintelligencerequiresactionaswell
as reasoning. Moreover only by understanding how actions are justified can we understand
howtobuildanagentwhoseactionsarejustifiableorrational. Aristotlearguedin De Motu
Animaliumthatactionsarejustifiedbyalogicalconnectionbetweengoalsandknowledgeof
theactionsoutcomethelastpartofthisextractalsoappearsonthefrontcoverofthisbook
intheoriginal Greek:
Buthowdoesithappenthatthinkingissometimesaccompaniedbyactionandsometimes
not sometimes by motion and sometimes not? It looks as if almost the same thing
happensasinthecaseofreasoningandmakinginferencesaboutunchangingobjects.But
in that case the end is a speculative proposition ... whereas here the conclusionwhich
resultsfromthe twopremisesisan action. ...I needcovering;a cloakis a covering. I
needacloak. What Ineed Ihavetomake;Ineedacloak. Ihavetomakeacloak. And
theconclusionthe Ihavetomakeacloakisanaction.
In the Nicomachean Ethics Book I II. 3 1112b Aristotle further elaborates on this topic
suggesting analgorithm:
Wedeliberatenotaboutendsbutaboutmeans. Foradoctordoesnotdeliberatewhether
he shall heal nor an orator whether he shall persuade ... They assume the end and
considerhowandbywhatmeansitisattainedandifitseemseasilyandbestproduced
thereby;whileifitisachievedbyonemeansonlytheyconsiderhowitwillbeachieved
bythisandbywhatmeansthiswillbeachievedtilltheycometothefirstcause...and
whatislastintheorderofanalysisseemstobefirstintheorderofbecoming. Andifwe
comeonanimpossibilitywegiveupthesearche.g.ifweneedmoneyandthiscannot
begot;butifathingappearspossiblewetrytodoit.
Aristotles algorithm was implemented 2300 years later by Newell and Simon in their G PS
program. Wewouldnowcallitaregression planning systemsee Chapter10.
Goal-based analysis is useful but does not say what to do when several actions will
achievethegoalorwhennoactionwillachieveitcompletely. Antoine Arnauld16121694
correctly described a quantitative formula for deciding what action to take in cases like this
see Chapter16. John Stuart Mills18061873 book Utilitarianism Mill1863promoted
theideaofrationaldecisioncriteriainallspheresofhumanactivity. Themoreformaltheory
ofdecisions isdiscussed inthefollowingsection.
Whataretheformalrulestodrawvalidconclusions?
Whatcanbecomputed?
Howdowereasonwithuncertain information?
Philosophersstakedoutsomeofthefundamentalideasof A Ibuttheleaptoaformalscience
required a level of mathematical formalization in three fundamental areas: logic computa-
tionandprobability.
Theidea of formal logic can be traced back to the philosophers of ancient Greece but
itsmathematicaldevelopmentreallybeganwiththeworkof George Boole18151864who
worked out the details of propositional or Boolean logic Boole 1847. In 1879 Gottlob
Frege18481925 extended Booleslogictoincludeobjectsandrelations creatingthefirst-
order logic that is used today.4 Alfred Tarski 19021983 introduced a theory of reference
thatshowshowtorelatetheobjectsinalogictoobjectsintherealworld.
The next step was to determine the limits of what could be done with logic and com-
putation. The first nontrivial algorithm is thought to be Euclids algorithm for computing
A LG OR IT HM
greatest common divisors. Theword algorithm and the idea of studying them comes from
al-Khowarazmi a Persian mathematician of the 9th century whose writings also introduced
Arabic numerals and algebra to Europe. Boole and others discussed algorithms for logical
deduction and bythelate 19thcentury efforts wereunder waytoformalize general mathe-
matical reasoning as logical deduction. In 1930 Kurt Godel 19061978 showed that there
exists aneffective procedure toprove anytrue statement in the first-order logic of Fregeand
Russell but that first-order logic could not capture the principle of mathematical induction
needed to characterize the natural numbers. In 1931 Godel showed that limits on deduc-
I NC OM PL ET EN ES S tion do exist. His incompleteness theorem showed that in any formal theory as strong as
T HE OR EM
Peano arithmetic the elementary theory of natural numbers there are true statements that
areundecidable inthesensethattheyhavenoproofwithinthetheory.
This fundamental result can also be interpreted as showing that some functions on the
integers cannot be represented by an algorithmthat is they cannot be computed. This
motivated Alan Turing 19121954 to try to characterize exactly which functions are com-
putablecapable of being computed. This notion is actually slightly problematic because
C OM PU TA BL E
thenotionofacomputation oreffectiveprocedure reallycannotbegivenaformaldefinition.
However the Church Turing thesis which states that the Turing machine Turing 1936 is
capableofcomputinganycomputablefunctionisgenerallyacceptedasprovidingasufficient
definition. Turing also showed that there were some functions that no Turing machine can
compute. For example no machine can tell in general whether a given program will return
anansweronagiveninputorrunforever.
Althoughdecidabilityandcomputabilityareimportanttoanunderstandingofcomputa-
tionthenotionoftractability hashadanevengreaterimpact. Roughlyspeaking aproblem
T RA CT AB IL IT Y
iscalledintractableifthetimerequiredtosolveinstancesoftheproblemgrowsexponentially
with the size of the instances. The distinction between polynomial and exponential growth
incomplexity wasfirstemphasized inthemid-1960s Cobham 1964; Edmonds 1965. Itis
important because exponential growthmeansthatevenmoderately largeinstances cannot be
solved in any reasonable time. Therefore one should strive to divide the overall problem of
generating intelligent behaviorintotractable subproblems ratherthanintractable ones.
Howcanone recognize an intractable problem? Thetheory of N P-completeness pio-
N P-C OM PL ET EN ES S
neeredby Steven Cook1971and Richard Karp1972 provides amethod. Cookand Karp
showed theexistence oflarge classes ofcanonical combinatorial search andreasoning prob-
lems that are N P-complete. Anyproblem class to which the class of N P-complete problems
canbereducedislikelytobeintractable. Althoughithasnotbeenprovedthat N P-complete
neverbecamepopular.
Section1.2. The Foundations of Artificial Intelligence 9
problems are necessarily intractable most theoreticians believe it. These results contrast
with the optimism with which the popular press greeted the first computers Electronic
Super-Brains that were Faster than Einstein! Despite the increasing speed of computers
careful use of resources will characterize intelligent systems. Put crudely the world is an
extremely large problem instance! Work in A I has helped explain why some instances of
N P-completeproblems arehardyetothersareeasy Cheesemanetal.1991.
Besideslogicandcomputation thethirdgreatcontribution ofmathematics to A Iisthe
theory of probability. The Italian Gerolamo Cardano 15011576 first framed the idea of
P RO BA BI LI TY
probability describing it in terms of the possible outcomes of gambling events. In 1654
Blaise Pascal 16231662 in a letter to Pierre Fermat 16011665 showed how to pre-
dict the future of an unfinished gambling game and assign average payoffs to the gamblers.
Probabilityquicklybecameaninvaluable partofallthequantitative sciences helpingtodeal
withuncertain measurements and incomplete theories. James Bernoulli 16541705 Pierre
Laplace 17491827 and others advanced the theory and introduced new statistical meth-
ods. Thomas Bayes 17021761 who appears on the front cover of this book proposed
a rule for updating probabilities in the light of new evidence. Bayes rule underlies most
modernapproaches touncertain reasoning in A Isystems.
Howshouldwemakedecisions soastomaximizepayoff?
Howshouldwedothiswhenothersmaynotgoalong?
Howshouldwedothiswhenthepayoffmaybefarinthefuture?
The science of economics got its start in 1776 when Scottish philosopher Adam Smith
17231790 published An Inquiry into the Nature and Causes of the Wealth of Nations.
Whiletheancient Greeksandothershadmadecontributions toeconomicthought Smithwas
the first to treat it as a science using the idea that economies can be thought of as consist-
ing of individual agents maximizing their own economic well-being. Most people think of
economics as being about money but economists will say that they are really studying how
people makechoices thatleadtopreferred outcomes. When Mc Donalds offersahamburger
foradollartheyareassertingthattheywouldpreferthedollarandhopingthatcustomerswill
prefer the hamburger. The mathematical treatment of preferred outcomes or utility was
U TI LI TY
firstformalized by Leon Walraspronounced Valrasse 1834-1910 andwasimproved by
Frank Ramsey1931 and laterby John von Neumann and Oskar Morgenstern in theirbook
The Theoryof Gamesand Economic Behavior1944.
Decisiontheorywhichcombinesprobability theorywithutilitytheoryprovidesafor-
D EC IS IO NT HE OR Y
malandcompleteframeworkfordecisionseconomicorotherwisemadeunderuncertainty
that is in cases where probabilistic descriptions appropriately capture the decision makers
environment. This issuitable forlarge economies whereeach agent need pay no attention
to the actions of other agents as individuals. For small economies the situation is much
more like a game: the actions of one player can significantly affect the utility of another
either positively or negatively. Von Neumann and Morgensterns development of game
theory see also Luceand Raiffa 1957 included the surprising result that forsomegames
G AM ET HE OR Y
arational agent should adoptpolicies thatareorleast appeartoberandomized. Unlikede-
cisiontheory gametheorydoesnotofferanunambiguous prescription forselecting actions.
For the most part economists did not address the third question listed above namely
how tomake rational decisions whenpayoffs from actions are not immediate but instead re-
sultfromseveralactionstakeninsequence. Thistopicwaspursuedinthefieldofoperations
O PE RA TI ON S research which emerged in World War I I from efforts in Britain to optimize radar installa-
R ES EA RC H
tions and later found civilian applications in complex management decisions. The work of
Richard Bellman 1957 formalized a class of sequential decision problems called Markov
decisionprocesses whichwestudyin Chapters17and21.
Work in economics and operations research has contributed much to our notion of ra-
tional agents yet for many years A I research developed along entirely separate paths. One
reason was the apparent complexity of making rational decisions. The pioneering A I re-
searcher Herbert Simon19162001 wonthe Nobel Prizeineconomicsin1978forhisearly
work showing that models based on satisficingmaking decisions that are good enough
S AT IS FI CI NG
rather than laboriously calculating an optimal decisiongave a better description of actual
human behavior Simon 1947. Since the 1990s there has been a resurgence of interest in
decision-theoretic techniques foragentsystems Wellman1995.
Howdobrainsprocess information?
Neuroscience is the study of the nervous system particularly the brain. Although the exact
N EU RO SC IE NC E
wayinwhichthebrainenablesthoughtisoneofthegreatmysteriesofsciencethefactthatit
doesenablethoughthasbeenappreciated forthousands ofyearsbecauseoftheevidencethat
strong blowsto the head can lead tomental incapacitation. Ithas also long been known that
humanbrains aresomehow different; inabout 335 B.C. Aristotle wrote Ofalltheanimals
man has the largest brain in proportion to his size.5 Still it was not until the middle of the
18th century thatthebrain waswidely recognized astheseat ofconsciousness. Before then
candidate locations included theheartandthespleen.
Paul Brocas 18241880 study of aphasia speech deficit in brain-damaged patients
in 1861 demonstrated the existence of localized areas of the brain responsible for specific
cognitive functions. In particular he showed that speech production was localized to the
portion of the left hemisphere now called Brocas area.6 By that time it was known that
the brain consisted of nerve cells or neurons but it was not until 1873 that Camillo Golgi
N EU RO N
18431926 developed a staining technique allowing the observation of individual neurons
in the brain see Figure 1.2. This technique was used by Santiago Ramon y Cajal 1852
1934inhispioneering studiesofthebrainsneuronalstructures.7 Nicolas Rashevsky1936
1938wasthefirsttoapplymathematical modelstothestudyofthenervous sytem.
whichneuronswereembedded whereas Cajalpropounded theneuronal doctrine. Thetwosharedthe Nobel
prizein1906butgavemutuallyantagonisticacceptancespeeches.
Section1.2. The Foundations of Artificial Intelligence 11
Axonal arborization
Axon from another cell
Synapse
Dendrite Axon
Nucleus
Synapses
Cell body or Soma
or soma that contains a cell nucleus. Branching out from the cell body are a number of
fiberscalleddendritesanda single longfibercalled the axon. Theaxonstretchesoutfora
long distance much longer than the scale in this diagram indicates. Typically an axon is
1cmlong100timesthediameterofthecellbodybutcanreachupto1meter. Aneuron
makesconnectionswith10to100000otherneuronsatjunctionscalledsynapses.Signalsare
propagatedfrom neuronto neuronby a complicated electrochemicalreaction. The signals
controlbrainactivityintheshorttermandalsoenablelong-termchangesintheconnectivity
ofneurons. Thesemechanismsarethoughttoformthebasisforlearninginthebrain. Most
informationprocessinggoesoninthecerebralcortextheouterlayerofthebrain. Thebasic
organizationalunit appears to be a column of tissue about0.5 mm in diameter containing
about20000neuronsandextendingthefulldepthofthecortexabout4mminhumans.
Wenowhavesomedataonthemappingbetweenareasofthebrain andthepartsofthe
body that they control orfrom which they receive sensory input. Such mappings are able to
change radically over the course of a few weeks and some animals seem to have multiple
maps. Moreover we do not fully understand how other areas can take over functions when
oneareaisdamaged. Thereisalmostnotheoryonhowanindividual memoryisstored.
The measurement of intact brain activity began in 1929 with the invention by Hans
Bergerofthe electroencephalograph E EG.Therecent development offunctional magnetic
resonance imaging f M RI Ogawa et al. 1990; Cabeza and Nyberg 2001 is giving neu-
roscientists unprecedentedly detailed images of brain activity enabling measurements that
correspond in interesting ways to ongoing cognitive processes. These are augmented by
advances in single-cell recording of neuron activity. Individual neurons can be stimulated
electrically chemically orevenoptically Hanand Boyden2007allowingneuronal input
output relationships to be mapped. Despite these advances we are still a long way from
understanding howcognitiveprocesses actually work.
The truly amazing conclusion is that a collection of simple cells can lead to thought
action and consciousness or in the pithy words of John Searle 1992 brains cause minds.
Supercomputer Personal Computer Human Brain
Computational units 104 C PUs1012 transistors 4 CP Us109 transistors 1011 neurons
Storageunits 1014 bits R AM 1011 bits R AM 1011 neurons
Cycletime
109
sec
109
sec
103
sec
Figure1.3 Acrudecomparisonoftherawcomputationalresourcesavailabletothe I BM
B LU E G EN Esupercomputeratypicalpersonalcomputerof2008andthehumanbrain.The
brains numbersare essentially fixed whereas the supercomputersnumbers have been in-
creasing by a factorof 10 every 5 years or so allowing it to achieveroughparity with the
brain.Thepersonalcomputerlagsbehindonallmetricsexceptcycletime.
Theonlyrealalternative theoryismysticism: thatmindsoperateinsomemysticalrealmthat
isbeyondphysical science.
Brainsanddigitalcomputershavesomewhatdifferentproperties. Figure1.3showsthat
computers have a cycle time that is a million times faster than a brain. The brain makes up
for that with far more storage and interconnection than even a high-end personal computer
although the largest supercomputers have a capacity that is similar to the brains. It should
be noted however that the brain does not seem to use all of its neurons simultaneously.
Futurists make much of these numbers pointing to an approaching singularity at which
S IN GU LA RI TY
computers reach asuperhuman level ofperformance Vinge 1993; Kurzweil 2005 but the
rawcomparisons arenotespecially informative. Evenwitha computerofvirtuallyunlimited
capacity westillwouldnotknowhowtoachievethebrainslevelofintelligence.
Howdohumansandanimalsthinkandact?
The origins of scientific psychology are usually traced to the work of the German physi-
cist Hermann von Helmholtz 18211894 and his student Wilhelm Wundt 18321920.
Helmholtz applied the scientific method to the study of human vision and his Handbook
of Physiological Optics is even now described as the single most important treatise on the
physics and physiology of human vision Nalwa 1993 p.15. In 1879 Wundt opened the
first laboratory of experimental psychology at the University of Leipzig. Wundt insisted
on carefully controlled experiments in which his workers would perform a perceptual oras-
sociative task while introspecting on their thought processes. The careful controls went a
long way toward making psychology a science but the subjective nature of the data made
it unlikely that an experimenter would ever disconfirm his or her own theories. Biologists
studying animal behavior onthe otherhand lacked introspective data and developed an ob-
jectivemethodologyasdescribedby H.S.Jennings1906inhisinfluentialwork Behaviorof
the Lower Organisms. Applying this viewpoint to humans the behaviorism movement led
B EH AV IO RI SM
by John Watson18781958 rejectedanytheoryinvolvingmentalprocessesonthegrounds
Section1.2. The Foundations of Artificial Intelligence 13
thatintrospection couldnotprovidereliableevidence. Behavioristsinsistedonstudyingonly
objective measures of the percepts or stimulus given to an animal and its resulting actions
or response. Behaviorism discovered a lot about rats and pigeons but had less success at
understanding humans.
C OG NI TI VE Cognitive psychology which views the brain as an information-processing device
P SY CH OL OG Y
can be traced back at least to the works of William James 18421910. Helmholtz also
insisted that perception involved a form of unconscious logical inference. The cognitive
viewpoint waslargely eclipsed by behaviorism in the United States but at Cambridges Ap-
plied Psychology Unit directed by Frederic Bartlett 18861969 cognitive modeling was
able to flourish. The Nature of Explanation by Bartletts student and successor Kenneth
Craik 1943 forcefully reestablished the legitimacy of such mental terms as beliefs and
goals arguing that they are just as scientific as say using pressure and temperature to talk
about gases despite their being made of molecules that have neither. Craik specified the
threekeystepsofaknowledge-based agent: 1thestimulusmustbetranslatedintoaninter-
nalrepresentation 2therepresentation ismanipulated bycognitiveprocessestoderivenew
internal representations and 3 these are in turn retranslated back into action. He clearly
explained whythiswasagooddesignforanagent:
Iftheorganismcarriesasmall-scalemodelofexternalrealityandofits ownpossible
actionswithinitsheaditisabletotryoutvariousalternativesconcludewhichisthebest
ofthemreacttofuturesituationsbeforetheyariseutilizetheknowledgeofpastevents
indealingwith thepresentandfutureandin everywaytoreactin amuchfullersafer
andmorecompetentmannertotheemergencieswhichfaceit. Craik1943
After Craiks death inabicycle accident in1945 hisworkwascontinued by Donald Broad-
bentwhosebook Perceptionand Communication1958wasoneofthefirstworkstomodel
psychological phenomena as information processing. Meanwhile in the United States the
development ofcomputermodeling ledtothecreation ofthefieldofcognitive science. The
fieldcanbe saidtohave started ataworkshop in September 1956 at M IT.Weshall see that
thisisjusttwomonthsaftertheconference atwhich A Iitselfwasborn. Attheworkshop
George Millerpresented The Magic Number Seven Noam Chomskypresented Three Models
of Language and Allen Newell and Herbert Simon presented The Logic Theory Machine.
These three influential papers showed how computer models could be used to address the
psychology of memory language and logical thinking respectively. It is now a common
although far from universal view among psychologists that a cognitive theory should be
likeacomputerprogram Anderson1980;thatisitshoulddescribeadetailedinformation-
processing mechanismwherebysomecognitivefunction mightbeimplemented.
Howcanwebuildanefficientcomputer?
For artificial intelligence to succeed we need two things: intelligence and an artifact. The
computer has been the artifact of choice. The modern digital electronic computer was in-
ventedindependently andalmostsimultaneously byscientistsinthreecountriesembattledin
World War I I. The first operational computer was the electromechanical Heath Robinson8
built in 1940 by Alan Turings team forasingle purpose: deciphering German messages. In
1943 the same group developed the Colossus a powerful general-purpose machine based
on vacuum tubes.9 The first operational programmable computer was the Z-3 the inven-
tionof Konrad Zusein Germanyin1941. Zusealsoinvented floating-point numbers andthe
first high-level programming language Plankalkul. The first electronic computer the A BC
at Iowa State University. Atanasoffs research received little support or recognition; it was
the E NI AC developed as part of a secret military project at the University of Pennsylvania
by a team including John Mauchly and John Eckert that proved to be the most influential
forerunnerofmoderncomputers.
Sincethattimeeachgenerationofcomputerhardwarehasbroughtanincreaseinspeed
andcapacityandadecreaseinprice. Performancedoubledevery18monthsorsountilaround
2005whenpowerdissipationproblemsledmanufacturers to startmultiplyingthenumberof
C PUcoresratherthantheclockspeed. Currentexpectations arethatfutureincreasesinpower
willcomefrommassiveparallelisma curious convergence withtheproperties ofthebrain.
Of course there were calculating devices before the electronic computer. The earliest
automated machines dating from the 17th century were discussed on page 6. The first pro-
grammable machine was a loom devised in 1805 by Joseph Marie Jacquard 17521834
that used punched cards to store instructions for the pattern to be woven. In the mid-19th
century Charles Babbage 17921871 designed two machines neither of which he com-
pleted. The Difference Enginewasintended tocomputemathematical tables forengineering
andscientificprojects. Itwasfinallybuiltandshowntoworkin1991atthe Science Museum
in London Swade2000. Babbages Analytical Enginewasfarmoreambitious: itincluded
addressable memory stored programs andconditional jumpsand wasthefirstartifact capa-
ble ofuniversal computation. Babbages colleague Ada Lovelace daughter ofthepoet Lord
Byronwasperhapstheworldsfirstprogrammer. Theprogramminglanguage Adaisnamed
afterher. Shewroteprogramsfortheunfinished Analytical Engineandevenspeculated that
themachinecouldplaychessorcomposemusic.
A I also owes a debt to the software side of computer science which has supplied the
operatingsystemsprogramminglanguages andtoolsneededtowritemodernprogramsand
papers aboutthem. Butthisisoneareawherethedebthasbeenrepaid: workin A Ihaspio-
neeredmanyideasthathavemadetheirwaybacktomainstream computerscienceincluding
time sharing interactive interpreters personal computers with windows and mice rapid de-
velopment environments the linked list data type automatic storage management and key
concepts ofsymbolic functional declarative andobject-oriented programming.
tionsforeverydaytaskssuchasbutteringtoast.
chessprograms Turingetal.1953.Hiseffortswereblockedbythe Britishgovernment.
Section1.2. The Foundations of Artificial Intelligence 15
Howcanartifactsoperate undertheirowncontrol?
Ktesibios of Alexandria c. 250 B.C. built the first self-controlling machine: a water clock
with a regulator that maintained a constant flow rate. This invention changed the definition
of what an artifact could do. Previously only living things could modify their behavior in
response to changes inthe environment. Otherexamples of self-regulating feedback control
systems include the steam engine governor created by James Watt 17361819 and the
thermostat invented by Cornelis Drebbel 15721633 who also invented the submarine.
Themathematical theoryofstablefeedback systemswasdevelopedinthe19thcentury.
The central figure in the creation of what is now called control theory was Norbert
C ON TR OL TH EO RY
Wiener18941964. Wienerwasabrilliant mathematician whoworkedwith Bertrand Rus-
sellamongothersbeforedevelopinganinterestinbiologicalandmechanicalcontrolsystems
andtheirconnectiontocognition. Like Craikwhoalsousedcontrolsystemsaspsychological
models Wiener and his colleagues Arturo Rosenblueth and Julian Bigelow challenged the
behaviorist orthodoxy Rosenblueth et al. 1943. They viewed purposive behavior as aris-
ingfromaregulatorymechanismtryingtominimizeerrorthedifferencebetweencurrent
state and goal state. In the late 1940s Wiener along with Warren Mc Culloch Walter Pitts
and John von Neumann organized a series of influential conferences that explored the new
mathematicalandcomputational modelsofcognition. Wienersbook Cybernetics1948be-
C YB ER NE TI CS
came a bestseller and awoke the public to the possibility of artificially intelligent machines.
Meanwhile in Britain W. Ross Ashby Ashby 1940 pioneered similar ideas. Ashby Alan
Turing Grey Walter and others formed the Ratio Club for those who had Wieners ideas
before Wieners bookappeared. Ashbys Designfora Brain1948 1952elaborated onhis
idea that intelligence could be created by the use of homeostatic devices containing appro-
H OM EO ST AT IC
priatefeedback loopstoachievestableadaptivebehavior.
Modern control theory especially the branch known as stochastic optimal control has
O BJ EC TI VE asitsgoalthedesignofsystemsthatmaximizeanobjectivefunctionovertime. Thisroughly
F UN CT IO N
matches our view of A I: designing systems that behave optimally. Why then are A I and
control theory twodifferent fields despite the close connections among theirfounders? The
answer lies in the close coupling between the mathematical techniques that were familiar to
theparticipantsandthecorresponding setsofproblemsthatwereencompassedineachworld
view. Calculusandmatrixalgebrathetoolsofcontroltheorylendthemselvestosystemsthat
aredescribablebyfixedsetsofcontinuousvariableswhereas A Iwasfoundedinpartasaway
toescapefromthetheseperceivedlimitations. Thetoolsoflogicalinferenceandcomputation
allowed A Iresearchers toconsider problems suchaslanguage vision andplanning that fell
completely outsidethecontroltheorists purview.
Howdoeslanguage relatetothought?
In 1957 B. F. Skinner published Verbal Behavior. This was a comprehensive detailed ac-
count of the behaviorist approach to language learning written by the foremost expert in
the field. But curiously a review of the book became as well known as the book itself and
served to almost kill off interest in behaviorism. The author of the review was the linguist
Noam Chomsky who had just published a book on his own theory Syntactic Structures.
Chomsky pointed out that the behaviorist theory did not address the notion of creativity in
languageit didnotexplain howachild could understand and makeupsentences thatheor
shehadneverheardbefore. Chomskystheorybased onsyntacticmodelsgoingbacktothe
Indian linguist Paninic. 350 B.C.could explain this andunlike previous theories itwas
formalenoughthatitcouldinprinciple beprogrammed.
Modern linguistics and A I then were born at about the same time and grew up
C OM PU TA TI ON AL together intersectinginahybridfieldcalled computationallinguisticsornaturallanguage
L IN GU IS TI CS
processing. Theproblemofunderstandinglanguagesoonturnedouttobeconsiderablymore
complex than it seemed in 1957. Understanding language requires an understanding of the
subjectmatterandcontextnotjustanunderstandingofthestructureofsentences. Thismight
seem obvious but it was not widely appreciated until the 1960s. Much of the early work in
knowledge representation the study of how to put knowledge into a form that a computer
can reason with was tied to language and informed by research in linguistics which was
connected inturntodecades ofworkonthephilosophical analysis oflanguage.
Withthebackground materialbehinduswearereadytocover thedevelopment of A Iitself.
The first work that is now generally recognized as A I was done by Warren Mc Culloch and
Walter Pitts 1943. They drew on three sources: knowledge of the basic physiology and
function of neurons in the brain; a formal analysis of propositional logic due to Russell and
Whitehead; and Turingstheoryofcomputation. Theyproposed amodelofartificialneurons
inwhicheachneuronischaracterized asbeingonoroff withaswitchtoonoccurring
in response to stimulation by a sufficient number of neighboring neurons. The state of a
neuronwasconceivedofasfactuallyequivalenttoapropositionwhichproposeditsadequate
stimulus. They showed for example that any computable function could be computed by
some network of connected neurons and that all the logical connectives and or not etc.
could be implemented by simple net structures. Mc Culloch and Pitts also suggested that
suitably defined networks could learn. Donald Hebb1949 demonstrated asimple updating
ruleformodifying theconnection strengths between neurons. Hisrule nowcalled Hebbian
learningremainsaninfluential modeltothisday.
H EB BI AN LE AR NI NG
Twoundergraduate students at Harvard Marvin Minsky and Dean Edmonds built the
first neural network computer in 1950. The S NA RC as it was called used 3000 vacuum
tubesandasurplusautomaticpilotmechanismfroma B-24bombertosimulateanetworkof
His Ph.D. committee was skeptical about whether this kind of work should be considered
Section1.3. The Historyof Artificial Intelligence 17
mathematics butvon Neumannreportedly said Ifitisntnowitwillbesomeday. Minsky
waslatertoproveinfluentialtheoremsshowingthelimitationsofneuralnetworkresearch.
There were a number of early examples of work that can be characterized as A I but
Alan Turings vision wasperhaps themostinfluential. Hegavelectures onthetopic asearly
article Computing Machinery and Intelligence. Therein he introduced the Turing Test
machine learning genetic algorithms and reinforcement learning. He proposed the Child
Programmeideaexplaining Insteadoftryingtoproduceaprogrammetosimulatetheadult
mindwhynotrathertrytoproduce onewhichsimulated thechilds?
Princeton was home to another influential figure in A I John Mc Carthy. After receiving his
Ph D there in 1951 and working for two years as an instructor Mc Carthy moved to Stan-
fordandthento Dartmouth Collegewhichwastobecometheofficialbirthplace ofthefield.
Mc Carthy convinced Minsky Claude Shannon and Nathaniel Rochester to help him bring
together U.S. researchers interested in automata theory neural nets and the study of intel-
ligence. They organized a two-month workshop at Dartmouth in the summer of 1956. The
proposal states:10
We propose that a 2 month 10 man study of artificial intelligence be carried
out during the summer of 1956 at Dartmouth College in Hanover New Hamp-
shire. The study is to proceed on the basis of the conjecture that every aspect of
learning or any other feature of intelligence can in principle be so precisely de-
scribedthatamachinecanbemadetosimulateit. Anattemptwillbemadetofind
howtomakemachinesuselanguage formabstractions andconcepts solvekinds
of problems now reserved for humans and improve themselves. We think that a
significant advance can be made in one or more of these problems if a carefully
selectedgroupofscientists workonittogetherforasummer.
There were 10 attendees in all including Trenchard More from Princeton Arthur Samuel
from I BMand Ray Solomonoffand Oliver Selfridgefrom M IT.
Two researchers from Carnegie Tech11 Allen Newell and Herbert Simon rather stole
the show. Although the others had ideas and in some cases programs for particular appli-
cations such as checkers Newell and Simon already had a reasoning program the Logic
Theorist L T about which Simonclaimed Wehave invented acomputerprogram capable
ofthinking non-numerically andtherebysolvedthevenerable mindbody problem.12 Soon
aftertheworkshoptheprogramwasabletoprovemostofthetheoremsin Chapter2of Rus-
wouldhavebeenmorepreciseandlessthreateningbut A Ihasstuck.Atthe50thanniversaryofthe Dartmouth
conference Mc Carthystatedthatheresistedthetermscomputerorcomputationalindeferenceto Norbert
Weinerwhowaspromotinganalogcyberneticdevicesratherthandigitalcomputers.
translateditintomachinecodebyhand. Toavoiderrorstheyworkedinparallelcallingoutbinarynumbersto
eachotherastheywroteeachinstructiontomakesuretheyagreed.
sell and Whiteheads Principia Mathematica. Russell wasreportedly delighted when Simon
showedhimthattheprogramhadcomeupwithaproofforonetheoremthatwasshorterthan
theonein Principia. Theeditors ofthe Journal of Symbolic Logicwerelessimpressed; they
rejectedapapercoauthored by Newell Simonand Logic Theorist.
The Dartmouth workshop did not lead to any new breakthroughs but it did introduce
all the major figures to each other. For the next 20 years the field would be dominated by
thesepeopleandtheirstudents andcolleagues at M IT CM UStanford and I BM.
Looking at the proposal for the Dartmouth workshop Mc Carthy et al. 1955 we can
see whyitwasnecessary for A Ito become aseparate field. Whycouldnt all the workdone
in A I have taken place under the name of control theory or operations research or decision
theory which after all have objectives similar to those of A I? Or why isnt A I a branch
of mathematics? The first answer is that A I from the start embraced the idea of duplicating
human faculties such as creativity self-improvement and language use. None of the other
fields were addressing these issues. The second answer is methodology. A I is the only one
ofthesefieldsthatisclearlyabranchofcomputersciencealthoughoperationsresearchdoes
share an emphasis on computer simulations and A I is the only field to attempt to build
machinesthatwillfunction autonomously incomplex changing environments.
Theearly yearsof A Iwerefullofsuccessesin alimitedway. Giventheprimitive comput-
ers and programming tools of the time and the fact that only a few years earlier computers
wereseenasthingsthatcoulddoarithmeticandnomoreitwasastonishingwheneveracom-
puterdidanything remotely clever. Theintellectual establishment byandlarge preferred to
believe that a machine can never do X. See Chapter 26 for a long list of Xs gathered
by Turing. A I researchers naturally responded by demonstrating one X after another. John
Mc Carthyreferredtothisperiodasthe Look Manohands! era.
Newell and Simons early success was followed up with the General Problem Solver
or G PS. Unlike Logic Theorist this program was designed from the start to imitate human
problem-solving protocols. Within the limited class of puzzles it could handle it turned out
that the order in which the program considered subgoals and possible actions was similarto
thatinwhichhumansapproached thesameproblems. Thus G PS wasprobably thefirstpro-
gramtoembodythethinkinghumanlyapproach. Thesuccess of G PS andsubsequentpro-
gramsasmodelsofcognitionled Newelland Simon1976toformulatethefamousphysical
P HY SI CA LS YM BO L symbolsystemhypothesiswhichstatesthataphysicalsymbolsystemhasthenecessaryand
S YS TE M
sufficient means forgeneral intelligent action. What they meant is that any system human
or machine exhibiting intelligence must operate by manipulating data structures composed
ofsymbols. Wewillseelaterthatthishypothesis hasbeenchallenged frommanydirections.
At I BM Nathaniel Rochester and his colleagues produced some of the first A I pro-
grams. Herbert Gelernter 1959 constructed the Geometry Theorem Prover which was
able to prove theorems that many students of mathematics would find quite tricky. Starting
in 1952 Arthur Samuel wrote a series of programs for checkers draughts that eventually
learned toplay atastrong amateurlevel. Along theway he disproved theidea that comput-
Section1.3. The Historyof Artificial Intelligence 19
erscandoonlywhattheyaretoldto: hisprogram quickly learned toplayabettergamethan
itscreator. Theprogram wasdemonstrated ontelevision in February 1956 creating astrong
impression. Like Turing Samuel had trouble finding computer time. Working at night he
used machines that were still on the testing floor at I BMs manufacturing plant. Chapter 5
coversgameplaying and Chapter21explainsthelearning techniques usedby Samuel.
John Mc Carthy movedfrom Dartmouth to M ITand there madethree crucial contribu-
tionsinonehistoricyear: 1958. In M IT AI Lab Memo No.1 Mc Carthydefinedthehigh-level
language Lispwhichwastobecomethedominant A Iprogramminglanguageforthenext30
L IS P
years. With Lisp Mc Carthyhadthetoolheneeded butaccess toscarceandexpensivecom-
putingresourceswasalsoaseriousproblem. Inresponse he andothersat M ITinventedtime
sharing. Also in 1958 Mc Carthy published a paper entitled Programs with Common Sense
in which he described the Advice Taker ahypothetical program that can be seen as the first
complete A I system. Like the Logic Theorist and Geometry Theorem Prover Mc Carthys
program was designed to use knowledge to search forsolutions to problems. But unlike the
others it was to embody general knowledge of the world. For example he showed how
somesimpleaxiomswouldenabletheprogram togenerate aplantodrivetotheairport. The
program wasalso designed toaccept new axioms inthe normal course ofoperation thereby
allowing it to achieve competence in new areas without being reprogrammed. The Advice
Taker thus embodied the central principles of knowledge representation and reasoning: that
it is useful to have a formal explicit representation of the world and its workings and to be
able to manipulate that representation with deductive processes. It is remarkable how much
ofthe1958paperremainsrelevanttoday.
1958alsomarkedtheyearthat Marvin Minskymovedto M IT.Hisinitialcollaboration
with Mc Carthydidnotlasthowever. Mc Carthystressedrepresentation andreasoning infor-
mal logic whereas Minsky was more interested in getting programs to work and eventually
developed an anti-logic outlook. In 1963 Mc Carthy started the A Ilab at Stanford. Hisplan
to use logic to build the ultimate Advice Taker was advanced by J. A. Robinsons discov-
ery in 1965 of the resolution method a complete theorem-proving algorithm for first-order
logic; see Chapter 9. Work at Stanford emphasized general-purpose methods for logical
reasoning. Applications of logic included Cordell Greens question-answering and planning
systems Green 1969b and the Shakey robotics project at the Stanford Research Institute
S RI. The latter project discussed further in Chapter 25 was the first to demonstrate the
completeintegration oflogicalreasoning andphysicalactivity.
Minsky supervised a series of students who chose limited problems that appeared to
require intelligence tosolve. Theselimited domains became known asmicroworlds. James
M IC RO WO RL D
Slagles S AI NT program 1963 wasable tosolveclosed-form calculus integration problems
typical of first-year college courses. Tom Evanss A NA LO GY program 1968 solved geo-
metricanalogyproblemsthatappearin I Qtests. Daniel Bobrows S TU DE NT program1967
solvedalgebrastoryproblemssuchasthefollowing:
If the number of customers Tom gets is twice the square of 20 percent of the number
of advertisementshe runs and the numberof advertisements he runs is 45 what is the
numberofcustomers Tomgets?
Blue
Red
Red Green
Blue
Green
Green
Red
Figure1.4 Ascenefromtheblocksworld. S HR DL UWinograd1972hasjustcompleted
thecommand Findablockwhichistallerthantheoneyouareholdingandputitinthebox.
The most famous microworld was the blocks world which consists of a set of solid blocks
placed on a tabletop or more often a simulation of a tabletop as shown in Figure 1.4.
A typical task in this world is to rearrange the blocks in a certain way using a robot hand
that can pick up one block at a time. The blocks world was home to the vision project of
David Huffman 1971 the vision and constraint-propagation work of David Waltz 1975
the learning theory of Patrick Winston 1970 the natural-language-understanding program
of Terry Winograd1972 andtheplannerof Scott Fahlman1974.
Early work building on the neural networks of Mc Culloch and Pitts also flourished.
The work of Winograd and Cowan 1963 showed how a large number of elements could
collectively represent anindividual concept withacorresponding increaseinrobustness and
parallelism. Hebbs learning methods were enhanced by Bernie Widrow Widrow and Hoff
1960; Widrow 1962 who called his networks adalines and by Frank Rosenblatt 1962
with his perceptrons. The perceptron convergence theorem Block et al. 1962 says that
thelearningalgorithmcanadjusttheconnection strengths ofaperceptrontomatchanyinput
dataprovidedsuchamatchexists. Thesetopicsarecovered in Chapter20.
From the beginning A I researchers were not shy about making predictions of their coming
successes. Thefollowingstatement by Herbert Simonin1957 isoftenquoted:
Itisnotmyaimtosurpriseorshockyoubutthesimplestway Icansummarizeistosay
thattherearenowintheworldmachinesthatthinkthatlearnandthatcreate. Moreover
Section1.3. The Historyof Artificial Intelligence 21
theirabilitytodothesethingsisgoingtoincreaserapidlyuntilinavisiblefuturethe
rangeofproblemstheycanhandlewillbecoextensivewiththerangetowhichthehuman
mindhasbeenapplied.
Terms such as visible future can be interpreted in various ways but Simon also made
more concrete predictions: that within 10 years a computer would be chess champion and
a significant mathematical theorem would be proved by machine. These predictions came
trueorapproximately truewithin40yearsratherthan10. Simonsoverconfidence wasdue
to the promising performance of early A I systems on simple examples. In almost all cases
however these early systems turned out to fail miserably when tried out onwiderselections
ofproblems andonmoredifficultproblems.
The first kind of difficulty arose because most early programs knew nothing of their
subject matter; they succeeded by means of simple syntactic manipulations. A typical story
occurred inearlymachinetranslation efforts whichweregenerously fundedbythe U.S.Na-
tional Research Councilinanattempttospeedupthetranslation of Russianscientificpapers
inthewakeofthe Sputniklaunchin1957. Itwasthoughtinitiallythatsimplesyntactictrans-
formations based on the grammars of Russian and English and word replacement from an
electronic dictionary would suffice to preserve the exact meanings of sentences. The fact is
that accurate translation requires background knowledge in order to resolve ambiguity and
establish the content of the sentence. The famous retranslation of the spirit is willing but
the fleshisweak asthe vodka isgood but themeat isrotten illustrates the difficulties en-
countered. In1966areportbyanadvisorycommitteefoundthattherehasbeennomachine
translationofgeneralscientifictextandnoneisinimmediateprospect. All U.S.government
fundingforacademictranslation projectswascanceled. Todaymachinetranslation isanim-
perfectbutwidelyusedtoolfortechnical commercial government and Internetdocuments.
Thesecondkindofdifficultywastheintractabilityofmanyoftheproblemsthat A Iwas
attempting to solve. Most of the early A I programs solved problems by trying out different
combinations of steps until the solution was found. This strategy worked initially because
microworlds contained very few objects and hence very few possible actions and very short
solution sequences. Before the theory of computational complexity was developed it was
widely thought that scaling up to larger problems was simply a matter of faster hardware
andlargermemories. Theoptimismthataccompaniedthedevelopmentofresolutiontheorem
provingforexamplewassoondampenedwhenresearchers failedtoprovetheoremsinvolv-
ingmorethanafewdozenfacts. Thefactthataprogramcanfindasolutioninprincipledoes
notmeanthattheprogramcontains anyofthemechanismsneededtofinditinpractice.
The illusion of unlimited computational power was not confined to problem-solving
programs. Earlyexperiments in machineevolutionnowcalledgeneticalgorithms Fried-
M AC HI NE EV OL UT IO N
G EN ET IC berg 1958; Friedberg et al. 1959 were based on the undoubtedly correct belief that by
A LG OR IT HM
making an appropriate series of small mutations to a machine-code program one can gen-
erate a program with good performance for any particular task. The idea then was to try
random mutations with a selection process to preserve mutations that seemed useful. De-
spitethousandsofhoursof C PUtimealmostnoprogresswasdemonstrated. Moderngenetic
algorithms usebetterrepresentations andhaveshownmoresuccess.
Failure to come to grips with the combinatorial explosion was one of the main criti-
cismsof A Icontainedinthe Lighthillreport Lighthill1973whichformedthebasisforthe
decision bythe Britishgovernment toendsupport for A Iresearch inallbuttwouniversities.
Oraltraditionpaintsasomewhatdifferentandmorecolorfulpicturewithpoliticalambitions
andpersonal animosities whosedescription isbesidethepoint.
Athirddifficultyarosebecauseofsomefundamentallimitationsonthebasicstructures
being used togenerate intelligent behavior. Forexample Minsky and Paperts book Percep-
trons 1969 proved that although perceptrons a simple form of neural network could be
showntolearnanythingtheywerecapableofrepresenting theycouldrepresentverylittle. In
particularatwo-inputperceptronrestrictedtobesimplerthantheform Rosenblattoriginally
studied couldnotbetrained torecognize whenitstwoinputs weredifferent. Althoughtheir
results did not apply to more complex multilayer networks research funding for neural-net
research soon dwindled to almost nothing. Ironically the newback-propagation learning al-
gorithms for multilayer networks that were to cause an enormous resurgence in neural-net
research inthelate1980swereactually discoveredfirstin1969 Brysonand Ho1969.
The picture of problem solving that had arisen during the first decade of A I research was of
a general-purpose search mechanism trying to string together elementary reasoning steps to
findcompletesolutions. Suchapproacheshavebeencalledweakmethodsbecausealthough
W EA KM ET HO D
general they do not scale up tolarge ordifficult problem instances. Thealternative to weak
methods is to use more powerful domain-specific knowledge that allows larger reasoning
stepsandcanmoreeasilyhandle typically occurring casesinnarrowareas ofexpertise. One
mightsaythattosolveahardproblemyouhavetoalmostknow theansweralready.
The D EN DR ALprogram Buchananetal.1969wasanearlyexampleofthisapproach.
It was developed at Stanford where Ed Feigenbaum a former student of Herbert Simon
Bruce Buchanan a philosopher turned computer scientist and Joshua Lederberg a Nobel
laureate geneticist teameduptosolvetheproblem ofinferring molecularstructure from the
information provided by a mass spectrometer. The input to the program consists of the ele-
mentaryformulaofthemoleculee.g.C H N O andthemassspectrumgivingthemasses
ofthevariousfragmentsofthemoleculegeneratedwhenitisbombardedbyanelectronbeam.
Forexample themassspectrum mightcontain apeakat m 15corresponding tothemass
ofamethyl C H fragment.
3
The naive version of the program generated all possible structures consistent with the
formulaandthenpredictedwhatmassspectrumwouldbeobservedforeachcomparingthis
with the actual spectrum. As one might expect this is intractable for even moderate-sized
molecules. The D EN DR AL researchers consulted analytical chemists and found that they
workedbylookingforwell-knownpatternsofpeaksinthespectrumthatsuggested common
substructures inthe molecule. Forexample the following rule isused torecognize aketone
C Osubgroup whichweighs28:
iftherearetwopeaksatx andx suchthat
ax x M 28 M isthemassofthewholemolecule;
Section1.3. The Historyof Artificial Intelligence 23
bx 28isahighpeak;
1
cx 28isahighpeak;
2
d Atleastoneofx andx ishigh.
thenthereisaketonesubgroup
Recognizing thatthe molecule contains aparticular substructure reduces thenumberofpos-
siblecandidates enormously. D EN DR AL waspowerfulbecause
Alltherelevanttheoreticalknowledgetosolvetheseproblemshasbeenmappedoverfrom
its general form in the spectrum prediction component first principles to efficient
specialformscookbookrecipes. Feigenbaumetal.1971
The significance of D EN DR AL was that it was the first successful knowledge-intensive sys-
tem: its expertise derived from large numbers of special-purpose rules. Later systems also
incorporated themainthemeof Mc Carthys Advice Takerapproachthe cleanseparation of
theknowledgeintheformofrulesfromthereasoning component.
With this lesson in mind Feigenbaum and others at Stanford began the Heuristic Pro-
gramming Project H PP to investigate the extent to which the new methodology of expert
systems could be applied to other areas of human expertise. The next major effort was in
E XP ER TS YS TE MS
theareaofmedical diagnosis. Feigenbaum Buchanan and Dr.Edward Shortliffe developed
M YC IN to diagnose blood infections. With about 450 rules M YC IN was able to perform
as well as some experts and considerably better than junior doctors. It also contained two
major differences from D EN DR AL. First unlike the D EN DR AL rules no general theoretical
modelexistedfromwhichthe M YC IN rulescouldbededuced. Theyhadtobeacquiredfrom
extensive interviewing of experts who in turn acquired them from textbooks other experts
anddirectexperienceofcases. Secondtheruleshadtoreflecttheuncertaintyassociatedwith
C ER TA IN TY FA CT OR
medical knowledge. M YC IN incorporated acalculus of uncertainty called certainty factors
see Chapter14whichseemedatthetimetofitwellwithhowdoctorsassessedtheimpact
ofevidence onthediagnosis.
The importance of domain knowledge was also apparent in the area of understanding
natural language. Although Winograds S HR DL U system forunderstanding natural language
hadengendered agooddealofexcitementitsdependence onsyntacticanalysiscausedsome
of the same problems as occurred in the early machine translation work. It was able to
overcome ambiguity and understand pronoun references but this wasmainly because itwas
designed specifically foroneareathe blocks world. Several researchers including Eugene
Charniak a fellow graduate student of Winograds at M IT suggested that robust language
understanding would require general knowledge about the world and a general method for
usingthatknowledge.
At Yale linguist-turned-A I-researcher Roger Schank emphasized this point claiming
Thereisnosuchthingassyntaxwhichupsetalotoflinguistsbutdidservetostartauseful
discussion. Schank and his students built a series of programs Schank and Abelson 1977;
Wilensky 1978; Schank and Riesbeck 1981; Dyer 1983 that all had the task of under-
standing naturallanguage. Theemphasis howeverwasless onlanguage perseandmoreon
theproblemsofrepresenting andreasoning withtheknowledgerequiredforlanguageunder-
standing. The problems included representing stereotypical situations Cullingford 1981
describing human memory organization Rieger 1976; Kolodner 1983 and understanding
plansandgoals Wilensky 1983.
Thewidespread growthof applications toreal-world problems caused aconcurrent in-
crease in the demands for workable knowledge representation schemes. A large number
of different representation and reasoning languages were developed. Some were based on
logicforexamplethe Prologlanguage becamepopularin Europeandthe P LA NN ER fam-
ily in the United States. Others following Minskys idea of frames 1975 adopted a more
F RA ME S
structured approach assembling facts about particular object and event types and arranging
thetypesintoalargetaxonomichierarchy analogous toabiological taxonomy.
Thefirstsuccessfulcommercialexpertsystem R1beganoperationatthe Digital Equipment
Corporation Mc Dermott 1982. The program helped configure orders for new computer
systems; by 1986 it was saving the company an estimated 40 million a year. By 1988
D ECs A Igroup had40expert systems deployed withmoreontheway. Du Ponthad100in
useand500indevelopmentsavinganestimated10millionayear. Nearlyeverymajor U.S.
corporation haditsown A Igroupandwaseitherusingorinvestigating expertsystems.
In1981the Japaneseannouncedthe Fifth Generationprojecta10-yearplantobuild
intelligent computers running Prolog. In response the United States formed the Microelec-
tronics and Computer Technology Corporation M CCas aresearch consortium designed to
assure national competitiveness. In both cases A I was part of a broad effort including chip
design andhuman-interface research. In Britain the Alvey report reinstated thefunding that
wascutbythe Lighthillreport.13 Inallthreecountries howevertheprojectsnevermettheir
ambitiousgoals.
Overallthe A Iindustryboomedfromafewmilliondollarsin1980tobillionsofdollars
in 1988 including hundreds of companies building expert systems vision systems robots
and software and hardware specialized for these purposes. Soon after that came a period
calledthe A IWinterinwhichmanycompaniesfellbythewaysideastheyfailedtodeliver
onextravagant promises.
In the mid-1980s at least four different groups reinvented the back-propagation learning
B AC K-P RO PA GA TI ON
algorithm first found in 1969 by Bryson and Ho. The algorithm was applied to many learn-
ing problems in computer science and psychology and the widespread dissemination of the
results in the collection Parallel Distributed Processing Rumelhart and Mc Clelland 1986
causedgreatexcitement.
These so-called connectionist models of intelligent systems were seen by some as di-
C ON NE CT IO NI ST
rect competitors both to the symbolic models promoted by Newell and Simon and to the
logicist approach of Mc Carthy and others Smolensky 1988. It might seem obvious that
at some level humans manipulate symbolsin fact Terrence Deacons book The Symbolic
Artificial Intelligencehadbeenofficiallycanceled.
Section1.3. The Historyof Artificial Intelligence 25
Species 1997 suggests that this is the defining characteristic of humansbut the most ar-
dentconnectionistsquestionedwhethersymbolmanipulationhadanyrealexplanatoryrolein
detailed modelsofcognition. Thisquestion remainsunanswered butthecurrent viewisthat
connectionistandsymbolicapproachesarecomplementary notcompeting. Asoccurredwith
the separation of A I and cognitive science modern neural network research has bifurcated
into two fields one concerned with creating effective network architectures and algorithms
and understanding theirmathematical properties the other concerned withcareful modeling
oftheempiricalproperties ofactualneuronsandensembles ofneurons.
Recent years have seen a revolution in both the content and the methodology of work in
artificialintelligence.14 Itisnowmorecommontobuild onexisting theories thantopropose
brand-new ones to base claims on rigorous theorems or hard experimental evidence rather
thanonintuition andtoshowrelevance toreal-world applications ratherthantoyexamples.
A Iwasfoundedinpartasarebellionagainstthelimitationsofexistingfieldslikecontrol
theoryandstatistics butnowitisembracingthosefields. As David Mc Allester1998putit:
In the early period of A I it seemed plausible that new forms of symbolic computation
e.g.framesandsemanticnetworksmademuchofclassicaltheoryobsolete. Thisledto
a formof isolationism in which A I became largelyseparatedfromthe rest of computer
science. This isolationism is currently being abandoned. There is a recognition that
machinelearningshouldnotbeisolatedfrominformationtheorythatuncertainreasoning
shouldnotbeisolatedfromstochasticmodelingthatsearchshouldnotbeisolatedfrom
classical optimizationand control and that automated reasoning should not be isolated
fromformalmethodsandstaticanalysis.
In terms of methodology A I has finally come firmly under the scientific method. To be ac-
ceptedhypothesesmustbesubjectedtorigorousempirical experimentsandtheresultsmust
be analyzed statistically for their importance Cohen 1995. It is now possible to replicate
experiments byusingsharedrepositories oftestdataandcode.
The field of speech recognition illustrates the pattern. In the 1970s a wide variety of
different architectures and approaches were tried. Many of these were rather ad hoc and
fragile and were demonstrated on only a few specially selected examples. In recent years
H ID DE NM AR KO V approachesbasedonhidden Markovmodels H MMshavecometodominatethearea. Two
M OD EL S
aspects of H MMsarerelevant. First theyarebased onarigorous mathematical theory. This
hasallowedspeechresearcherstobuildonseveraldecadesofmathematicalresultsdeveloped
in other fields. Second they are generated by a process of training on a large corpus of
real speech data. This ensures that the performance is robust and in rigorous blind tests the
H MMshavebeenimprovingtheirscoressteadily. Speechtechnology andtherelatedfieldof
handwritten character recognition are already making the transition towidespread industrial
grounded inmathematicalrigoroverthe scruffiesthosewhowouldrathertryoutlotsof ideas writesome
programs andthenassesswhatseemstobeworking. Bothapproachesareimportant. Ashifttowardneatness
impliesthatthefieldhasreachedalevelofstabilityandmaturity. Whetherthatstabilitywillbedisruptedbya
newscruffyideaisanotherquestion.
and consumer applications. Note that there is no scientific claim that humans use H MMsto
recognize speech; rather H MMs provide a mathematical framework for understanding the
problem andsupporttheengineering claimthattheyworkwellinpractice.
Machine translation follows the samecourse asspeech recognition. In the1950s there
was initial enthusiasm for an approach based on sequences of words with models learned
according to the principles of information theory. That approach fell out of favor in the
1960s butreturnedinthelate1990sandnowdominatesthefield.
Neural networks also fitthis trend. Much of the work on neural nets in the 1980s was
done inanattempt toscope out whatcould bedone and tolearn howneural netsdifferfrom
traditional techniques. Usingimprovedmethodology and theoretical frameworks thefield
arrived at an understanding in which neural nets can now be compared with corresponding
techniquesfromstatisticspatternrecognitionandmachinelearningandthemostpromising
technique can be applied to each application. As a result of these developments so-called
dataminingtechnology hasspawnedavigorous newindustry.
D AT AM IN IN G
Judea Pearls1988 Probabilistic Reasoningin Intelligent Systemsledtoanewaccep-
tance ofprobability anddecision theory in A Ifollowing aresurgence ofinterest epitomized
by Peter Cheesemans 1985 article In Defense of Probability. The Bayesian network
B AY ES IA NN ET WO RK
formalism was invented to allow efficient representation of and rigorous reasoning with
uncertain knowledge. This approach largely overcomes many problems of the probabilistic
reasoningsystemsofthe1960sand1970s;itnowdominates A Iresearchonuncertainreason-
ing and expert systems. The approach allows for learning from experience and it combines
thebestofclassical A Iandneuralnets. Workby Judea Pearl1982aandby Eric Horvitzand
David Heckerman Horvitzand Heckerman1986;Horvitzetal.1986promotedtheideaof
normative expert systems: ones that act rationally according to the laws of decision theory
anddonottrytoimitatethethoughtstepsofhumanexperts. The Windows T M operatingsys-
tem includes several normative diagnostic expert systems for correcting problems. Chapters
13to16coverthisarea.
Similar gentle revolutions have occurred in robotics computer vision and knowledge
representation. Abetterunderstanding oftheproblemsand theircomplexitypropertiescom-
bined withincreased mathematical sophistication has led to workable research agendas and
robustmethods. Althoughincreasedformalizationandspecializationledfieldssuchasvision
androboticstobecomesomewhatisolatedfrommainstream A Iinthe1990sthistrendhas
reversedinrecentyearsastoolsfrommachinelearninginparticularhaveprovedeffectivefor
manyproblems. Theprocessofreintegration isalreadyyielding significantbenefits
Perhaps encouraged bytheprogress insolving thesubproblems of A Iresearchers havealso
started to look at the whole agent problem again. The work of Allen Newell John Laird
and Paul Rosenbloomon S OA RNewell1990;Lairdetal.1987isthebest-knownexample
of a complete agent architecture. One of the most important environments for intelligent
agents is the Internet. A I systems have become so common in Web-based applications that
the -bot suffix has entered everyday language. Moreover A I technologies underlie many
Section1.3. The Historyof Artificial Intelligence 27
Internettools suchassearchengines recommendersystems and Websiteaggregators.
Oneconsequenceoftryingtobuildcompleteagentsistherealizationthatthepreviously
isolated subfields of A I might need to be reorganized somewhat when their results are to be
tied together. In particular it is now widely appreciated that sensory systems vision sonar
speechrecognition etc. cannotdeliverperfectlyreliableinformationabouttheenvironment.
Hence reasoning and planning systems must be able to handle uncertainty. Asecond major
consequence of the agent perspective is that A I has been drawn into much closer contact
with other fields such as control theory and economics that also deal with agents. Recent
progressinthecontrolofroboticcarshasderivedfromamixtureofapproachesrangingfrom
better sensors control-theoretic integration of sensing localization and mapping as well as
adegreeofhigh-level planning.
Despite these successes some influential founders of A I including John Mc Carthy
2007 Marvin Minsky 2007 Nils Nilsson 1995 2005 and Patrick Winston Beal and
Winston2009haveexpresseddiscontentwiththeprogressof A I.Theythinkthat A Ishould
put less emphasis on creating ever-improved versions of applications that are good at a spe-
cific task such as driving a car playing chess or recognizing speech. Instead they believe
A Ishouldreturntoitsrootsofstrivingforin Simonswordsmachinesthatthinkthatlearn
andthatcreate. Theycalltheeffort human-level A Ior H LA I;theirfirstsymposium wasin
H UM AN-L EV EL AI
2004 Minskyetal.2004. Theeffortwillrequireverylargeknowledgebases; Hendleretal.
1995discusswheretheseknowledgebasesmightcomefrom.
A RT IF IC IA LG EN ER AL A related idea is the subfield of Artificial General Intelligence or A GI Goertzel and
I NT EL LI GE NC E
Pennachin2007whichhelditsfirstconferenceandorganizedthe Journalof Artificial Gen-
eral Intelligence in 2008. A GI looks for a universal algorithm for learning and acting in
any environment and has its roots in the work of Ray Solomonoff 1964 one of the atten-
dees of the original 1956 Dartmouth conference. Guaranteeing that what we create is really
Friendly A I isalso aconcern Yudkowsky 2008; Omohundro 2008 one we will return to
F RI EN DL YA I
in Chapter26.
Throughoutthe60-yearhistoryofcomputersciencetheemphasishasbeenonthealgorithm
asthemainsubject ofstudy. Butsomerecentworkin A Isuggests thatformanyproblemsit
makes more sense to worry about the data and be less picky about what algorithm to apply.
This is true because of the increasing availability of very large data sources: for example
trillionsofwordsof Englishandbillionsofimagesfromthe Web Kilgarriffand Grefenstette
2006;orbillionsofbasepairsofgenomicsequences Collinsetal.2003.
One influential paper in this line was Yarowskys 1995 work on word-sense disam-
biguation: giventheuseofthewordplant inasentence doesthatrefertofloraorfactory?
Previous approaches to the problem had relied on human-labeled examples combined with
machine learning algorithms. Yarowsky showed that the task can be done with accuracy
above 96 with no labeled examples at all. Instead given a very large corpus of unanno-
tatedtextandjustthedictionary definitions ofthetwosensesworks industrial plant and
flora plant lifeone can label examples in the corpus and from there bootstrap to learn
new patterns that help label new examples. Banko and Brill 2001 show that techniques
like this perform even better as the amount of available text goes from a million words to a
billion and that the increase in performance from using more data exceeds any difference in
algorithm choice; a mediocre algorithm with 100 million words of unlabeled training data
outperforms thebestknownalgorithm with1millionwords.
Asanotherexample Haysand Efros2007discuss theproblem offillinginholesina
photograph. Suppose you use Photoshop to mask out an ex-friend from a group photo but
now you need to fill in the masked area with something that matches the background. Hays
and Efrosdefinedanalgorithmthatsearchesthroughacollectionofphotostofindsomething
that will match. They found the performance of their algorithm was poor when they used
a collection of only ten thousand photos but crossed a threshold into excellent performance
whentheygrewthecollection totwomillionphotos.
Worklikethissuggests thattheknowledge bottleneck in A Itheproblem ofhowto
expressalltheknowledgethatasystemneedsmaybesolvedinmanyapplicationsbylearn-
ingmethodsratherthanhand-codedknowledgeengineering providedthelearningalgorithms
haveenoughdatatogoon Halevyetal.2009. Reportershavenoticedthesurgeofnewap-
plications and have written that A I Winter may be yielding to a new Spring Havenstein
2005. As Kurzweil 2005 writes today many thousands of A I applications are deeply
embeddedintheinfrastructure ofeveryindustry.
What can A I do today? A concise answer isdifficult because there are so many activities in
somanysubfields. Herewesampleafewapplications; othersappearthroughout thebook.
Robotic vehicles: A driverless robotic car named S TA NL EY sped through the rough
D AR PA Grand Challenge. S TA NL EY isa Volkswagen Touaregoutfittedwithcamerasradar
andlaserrangefinders tosensetheenvironment andonboard softwaretocommandthesteer-
ing braking andacceleration Thrun 2006. Thefollowing year C MUs B OS S wonthe Ur-
ban Challengesafelydrivingintrafficthroughthestreets ofaclosed Air Forcebaseobeying
trafficrulesandavoiding pedestrians andothervehicles.
Speechrecognition: Atravelercalling United Airlinestobookaflightcanhavetheen-
tireconversationguidedbyanautomatedspeechrecognitionanddialogmanagementsystem.
Autonomousplanningandscheduling: Ahundredmillionmilesfrom Earth N AS As
Remote Agent program became the first on-board autonomous planning program to control
the scheduling of operations for a spacecraft Jonsson et al. 2000. R EM OT E A GE NT gen-
erated plansfrom high-level goals specified fromtheground andmonitored theexecution of
thoseplansdetecting diagnosing andrecoveringfromproblemsastheyoccurred. Succes-
sorprogram M AP GE N Al-Chang etal.2004plans thedailyoperations for N AS As Mars
Exploration Roversand M EX AR2 Cestaetal.2007didmissionplanningboth logistics
andscienceplanningfor the European Space Agencys Mars Expressmissionin2008.
Section1.5. Summary 29
Game playing: I BMs D EE P B LU E became the first computer program to defeat the
world champion in a chess match when it bested Garry Kasparov by a score of 3.5 to 2.5 in
anexhibition match Goodman and Keene 1997. Kasparov said that hefelt anew kind of
intelligence across the board from him. Newsweek magazine described the match as The
brains last stand. The value of I BMs stock increased by 18 billion. Human champions
studied Kasparovs loss and were able to draw a few matches in subsequent years but the
mostrecenthuman-computer matcheshavebeenwonconvincingly bythecomputer.
Spamfighting: Eachdaylearningalgorithmsclassifyoverabillionmessagesasspam
savingtherecipientfromhavingtowastetimedeletingwhatformanyuserscouldcomprise
80or90ofallmessagesifnotclassifiedawaybyalgorithms. Becausethespammersare
continually updating theirtactics itisdifficultforastaticprogrammed approach tokeepup
andlearning algorithmsworkbest Sahami etal.1998;Goodmanand Heckerman 2004.
Logistics planning: During the Persian Gulf crisis of 1991 U.S. forces deployed a
Dynamic Analysis and Replanning Tool D AR T Cross and Walker 1994 to doautomated
logistics planning and scheduling for transportation. This involved up to 50000 vehicles
cargo and people at a time and had to account for starting points destinations routes and
conflict resolution among all parameters. The A I planning techniques generated in hours
a plan that would have taken weeks with older methods. The Defense Advanced Research
Project Agency D AR PA stated that this single application more than paid back D AR PAs
30-yearinvestment in A I.
Robotics: The i Robot Corporation has sold overtwomillion Roomba robotic vacuum
cleaners for home use. The company also deploys the more rugged Pack Bot to Iraq and
Afghanistan where it is used to handle hazardous materials clear explosives and identify
thelocationofsnipers.
Machine Translation: A computer program automatically translates from Arabic to
English allowing an English speaker to see the headline Ardogan Confirms That Turkey
Would Not Accept Any Pressure Urging Them to Recognize Cyprus. The program uses a
statisticalmodelbuiltfromexamplesof Arabic-to-English translationsandfromexamplesof
Englishtexttotalingtwotrillionwords Brants etal.2007. Noneofthecomputerscientists
ontheteamspeak Arabicbuttheydounderstand statisticsandmachinelearningalgorithms.
These are just a few examples of artificial intelligence systems that exist today. Not
magic or science fictionbut rather science engineering and mathematics to which this
bookprovidesanintroduction.
This chapter defines A I and establishes the cultural background against which it has devel-
oped. Someoftheimportantpointsareasfollows:
Differentpeopleapproach A Iwithdifferent goalsinmind. Twoimportant questions to
askare: Areyouconcerned withthinking orbehavior? Doyou wanttomodelhumans
orworkfromanidealstandard?
In this book we adopt the view that intelligence is concerned mainly with rational
action. Ideally an intelligent agent takes the best possible action in a situation. We
studytheproblem ofbuilding agentsthatareintelligent inthissense.
Philosophers going back to 400 B.C. made A I conceivable by considering the ideas
thatthemindisinsomewayslikeamachinethatitoperatesonknowledgeencodedin
someinternal language andthatthought canbeusedtochoosewhatactionstotake.
Mathematiciansprovidedthetoolstomanipulatestatementsoflogicalcertaintyaswell
asuncertain probabilistic statements. Theyalso setthe groundwork forunderstanding
computation andreasoning aboutalgorithms.
Economists formalized the problem of making decisions that maximize the expected
outcometothedecisionmaker.
Neuroscientistsdiscoveredsomefactsabouthowthebrainworksandthewaysinwhich
itissimilartoanddifferentfromcomputers.
Psychologistsadoptedtheideathathumansandanimalscanbeconsideredinformation-
processing machines. Linguists showedthatlanguage usefitsintothismodel.
Computerengineers provided the ever-more-powerful machines that make A I applica-
tionspossible.
Controltheorydealswithdesigning devicesthatactoptimally onthebasisoffeedback
from the environment. Initially the mathematical tools of control theory were quite
differentfrom A Ibutthefieldsarecomingclosertogether.
Thehistoryof A Ihashadcyclesofsuccessmisplacedoptimismandresultingcutbacks
in enthusiasm and funding. There have also been cycles of introducing new creative
approaches andsystematically refiningthebestones.
A Ihasadvancedmorerapidlyinthepastdecadebecauseofgreateruseofthescientific
methodinexperimenting withandcomparing approaches.
Recentprogressinunderstanding thetheoretical basisfor intelligence hasgonehandin
hand with improvements in the capabilities of real systems. The subfields of A I have
becomemoreintegrated and A Ihasfoundcommonground withotherdisciplines.
B IB LI OG RA PH IC AL A ND H IS TO RI CA L N OT ES
Themethodologicalstatusofartificialintelligenceisinvestigatedin The Sciencesofthe Artifi-
cialby Herb Simon1981whichdiscussesresearchareasconcernedwithcomplexartifacts.
It explains how A I can be viewed as both science and mathematics. Cohen 1995 gives an
overviewofexperimental methodology within A I.
The Turing Test Turing 1950 isdiscussed by Shieber1994 whoseverely criticizes
the usefulness of its instantiation in the Loebner Prize competition and by Ford and Hayes
1995 whoarguethatthetestitselfisnothelpfulfor A I.Bringsjord 2008 givesadvicefor
a Turing Test judge. Shieber 2004 and Epstein et al. 2008 collect a number of essays on
the Turing Test. Artificial Intelligence: The Very Idea by John Haugeland 1985 gives a
2
I NT EL LI GE NT A GE NT S
In which we discuss the nature of agents perfect or otherwise the diversity of
environments andtheresulting menagerieofagenttypes.
intelligence. Inthischapterwemakethisnotionmoreconcrete. Wewillseethattheconcept
ofrationality canbeappliedtoawidevarietyofagentsoperating inanyimaginable environ-
ment. Ourplanin thisbook istousethis concept todevelop asmallsetof design principles
forbuilding successful agentssystems thatcanreasonably becalled intelligent.
We begin by examining agents environments and the coupling between them. The
observationthatsomeagentsbehavebetterthanothersleadsnaturallytotheideaofarational
agentone that behaves as well as possible. How well an agent can behave depends on
the nature of the environment; some environments are more difficult than others. We give a
crude categorization of environments and show how properties of an environment influence
thedesignofsuitableagentsforthatenvironment. Wedescribe anumberofbasicskeleton
agentdesigns whichwefleshoutintherestofthebook.
Anagentisanything thatcanbeviewedasperceiving itsenvironmentthrough sensors and
E NV IR ON ME NT
actinguponthatenvironmentthrough actuators. Thissimpleideaisillustratedin Figure2.1.
S EN SO R
Ahumanagenthaseyesearsandotherorgansforsensorsandhandslegsvocaltractandso
A CT UA TO R
on for actuators. A robotic agent might have cameras and infrared range finders for sensors
and various motors for actuators. A software agent receives keystrokes file contents and
network packets as sensory inputs and acts on the environment by displaying on the screen
writingfilesandsending networkpackets.
Weusethetermpercepttorefertotheagentsperceptualinputsatanygiveninstant. An
P ER CE PT
agentsperceptsequenceisthecompletehistoryofeverythingtheagenthaseverperceived.
P ER CE PT SE QU EN CE
Ingeneral an agents choice ofaction atany given instant candepend on theentire percept
sequenceobservedtodatebutnotonanything ithasntperceived. Byspecifying theagents
choice of action for every possible percept sequence we have said more or less everything
34
Section2.1. Agentsand Environments 35
Agent
Sensors
Actuators
Environment
Percepts
?
Actions
Figure2.1 Agentsinteractwithenvironmentsthroughsensorsandactuators.
there is to say about the agent. Mathematically speaking we say that an agents behavior is
described bytheagentfunctionthatmapsanygivenperceptsequence toanaction.
A GE NT FU NC TI ON
Wecan imagine tabulating the agent function that describes any given agent; formost
agents this would be a very large tableinfinite in fact unless we place a bound on the
lengthofperceptsequenceswewanttoconsider. Givenanagenttoexperimentwithwecan
in principle construct this table by trying out all possible percept sequences and recording
whichactionstheagentdoesinresponse.1 Thetableisofcourseanexternalcharacterization
of the agent. Internally the agent function for an artificial agent will be implemented by an
agent program. It is important to keep these two ideas distinct. The agent function is an
A GE NT PR OG RA M
abstract mathematical description; the agent program is a concrete implementation running
withinsomephysicalsystem.
To illustrate these ideas we use a very simple examplethe vacuum-cleaner world
shown in Figure 2.2. This world is so simple that we can describe everything that happens;
itsalsoamade-upworldsowecaninventmanyvariations. Thisparticularworldhasjusttwo
locations: squares A and B. The vacuum agent perceives which square it is in and whether
there is dirt in the square. It can choose to move left move right suck up the dirt or do
nothing. One very simple agent function is the following: if the current square is dirty then
suck;otherwisemovetotheothersquare. Apartialtabulationofthisagentfunctionisshown
in Figure2.3andanagentprogram thatimplementsitappears in Figure2.8onpage48.
Lookingat Figure2.3weseethatvarious vacuum-world agentscanbedefinedsimply
byfillingintheright-handcolumninvariousways. Theobviousquestionthenisthis: What
is the right way to fill out the table? In other words what makes an agent good or bad
intelligent orstupid? Weanswerthesequestions inthenextsection.
timestoidentifytheprobabilityofeachaction. Onemightimaginethatactingrandomlyisrathersillybutwe
showlaterinthischapterthatitcanbeveryintelligent.
A B
Figure2.2 Avacuum-cleanerworldwithjusttwolocations.
Perceptsequence Action
A Clean Right
A Dirty Suck
B Clean Left
B Dirty Suck
A Clean A Clean Right
A Clean A Dirty Suck
. .
. .
. .
A Clean A Clean A Clean Right
A Clean A Clean A Dirty Suck
. .
. .
. .
shownin Figure2.2.
Beforeclosingthissectionweshouldemphasizethatthenotionofanagentismeantto
be a tool for analyzing systems not an absolute characterization that divides the world into
agents and non-agents. One could view a hand-held calculator as an agent that chooses the
action of displaying 4 when given the percept sequence 2 2 but such an analysis
wouldhardly aidourunderstanding ofthecalculator. Inasense allareasofengineering can
be seen as designing artifacts that interact with the world; A I operates at what the authors
consider to be the most interesting end of the spectrum where the artifacts have significant
computational resources andthetaskenvironment requires nontrivial decision making.
Arational agent is one that does the right thingconceptually speaking every entry in the
R AT IO NA LA GE NT
table for the agent function is filled out correctly. Obviously doing the right thing is better
thandoingthewrongthingbutwhatdoesitmeantodotherightthing?
Section2.2. Good Behavior: The Conceptof Rationality 37
We answer this age-old question in an age-old way: by considering the consequences
of the agents behavior. When an agent is plunked down in an environment it generates a
sequenceofactionsaccordingtotheperceptsitreceives. Thissequenceofactionscausesthe
environment to go through a sequence of states. If the sequence is desirable then the agent
P ER FO RM AN CE has performed well. This notion of desirability is captured by aperformance measure that
M EA SU RE
evaluatesanygivensequence ofenvironment states.
Notice that we said environment states not agent states. If we define success in terms
of agents opinion ofits own performance an agent could achieve perfect rationality simply
bydeluding itselfthatitsperformance wasperfect. Humanagentsinparticular arenotorious
for sour grapesbelieving they did not really want something e.g. a Nobel Prize after
notgettingit.
Obviouslythereisnotonefixedperformancemeasureforalltasksandagents;typically
adesigner willdevise one appropriate tothe circumstances. Thisis notas easy asit sounds.
Consider for example the vacuum-cleaner agent from the preceding section. We might
proposetomeasureperformancebytheamountofdirtcleaned upinasingleeight-hourshift.
With a rational agent of course what you ask for is what you get. A rational agent can
maximizethisperformance measurebycleaning upthedirtthendumpingitallonthefloor
thencleaningitupagainandsoon. Amoresuitableperformancemeasurewouldrewardthe
agentforhavingacleanfloor. Forexampleonepointcouldbeawardedforeachcleansquare
at each time step perhaps with a penalty for electricity consumed and noise generated. As
a general rule it is better to design performance measures according to what one actually
wantsintheenvironment ratherthanaccording tohowonethinkstheagentshouldbehave.
Evenwhentheobviouspitfallsareavoidedthereremainsomeknottyissuestountangle.
For example the notion of clean floor in the preceding paragraph is based on average
cleanliness over time. Yet the same average cleanliness can be achieved by two different
agentsoneofwhichdoesamediocrejoballthetimewhiletheothercleansenergetically but
takeslongbreaks. Whichispreferablemightseemtobeafinepointofjanitorialscience but
in fact it is a deep philosophical question with far-reaching implications. Which is better
a reckless life of highs and lows or a safe but humdrum existence? Which is betteran
economy where everyone lives in moderate poverty or one in which some live in plenty
whileothersareverypoor? Weleavethesequestions asanexerciseforthediligentreader.
Whatisrationalatanygiventimedepends onfourthings:
Theperformance measurethatdefinesthecriterionofsuccess.
Theagentspriorknowledge oftheenvironment.
Theactionsthattheagentcanperform.
Theagentsperceptsequence todate.
D EF IN IT IO NO FA Thisleadstoadefinitionofarationalagent:
R AT IO NA LA GE NT
Foreachpossible perceptsequence a rationalagentshouldselect anaction thatis ex-
pectedtomaximizeitsperformancemeasuregiventheevidenceprovidedbythepercept
sequenceandwhateverbuilt-inknowledgetheagenthas.
Considerthesimplevacuum-cleaner agent thatcleans asquare ifitisdirty andmovestothe
othersquare ifnot; thisistheagentfunction tabulated in Figure2.3. Isthisarational agent?
That depends! First we need to say what the performance measure is what is known about
theenvironment andwhatsensorsandactuators theagenthas. Letusassumethefollowing:
The performance measure awards one point for each clean square at each time step
overalifetimeof1000timesteps.
The geography of the environment is known a priori Figure 2.2 but the dirt distri-
butionandtheinitiallocationoftheagentarenot. Cleansquaresstaycleanandsucking
cleans the current square. The Left and Right actions move the agent left and right
exceptwhenthiswouldtaketheagentoutsidetheenvironment inwhichcasetheagent
remainswhereitis.
Theonlyavailable actionsare Left Rightand Suck.
Theagentcorrectly perceivesitslocation andwhetherthat location contains dirt.
We claim that under these circumstances the agent is indeed rational; its expected perfor-
manceisatleastashighasanyotheragents. Exercise2.2asksyoutoprovethis.
One can see easily that the same agent would be irrational under different circum-
stances. Forexample once allthedirtiscleaned up theagent willoscillate needlessly back
andforth;iftheperformancemeasureincludesapenaltyofonepointforeachmovementleft
or right the agent will fare poorly. A better agent for this case would do nothing once it is
sure that allthe squares are clean. Ifclean squares can become dirty again the agent should
occasionally check and re-clean them if needed. If the geography of the environment is un-
known the agent will need to explore it rather than stick to squares A and B. Exercise 2.2
asksyoutodesignagentsforthesecases.
We need to be careful to distinguish between rationality and omniscience. An omniscient
O MN IS CI EN CE
agent knows the actual outcome of its actions and can act accordingly; but omniscience is
impossible in reality. Consider the following example: I am walking along the Champs
Elysees one day and I see an old friend across the street. There is no traffic nearby and Im
not otherwise engaged so being rational I start to cross the street. Meanwhile at 33000
feet a cargo door falls off a passing airliner2 and before I make it to the other side of the
street Iamflattened. Was Iirrationaltocrossthestreet? Itisunlikelythatmyobituarywould
read Idiotattemptstocrossstreet.
This example shows that rationality is not the same as perfection. Rationality max-
imizes expected performance while perfection maximizes actual performance. Retreating
from a requirement of perfection is not just a question of being fair to agents. The point is
that if we expect an agent to do what turns out to be the best action after the fact it will be
impossibletodesignanagenttofulfillthisspecificationunlessweimprovetheperformance
ofcrystal ballsortimemachines.
Section2.2. Good Behavior: The Conceptof Rationality 39
Our definition of rationality does not require omniscience then because the rational
choice depends only on the percept sequence to date. We must also ensure that we havent
inadvertently allowedtheagenttoengageindecidedly underintelligent activities. Forexam-
pleifanagentdoesnotlookbothwaysbeforecrossingabusyroadthenitsperceptsequence
will not tell it that there is a large truck approaching at high speed. Does our definition of
rationality saythatitsnow O K tocross theroad? Farfromit! Firstitwould notberational
tocross the road given thisuninformative percept sequence: the riskofaccident from cross-
ingwithoutlookingistoogreat. Secondarationalagentshouldchoosethelooking action
before stepping into the street because looking helps maximize the expected performance.
Doing actions in order to modify future perceptssometimes called information gather-
I NF OR MA TI ON ingis an important part of rationality and is covered in depth in Chapter 16. A second
G AT HE RI NG
exampleofinformationgathering isprovidedbythe exploration thatmustbeundertaken by
E XP LO RA TI ON
avacuum-cleaning agentinaninitially unknownenvironment.
Ourdefinition requires arational agentnotonlytogatherinformation butalsotolearn
L EA RN IN G
as much as possible from what it perceives. The agents initial configuration could reflect
some prior knowledge of the environment but as the agent gains experience this may be
modified and augmented. There are extreme cases in which the environment is completely
known a priori. In such cases the agent need not perceive orlearn; it simply acts correctly.
Ofcoursesuchagentsarefragile. Considerthelowlydungbeetle. Afterdiggingitsnestand
layingitseggsitfetchesaballofdungfromanearbyheaptoplugtheentrance. Iftheballof
dung isremoved from itsgrasp en route the beetle continues its task and pantomimes plug-
ging the nest withthe nonexistent dung ball never noticing that it is missing. Evolution has
builtanassumption intothebeetlesbehavior andwhenitisviolated unsuccessful behavior
results. Slightly more intelligent is the sphex wasp. The female sphex will dig a burrow go
out and sting a caterpillar and drag it to the burrow enter the burrow again to check all is
welldragthecaterpillarinsideandlayitseggs. Thecaterpillarservesasafoodsourcewhen
the eggs hatch. So far so good but if an entomologist moves the caterpillar a few inches
away while the sphex isdoing the check it willrevert to the drag step ofits plan and will
continuetheplanwithoutmodificationevenafterdozensofcaterpillar-movinginterventions.
Thesphexisunabletolearnthatitsinnateplanisfailing andthuswillnotchange it.
To the extent that an agent relies on the prior knowledge of its designer rather than
A UT ON OM Y on its own percepts we say that the agent lacks autonomy. A rational agent should be
autonomousit should learn what it can to compensate forpartial or incorrect prior knowl-
edge. Forexampleavacuum-cleaningagentthatlearnstoforeseewhereandwhenadditional
dirt will appear will do better than one that does not. As a practical matter one seldom re-
quires complete autonomy from the start: when the agent has had little or no experience it
would have to act randomly unless the designer gave some assistance. So just as evolution
providesanimalswithenoughbuilt-inreflexestosurvivelongenoughtolearnforthemselves
it would be reasonable to provide an artificial intelligent agent with some initial knowledge
as well as an ability to learn. After sufficient experience of its environment the behavior
of a rational agent can become effectively independent of its prior knowledge. Hence the
incorporation of learning allows one to design a single rational agent that will succeed in a
vastvarietyofenvironments.
Now that we have a definition of rationality we are almost ready to think about building
rational agents. First however we must think about task environments which are essen-
T AS KE NV IR ON ME NT
tiallytheproblems towhichrational agentsarethesolutions. Webeginbyshowinghow
to specify a task environment illustrating the process with a number of examples. We then
showthattask environments comeinavarietyofflavors. Theflavorofthetaskenvironment
directlyaffectstheappropriate designfortheagentprogram.
In our discussion of the rationality of the simple vacuum-cleaner agent we had to specify
theperformance measure theenvironment and theagents actuators and sensors. Wegroup
alltheseundertheheading ofthetaskenvironment. Fortheacronymically minded wecall
thisthe P EA SPerformance Environment Actuators Sensorsdescription. Indesigning an
P EA S
agentthefirststepmustalwaysbetospecify thetaskenvironment asfullyaspossible.
Thevacuum worldwasasimpleexample;letusconsider amorecomplexproblem: an
automated taxi driver. We should point out before the reader becomes alarmed that a fully
automatedtaxiiscurrentlysomewhatbeyondthecapabilitiesofexistingtechnology. page28
describes anexisting driving robot. Thefulldriving task isextremely open-ended. Thereis
nolimittothenovelcombinations ofcircumstances thatcanariseanother reasonwechose
it as a focus for discussion. Figure 2.4 summarizes the P EA S description for the taxis task
environment. Wediscuss eachelementinmoredetailinthefollowingparagraphs.
Agent Type Performance Environment Actuators Sensors
Measure
Taxidriver Safefastlegal Roadsother Steering Camerassonar
comfortabletrip traffic accelerator speedometer
maximizeprofits pedestrians brakesignal G PSodometer
customers horndisplay accelerometer
enginesensors
keyboard
Figure2.4 P EA Sdescriptionofthetaskenvironmentforanautomatedtaxi.
First what isthe performance measure towhich wewould like ourautomated driver
toaspire? Desirablequalities includegetting tothecorrect destination; minimizingfuelcon-
sumptionandwearandtear;minimizingthetriptimeorcost; minimizingviolationsoftraffic
laws and disturbances to other drivers; maximizing safety and passenger comfort; maximiz-
ingprofits. Obviously someofthesegoalsconflict sotradeoffs willberequired.
Next what is the driving environment that the taxi will face? Any taxi driver must
deal with a variety of roads ranging from rural lanes and urban alleys to 12-lane freeways.
The roads contain other traffic pedestrians stray animals road works police cars puddles
Section2.3. The Natureof Environments 41
andpotholes. Thetaximustalsointeractwithpotentialandactualpassengers. Therearealso
some optional choices. The taxi might need to operate in Southern California where snow
is seldom aproblem orin Alaska where it seldom isnot. Itcould always be driving on the
right orwemightwantittobeflexibleenough todriveontheleftwhenin Britain or Japan.
Obviously themorerestricted theenvironment theeasier thedesignproblem.
Theactuatorsforanautomatedtaxiincludethoseavailable toahumandriver: control
overtheengine through theaccelerator andcontrol oversteering andbraking. Inaddition it
will need output to a display screen or voice synthesizer to talk back to the passengers and
perhapssomewaytocommunicatewithothervehicles politely orotherwise.
The basic sensors for the taxi will include one or more controllable video cameras so
that it can see the road; it might augment these with infrared or sonar sensors to detect dis-
tancestoothercarsandobstacles. Toavoidspeedingticketsthetaxishouldhaveaspeedome-
terandtocontrolthevehicleproperly especiallyoncurvesitshouldhaveanaccelerometer.
Todetermine themechanical state ofthevehicle itwillneed theusual array ofengine fuel
and electrical system sensors. Like many human drivers it might want a global positioning
system G PS so that it doesnt get lost. Finally it will need a keyboard or microphone for
thepassengertorequest adestination.
In Figure 2.5 we have sketched the basic P EA S elements for a number of additional
agenttypes. Furtherexamplesappearin Exercise2.4. Itmaycomeasasurprisetosomeread-
ers that our list of agent types includes some programs that operate in the entirely artificial
environmentdefinedbykeyboardinputandcharacteroutputonascreen. Surelyonemight
saythisisnotarealenvironment isit? Infactwhatmattersisnotthedistinction between
realandartificialenvironments butthecomplexityof therelationship amongthebehav-
ior of the agent the percept sequence generated by the environment and the performance
measure. Somerealenvironments areactuallyquitesimple. Forexamplearobotdesigned
toinspectpartsastheycomebyonaconveyorbeltcanmakeuse ofanumberofsimplifying
assumptions: that the lighting is always just so that the only thing onthe conveyor belt will
bepartsofakindthatitknowsaboutandthatonlytwoactionsacceptorrejectarepossible.
Incontrast somesoftwareagentsorsoftwarerobotsor softbotsexistinrichunlim-
S OF TW AR EA GE NT
iteddomains. Imagineasoftbot Websiteoperatordesignedtoscan Internetnewssourcesand
S OF TB OT
show the interesting items to its users while selling advertising space to generate revenue.
To do well that operator will need some natural language processing abilities it will need
to learn what each user and advertiser is interested in and it will need to change its plans
dynamicallyfor example when the connection for one news source goes down or when a
new one comes online. The Internet is an environment whose complexity rivals that of the
physicalworldandwhoseinhabitants includemanyartificialandhumanagents.
The range of task environments that might arise in A I is obviously vast. We can however
identify a fairly small number of dimensions along which task environments can be catego-
rized. These dimensions determine to a large extent the appropriate agent design and the
applicability of each of the principal families of techniques foragent implementation. First
Agent Type Performance Environment Actuators Sensors
Measure
Medical Healthypatient Patienthospital Displayof Keyboardentry
diagnosissystem reducedcosts staff questionstests ofsymptoms
diagnoses findingspatients
treatments answers
referrals
Satelliteimage Correctimage Downlinkfrom Displayofscene Colorpixel
analysissystem categorization orbitingsatellite categorization arrays
Part-picking Percentageof Conveyorbelt Jointedarmand Camerajoint
robot partsincorrect withparts;bins hand anglesensors
bins
Refinery Purityyield Refinery Valvespumps Temperature
controller safety operators heatersdisplays pressure
chemicalsensors
Interactive Studentsscore Setofstudents Displayof Keyboardentry
Englishtutor ontest testingagency exercises
suggestions
corrections
Figure2.5 Examplesofagenttypesandtheir P EA Sdescriptions.
welistthedimensions thenweanalyzeseveraltaskenvironments toillustrate theideas. The
definitionshereareinformal;laterchaptersprovidemoreprecisestatementsandexamplesof
eachkindofenvironment.
Fully observable vs. partially observable: If an agents sensors give it access to the
F UL LY OB SE RV AB LE
P AR TI AL LY complete state of the environment at each point in time then we say that the task environ-
O BS ER VA BL E
ment is fully observable. A task environment is effectively fully observable if the sensors
detect all aspects that are relevant tothe choice of action; relevance inturn depends on the
performance measure. Fullyobservable environments areconvenient because theagentneed
notmaintainanyinternal statetokeeptrackoftheworld. An environment mightbepartially
observable because of noisy and inaccurate sensors or because parts of the state are simply
missing from the sensor datafor example a vacuum agent with only a local dirt sensor
cannottellwhetherthereisdirtinothersquaresandanautomatedtaxicannotseewhatother
drivers are thinking. If the agent has no sensors at all then the environment is unobserv-
able. One might think that in such cases the agents plight is hopeless but as wediscuss in
U NO BS ER VA BL E
Chapter4theagents goalsmaystillbeachievable sometimeswithcertainty.
Singleagent vs. multiagent: Thedistinction between single-agent andmultiagent en-
S IN GL EA GE NT
M UL TI AG EN T
Section2.3. The Natureof Environments 43
vironments may seem simple enough. Forexample anagent solving acrossword puzzle by
itself is clearly in a single-agent environment whereas an agent playing chess is in a two-
agentenvironment. Therearehowever somesubtle issues. Firstwehavedescribed howan
entity may be viewed as an agent but we have not explained which entities must be viewed
as agents. Does an agent A the taxi driver for example have to treat an object B another
vehicleasanagentorcanitbetreatedmerelyasanobjectbehavingaccordingtothelawsof
physics analogous towavesatthebeach orleaves blowing inthewind? Thekeydistinction
iswhether Bsbehaviorisbestdescribedasmaximizingaperformancemeasurewhosevalue
depends on agent As behavior. For example in chess the opponent entity B is trying to
maximize its performance measure which by the rules of chess minimizes agent As per-
formancemeasure. Thuschessisa competitivemultiagent environment. Inthetaxi-driving
C OM PE TI TI VE
environment on the other hand avoiding collisions maximizes the performance measure of
all agents so it is a partially cooperative multiagent environment. It is also partially com-
C OO PE RA TI VE
petitive because for example only one car can occupy a parking space. The agent-design
problems inmultiagent environments areoftenquite different fromthose insingle-agent en-
vironments;forexample communicationoftenemergesasarationalbehaviorinmultiagent
environments; insomecompetitive environments randomizedbehaviorisrational because
itavoids thepitfallsofpredictability.
Deterministic vs. stochastic. If the next state of the environment is completely deter-
D ET ER MI NI ST IC
minedbythecurrentstateandtheactionexecutedbytheagentthenwesaytheenvironment
S TO CH AS TI C
isdeterministic;otherwiseitisstochastic. Inprincipleanagentneednotworryaboutuncer-
tainty in a fully observable deterministic environment. In our definition we ignore uncer-
tainty that arises purely from the actions of other agents in a multiagent environment; thus
a game can be deterministic even though each agent may be unable to predict the actions of
the others. If the environment is partially observable however then it could appear to be
stochastic. Most real situations are so complex that it is impossible to keep track of all the
unobserved aspects;forpracticalpurposes theymustbetreatedasstochastic. Taxidrivingis
clearly stochastic inthissense because one canneverpredict thebehavior oftrafficexactly;
moreover ones tires blow out and ones engine seizes up without warning. The vacuum
worldaswedescribed itisdeterministic butvariations caninclude stochastic elementssuch
asrandomly appearing dirtandanunreliable suction mechanism Exercise 2.13. Wesayan
environment is uncertain if it is not fully observable or not deterministic. One final note:
U NC ER TA IN
our use of the word stochastic generally implies that uncertainty about outcomes is quan-
tified in terms of probabilities; a nondeterministic environment is one in which actions are
N ON DE TE RM IN IS TI C
characterized by their possible outcomes but no probabilities are attached to them. Nonde-
terministic environment descriptions are usually associated with performance measures that
requiretheagenttosucceed forallpossible outcomesofitsactions.
Episodic vs. sequential: In an episodic task environment the agents experience is
E PI SO DI C
dividedintoatomicepisodes. Ineachepisodetheagentreceivesaperceptandthenperforms
S EQ UE NT IA L
asingle action. Crucially the next episode does not depend on theactions taken inprevious
episodes. Many classification tasks are episodic. For example an agent that has to spot
defective parts on an assembly line bases each decision on the current part regardless of
previous decisions; moreover the current decision doesnt affect whether the next part is
defective. In sequential environments on the other hand the current decision could affect
allfuture decisions.3 Chessand taxidriving aresequential: inboth cases short-term actions
can have long-term consequences. Episodic environments are much simpler than sequential
environments becausetheagentdoesnotneedtothinkahead.
Staticvs.dynamic: Iftheenvironment canchangewhileanagentisdeliberating then
S TA TI C
wesaytheenvironment isdynamic forthatagent;otherwise itisstatic. Staticenvironments
D YN AM IC
areeasytodealwithbecausetheagentneednotkeeplookingattheworldwhileitisdeciding
on an action nor need it worry about the passage of time. Dynamic environments on the
other hand are continuously asking the agent what it wants to do; if it hasnt decided yet
that counts as deciding to do nothing. If the environment itself does not change with the
passage of time but the agents performance score does then we say the environment is
semidynamic. Taxidrivingisclearlydynamic: theothercarsandthetaxi itselfkeepmoving
S EM ID YN AM IC
whilethe driving algorithm dithers about whattodonext. Chess whenplayed withaclock
issemidynamic. Crosswordpuzzlesarestatic.
Discretevs.continuous: Thediscretecontinuous distinction applies tothestateofthe
D IS CR ET E
environment to the way time is handled and to the percepts and actions of the agent. For
C ON TI NU OU S
example the chess environment has a finite number of distinct states excluding the clock.
Chess also has a discrete set of percepts and actions. Taxi driving is a continuous-state and
continuous-time problem: the speed and location ofthe taxi and ofthe other vehicles sweep
through arangeofcontinuous values anddososmoothlyovertime. Taxi-driving actions are
also continuous steering angles etc.. Input from digital cameras is discrete strictly speak-
ingbutistypically treatedasrepresenting continuously varyingintensities andlocations.
Knownvs. unknown: Strictly speaking this distinction refers not to the environment
K NO WN
itself but to the agents or designers state of knowledge about the laws of physics of
U NK NO WN
the environment. In a known environment the outcomes or outcome probabilities if the
environmentisstochasticforallactionsaregiven. Obviouslyiftheenvironmentisunknown
the agent will have to learn how it works in order to make good decisions. Note that the
distinction between known and unknown environments is not the same as the one between
fully and partially observable environments. It is quite possible for a known environment
to be partially observablefor example in solitaire card games I know the rules but am
still unable to see the cards that have not yet been turned over. Conversely an unknown
environment can be fully observablein a new video game the screen may show the entire
gamestatebut Istilldontknowwhatthebuttonsdountil Itrythem.
As one might expect the hardest case is partially observable multiagent stochastic
sequentialdynamiccontinuousandunknown. Taxidrivingishardinallthesesensesexcept
thatforthemostpartthedriversenvironmentisknown. Drivingarentedcarinanewcountry
withunfamiliargeography andtrafficlawsisalotmoreexciting.
answers are not always cut and dried. For example we describe the part-picking robot as
episodic because itnormally considers each part in isolation. Butif oneday there isalarge
largelyunrelated.
Section2.3. The Natureof Environments 45
Task Environment Observable Agents Deterministic Episodic Static Discrete
Crosswordpuzzle Fully Single Deterministic Sequential Static Discrete
Chesswithaclock Fully Multi Deterministic Sequential Semi Discrete
Poker Partially Multi Stochastic Sequential Static Discrete
Backgammon Fully Multi Stochastic Sequential Static Discrete
Taxidriving Partially Multi Stochastic Sequential Dynamic Continuous
Medicaldiagnosis Partially Single Stochastic Sequential Dynamic Continuous
Imageanalysis Fully Single Deterministic Episodic Semi Continuous
Part-pickingrobot Partially Single Stochastic Episodic Dynamic Continuous
Refinerycontroller Partially Single Stochastic Sequential Dynamic Continuous
Interactive Englishtutor Partially Multi Stochastic Sequential Dynamic Discrete
Figure2.6 Examplesoftaskenvironmentsandtheircharacteristics.
batchofdefectivepartstherobotshouldlearnfromseveralobservations thatthedistribution
of defects has changed and should modify its behavior for subsequent parts. We have not
includedaknownunknowncolumnbecauseasexplainedearlierthisisnotstrictlyaprop-
erty of the environment. Forsome environments such aschess and poker it isquite easy to
supplytheagentwithfullknowledgeoftherulesbutitisnonetheless interesting toconsider
howanagentmightlearntoplaythesegameswithoutsuchknowledge.
Severaloftheanswersinthetabledepend onhowthetaskenvironment isdefined. We
havelistedthemedical-diagnosis taskassingle-agentbecausethediseaseprocessinapatient
is not profitably modeled as an agent; but a medical-diagnosis system might also have to
dealwithrecalcitrantpatientsandskepticalstaffsotheenvironmentcouldhaveamultiagent
aspect. Furthermore medicaldiagnosis isepisodic ifoneconceives ofthetaskasselecting a
diagnosisgivenalistofsymptoms;theproblemissequentialifthetaskcanincludeproposing
a series of tests evaluating progress over the course of treatment and so on. Also many
environments are episodic at higher levels than the agents individual actions. For example
a chess tournament consists of a sequence of games; each game is an episode because by
and large the contribution of the moves in one game to the agents overall performance is
notaffected bythemovesinitsprevious game. Ontheotherhand decision makingwithina
singlegameiscertainly sequential.
The code repository associated with this book aima.cs.berkeley.edu includes imple-
mentationsofanumberofenvironments togetherwithageneral-purpose environmentsimu-
latorthatplacesoneormoreagentsinasimulatedenvironment observestheirbehaviorover
time and evaluates them according to a given performance measure. Such experiments are
oftencarried outnotforasingle environment butformanyenvironments drawnfroman en-
E NV IR ON ME NT vironmentclass. Forexampletoevaluateataxidriverinsimulatedtraffic wewouldwantto
C LA SS
runmanysimulations withdifferent traffic lighting and weatherconditions. Ifwedesigned
the agent for a single scenario we might be able to take advantage of specific properties
of the particular case but might not identify a good design for driving in general. For this
E NV IR ON ME NT reason the code repository also includes an environment generator for each environment
G EN ER AT OR
classthatselectsparticularenvironments withcertainlikelihoods inwhichtoruntheagent.
Forexamplethevacuumenvironmentgeneratorinitializes thedirtpatternandagentlocation
randomly. We are then interested in the agents average performance over the environment
class. A rational agent for a given environment class maximizes this average performance.
Exercises 2.8 to 2.13 take you through the process of developing an environment class and
evaluating variousagentstherein.
Sofarwehavetalkedaboutagentsbydescribing behaviortheactionthatisperformedafter
any given sequence ofpercepts. Nowwemust bite the bullet and talk about how theinsides
work. The job of A I is to design an agent program that implements the agent function
A GE NT PR OG RA M
the mapping from percepts to actions. We assume this program will run on some sort of
computing devicewithphysicalsensorsandactuatorswe callthisthearchitecture:
A RC HI TE CT UR E
agent architectureprogram .
Obviouslytheprogramwechoosehastobeonethatisappropriateforthearchitecture. Ifthe
program isgoing torecommend actions like Walkthearchitecture hadbetterhave legs. The
architecture might be just an ordinary P C or it might be a robotic car with several onboard
computers cameras and other sensors. In general the architecture makes the percepts from
thesensorsavailabletotheprogramrunstheprogramandfeedstheprogramsactionchoices
to the actuators asthey are generated. Most ofthis book is about designing agent programs
although Chapters24and25dealdirectly withthesensorsandactuators.
The agent programs that we design in this book all have the same skeleton: they take the
current percept as input from the sensors and return an action to the actuators.4 Notice the
differencebetweentheagentprogramwhichtakesthecurrentperceptasinputandtheagent
function which takes the entire percept history. The agent program takes just the current
perceptasinputbecausenothingmoreisavailablefromtheenvironment;iftheagentsactions
needtodependontheentireperceptsequence theagentwillhavetorememberthepercepts.
We describe the agent programs in the simple pseudocode language that is defined in
Appendix B. The online code repository contains implementations in real programming
languages. Forexample Figure 2.7shows arathertrivial agent program thatkeeps track of
the percept sequence and then uses it to index into a table of actions to decide what to do.
The tablean example of which is given for the vacuum world in Figure 2.3represents
explicitly the agent function that the agent program embodies. To build a rational agent in
coroutinesthatrunasynchronouslywiththeenvironment. Eachsuchcoroutinehasaninputandoutputportand
consistsofaloopthatreadstheinputportforperceptsandwritesactionstotheoutputport.
Section2.4. The Structureof Agents 47
function T AB LE-D RI VE N-A GE NTperceptreturnsanaction
persistent: perceptsasequenceinitiallyempty
tableatableofactionsindexedbyperceptsequencesinitiallyfullyspecified
appendpercept totheendofpercepts
action L OO KU Pperceptstable
returnaction
returnsanactioneachtime. Itretainsthecompleteperceptsequenceinmemory.
thiswayweasdesignersmustconstructatablethatcontainstheappropriateactionforevery
possible perceptsequence.
It is instructive to consider why the table-driven approach to agent construction is
doomed to failure. Let P be the set of possible percepts and let T be the lifetcid:2ime of the
agentthetotalnumberofperceptsitwillreceive. Thelookuptablewillcontain T Pt
t1
entries. Consider the automated taxi: the visual input from a single camera comes in at the
rate of roughly 27 megabytes per second 30 frames per second 640480 pixels with 24
bitsofcolorinformation. Thisgivesalookup table withover10250000000000 entries foran
hours driving. Even the lookup table for chessa tiny well-behaved fragment of the real
worldwould have at least 10150 entries. The daunting size of these tables the number of
atoms in the observable universe is less than 1080 means that a no physical agent in this
universe willhavethespacetostorethetable bthedesignerwouldnothavetimetocreate
the table cno agent could everlearn allthe right table entries from itsexperience and d
even if the environment is simple enough to yield a feasible table size the designer still has
noguidance abouthowtofillinthetableentries.
Despite all this T AB LE-D RI VE N-A GE NT does do what we want: it implements the
desired agent function. The key challenge for A I is to find out how to write programs that
to the extent possible produce rational behavior from a smallish program rather than from
a vast table. We have many examples showing that this can be done successfully in other
areas: forexamplethehugetablesofsquarerootsusedbyengineersandschoolchildrenprior
to the 1970s have now been replaced by a five-line program for Newtons method running
on electronic calculators. The question is can A I do for general intelligent behavior what
Newtondidforsquareroots? Webelievetheanswerisyes.
In the remainder of this section we outline four basic kinds of agent programs that
embodytheprinciples underlying almostallintelligent systems:
Simplereflexagents;
Model-based reflexagents;
Goal-basedagents; and
Utility-based agents.
Each kind of agent program combines particular components in particular ways to generate
actions. Section2.4.6explains ingeneral termshowtoconvert alltheseagents into learning
function R EF LE X-V AC UU M-A GE NTlocationstatusreturnsanaction
ifstatus Dirty thenreturn Suck
elseiflocation Athenreturn Right
elseiflocation B thenreturn Left
Figure2.8 Theagentprogramforasimplereflexagentinthetwo-statevacuumenviron-
ment.Thisprogramimplementstheagentfunctiontabulatedin Figure2.3.
agentsthatcanimprovetheperformanceoftheircomponentssoasto generatebetteractions.
Finally Section2.4.7describes thevariety ofwaysinwhichthecomponents themselves can
be represented within the agent. This variety provides a major organizing principle for the
fieldandforthebookitself.
S IM PL ER EF LE X Thesimplestkindofagentisthesimplereflexagent. Theseagentsselectactionsonthebasis
A GE NT
ofthecurrentperceptignoringtherestofthepercepthistory. Forexamplethevacuumagent
whose agent function is tabulated in Figure 2.3is asimple reflexagent because its decision
is based only on the current location and on whether that location contains dirt. An agent
program forthisagentisshownin Figure2.8.
Noticethatthevacuumagentprogramisverysmallindeedcomparedtothecorrespond-
ing table. The most obvious reduction comes from ignoring the percept history which cuts
down the number of possibilities from 4 T to just 4. A further small reduction comes from
thefactthatwhenthecurrent squareisdirtytheactiondoesnotdepend onthelocation.
Simple reflex behaviors occur even in more complex environments. Imagine yourself
asthedriveroftheautomatedtaxi. Ifthecarinfrontbrakes anditsbrakelightscomeonthen
you should notice this and initiate braking. In other words some processing is done on the
visualinputtoestablishtheconditionwecall Thecarinfrontisbraking. Thenthistriggers
some established connection in the agent program to the action initiate braking. We call
C ON DI TI ON AC TI ON suchaconnection aconditionaction rule5 writtenas
R UL E
ifcar-in-front-is-braking theninitiate-braking.
Humansalsohavemanysuchconnections someofwhicharelearnedresponses asfordriv-
ingandsomeofwhichareinnatereflexessuchasblinking whensomething approaches the
eye. In the course of the book we show several different ways in which such connections
canbelearned andimplemented.
The program in Figure 2.8 is specific to one particular vacuum environment. A more
general and flexible approach is first to build a general-purpose interpreter for condition
action rules and then to create rule sets for specific task environments. Figure 2.9 gives the
structure ofthisgeneral programinschematicformshowinghowtheconditionaction rules
allow the agent to make the connection from percept to action. Do not worry if this seems
Section2.4. The Structureof Agents 49
Agent
Environment
Sensors
What the world
is like now
What action I
Condition-action rules
should do now
Actuators
Figure2.9 Schematicdiagramofasimplereflexagent.
function S IM PL E-R EF LE X-A GE NTperceptreturnsanaction
persistent: rulesasetofconditionactionrules
state I NT ER PR ET-I NP UTpercept
rule R UL E-M AT CHstaterules
actionrule.A CT IO N
returnaction
thecurrentstateasdefinedbythepercept.
trivial;itgetsmoreinteresting shortly. Weuserectangles todenotethecurrentinternalstate
of the agents decision process and ovals to represent the background information used in
the process. The agent program which is also very simple is shown in Figure 2.10. The
I NT ER PR ET-I NP UT function generates anabstracted description ofthecurrentstatefromthe
percept and the R UL E-M AT CH function returns the firstrule inthe setof rules that matches
the given state description. Note that the description in terms of rules and matching is
purely conceptual; actual implementations can be as simple as a collection of logic gates
implementing a Booleancircuit.
Simplereflexagentshavetheadmirablepropertyofbeingsimplebuttheyturnouttobe
oflimitedintelligence. Theagentin Figure2.10willworkonlyifthecorrectdecisioncanbe
madeonthebasisofonlythecurrentperceptthatisonlyiftheenvironmentisfullyobserv-
able. Evenalittle bitofunobservability cancause serious trouble. Forexample thebraking
rule given earlier assumes that the condition car-in-front-is-braking can be determined from
the current percepta single frame of video. This works if the car in front has a centrally
mounted brake light. Unfortunately older models have different configurations of taillights
brake lights and turn-signal lights and it is not always possible to tell from a single image
whetherthecarisbraking. Asimplereflexagentdrivingbehindsuchacarwouldeitherbrake
continuously andunnecessarily orworseneverbrakeatall.
Wecanseeasimilarproblemarisinginthevacuumworld. Supposethatasimplereflex
vacuum agent is deprived of its location sensor and has only a dirt sensor. Such an agent
hasjusttwopossiblepercepts: Dirtyand Clean. Itcan Suck inresponseto Dirty;what
shoulditdoinresponseto Clean? Moving Left failsforeverifithappenstostartinsquare
Aandmoving Right failsforeverifithappenstostartinsquare B. Infiniteloopsareoften
unavoidable forsimplereflexagentsoperating inpartially observable environments.
Escape from infinite loops is possible if the agent can randomize its actions. For ex-
R AN DO MI ZA TI ON
ampleifthevacuumagentperceives Cleanitmightflipacointochoosebetween Left and
Right. Itiseasytoshowthattheagentwillreachtheothersquareinanaverageoftwosteps.
Then if that square is dirty the agent will clean it and the task will be complete. Hence a
randomized simplereflexagentmightoutperform adeterministic simplereflexagent.
Wementionedin Section2.3thatrandomizedbehavioroftherightkindcanberational
insomemultiagentenvironments. Insingle-agentenvironmentsrandomizationisusuallynot
rational. It is a useful trick that helps a simple reflex agent in some situations but in most
caseswecandomuchbetterwithmoresophisticated deterministic agents.
The most effective way to handle partial observability is for the agent to keep track of the
part of the world it cant see now. That is the agent should maintain some sort of internal
statethatdepends onthepercepthistoryandtherebyreflectsatleastsomeoftheunobserved
I NT ER NA LS TA TE
aspects ofthecurrentstate. Forthebraking problem theinternal stateisnottooextensive
just the previous frame from the camera allowing the agent to detect when twored lights at
theedgeofthevehicle goonoroffsimultaneously. Forother driving taskssuchaschanging
lanestheagentneedstokeeptrackofwheretheothercarsareifitcantseethemallatonce.
Andforanydrivingtobepossibleatalltheagentneedstokeeptrackofwhereitskeysare.
Updating this internal state information as time goes by requires two kinds of knowl-
edge to be encoded in the agent program. First we need some information about how the
worldevolvesindependently oftheagentforexamplethatanovertakingcargenerallywill
be closer behind than it was a moment ago. Second we need some information about how
theagentsownactionsaffecttheworldforexamplethatwhentheagentturnsthesteering
wheel clockwise the car turns to the right or that after driving for five minutes northbound
onthefreewayoneisusuallyaboutfivemilesnorthofwhereonewasfiveminutesago. This
knowledge about how the world workswhether implemented in simple Boolean circuits
orincomplete scientific theoriesis called a modelofthe world. Anagent that uses such a
M OD EL-B AS ED modeliscalledamodel-basedagent.
A GE NT
Figure2.11givesthestructureofthemodel-basedreflexagentwithinternalstateshow-
ing how the current percept is combined with the old internal state to generate the updated
descriptionofthecurrentstatebasedontheagentsmodelofhowtheworldworks. Theagent
programisshownin Figure2.12. Theinterestingpartisthefunction U PD AT E-S TA TEwhich
Section2.4. The Structureof Agents 51
Agent
Environment
Sensors
State
How the world evolves What the world
is like now
What my actions do
What action I
Condition-action rules
should do now
Actuators
Figure2.11 Amodel-basedreflexagent.
function M OD EL-B AS ED-R EF LE X-A GE NTperceptreturnsanaction
persistent: statetheagentscurrentconceptionoftheworldstate
modeladescriptionofhowthenextstatedependsoncurrentstateandaction
rulesasetofconditionactionrules
actionthemostrecentactioninitiallynone
state U PD AT E-S TA TEstateactionperceptmodel
rule R UL E-M AT CHstaterules
actionrule.A CT IO N
returnaction
Figure2.12 Amodel-basedreflexagent. Itkeepstrackofthecurrentstateoftheworld
usinganinternalmodel.Itthenchoosesanactioninthesamewayasthereflexagent.
is responsible forcreating the new internal state description. Thedetails of how models and
states are represented vary widely depending on the type of environment and the particular
technology used in the agent design. Detailed examples of models and updating algorithms
appearin Chapters412111517and25.
Regardless of the kind of representation used it is seldom possible for the agent to
determine the current state of a partially observable environment exactly. Instead the box
labeled what the world is like now Figure 2.11 represents the agents best guess or
sometimes best guesses. Forexample an automated taxi may not beable tosee around the
large truck that has stopped in front of it and can only guess about what may be causing the
hold-up. Thusuncertainty aboutthecurrent statemaybeunavoidable buttheagentstillhas
tomakeadecision.
A perhaps less obvious point about the internal state maintained by a model-based
agent is that it does not have to describe what the world is like now in a literal sense. For
Agent
Environment
Sensors
State
What the world
How the world evolves is like now
What it will be like
What my actions do if I do action A
What action I
Goals should do now
Actuators
Figure2.13 Amodel-basedgoal-basedagent.Itkeepstrackoftheworldstateaswellas
asetofgoalsitistryingtoachieveandchoosesanactionthatwilleventuallyleadtothe
achievementofitsgoals.
example the taxi may be driving back home and it may have a rule telling it to fill up with
gas on the way home unless it has at least half a tank. Although driving back home may
seem toan aspect of theworld state the fact ofthe taxis destination isactually an aspect of
the agents internal state. Ifyou findthis puzzling consider that the taxi could be in exactly
thesameplaceatthesametimebutintending toreachadifferent destination.
Knowingsomethingaboutthecurrentstateoftheenvironmentisnotalwaysenoughtodecide
what to do. For example at a road junction the taxi can turn left turn right or go straight
on. Thecorrectdecisiondependsonwherethetaxiistryingtogetto. Inotherwordsaswell
as a current state description the agent needs some sort of goal information that describes
G OA L
situations that are desirablefor example being at the passengers destination. The agent
program can combine this with the model the same information as was used in the model-
basedreflexagenttochooseactionsthatachievethegoal. Figure2.13showsthegoal-based
agentsstructure.
Sometimesgoal-basedactionselectionisstraightforwardforexamplewhengoalsat-
isfaction results immediately from a single action. Sometimes it will be more trickyfor
examplewhentheagenthastoconsiderlongsequences oftwistsandturnsinordertofinda
waytoachievethegoal. Search Chapters3to5andplanning Chapters10and11arethe
subfieldsof A Idevoted tofindingactionsequences thatachievetheagentsgoals.
Noticethatdecisionmakingofthiskindisfundamentally differentfromthecondition
actionrulesdescribed earlierinthatitinvolvesconsideration ofthefutureboth Whatwill
happen if Idosuch-and-such? and Willthatmakemehappy? Inthereflexagentdesigns
this information is not explicitly represented because the built-in rules map directly from
Section2.4. The Structureof Agents 53
percepts toactions. Thereflexagentbrakeswhenitseesbrakelights. Agoal-based agent in
principle couldreasonthatifthecarinfronthasitsbrake lightsonitwillslowdown. Given
the way the world usually evolves the only action that will achieve the goal of not hitting
othercarsistobrake.
Although the goal-based agent appears less efficient it is more flexible because the
knowledgethatsupportsitsdecisionsisrepresentedexplicitlyandcanbemodified. Ifitstarts
toraintheagentcanupdateitsknowledgeofhoweffectivelyitsbrakeswilloperate;thiswill
automatically cause all of the relevant behaviors to be altered to suit the new conditions.
For the reflex agent on the other hand we would have to rewrite many conditionaction
rules. Thegoal-based agentsbehaviorcaneasilybechanged togotoadifferentdestination
simply by specifying that destination as the goal. The reflex agents rules for when to turn
and when to go straight will work only fora single destination; they must all be replaced to
gosomewherenew.
Goals alone are not enough to generate high-quality behavior in most environments. For
example many action sequences will get the taxi to its destination thereby achieving the
goal but somearequicker safer morereliable orcheaper thanothers. Goalsjust provide a
crudebinarydistinctionbetweenhappyandunhappystates. Amoregeneralperformance
measureshouldallowacomparisonofdifferentworldstates accordingtoexactlyhowhappy
theywouldmaketheagent. Becausehappy doesnotsoundveryscientific economists and
computerscientists usetheterm utilityinstead.6
U TI LI TY
Wehavealreadyseenthataperformancemeasureassignsascoretoanygivensequence
of environment states so it can easily distinguish between more and less desirable ways of
getting to the taxis destination. An agents utility function is essentially an internalization
U TI LI TY FU NC TI ON
of the performance measure. If the internal utility function and the external performance
measure are in agreement then an agent that chooses actions to maximize its utility will be
rationalaccording totheexternalperformance measure.
Let us emphasize again that this is not the only way to be rationalwe have already
seen a rational agent program for the vacuum world Figure 2.8 that has no idea what its
utility function isbut likegoal-based agents autility-based agent hasmanyadvantages in
termsofflexibilityandlearning. Furthermoreintwokinds ofcasesgoalsareinadequatebut
autility-based agentcanstillmakerationaldecisions. Firstwhenthereareconflictinggoals
only some of which can be achieved for example speed and safety the utility function
specifies the appropriate tradeoff. Second when there are several goals that the agent can
aim for none of which can be achieved with certainty utility provides a way in which the
likelihood ofsuccesscanbeweighedagainsttheimportance ofthegoals.
Partialobservabilityandstochasticityareubiquitousintherealworldandsotherefore
is decision making under uncertainty. Technically speaking a rational utility-based agent
chooses the action that maximizes the expected utility of the action outcomesthat is the
E XP EC TE DU TI LI TY
utility the agent expects to derive on average given the probabilities and utilities of each
Agent
Environment
Sensors
State
What the world
How the world evolves is like now
What it will be like
What my actions do if I do action A
How happy I will be
Utility
in such a state
What action I
should do now
Actuators
Figure2.14 Amodel-basedutility-basedagent. Itusesamodeloftheworldalongwith
autilityfunctionthatmeasuresitspreferencesamongstatesoftheworld.Thenitchoosesthe
actionthatleadstothebestexpectedutilitywhereexpectedutilityiscomputedbyaveraging
overallpossibleoutcomestatesweightedbytheprobabilityoftheoutcome.
outcome. Appendix Adefinesexpectation moreprecisely. In Chapter16weshowthatany
rational agent must behave as if it possesses a utility function whose expected value it tries
tomaximize. Anagentthatpossesses anexplicit utility function canmakerational decisions
with a general-purpose algorithm that does not depend on the specific utility function being
maximized. In this way the global definition of rationalitydesignating as rational those
agent functions that have the highest performanceis turned into a local constraint on
rational-agent designsthatcanbeexpressedinasimpleprogram.
Theutility-based agent structure appears in Figure 2.14. Utility-based agent programs
appearin Part I Vwherewedesign decision-making agents thatmusthandle theuncertainty
inherent instochastic orpartially observable environments.
Atthispointthereadermaybewondering Isitthatsimple? Wejustbuildagentsthat
maximize expected utility and were done? Its true that such agents would be intelligent
but its not simple. A utility-based agent has to model and keep track of its environment
tasks that have involved a great deal of research on perception representation reasoning
and learning. The results of this research fill many of the chapters of this book. Choosing
theutility-maximizing courseofactionisalsoadifficulttaskrequiringingeniousalgorithms
that fill several more chapters. Even with these algorithms perfect rationality is usually
unachievable inpractice becauseofcomputational complexity aswenotedin Chapter1.
We have described agent programs with various methods for selecting actions. We have
not so far explained how the agent programs come into being. In his famous early paper
Turing 1950 considers the idea of actually programming his intelligent machines by hand.
Section2.4. The Structureof Agents 55
Performance standard
Agent
Environment
Critic Sensors
feedback
changes
Learning Performance
element element
knowledge
learning
goals
Problem
generator
Actuators
Figure2.15 Agenerallearningagent.
Heestimateshowmuchworkthismighttakeandconcludes Somemoreexpeditiousmethod
seems desirable. The method he proposes is to build learning machines and then to teach
them. In many areas of A I this is now the preferred method for creating state-of-the-art
systems. Learning has another advantage as wenoted earlier: it allows the agent to operate
ininitially unknown environments andtobecomemorecompetent thanitsinitial knowledge
alone might allow. In this section we briefly introduce the main ideas of learning agents.
Throughout the book we comment on opportunities and methods for learning in particular
kindsofagents. Part Vgoesintomuchmoredepthonthelearningalgorithms themselves.
A learning agent can be divided into four conceptual components as shown in Fig-
ure 2.15. The most important distinction is between the learning element which is re-
L EA RN IN GE LE ME NT
P ER FO RM AN CE sponsibleformakingimprovementsandtheperformanceelementwhichisresponsiblefor
E LE ME NT
selecting external actions. The performance element is what we have previously considered
tobetheentire agent: ittakes inpercepts and decides onactions. Thelearning element uses
feedback from the critic on how the agent is doing and determines how the performance
C RI TI C
elementshouldbemodifiedtodobetterinthefuture.
Thedesignofthelearningelementdependsverymuchonthedesignoftheperformance
element. When trying to design an agent that learns a certain capability the first question is
not Howam Igoingtogetittolearnthis? but Whatkindofperformanceelementwillmy
agentneedtodothisonceithaslearned how? Givenanagentdesign learningmechanisms
canbeconstructed toimproveeverypartoftheagent.
Thecritic tellsthe learning element how welltheagent isdoing withrespect toafixed
performance standard. The critic is necessary because the percepts themselves provide no
indication of the agents success. For example a chess program could receive a percept
indicating that it has checkmated its opponent but it needs a performance standard to know
thatthisisagoodthing;theperceptitselfdoesnotsayso. Itisimportantthattheperformance
standard be fixed. Conceptually one should think of it as being outside the agent altogether
becausetheagentmustnotmodifyittofititsownbehavior.
P RO BL EM The last component of the learning agent is the problem generator. It is responsible
G EN ER AT OR
for suggesting actions that will lead to new and informative experiences. The point is that
if the performance element had its way it would keep doing the actions that are best given
whatitknows. Butiftheagent iswillingtoexplore alittle anddosomeperhaps suboptimal
actions inthe short run itmight discover muchbetteractions forthelong run. Theproblem
generators job is to suggest these exploratory actions. This is what scientists do when they
carry out experiments. Galileo did not think that dropping rocks from the top of a tower in
Pisa was valuable in itself. He was not trying to break the rocks or to modify the brains of
unfortunate passers-by. His aim was to modify his own brain by identifying a better theory
ofthemotionofobjects.
Tomake theoverall design moreconcrete letusreturn tothe automated taxi example.
The performance element consists of whatever collection of knowledge and procedures the
taxi has for selecting its driving actions. The taxi goes out on the road and drives using
thisperformance element. Thecritic observes theworldand passes information along tothe
learningelement. Forexampleafterthetaximakesaquickleftturnacrossthreelanesoftraf-
ficthecriticobservestheshockinglanguageusedbyotherdrivers. Fromthisexperience the
learningelementisabletoformulatearulesayingthiswasabadactionandtheperformance
element is modified by installation of the new rule. The problem generator might identify
certainareasofbehaviorinneedofimprovementandsuggest experimentssuchastryingout
thebrakesondifferentroadsurfaces underdifferentconditions.
Thelearning elementcanmakechanges toanyoftheknowledge components shown
intheagentdiagrams Figures2.92.112.13and2.14. Thesimplestcasesinvolvelearning
directly from the percept sequence. Observation of pairs of successive states of the environ-
mentcanallow theagent tolearn Howtheworldevolves and observation oftheresults of
itsactions canallowtheagenttolearn Whatmyactions do. Forexample ifthetaxiexerts
a certain braking pressure when driving on a wet road then it will soon find out how much
deceleration is actually achieved. Clearly these two learning tasks are more difficult if the
environment isonlypartially observable.
The forms of learning in the preceding paragraph do not need to access the external
performance standardin a sense the standard is the universal one of making predictions
that agree with experiment. The situation is slightly more complex for a utility-based agent
thatwishestolearnutilityinformation. Forexamplesuppose thetaxi-driving agentreceives
no tips from passengers who have been thoroughly shaken up during the trip. The external
performance standard mustinform theagentthatthelossoftipsisanegativecontribution to
its overall performance; then the agent might be able to learn that violent maneuvers do not
contribute to its own utility. In a sense the performance standard distinguishes part of the
incomingperceptasarewardorpenaltythatprovidesdirectfeedbackonthequalityofthe
agents behavior. Hard-wired performance standards such aspainandhungerinanimals can
beunderstood inthisway. Thisissueisdiscussed furtherin Chapter21.
Insummary agents haveavarietyofcomponents andthose components canberepre-
sented in many ways within the agent program so there appears to be great variety among
Section2.4. The Structureof Agents 57
learning methods. Thereis however asingle unifying theme. Learning inintelligent agents
canbesummarized asaprocess ofmodification ofeachcomponent oftheagenttobring the
components into closer agreement withthe available feedback information thereby improv-
ingtheoverallperformance oftheagent.
Wehavedescribedagentprogramsinveryhigh-levelterms asconsistingofvariouscompo-
nentswhosefunctionitistoanswerquestionssuchas: Whatistheworldlikenow? What
action should I do now? What do my actions do? The next question for a student of A I
is How on earth do these components work? It takes about a thousand pages to begin to
answerthat question properly but here wewantto draw thereaders attention to somebasic
distinctions among thevarious waysthatthecomponents can represent theenvironment that
theagentinhabits.
Roughly speaking we can place the representations along an axis of increasing com-
plexity and expressive poweratomic factored and structured. To illustrate these ideas
it helps to consider a particular agent component such as the one that deals with What my
actions do. This component describes the changes that might occur in the environment as
the result of taking an action and Figure 2.16 provides schematic depictions of how those
transitions mightberepresented.
B C
B C
a Atomic b Factored b Structured
Figure2.16 Threewaystorepresentstatesandthetransitionsbetweenthem. a Atomic
representation:astatesuchas Bor Cisablackboxwithnointernalstructure;b Factored
representation: a state consists of a vectorof attribute values; valuescan be Boolean real-
valued or one of a fixed set of symbols. c Structured representation: a state includes
objectseachofwhichmayhaveattributesofitsownaswellasrelationshipstootherobjects.
A TO MI C In an atomic representation each state of the world is indivisibleit has no internal
R EP RE SE NT AT IO N
structure. Consider the problem of finding a driving route from one end of a country to the
otherviasomesequenceofcitiesweaddressthisproblemin Figure3.2onpage68. Forthe
purposes ofsolving this problem itmaysufficetoreduce the stateofworldtojustthename
of the city we are ina single atom of knowledge; a black box whose only discernible
property is that of being identical to or different from another black box. The algorithms
underlyingsearchandgame-playing Chapters35 Hidden Markovmodels Chapter15
and Markov decision processes Chapter 17 all work with atomic representationsor at
leasttheytreatrepresentations asiftheywereatomic.
Nowconsider a higher-fidelity description for the same problem where weneed to be
concerned with more than just atomic location in one city oranother; wemight need to pay
attention tohowmuchgasisinthetank ourcurrent G PScoordinates whetherornottheoil
warning light isworking howmuch spare change wehave fortoll crossings whatstation is
F AC TO RE D on the radio and so on. A factored representation splits up each state into a fixed set of
R EP RE SE NT AT IO N
variables or attributes each of which can have a value. While two different atomic states
V AR IA BL E
have nothing in commonthey are just different black boxestwo different factored states
A TT RI BU TE
cansharesomeattributessuchasbeingatsomeparticular G PSlocationandnototherssuch
V AL UE
as having lots of gas or having no gas; this makes it much easier to work out how to turn
onestateintoanother. Withfactored representations wecanalsorepresent uncertaintyfor
example ignorance about the amount of gas in the tank can be represented by leaving that
attribute blank. Manyimportant areasof A Iarebased onfactored representations including
constraint satisfaction algorithms Chapter 6 propositional logic Chapter 7 planning
Chapters 10 and 11 Bayesian networks Chapters 1316 and the machine learning al-
gorithmsin Chapters1820and21.
For many purposes we need to understand the world as having things in it that are
related to each other not just variables with values. For example we might notice that a
largetruck aheadofusisreversing intothedrivewayofadairy farmbutacowhasgotloose
and is blocking the trucks path. A factored representation is unlikely to be pre-equipped
withtheattribute Truck Ahead Backing Into Dairy Farm Driveway Blocked By Loose Cow with
S TR UC TU RE D value true or false. Instead we would need a structured representation in which ob-
R EP RE SE NT AT IO N
jects such as cows and trucks and their various and varying relationships can be described
explicitly. See Figure 2.16c. Structured representations underlie relational databases
and first-order logic Chapters 8 9 and 12 first-order probability models Chapter 14
knowledge-based learning Chapter 19 and much of natural language understanding
Chapters 22 and 23. In fact almost everything that humans express in natural language
concerns objectsandtheirrelationships.
As we mentioned earlier the axis along which atomic factored and structured repre-
sentations lieistheaxis ofincreasing expressiveness. Roughly speaking amoreexpressive
E XP RE SS IV EN ES S
representation cancaptureatleastasconcisely everythingalessexpressiveonecancapture
plussomemore. Oftenthemoreexpressivelanguageismuchmoreconcise;forexamplethe
rules of chess can be written in a page or two of a structured-representation language such
as first-order logic but require thousands of pages when written in a factored-representation
language such as propositional logic. On the other hand reasoning and learning become
more complex as the expressive power of the representation increases. To gain the benefits
ofexpressive representations whileavoiding theirdrawbacks intelligent systems forthereal
worldmayneedtooperateatallpointsalongtheaxissimultaneously.
Section2.5. Summary 59
This chapter has been something of a whirlwind tour of A I which we have conceived of as
thescienceofagentdesign. Themajorpointstorecallareas follows:
Anagent issomething that perceives and acts in anenvironment. The agent function
foranagentspecifiestheactiontakenbytheagentinresponsetoanyperceptsequence.
The performance measure evaluates the behavior of the agent in an environment. A
rational agentactssoastomaximize theexpected valueoftheperformance measure
giventhepercept sequence ithasseensofar.
A task environment specification includes the performance measure the external en-
vironment the actuators and the sensors. In designing an agent the first step must
alwaysbetospecifythetaskenvironment asfullyaspossible.
Task environments vary along several significant dimensions. They can be fully or
partiallyobservable single-agent ormultiagent deterministic orstochastic episodicor
sequential staticordynamicdiscrete orcontinuous and knownorunknown.
The agent program implements the agent function. There exists a variety of basic
agent-program designs reflecting thekindofinformation madeexplicit andusedinthe
decision process. The designs vary in efficiency compactness and flexibility. The
appropriate designoftheagentprogram depends onthenatureoftheenvironment.
Simplereflexagentsresponddirectlytopercepts whereas model-basedreflexagents
maintain internal state to track aspects of the world that are not evident in the current
percept. Goal-based agents act to achieve their goals and utility-based agents try to
maximizetheirownexpected happiness.
Allagentscanimprovetheirperformance through learning.
B IB LI OG RA PH IC AL A ND H IS TO RI CA L N OT ES
The central role of action in intelligencethe notion of practical reasoninggoes back at
least as far as Aristotles Nicomachean Ethics. Practical reasoning was also the subject of
Mc Carthys1958influential paper Programswith Common Sense. Thefieldsofrobotics
andcontrol theory are bytheirverynature concerned principally withphysical agents. The
concept of a controller in control theory is identical to that of an agent in A I. Perhaps sur-
C ON TR OL LE R
prisingly A I has concentrated for most of its history on isolated components of agents
question-answering systems theorem-provers vision systems and so onrather than on
wholeagents. Thediscussion ofagents inthetextby Genesereth and Nilsson1987 wasan
influentialexception. Thewhole-agentviewisnowwidelyacceptedandisacentralthemein
recenttexts Pooleetal.1998;Nilsson1998;Padghamand Winikoff 2004;Jones2007.
Chapter1tracedtherootsoftheconceptofrationalityinphilosophyandeconomics. In
A Itheconceptwasofperipheralinterestuntilthemid-1980swhenitbegantosuffusemany
discussions aboutthepropertechnical foundations ofthefield. Apaperby Jon Doyle1983
predicted that rational agent design would come to be seen as the core mission of A I while
otherpopulartopicswouldspinofftoformnewdisciplines.
Careful attention to the properties of the environment and their consequences for ra-
tional agent design is most apparent in the control theory traditionfor example classical
control systems Dorf and Bishop 2004; Kirk 2004 handle fully observable deterministic
environments; stochastic optimal control Kumar and Varaiya 1986; Bertsekas and Shreve
2007 handles partially observable stochastic environments; and hybrid control Henzinger
and Sastry 1998; Cassandras and Lygeros 2006 deals with environments containing both
discrete andcontinuous elements. Thedistinction between fully andpartially observable en-
vironments is also central in the dynamic programming literature developed in the field of
operations research Puterman1994 whichwediscuss in Chapter17.
Reflex agents were the primary model for psychological behaviorists such as Skinner
1953whoattemptedtoreducethepsychologyoforganismsstrictlytoinputoutputorstim-
ulusresponse mappings. The advance from behaviorism to functionalism in psychology
which wasatleast partly driven bythe application ofthe computer metaphortoagents Put-
nam 1960; Lewis 1966 introduced the internal state of the agent into the picture. Most
work in A I views the idea of pure reflex agents with state as too simple to provide much
leverage but work by Rosenschein 1985 and Brooks 1986 questioned this assumption
see Chapter 25. In recent years a great deal of work has gone into finding efficient algo-
rithmsforkeepingtrackofcomplexenvironments Hamscheretal.1992;Simon2006. The
Remote Agentprogramdescribedonpage28thatcontrolled the Deep Space Onespacecraft
isaparticularly impressiveexample Muscettola etal.1998;Jonssonetal.2000.
Goal-basedagentsarepresupposedineverythingfrom Aristotlesviewofpracticalrea-
soning to Mc Carthys early papers on logical A I. Shakey the Robot Fikes and Nilsson
1971; Nilsson 1984 was the first robotic embodiment of a logical goal-based agent. A
full logical analysis of goal-based agents appeared in Genesereth and Nilsson 1987 and a
goal-basedprogrammingmethodologycalledagent-orientedprogrammingwasdevelopedby
Shoham 1993. The agent-based approach is now extremely popular in software engineer-
ing Ciancarini and Wooldridge 2001. It has also infiltrated the area of operating systems
A UT ON OM IC whereautonomiccomputingreferstocomputersystemsandnetworksthatmonitorandcon-
C OM PU TI NG
trolthemselves withaperceiveact loopandmachinelearning methods Kephartand Chess
2003. Noting that a collection of agent programs designed to work well together in a true
multiagentenvironmentnecessarilyexhibitsmodularitytheprogramssharenointernalstate
and communicate with each other only through the environmentit is common within the
M UL TI AG EN T field of multiagent systems to design the agent program of asingle agent as a collection of
S YS TE MS
autonomous sub-agents. In some cases one can even prove that the resulting system gives
thesameoptimalsolutions asamonolithicdesign.
Thegoal-based viewofagentsalsodominatesthecognitivepsychology traditioninthe
area of problem solving beginning with the enormously influential Human Problem Solv-
ing Newelland Simon1972andrunningthroughallof Newellslaterwork Newell1990.
Goals furtheranalyzed as desires general and intentions currently pursued arecentral to
the theory of agents developed by Bratman 1987. This theory has been influential both in
3
S OL VI NG P RO BL EM S B Y
S EA RC HI NG
In which we see how an agent can find a sequence of actions that achieves its
goalswhennosingleactionwilldo.
Thesimplestagentsdiscussedin Chapter2werethereflexagentswhichbasetheiractionson
adirectmappingfromstatestoactions. Suchagentscannotoperate wellinenvironments for
whichthismappingwouldbetoolargetostoreandwouldtaketoolongtolearn. Goal-based
agents ontheotherhandconsiderfutureactions andthedesirability oftheiroutcomes.
P RO BL EM-S OL VI NG This chapter describes one kind of goal-based agent called a problem-solving agent.
A GE NT
Problem-solving agents use atomic representations as described in Section 2.4.7that is
statesoftheworldareconsideredaswholeswithnointernalstructurevisibletotheproblem-
solving algorithms. Goal-based agents that use more advanced factored or structured rep-
resentations areusuallycalled planningagentsandarediscussed in Chapters7and10.
Ourdiscussionofproblemsolvingbeginswithprecisedefinitionsofproblemsandtheir
solutions and give several examples to illustrate these definitions. We then describe several
general-purpose search algorithms that can be used to solve these problems. We will see
several uninformed search algorithmsalgorithms that are given no information about the
problem otherthan itsdefinition. Although some ofthese algorithms can solve anysolvable
problem noneofthemcandosoefficiently. Informedsearchalgorithms ontheotherhand
candoquitewellgivensomeguidance onwheretolookforsolutions.
In this chapter we limit ourselves to the simplest kind of task environment for which
thesolutiontoaproblemisalwaysafixedsequenceofactions. Themoregeneralcasewhere
theagentsfutureactionsmayvarydepending onfutureperceptsis handledin Chapter4.
This chapter uses the concepts of asymptotic complexity that is O notation and
N P-completeness. Readersunfamiliarwiththeseconcepts shouldconsult Appendix A.
Intelligent agents are supposed to maximize their performance measure. As we mentioned
in Chapter2 achieving this issometimes simplified iftheagent canadopt agoal andaim at
satisfying it. Letusfirstlookatwhyandhowanagentmightdothis.
64
Section3.1. Problem-Solving Agents 65
Imagineanagentinthecityof Arad Romaniaenjoying atouringholiday. Theagents
performance measure contains manyfactors: itwants toimprove itssuntan improve its Ro-
maniantakeinthesightsenjoythenightlife suchasitisavoidhangovers andsoon. The
decision problem is a complex one involving many tradeoffs and careful reading of guide-
books. Nowsuppose theagenthasanonrefundable tickettoflyoutof Bucharest thefollow-
ing day. In that case it makes sense for the agent to adopt the goal of getting to Bucharest.
Courses of action that dont reach Bucharest on timecan berejected without further consid-
eration and the agents decision problem is greatly simplified. Goals help organize behavior
by limiting the objectives that the agent is trying to achieve and hence the actions it needs
to consider. Goal formulation based on the current situation and the agents performance
G OA LF OR MU LA TI ON
measureisthefirststepinproblem solving.
We will consider a goal to be a set of world statesexactly those states in which the
goal is satisfied. The agents task is to find out how to act now and in the future so that it
reaches a goal state. Before it can do this it needs to decide or we need to decide on its
behalf what sorts of actions and states it should consider. If it were to consider actions at
thelevelofmovetheleftfootforwardaninchorturnthesteering wheelonedegreeleft
the agent would probably never find its way out of the parking lot let alone to Bucharest
because at that level of detail there is too much uncertainty in the world and there would be
P RO BL EM too many steps in a solution. Problem formulation is the process of deciding what actions
F OR MU LA TI ON
andstatestoconsider givenagoal. Wediscuss thisprocess inmoredetaillater. Fornowlet
usassume thattheagent willconsider actions atthe levelof driving from onemajortownto
another. Eachstatetherefore corresponds tobeinginaparticulartown.
Our agent has now adopted the goal of driving to Bucharest and is considering where
togofrom Arad. Threeroads lead outof Arad onetoward Sibiu oneto Timisoara and one
to Zerind. Noneoftheseachievesthegoalsounlesstheagentisfamiliarwiththegeography
of Romania itwillnotknow whichroadtofollow.1 Inotherwords theagent willnotknow
which of its possible actions is best because it does not yet know enough about the state
that results from taking each action. If the agent has no additional informationi.e. if the
environment is unknowninthesense defined in Section 2.3then itishas nochoice but to
tryoneoftheactionsatrandom. Thissadsituationisdiscussed in Chapter4.
But suppose the agent has a map of Romania. The point of a map is to provide the
agentwithinformationaboutthestatesitmightgetitselfintoandtheactionsitcantake. The
agent can use this information to consider subsequent stages of a hypothetical journey via
eachofthethreetownstryingtofindajourneythateventuallygetsto Bucharest. Onceithas
found a path on the map from Arad to Bucharest it can achieve its goal by carrying out the
driving actions that correspond to the legs of the journey. In general an agent with several
immediate options ofunknown valuecan decide whattodobyfirst examining future actions
thateventually leadtostatesofknownvalue.
To be more specific about what we mean by examining future actions we have to
be more specific about properties of the environment as defined in Section 2.3. For now
asouragent.Weapologizeto Romanianreaderswhoareunabletotakeadvantageofthispedagogicaldevice.
weassume that the environment is observable so the agent always knows the current state.
Forthe agent driving in Romania its reasonable to suppose that each city on the maphas a
sign indicating its presence to arriving drivers. Wealso assume the environment is discrete
so at any given state there are only finitely many actions to choose from. This is true for
navigating in Romania because each city isconnected toa small numberof other cities. We
willassumetheenvironment isknownsotheagentknowswhichstatesarereachedbyeach
action. Having an accurate map suffices to meet this condition for navigation problems.
Finally we assume that the environment is deterministic so each action has exactly one
outcome. Under ideal conditions this is true for the agent in Romaniait means that if it
chooses to drive from Arad to Sibiu it does end up in Sibiu. Of course conditions are not
alwaysideal asweshowin Chapter4.
Under these assumptions the solution to any problem is a fixed sequence of actions.
Ofcourse! onemightsay Whatelsecoulditbe? Wellingeneralitcouldbeabranching
strategy that recommends different actions in the future depending on what percepts arrive.
For example under less than ideal conditions the agent might plan to drive from Arad to
Sibiu and then to Rimnicu Vilcea but may also need to have a contingency plan in case it
arrivesbyaccidentin Zerindinstead of Sibiu. Fortunately iftheagentknowstheinitialstate
and the environment is known and deterministic it knows exactly where it will be after the
firstaction and whatitwillperceive. Sinceonlyonepercept ispossible afterthefirstaction
thesolution canspecify onlyonepossible secondaction andsoon.
Theprocess oflooking forasequence ofactions thatreaches thegoaliscalledsearch.
S EA RC H
A search algorithm takes a problem as input and returns a solution in the form of an action
S OL UT IO N
sequence. Once a solution is found the actions it recommends can be carried out. This
is called the execution phase. Thus we have a simple formulate search execute design
E XE CU TI ON
for the agent as shown in Figure 3.1. After formulating a goal and a problem to solve
the agent calls a search procedure to solve it. It then uses the solution to guide its actions
doingwhateverthesolutionrecommendsasthenextthingtodotypically thefirstactionof
the sequenceand then removing that step from the sequence. Once the solution has been
executed theagentwillformulate anewgoal.
Notice that while the agent is executing the solution sequence it ignores its percepts
when choosing an action because it knows in advance what they will be. An agent that
carries out its plans with its eyes closed so to speak must be quite certain of what is going
on. Controltheorists callthisan open-loopsystembecause ignoringthepercepts breaksthe
O PE N-L OO P
loopbetweenagentandenvironment.
We first describe the process of problem formulation and then devote the bulk of the
the U PD AT E-S TA TE and F OR MU LA TE-G OA L functions furtherinthischapter.
Aproblemcanbedefinedformallybyfivecomponents:
P RO BL EM
The initial state that the agent starts in. For example the initial state for our agent in
I NI TI AL ST AT E
Romaniamightbedescribed as In Arad.
Section3.1. Problem-Solving Agents 67
function S IM PL E-P RO BL EM-S OL VI NG-A GE NTperceptreturnsanaction
persistent: seqanactionsequenceinitiallyempty
statesomedescriptionofthecurrentworldstate
goalagoalinitiallynull
problemaproblemformulation
state U PD AT E-S TA TEstatepercept
ifseq isemptythen
goal F OR MU LA TE-G OA Lstate
problem F OR MU LA TE-P RO BL EMstategoal
seq S EA RC Hproblem
ifseq failure thenreturnanullaction
action F IR STseq
seq R ES Tseq
returnaction
searchesforasequenceofactionsthatwouldsolvetheproblemandthenexecutestheactions
oneatatime. Whenthisiscompleteitformulatesanothergoalandstartsover.
A description of the possible actions available to the agent. Given a particular state s
A CT IO NS
A CT IO NSs returns the set of actions that can be executed in s. We say that each of
these actions is applicable in s. Forexample from the state In Arad the applicable
A PP LI CA BL E
actionsare Go Sibiu Go Timisoara Go Zerind.
A description of what each action does; the formal name for this is the transition
T RA NS IT IO NM OD EL
model specified by a function R ES UL Tsa that returns the state that results from
doingactionainstates. Wealsousethetermsuccessortorefertoanystatereachable
S UC CE SS OR
fromagivenstatebyasingleaction.2 Forexamplewehave
R ES UL TIn Arad Go Zerind In Zerind.
Togethertheinitialstateactionsandtransitionmodelimplicitlydefinethestatespace
S TA TE SP AC E
of the problemthe set of all states reachable from the initial state by any sequence
of actions. The state space forms a directed network or graph in which the nodes
G RA PH
are states and the links between nodes are actions. The map of Romania shown in
fortwodriving actions one ineach direction. A pathin the state space is asequence
P AT H
ofstatesconnected byasequence ofactions.
Thegoal test whichdetermines whetheragiven stateisagoal state. Sometimesthere
G OA LT ES T
is an explicit set of possible goal states and the test simply checks whether the given
stateisoneofthem. Theagentsgoalin Romaniaisthesingletonset In Bucharest.
returnsthe set of allsuccessors instead of separate A CT IO NSand R ES UL Tfunctions. The successor function
makesitdifficulttodescribeanagentthatknowswhatactionsitcantrybutnotwhattheyachieve. Alsonote
someauthoruse R ES UL Tasinsteadof R ES UL Tsaandsomeuse D Oinsteadof R ES UL T.
Oradea
71
Neamt
Zerind 87
151
75
Iasi
Arad
140
92
Sibiu Fagaras
99
118
Vaslui
80
Rimnicu Vilcea
Timisoara
142
Hirsova
Mehadia 146 101 85 Urziceni
86
Bucharest
Drobeta 120
90
Craiova Giurgiu Eforie
Figure3.2 Asimplifiedroadmapofpartof Romania.
Sometimesthegoalisspecifiedbyanabstractpropertyratherthananexplicitlyenumer-
atedsetofstates. Forexampleinchessthegoalistoreach astatecalledcheckmate
wheretheopponents kingisunderattackandcantescape.
A path cost function that assigns a numeric cost to each path. The problem-solving
P AT HC OS T
agent chooses acost function that reflects its ownperformance measure. Forthe agent
tryingtogetto Bucharesttimeisoftheessencesothecostofapathmightbeitslength
inkilometers. Inthischapter weassumethatthecostofapathcanbedescribed asthe
sumofthecostsoftheindividualactionsalongthepath.3 Thestepcostoftakingaction
S TE PC OS T
cid:2 cid:2
a in state s to reach state s is denoted by csas. The step costs for Romania are
shownin Figure3.2asroutedistances. Weassumethatstepcostsarenonnegative.4
The preceding elements define a problem and can be gathered into a single data structure
that is given as input to a problem-solving algorithm. A solution to a problem is an action
sequence that leads from the initial state to agoal state. Solution quality is measured by the
pathcostfunction andanoptimalsolutionhasthelowestpathcostamongallsolutions.
O PT IM AL SO LU TI ON
Intheprecedingsectionweproposed aformulationoftheproblemofgettingto Bucharestin
termsoftheinitial state actions transition model goal test andpath cost. Thisformulation
seems reasonable but it is still a modelan abstract mathematical descriptionand not the
Section3.2. Example Problems 69
realthing. Comparethesimplestatedescriptionwehavechosen In Aradtoanactualcross-
countrytripwherethestateoftheworldincludessomanythings: thetravelingcompanions
the current radio program the scenery out of the window the proximity of law enforcement
officers the distance to the next rest stop the condition of the road the weather and so on.
Alltheseconsiderations areleftoutofourstatedescriptionsbecausetheyareirrelevanttothe
problemoffindingarouteto Bucharest. Theprocessofremovingdetailfromarepresentation
iscalledabstraction.
A BS TR AC TI ON
Inadditiontoabstractingthestatedescription wemustabstracttheactionsthemselves.
A driving action has many effects. Besides changing the location of the vehicle and its oc-
cupants ittakes uptime consumes fuel generates pollution andchanges the agent as they
say travel is broadening. Our formulation takes into account only the change in location.
Also there are many actions that we omit altogether: turning on the radio looking out of
thewindowslowing downforlawenforcement officers andso on. Andofcourse wedont
specifyactions atthelevelofturnsteeringwheeltotheleftbyonedegree.
Canwebemorepreciseaboutdefiningtheappropriatelevelofabstraction? Thinkofthe
abstract states and actions we have chosen as corresponding to large sets of detailed world
states and detailed action sequences. Now consider a solution to the abstract problem: for
examplethepathfrom Aradto Sibiuto Rimnicu Vilceato Pitestito Bucharest. Thisabstract
solution corresponds to alarge numberofmore detailed paths. Forexample wecould drive
with the radio on between Sibiu and Rimnicu Vilcea and then switch it off for the rest of
thetrip. Theabstraction is valid ifwecanexpand anyabstract solution intoasolution inthe
more detailed world; a sufficient condition is that for every detailed state that is in Arad
there is a detailed path to some state that is in Sibiu and so on.5 The abstraction is useful
if carrying out each of the actions in the solution is easier than the original problem; in this
case they are easy enough that they can be carried out without further search orplanning by
an average driving agent. The choice of a good abstraction thus involves removing as much
detail as possible while retaining validity and ensuring that the abstract actions are easy to
carryout. Wereitnotfortheabilitytoconstruct usefulabstractions intelligent agentswould
becompletelyswampedbytherealworld.
The problem-solving approach has been applied to a vast array of task environments. We
list some of the best known here distinguishing between toy and real-world problems. A
toyproblem isintended toillustrate orexercise various problem-solving methods. Itcanbe
T OY PR OB LE M
givenaconciseexactdescriptionandhenceisusablebydifferentresearcherstocomparethe
R EA L-W OR LD performance of algorithms. A real-world problem is one whose solutions people actually
P RO BL EM
careabout. Suchproblemstendnottohaveasingleagreed-upondescription butwecangive
thegeneralflavoroftheirformulations.
R
L R
L
S S
R R
L R L R
L L
S S
S S
R
L R
L
S S
Right S Suck.
The first example we examine is the vacuum world first introduced in Chapter 2. See
Figure2.2. Thiscanbeformulatedasaproblem asfollows:
States: The state is determined by both the agent location and the dirt locations. The
agent is in one of two locations each of which might or might not contain dirt. Thus
there are 222 8 possible world states. A larger environment with n locations has
n2n states.
Initialstate: Anystatecanbedesignated astheinitialstate.
Actions: In this simple environment each state has just three actions: Left Right and
Suck. Largerenvironments mightalsoinclude Upand Down.
Transition model: Theactions have their expected effects except that moving Left in
theleftmostsquaremoving Rightintherightmostsquareand Suckinginacleansquare
havenoeffect. Thecompletestatespaceisshownin Figure3.3.
Goaltest: Thischeckswhetherallthesquares areclean.
Pathcost: Eachstepcosts1sothepathcostisthenumberofstepsinthepath.
Compared with the real world this toy problem has discrete locations discrete dirt reliable
cleaning anditnevergetsanydirtier. Chapter4relaxessomeoftheseassumptions.
The8-puzzleaninstanceofwhichisshownin Figure3.4consistsofa33boardwith
8-P UZ ZL E
eight numbered tiles and a blank space. A tile adjacent to the blank space can slide into the
space. Theobject istoreach aspecified goalstate such astheoneshownontherightofthe
figure. Thestandard formulation isasfollows:
Section3.2. Example Problems 71
Start State Goal State
Figure3.4 Atypicalinstanceofthe8-puzzle.
States: Astatedescription specifiesthelocationofeachoftheeighttilesandtheblank
inoneoftheninesquares.
Initial state: Any state can be designated as the initial state. Note that any given goal
canbereachedfromexactlyhalfofthepossible initialstates Exercise3.4.
Actions: Thesimplestformulationdefinestheactionsasmovementsoftheblankspace
Left Right Up or Down. Different subsets of these are possible depending on where
theblankis.
Transitionmodel: Givenastateandactionthisreturnstheresultingstate;forexample
ifweapply Lefttothestartstatein Figure3.4theresultingstatehasthe5andtheblank
switched.
Goaltest: Thischecks whetherthe state matches thegoal configuration shownin Fig-
ure3.4. Othergoalconfigurations arepossible.
Pathcost: Eachstepcosts1sothepathcostisthenumberofstepsinthepath.
What abstractions have weincluded here? The actions are abstracted to their beginning and
finalstatesignoringtheintermediatelocationswheretheblockissliding. Wehaveabstracted
away actions such as shaking the board when pieces get stuck and ruled out extracting the
pieceswithaknifeandputtingthembackagain. Weareleftwithadescriptionoftherulesof
thepuzzle avoiding allthedetailsofphysicalmanipulations.
S LI DI NG-B LO CK The 8-puzzle belongs to the family of sliding-block puzzles which are often used as
P UZ ZL ES
test problems for new search algorithms in A I. This family is known to be N P-complete
so one does not expect to find methods significantly better in the worst case than the search
algorithmsdescribedinthischapterandthenext. The8-puzzlehas9!2181440reachable
statesandiseasilysolved. The15-puzzleona44boardhasaround1.3trillionstatesand
randominstancescanbesolvedoptimallyinafewmillisecondsbythebestsearchalgorithms.
The24-puzzle on a 55board has around 1025 states and random instances take several
hourstosolveoptimally.
The goal of the 8-queens problem is to place eight queens on a chessboard such that
8-Q UE EN SP RO BL EM
no queen attacks any other. A queen attacks any piece in the same row column or diago-
nal. Figure3.5shows anattempted solution thatfails: the queen inthe rightmost column is
attacked bythequeenatthetopleft.
Figure3.5 Almostasolutiontothe8-queensproblem.Solutionisleftasanexercise.
Although efficient special-purpose algorithms exist forthis problem and forthe whole
n-queens family it remains auseful test problem forsearch algorithms. Thereare twomain
I NC RE ME NT AL kindsofformulation. Anincrementalformulationinvolvesoperatorsthataugmentthestate
F OR MU LA TI ON
description starting with an empty state; for the 8-queens problem this means that each
C OM PL ET E-S TA TE action adds a queen to the state. A complete-state formulation starts with all 8 queens on
F OR MU LA TI ON
theboard andmovesthemaround. Ineithercasethepathcost isofnointerest because only
thefinalstatecounts. Thefirstincremental formulation one mighttryisthefollowing:
States: Anyarrangement of0to8queensontheboardisastate.
Initialstate: Noqueensontheboard.
Actions: Addaqueentoanyemptysquare.
Transitionmodel: Returnstheboardwithaqueenaddedtothespecifiedsquare.
Goaltest: 8queensareontheboard noneattacked.
Inthisformulation wehave646357 1.81014 possible sequences toinvestigate. A
betterformulation wouldprohibitplacing aqueeninanysquarethatisalready attacked:
States: All possible arrangements of n queens 0 n 8 one per column in the
leftmostncolumnswithnoqueenattacking another.
Actions: Add a queen to any square in the leftmost empty column such that it is not
attackedbyanyotherqueen.
Thisformulationreducesthe8-queensstatespacefrom1.81014tojust2057andsolutions
areeasytofind. Ontheotherhandfor100queensthereductionisfromroughly 10400 states
toabout1052 states Exercise3.5abigimprovementbutnotenoughtomaketheproblem
tractable. Section4.1describes thecomplete-state formulation and Chapter6givesasimple
algorithm thatsolveseventhemillion-queens problemwith ease.
Section3.2. Example Problems 73
Ourfinaltoyproblem wasdevisedby Donald Knuth1964andillustrates howinfinite
statespacescanarise. Knuthconjectured thatstarting withthenumber4asequence offac-
torial square root andflooroperations willreach anydesired positive integer. Forexample
wecanreach5from4asfollows:
cid:4
cid:5cid:7
cid:8
cid:3cid:5 cid:6 cid:9 cid:10 cid:11
4!! 5.
Theproblem definitionisverysimple:
States: Positivenumbers.
Initialstate: 4.
Actions: Applyfactorial squarerootorflooroperation factorial forintegersonly.
Transitionmodel: Asgivenbythemathematicaldefinitions oftheoperations.
Goaltest: Stateisthedesiredpositiveinteger.
Toourknowledge thereisnobound onhowlarge anumbermightbeconstructed inthepro-
cessofreaching agiventargetfor example thenumber620448401733239439360000
is generated in the expression for 5so the state space for this problem is infinite. Such
state spaces arise frequently in tasks involving the generation of mathematical expressions
circuits proofs programs andotherrecursively definedobjects.
R OU TE-F IN DI NG We have already seen how the route-finding problem is defined in terms of specified loca-
P RO BL EM
tionsandtransitionsalonglinksbetweenthem. Route-findingalgorithmsareusedinavariety
of applications. Some such as Web sites and in-car systems that provide driving directions
are relatively straightforward extensions of the Romania example. Others such as routing
videostreamsincomputernetworksmilitaryoperationsplanningandairlinetravel-planning
systemsinvolvemuchmorecomplexspecifications. Considertheairlinetravelproblemsthat
mustbesolvedbyatravel-planning Website:
States: Eachstate obviously includes alocation e.g. anairport and the current time.
Furthermore because the cost of an action a flight segment may depend on previous
segments their fare bases and their status as domestic or international the state must
recordextrainformation aboutthesehistorical aspects.
Initialstate: Thisisspecifiedbytheusersquery.
Actions: Take any flight from the current location in any seat class leaving after the
currenttimeleavingenough timeforwithin-airport transferifneeded.
Transition model: The state resulting from taking a flight will have the flights desti-
nationasthecurrentlocation andtheflightsarrivaltimeasthecurrenttime.
Goaltest: Areweatthefinaldestination specifiedbytheuser?
Path cost: This depends on monetary cost waiting time flight time customs and im-
migrationprocedures seatquality timeofdaytypeofairplane frequent-flyer mileage
awardsandsoon.
Commercial travel advice systems use a problem formulation of this kind with many addi-
tional complications to handle the byzantine fare structures that airlines impose. Any sea-
soned traveler knows however that not all air travel goes according to plan. A really good
systemshouldincludecontingencyplanssuchasbackupreservationsonalternateflights
totheextentthatthesearejustifiedbythecostandlikelihood offailureoftheoriginalplan.
Touring problems are closely related to route-finding problems but with an impor-
T OU RI NG PR OB LE M
tant difference. Consider for example the problem Visit every city in Figure 3.2 at least
once starting and ending in Bucharest. As with route finding the actions correspond
to trips between adjacent cities. The state space however is quite different. Each state
must include not just the current location but also the set of cities the agent has visited.
So the initial state would be In Bucharest Visited Bucharest a typical intermedi-
ate state would be In Vaslui Visited Bucharest Urziceni Vaslui and the goal test
wouldcheckwhethertheagentisin Bucharestandall20citieshavebeenvisited.
T RA VE LI NG
The traveling salesperson problem T SP is a touring problem in which each city
S AL ES PE RS ON
P RO BL EM
must be visited exactly once. The aim is to find the shortest tour. The problem is known to
be N P-hardbutanenormousamountofefforthasbeenexpendedtoimprovethecapabilities
of T SP algorithms. In addition to planning trips for traveling salespersons these algorithms
havebeenusedfortaskssuchasplanningmovementsofautomaticcircuit-board drillsandof
stocking machinesonshopfloors.
A V LS Ilayout problem requires positioning millions of components and connections
V LS IL AY OU T
on a chip to minimize area minimize circuit delays minimize stray capacitances and max-
imize manufacturing yield. The layout problem comes after the logical design phase and is
usually split into two parts: cell layout and channel routing. In cell layout the primitive
components of the circuit are grouped into cells each of which performs some recognized
function. Each cell has a fixed footprint size and shape and requires a certain number of
connectionstoeachoftheothercells. Theaimistoplacethecellsonthechipsothattheydo
notoverlapandsothatthereisroomfortheconnecting wires tobeplacedbetweenthecells.
Channelroutingfindsaspecificrouteforeachwirethroughthegapsbetweenthecells. These
search problems are extremely complex but definitely worth solving. Later in this chapter
wepresentsomealgorithmscapable ofsolving them.
Robot navigation is a generalization of the route-finding problem described earlier.
R OB OT NA VI GA TI ON
Rather than following a discrete set of routes a robot can move in a continuous space with
in principle an infinite set of possible actions and states. For a circular robot moving on a
flat surface the space is essentially two-dimensional. When the robot has arms and legs or
wheelsthatmustalsobecontrolled thesearchspacebecomesmany-dimensional. Advanced
techniques are required just to make the search space finite. We examine some of these
methods in Chapter 25. In addition to the complexity of the problem real robots must also
dealwitherrorsintheirsensorreadings andmotorcontrols.
A UT OM AT IC
A SS EM BL Y
Automaticassemblysequencingofcomplexobjectsbyarobotwasfirstdemonstrated
S EQ UE NC IN G
by F RE DD Y Michie 1972. Progress since then has been slow but sure to the point where
theassemblyofintricateobjectssuchaselectricmotorsiseconomicallyfeasible. Inassembly
problems the aim is to find an order in which to assemble the parts of some object. If the
wrong order is chosen there will be no way to add some part later in the sequence without
Section3.3. Searchingfor Solutions 75
undoing some of the work already done. Checking a step in the sequence for feasibility is a
difficultgeometricalsearchproblemcloselyrelatedtorobotnavigation. Thusthegeneration
of legal actions is the expensive part of assembly sequencing. Any practical algorithm must
avoidexploringallbutatinyfractionofthestatespace. Anotherimportantassemblyproblem
isprotein design inwhichthegoalistofindasequence ofaminoacids thatwillfoldinto a
P RO TE IN DE SI GN
three-dimensional proteinwiththerightproperties tocuresomedisease.
Having formulated some problems we now need to solve them. A solution is an action
sequence so search algorithms work by considering various possible action sequences. The
possible action sequences starting at the initial state form asearch tree with the initial state
S EA RC HT RE E
at the root; the branches are actions and the nodes correspond to states in the state space of
N OD E
theproblem. Figure3.6showsthefirstfewstepsingrowingthesearchtreeforfindingaroute
from Arad to Bucharest. The root node of the tree corresponds to the initial state In Arad.
The first step is to test whether this is a goal state. Clearly it is not but it is important to
check so that we can solve trick problems like starting in Arad get to Arad. Then we
need to consider taking various actions. We do this by expanding the current state; that is
E XP AN DI NG
applying each legal action to the current state thereby generating a new set of states. In
G EN ER AT IN G
this case we add three branches from the parent node In Arad leading to three new child
P AR EN TN OD E
nodes: In Sibiu In Timisoara and In Zerind. Now we must choose which of these three
C HI LD NO DE
possibilities toconsiderfurther.
Thisistheessenceofsearchfollowinguponeoptionnowandputtingtheothersaside
for later in case the first choice does not lead to a solution. Suppose we choose Sibiu first.
We check to see whether it is a goal state it is not and then expand it to get In Arad
In Fagaras In Oradeaand In Rimnicu Vilcea. Wecanthenchooseanyofthesefourorgo
back and choose Timisoara or Zerind. Each ofthese sixnodes isa leaf node that is anode
L EA FN OD E
with no children in the tree. The set of all leaf nodes available for expansion at any given
pointiscalledthefrontier. Manyauthors callittheopenlist whichisbothgeographically
F RO NT IE R
lessevocativeandlessaccurate becauseotherdatastructures arebettersuitedthanalist. In
O PE NL IS T
Figure3.6thefrontierofeachtreeconsists ofthosenodes withboldoutlines.
Theprocessofexpandingnodesonthefrontiercontinuesuntileitherasolutionisfound
ortherearenomorestatestoexpand. Thegeneral T RE E-S EA RC H algorithm isshowninfor-
mally in Figure 3.7. Search algorithms all share this basic structure; they vary primarily
according tohowtheychoosewhichstatetoexpandnexttheso-called search strategy.
S EA RC HS TR AT EG Y
Theeagle-eyedreaderwillnoticeonepeculiarthingaboutthesearchtreeshownin Fig-
ure3.6: itincludesthepathfrom Aradto Sibiuandbackto Aradagain! Wesaythat In Arad
is a repeated state in the search tree generated in this case by a loopy path. Considering
R EP EA TE DS TA TE
such loopy paths means that the complete search tree for Romania is infinite because there
L OO PY PA TH
is no limit to how often one can traverse a loop. On the other hand the state spacethe
mapshownin Figure3.2hasonly 20states. Aswediscuss in Section 3.4loops cancause
certainalgorithmstofailmakingotherwisesolvableproblemsunsolvable. Fortunatelythere
isnoneedtoconsiderloopypaths. Wecanrelyonmorethanintuition forthis: because path
costs are additive and step costs are nonnegative a loopy path to any given state is never
betterthanthesamepathwiththeloopremoved.
Loopypathsareaspecialcaseofthemoregeneralconceptofredundantpathswhich
R ED UN DA NT PA TH
existwheneverthereismorethanonewaytogetfromonestatetoanother. Considerthepaths
Arad Sibiu 140 km long and Arad Zerind Oradea Sibiu 297 km long. Obviously the
secondpathisredundantits justaworsewaytogettothesamestate. Ifyouareconcerned
about reaching the goal theres never any reason to keep more than one path to any given
state because any goal state that is reachable by extending one path is also reachable by
extending theother.
In some cases it is possible to define the problem itself so as to eliminate redundant
paths. Forexample if we formulate the 8-queens problem page 71 so that a queen can be
placedinanycolumntheneachstatewithnqueenscanbereachedbyn!differentpaths;but
ifwereformulatetheproblemsothateachnewqueenisplacedintheleftmostemptycolumn
theneachstatecanbereached onlythrough onepath.
a The initial state Arad
Sibiu Timisoara Zerind
Arad Fagaras Oradea Rimnicu Vilcea Arad Lugoj Arad Oradea
b After expanding Arad Arad
Sibiu Timisoara Zerind
Arad Fagaras Oradea Rimnicu Vilcea Arad Lugoj Arad Oradea
c After expanding Sibiu Arad
Sibiu Timisoara Zerind
Arad Fagaras Oradea Rimnicu Vilcea Arad Lugoj Arad Oradea
have been expanded are shaded; nodes that have been generated but not yet expanded are
outlinedinbold;nodesthathavenotyetbeengeneratedareshowninfaintdashedlines.
Section3.3. Searchingfor Solutions 77
function T RE E-S EA RC Hproblemreturnsasolutionorfailure
initializethefrontierusingtheinitialstateofproblem
loopdo
ifthefrontierisemptythenreturnfailure
choosealeafnodeandremoveitfromthefrontier
ifthenodecontainsagoalstatethenreturnthecorrespondingsolution
expandthechosennodeaddingtheresultingnodestothefrontier
function G RA PH-S EA RC Hproblemreturnsasolutionorfailure
initializethefrontierusingtheinitialstateofproblem
initializetheexploredsettobeempty
loopdo
ifthefrontierisemptythenreturnfailure
choosealeafnodeandremoveitfromthefrontier
ifthenodecontainsagoalstatethenreturnthecorrespondingsolution
addthenodetotheexploredset
expandthechosennodeaddingtheresultingnodestothefrontier
onlyifnotinthefrontierorexploredset
rithms. The parts of G RA PH-S EA RC H marked in bold italic are the additions needed to
handlerepeatedstates.
In other cases redundant paths are unavoidable. This includes all problems where
the actions are reversible such as route-finding problems and sliding-block puzzles. Route-
findingonarectangular gridliketheoneusedlaterfor Figure3.9isaparticularly impor-
R EC TA NG UL AR GR ID
tant example in computer games. In such a grid each state has four successors so a search
treeofdepthdthatincludesrepeatedstateshas4dleaves;butthereareonlyabout2d2distinct
stateswithindstepsofanygivenstate. Ford 20thismeansaboutatrillionnodesbutonly
about 800 distinct states. Thus following redundant paths can cause a tractable problem to
becomeintractable. Thisistrueevenforalgorithms thatknowhowtoavoidinfiniteloops.
As the saying goes algorithms that forget their history are doomed to repeat it. The
way to avoid exploring redundant paths is to remember where one has been. To do this we
E XP LO RE DS ET
augment the T RE E-S EA RC H algorithm with a data structure called the explored set also
known as the closed list which remembers every expanded node. Newly generated nodes
C LO SE DL IS T
thatmatchpreviously generated nodesones intheexplored setorthefrontiercan bedis-
cardedinsteadofbeingaddedtothefrontier. Thenewalgorithm called G RA PH-S EA RC His
shown informally in Figure 3.7. Thespecific algorithms inthis chapter draw onthis general
design.
Clearlythesearchtreeconstructedbythe G RA PH-S EA RC H algorithmcontainsatmost
onecopyofeachstatesowecanthinkofitasgrowingatreedirectlyonthestate-spacegraph
as shown in Figure 3.8. The algorithm has another nice property: the frontier separates the
S EP AR AT OR
state-space graphintotheexploredregionandtheunexplored regionsothateverypathfrom
Figure3.8 Asequenceofsearchtreesgeneratedbyagraphsearchonthe Romaniaprob-
lemof Figure3.2. Ateachstagewehaveextendedeachpathbyonestep. Noticethatatthe
thirdstagethenorthernmostcity Oradeahasbecomeadeadend:bothofitssuccessorsare
alreadyexploredviaotherpaths.
a b c
Figure3.9 Theseparationpropertyof G RA PH-S EA RC Hillustratedonarectangular-grid
problem. Thefrontierwhitenodesalwaysseparatestheexploredregionofthestatespace
black nodes from the unexplored region gray nodes. In a just the root has been ex-
panded.Inboneleafnodehasbeenexpanded.Inctheremainingsuccessorsoftheroot
havebeenexpandedinclockwiseorder.
the initial state to an unexplored state has to pass through a state in the frontier. If this
seemscompletely obvious try Exercise 3.13now. Thisproperty isillustrated in Figure 3.9.
As every step moves a state from the frontier into the explored region while moving some
statesfromtheunexploredregionintothefrontierweseethatthealgorithmissystematically
examiningthestatesinthestatespace onebyoneuntilitfindsasolution.
Search algorithms require a data structure to keep track of the search tree that is being con-
structed. Foreachnode nofthetreewehaveastructure thatcontains fourcomponents:
n.S TA TE: thestateinthestatespacetowhichthenodecorresponds;
n.P AR EN T: thenodeinthesearchtreethatgenerated thisnode;
n.A CT IO N: theactionthatwasappliedtotheparenttogeneratethenode;
n.P AT H-C OS T: thecosttraditionally denotedbygnofthepathfromtheinitialstate
tothenodeasindicated bytheparentpointers.
Section3.3. Searchingfor Solutions 79
P AR EN T
Node
P AT H-C OS T 6
S TA TE
Figure3.10 Nodesarethedatastructuresfromwhichthesearchtreeisconstructed.Each
hasaparentastateandvariousbookkeepingfields. Arrowspointfromchildtoparent.
Given the components for a parent node it is easy to see how to compute the necessary
components fora child node. The function C HI LD-N OD E takes a parent node and an action
andreturnstheresulting childnode:
function C HI LD-N OD Eproblemparentactionreturnsanode
returnanodewith
S TA TEproblem.R ES UL Tparent.S TA TEaction
P AR EN Tparent A CT IO Naction
P AT H-C OS Tparent.P AT H-C OS Tproblem.S TE P-C OS Tparent.S TA TEaction
The node data structure is depicted in Figure 3.10. Notice how the P AR EN T pointers
stringthenodestogetherintoatreestructure. Thesepointersalsoallowthesolutionpathtobe
extracted when agoal node is found; weuse the S OL UT IO N function to return the sequence
ofactions obtained byfollowingparentpointers backtotheroot.
Uptonowwehavenotbeenverycarefultodistinguishbetweennodesandstatesbutin
writing detailed algorithms its important to make that distinction. A node isa bookkeeping
data structure usedtorepresent the search tree. Astate corresponds toaconfiguration ofthe
world. Thus nodes are on particular paths as defined by P AR EN T pointers whereas states
are not. Furthermore two different nodes can contain the same world state if that state is
generated viatwodifferent searchpaths.
Now that we have nodes we need somewhere to put them. The frontier needs to be
stored in such a way that the search algorithm can easily choose the next node to expand
according to its preferred strategy. The appropriate data structure for this is a queue. The
Q UE UE
operations onaqueueareasfollows:
E MP TY?queuereturnstrueonlyiftherearenomoreelementsinthequeue.
P OPqueueremovesthefirstelementofthequeueandreturnsit.
I NS ER Telementqueueinsertsanelementandreturns theresulting queue.
Queuesarecharacterized bythe orderinwhichtheystoretheinsertednodes. Threecommon
variantsarethefirst-infirst-outor F IF Oqueuewhichpopstheoldestelementofthequeue;
F IF OQ UE UE
thelast-in first-outor L IF Oqueuealso knownasastack whichpops thenewestelement
L IF OQ UE UE
of the queue; and the priority queuewhich pops the element of the queue with the highest
P RI OR IT YQ UE UE
priorityaccording tosomeorderingfunction.
The explored set can be implemented with a hash table to allow efficient checking for
repeated states. With a good implementation insertion and lookup can be done in roughly
constant time no matter how many states are stored. One must take care to implement the
hash table with the right notion of equality between states. For example in the traveling
salesperson problem page 74 the hash table needs to know that the set of visited cities
Bucharest Urziceni Vaslui isthesameas Urziceni Vaslui Bucharest. Sometimesthiscan
be achieved most easily by insisting that the data structures for states be in some canonical
form; that is logically equivalent states should map to the same data structure. In the case
C AN ON IC AL FO RM
of states described by sets for example a bit-vector representation or a sorted list without
repetition wouldbecanonical whereasanunsorted listwouldnot.
Before we get into the design of specific search algorithms we need to consider the criteria
that might be used to choose among them. We can evaluate an algorithms performance in
fourways:
Completeness: Isthealgorithm guaranteed tofindasolution whenthereisone?
C OM PL ET EN ES S
Optimality: Doesthestrategyfindtheoptimalsolution asdefinedonpage68?
O PT IM AL IT Y
Timecomplexity: Howlongdoesittaketofindasolution?
T IM EC OM PL EX IT Y
Spacecomplexity: Howmuchmemoryisneededtoperformthesearch?
S PA CE CO MP LE XI TY
Timeandspacecomplexityarealwaysconsidered withrespecttosomemeasureoftheprob-
lem difficulty. In theoretical computer science the typical measure is the size of the state
space graph V E where V is the set of vertices nodes of the graph and E is the set
ofedges links. Thisisappropriate whenthe graph isan explicit data structure that isinput
tothesearchprogram. Themapof Romaniaisanexampleofthis. In A Ithegraphisoften
represented implicitly bytheinitialstate actions andtransition modelandisfrequently infi-
nite. Forthesereasonscomplexityisexpressedintermsofthreequantities: bthebranching
factor or maximum number of successors of any node; d the depth of the shallowest goal
B RA NC HI NG FA CT OR
nodei.e.thenumberofstepsalong thepathfromtheroot; andmthemaximumlength of
D EP TH
anypathinthestatespace. Timeisoftenmeasuredintermsofthenumberofnodesgenerated
during the search and space in terms of the maximum number of nodes stored in memory.
For the most part we describe time and space complexity for search on a tree; for a graph
theanswerdepends onhowredundant thepathsinthestatespaceare.
Toassesstheeffectivenessofasearchalgorithmwecanconsiderjustthesearchcost
S EA RC HC OS T
which typically depends on the time complexity but can also include a term for memory
usageorwecanusethetotalcostwhichcombinesthesearchcostandthepathcostofthe
T OT AL CO ST
solution found. For the problem of finding a route from Arad to Bucharest the search cost
isthe amount of timetaken bythe search and thesolution cost isthe total length of thepath
Section3.4. Uninformed Search Strategies 81
in kilometers. Thus to compute the total cost we have to add milliseconds and kilometers.
Thereisnoofficialexchangeratebetweenthetwobutitmightbereasonableinthiscaseto
convertkilometersintomillisecondsbyusinganestimateofthecarsaveragespeedbecause
time is what the agent cares about. This enables the agent to find an optimal tradeoff point
at which further computation to find a shorter path becomes counterproductive. The more
generalproblem oftradeoffs betweendifferentgoodsistakenupin Chapter16.
This section covers several search strategies that come under the heading of uninformed
U NI NF OR ME D search also called blind search. The term means that the strategies have no additional
S EA RC H
information about states beyond that provided in the problem definition. All they can do is
B LI ND SE AR CH
generate successors and distinguish a goal state from a non-goal state. All search strategies
are distinguished by the order in which nodes are expanded. Strategies that know whether
onenon-goalstateismorepromisingthananotherarecalledinformedsearchorheuristic
I NF OR ME DS EA RC H
search strategies; theyarecoveredin Section3.5.
H EU RI ST IC SE AR CH
B RE AD TH-F IR ST Breadth-firstsearchisasimplestrategyinwhichtherootnodeisexpandedfirstthenallthe
S EA RC H
successors of the root node are expanded next then their successors and so on. In general
all the nodes are expanded at a given depth in the search tree before any nodes at the next
levelareexpanded.
Breadth-firstsearchisaninstanceofthegeneralgraph-search algorithm Figure3.7in
whichtheshallowestunexpandednodeischosenforexpansion. Thisisachievedverysimply
byusinga F IF Oqueueforthefrontier. Thusnewnodeswhich arealwaysdeeperthantheir
parentsgotothebackofthequeueandoldnodeswhichareshallowerthanthenewnodes
getexpandedfirst. Thereisoneslighttweakonthegeneralgraph-search algorithmwhichis
thatthegoaltestisappliedtoeachnodewhenitisgeneratedratherthanwhenitisselectedfor
expansion. This decision is explained below where we discuss time complexity. Note also
thatthealgorithm followingthegeneral templateforgraphsearch discards anynewpathto
a state already in the frontier or explored set; it is easy to see that any such path must be at
least as deep as the one already found. Thus breadth-first search always has the shallowest
pathtoeverynodeonthefrontier.
Pseudocode isgiven in Figure 3.11. Figure 3.12 shows the progress ofthe search on a
simplebinarytree.
Howdoesbreadth-first search rateaccording tothefourcriteria from theprevious sec-
tion? Wecaneasilyseethatitiscompleteiftheshallowestgoalnodeisatsomefinitedepth
d breadth-first search will eventually find it after generating all shallower nodes provided
the branching factor b is finite. Note that as soon as a goal node is generated we know it
is the shallowest goal node because all shallower nodes must have been generated already
and failed the goal test. Now the shallowest goal node is not necessarily the optimal one;
function B RE AD TH-F IR ST-S EA RC Hproblemreturnsasolutionorfailure
nodeanodewith S TA TEproblem.I NI TI AL-S TA TE PA TH-C OS T0
ifproblem.G OA L-T ES Tnode.S TA TEthenreturn S OL UT IO Nnode
frontiera F IF Oqueuewithnode astheonlyelement
exploredanemptyset
loopdo
if E MP TY?frontierthenreturnfailure
node P OPfrontier choosestheshallowestnodeinfrontier
addnode.S TA TEtoexplored
foreachaction inproblem.A CT IO NSnode.S TA TEdo
child C HI LD-N OD Eproblemnodeaction
ifchild.S TA TEisnotinexplored orfrontier then
ifproblem.G OA L-T ES Tchild.S TA TEthenreturn S OL UT IO Nchild
frontier I NS ER Tchildfrontier
Figure3.11 Breadth-firstsearchonagraph.
technically breadth-first search isoptimal ifthe path cost is anondecreasing function of the
depthofthenode. Themostcommonsuchscenarioisthatallactions havethesamecost.
So far the news about breadth-first search has been good. The news about time and
space is not so good. Imagine searching a uniform tree where every state has b successors.
Therootofthesearchtreegenerates bnodesatthefirstleveleachofwhichgenerates bmore
nodes foratotalofb2 atthesecond level. Eachofthesegenerates bmorenodes yielding b3
nodes atthe third level and soon. Nowsuppose that the solution isatdepth d. Inthe worst
caseitisthelastnodegenerated atthatlevel. Thenthetotalnumberofnodesgenerated is
bb2b3bd Obd.
Ifthealgorithmweretoapplythegoaltesttonodeswhenselectedforexpansionratherthan
whengenerated thewholelayerofnodesatdepth dwouldbeexpanded before thegoalwas
detected andthetimecomplexitywouldbe Obd1.
As for space complexity: for any kind of graph search which stores every expanded
node in the explored set the space complexity is always within a factor of b of the time
complexity. For breadth-first graph search in particular every node generated remains in
memory. There will be Obd1 nodes in the explored set and Obd nodes in the frontier
A A A A
B C B C B C B C
D E F G D E F G D E F G D E F G
expandednextisindicatedbyamarker.
Section3.4. Uninformed Search Strategies 83
so the space complexity is Obd i.e. it is dominated by the size of the frontier. Switching
toatreesearch would notsavemuchspace andinastate space withmanyredundant paths
switchingcouldcostagreatdealoftime.
An exponential complexity bound such as Obd is scary. Figure 3.13 shows why. It
lists forvarious values ofthesolution depth dthetimeandmemoryrequired forabreadth-
first search with branching factor b 10. The table assumes that 1 million nodes can be
generated persecond and that anode requires 1000 bytes ofstorage. Manysearch problems
fit roughly within these assumptions give or take a factor of 100 when run on a modern
personal computer.
Depth Nodes Time Memory
Figure3.13 Timeandmemoryrequirementsforbreadth-firstsearch.Thenumbersshown
assumebranchingfactorb10;1millionnodessecond;1000bytesnode.
Two lessons can be learned from Figure 3.13. First the memory requirements are a
bigger problem for breadth-first search than is the execution time. One might wait 13 days
forthesolution toanimportant problem withsearch depth 12butnopersonal computerhas
thepetabyte ofmemoryitwouldtake. Fortunately otherstrategies requirelessmemory.
The second lesson is that time is still a major factor. If your problem has a solution at
depth16thengivenourassumptionsitwilltakeabout350yearsforbreadth-firstsearchor
indeedanyuninformedsearchtofindit. Ingeneral exponential-complexity searchproblems
cannotbesolvedbyuninformed methodsforanybutthesmallestinstances.
When all step costs are equal breadth-first search is optimal because it always expands the
shallowestunexpandednode. Byasimpleextensionwecanfindanalgorithmthatisoptimal
U NI FO RM-C OS T withanystep-cost function. Instead ofexpanding theshallowest node uniform-costsearch
S EA RC H
expands the node n with the lowest path cost gn. This is done by storing the frontier as a
priorityqueueordered by g. Thealgorithm isshownin Figure3.14.
In addition to the ordering of the queue by path cost there are two other significant
differences from breadth-first search. Thefirstisthat thegoal test isapplied toanode when
it is selected for expansion as in the generic graph-search algorithm shown in Figure 3.7
rather than when it is first generated. The reason is that the first goal node that is generated
function U NI FO RM-C OS T-S EA RC Hproblemreturnsasolutionorfailure
nodeanodewith S TA TEproblem.I NI TI AL-S TA TE PA TH-C OS T0
frontierapriorityqueueorderedby P AT H-C OS Twithnode astheonlyelement
exploredanemptyset
loopdo
if E MP TY?frontierthenreturnfailure
node P OPfrontier choosesthelowest-costnodeinfrontier
ifproblem.G OA L-T ES Tnode.S TA TEthenreturn S OL UT IO Nnode
addnode.S TA TEtoexplored
foreachaction inproblem.A CT IO NSnode.S TA TEdo
child C HI LD-N OD Eproblemnodeaction
ifchild.S TA TEisnotinexplored orfrontier then
frontier I NS ER Tchildfrontier
elseifchild.S TA TEisinfrontier withhigher P AT H-C OS Tthen
replacethatfrontier nodewithchild
graphsearchalgorithmin Figure3.7exceptfortheuseofapriorityqueueandtheaddition
ofanextracheckincaseashorterpathtoafrontierstateisdiscovered.Thedatastructurefor
frontier needstosupportefficientmembershiptestingsoitshouldcombinethecapabilities
ofapriorityqueueandahashtable.
Sibiu Fagaras
99
80
Rimnicu Vilcea
211
Pitesti
97
101
Bucharest
Figure3.15 Partofthe Romaniastatespaceselectedtoillustrateuniform-costsearch.
may be on a suboptimal path. The second difference is that a test is added in case a better
pathisfoundtoanodecurrently onthefrontier.
Bothofthesemodificationscomeintoplayintheexampleshownin Figure3.15where
theproblemistogetfrom Sibiuto Bucharest. Thesuccessors of Sibiuare Rimnicu Vilceaand
Fagaraswithcosts80and99respectively. Theleast-cost node Rimnicu Vilceaisexpanded
next adding Pitesti with cost 80 97177. The least-cost node is now Fagaras so it is
expanded adding Bucharestwithcost99211310. Nowagoalnodehasbeengenerated
butuniform-costsearchkeepsgoingchoosing Pitestiforexpansionandaddingasecondpath
Section3.4. Uninformed Search Strategies 85
to Bucharestwithcost8097101278. Nowthealgorithmcheckstoseeifthisnewpath
is better than the old one; it is so the old one is discarded. Bucharest now with g-cost 278
isselected forexpansion andthesolution isreturned.
It is easy to see that uniform-cost search is optimal in general. First we observe that
whenever uniform-cost search selects a node n for expansion the optimal path to that node
has been found. Were this not the case there would have to be another frontier node
ncid:2
on
the optimal path from the start node to n by the graph separation property of Figure 3.9;
cid:2
by definition n would have lower g-cost than n and would have been selected first. Then
because step costs are nonnegative paths never get shorter as nodes are added. These two
facts together imply that uniform-cost search expands nodes in order of their optimal path
cost. Hencethefirstgoalnodeselectedforexpansion mustbetheoptimalsolution.
Uniform-costsearchdoesnotcareaboutthe numberofstepsapathhasbutonlyabout
theirtotalcost. Thereforeitwillgetstuckinaninfiniteloopifthereisapathwithaninfinite
sequence ofzero-cost actionsfor example asequence of No Op actions.6 Completeness is
guaranteed providedthecostofeverystepexceedssomesmallpositiveconstant cid:2.
Uniform-costsearchisguidedbypathcostsratherthandepthssoitscomplexityisnot
easily characterized in terms of band d. Instead let C bethe cost ofthe optimal solution7
andassumethateveryactioncostsatleastcid:2. Thenthealgorithms worst-casetimeandspace
complexity is Ob1cid:4 Ccid:2cid:5 which can be much greater than bd. This is because uniform-
cost search can explore large trees of small steps before exploring paths involving large and
perhaps useful steps. When all step costs are equal b1cid:4 Ccid:2cid:5 is just bd1. When all step
costsarethesameuniform-costsearchissimilartobreadth-firstsearchexceptthatthelatter
stops as soon as it generates a goal whereas uniform-cost search examines all the nodes at
the goals depth to see if one has a lower cost; thus uniform-cost search does strictly more
workbyexpanding nodesatdepthdunnecessarily.
D EP TH-F IR ST Depth-firstsearchalwaysexpandsthedeepestnodeinthecurrentfrontierofthesearchtree.
S EA RC H
The progress of the search is illustrated in Figure 3.16. The search proceeds immediately
to the deepest level of the search tree where the nodes have no successors. As those nodes
are expanded they are dropped from the frontier so then the search backs up to the next
deepestnodethatstillhasunexplored successors.
The depth-first search algorithm is an instance of the graph-search algorithm in Fig-
ure3.7;whereasbreadth-first-searchusesa F IF Oqueuedepth-firstsearchusesa L IF Oqueue.
A L IF O queue means that the most recently generated node is chosen for expansion. This
mustbethedeepestunexpandednodebecauseitisonedeeperthanitsparentwhichinturn
wasthedeepestunexpanded nodewhenitwasselected.
As an alternative to the G RA PH-S EA RC H-style implementation it is common to im-
plement depth-first search witharecursive function that callsitself oneach ofitschildren in
turn. Arecursivedepth-first algorithm incorporating adepthlimitisshownin Figure3.17.
A A A
B C B C B C
D E F G D E F G D E F G
H I J K L M N O H I J K L M N O H I J K L M N O
A A A
B C B C C
D E F G D E F G E F G
H I J K L M N O I J K L M N O J K L M N O
A A
C B C C
E F G E F G F G
J K L M N O K L M N O L M N O
A A A
C C C
F G F G F G
L M N O L M N O M N O
Figure3.16 Depth-firstsearchonabinarytree. Theunexploredregionis showninlight
gray. Explorednodeswithnodescendantsinthefrontierare removedfrommemory. Nodes
atdepth3havenosuccessorsand M istheonlygoalnode.
The properties of depth-first search depend strongly on whether the graph-search or
tree-search version is used. The graph-search version which avoids repeated states and re-
dundantpathsiscompleteinfinitestatespacesbecauseitwilleventuallyexpandeverynode.
The tree-search version on the other hand is not completefor example in Figure 3.6 the
algorithmwillfollowthe Arad Sibiu Arad Sibiuloopforever. Depth-firsttreesearchcanbe
modifiedatnoextra memorycostsothatitchecks newstates against those onthepathfrom
theroottothecurrent node;thisavoids infiniteloopsinfinitestatespaces butdoesnotavoid
the proliferation of redundant paths. In infinite state spaces both versions fail if an infinite
non-goal path is encountered. For example in Knuths 4 problem depth-first search would
keepapplying thefactorial operatorforever.
Forsimilarreasons both versions arenonoptimal. Forexample in Figure3.16 depth-
firstsearch willexplore theentire left subtree evenifnode C isagoal node. Ifnode J were
also a goal node then depth-first search would return it as a solution instead of C which
wouldbeabettersolution; hence depth-firstsearchisnotoptimal.
Section3.4. Uninformed Search Strategies 87
Thetimecomplexityofdepth-firstgraphsearchisboundedbythesizeofthestatespace
whichmaybeinfiniteofcourse. Adepth-firsttreesearch ontheotherhandmaygenerate
all of the Obm nodes in the search tree where m is the maximum depth of any node; this
can be much greater than the size of the state space. Note that m itself can be much larger
thandthedepthoftheshallowest solution andisinfiniteifthetreeisunbounded.
So far depth-first search seems to have no clear advantage over breadth-first search
so why do we include it? The reason is the space complexity. For a graph search there is
no advantage but a depth-first tree search needs to store only a single path from the root
to a leaf node along with the remaining unexpanded sibling nodes for each node on the
path. Once a node has been expanded it can be removed from memory as soon as all its
descendants have been fully explored. See Figure 3.16. For a state space with branching
factor b and maximum depth m depth-first search requires storage of only Obm nodes.
Usingthesameassumptions asfor Figure3.13andassumingthatnodesatthesamedepthas
thegoalnodehavenosuccessors wefindthatdepth-firstsearchwouldrequire156kilobytes
instead of 10 exabytes at depth d 16 a factor of 7 trillion times less space. This has
led to the adoption of depth-first tree search as the basic workhorse of many areas of A I
includingconstraintsatisfaction Chapter6propositionalsatisfiability Chapter7andlogic
programming Chapter 9. Forthe remainder ofthis section wefocus primarily on thetree-
searchversionofdepth-first search.
B AC KT RA CK IN G Avariantofdepth-firstsearchcalled backtrackingsearchusesstilllessmemory. See
S EA RC H
Chapter6formoredetails. Inbacktracking onlyonesuccessor isgenerated atatimerather
than all successors; each partially expanded node remembers which successor to generate
next. In this way only Om memory is needed rather than Obm. Backtracking search
facilitates yet another memory-saving and time-saving trick: the idea of generating a suc-
cessor by modifying the current state description directly rather than copying it first. This
reduces thememoryrequirements tojustonestatedescription and Omactions. Forthisto
workwemustbeabletoundoeachmodification whenwegobacktogenerate thenextsuc-
cessor. Forproblemswithlargestatedescriptions suchas roboticassemblythesetechniques
arecriticaltosuccess.
The embarrassing failure of depth-first search in infinite state spaces can be alleviated by
supplying depth-first searchwithapredetermined depth limitcid:3. Thatisnodes atdepthcid:3are
D EP TH-L IM IT ED treated as if they have no successors. This approach is called depth-limited search. The
S EA RC H
depth limit solves the infinite-path problem. Unfortunately it also introduces an additional
sourceofincompleteness ifwechoose cid:3 dthatistheshallowest goalisbeyondthedepth
limit. This is likely when d is unknown. Depth-limited search will also be nonoptimal if
wechoosecid:3 d. Itstimecomplexityis Obcid:3anditsspacecomplexityis Obcid:3. Depth-first
searchcanbeviewedasaspecialcaseofdepth-limited searchwithcid:3.
Sometimes depth limits can be based on knowledge of the problem. Forexample on
themapof Romaniathereare20cities. Thereforeweknowthatifthereisasolutionitmust
be of length 19 at the longest so cid:3 19 is a possible choice. But in fact if we studied the
function D EP TH-L IM IT ED-S EA RC Hproblemlimitreturnsasolutionorfailurecutoff
return R EC UR SI VE-D LS MA KE-N OD Eproblem.I NI TI AL-S TA TEproblemlimit
function R EC UR SI VE-D LSnodeproblemlimitreturnsasolutionorfailurecutoff
ifproblem.G OA L-T ES Tnode.S TA TEthenreturn S OL UT IO Nnode
elseiflimit 0thenreturncutoff
else
cutoff occurred?false
foreachaction inproblem.A CT IO NSnode.S TA TEdo
child C HI LD-N OD Eproblemnodeaction
result R EC UR SI VE-D LSchildproblemlimit 1
ifresult cutoff thencutoff occurred?true
elseifresult cid:7failure thenreturnresult
ifcutoff occurred?thenreturncutoff elsereturnfailure
Figure3.17 Arecursiveimplementationofdepth-limitedtreesearch.
mapcarefully wewoulddiscoverthatanycitycanbereached fromanyothercityinatmost
9steps. Thisnumber knownasthediameterofthestatespacegivesusabetterdepthlimit
D IA ME TE R
which leads to a more efficient depth-limited search. For most problems however we will
notknowagooddepthlimituntilwehavesolvedtheproblem.
Depth-limited search can beimplemented asasimple modification to thegeneral tree-
or graph-search algorithm. Alternatively it can be implemented as a simple recursive al-
gorithm as shown in Figure 3.17. Notice that depth-limited search can terminate with two
kinds of failure: the standard failure value indicates no solution; the cutoff value indicates
nosolutionwithinthedepthlimit.
I TE RA TI VE Iterative deepening search or iterative deepening depth-first search is a general strategy
D EE PE NI NG SE AR CH
often used incombination withdepth-first tree search that findsthe bestdepth limit. Itdoes
thisbygraduallyincreasingthelimitfirst0then1then2andsoonuntilagoalisfound.
This will occur when the depth limit reaches d the depth of the shallowest goal node. The
algorithm is shown in Figure 3.18. Iterative deepening combines the benefits of depth-first
andbreadth-firstsearch. Likedepth-firstsearchitsmemoryrequirementsaremodest: Obd
tobeprecise. Likebreadth-first search itiscomplete when thebranching factorisfiniteand
optimalwhenthepathcostisanondecreasing function ofthedepthofthenode. Figure3.19
showsfouriterationsof I TE RA TI VE-D EE PE NI NG-S EA RC H onabinarysearchtreewherethe
solution isfoundonthefourthiteration.
Iterative deepening search may seem wasteful because states are generated multiple
times. Itturns out this isnot too costly. Thereason is that inasearch tree with thesame or
nearly the same branching factor at each level most of the nodes are in the bottom level
so it does not mattermuch that the upper levels are generated multiple times. In an iterative
deepening search the nodes on the bottom level depth d are generated once those on the
Section3.4. Uninformed Search Strategies 89
function I TE RA TI VE-D EE PE NI NG-S EA RC Hproblemreturnsasolutionorfailure
fordepth 0todo
result D EP TH-L IM IT ED-S EA RC Hproblemdepth
ifresult cid:7cutoffthenreturnresult
limitedsearchwithincreasinglimits. Itterminateswhena solutionisfoundorifthedepth-
limitedsearchreturnsfailuremeaningthatnosolutionexists.
Limit 0 A A
Limit 1 A A A A
B C B C B C B C
Limit 2 A A A A
B C B C B C B C
D E F G D E F G D E F G D E F G
A A A A
B C B C B C B C
D E F G D E F G D E F G D E F G
Limit 3 A A A A
B C B C B C B C
D E F G D E F G D E F G D E F G
H I J K L M N O H I J K L M N O H I J K L M N O H I J K L M N O
A A A A
B C B C B C B C
D E F G D E F G D E F G D E F G
H I J K L M N O H I J K L M N O H I J K L M N O H I J K L M N O
A A A A
B C B C B C B C
D E F G D E F G D E F G D E F G
H I J K L M N O H I J K L M N O H I J K L M N O H I J K L M N O
Figure3.19 Fouriterationsofiterativedeepeningsearchonabinarytree.
next-to-bottom levelaregenerated twice andsoon uptothechildren oftheroot whichare
generated dtimes. Sothetotalnumberofnodesgenerated intheworstcaseis
N ID S dbd1b21bd
which gives a time complexity of Obdasymptotically the same as breadth-first search.
Thereissomeextracostforgeneratingtheupperlevelsmultipletimesbutitisnotlarge. For
exampleifb 10andd 5thenumbersare
N ID S 50400300020000100000 123450
N BF S 10100100010000100000 111110.
If you are really concerned about repeating the repetition you can use a hybrid approach
that runs breadth-first search until almost all the available memory is consumed and then
runs iterative deepening from all the nodes in the frontier. In general iterative deepening is
the preferred uninformed search method when thesearch space islarge and thedepth ofthe
solution isnotknown.
Iterativedeepeningsearchisanalogous tobreadth-first searchinthatitexploresacom-
plete layer of new nodes at each iteration before going on to the next layer. It would seem
worthwhile to develop an iterative analog to uniform-cost search inheriting the latter algo-
rithms optimality guarantees while avoiding its memory requirements. The idea is to use
increasing path-cost limitsinstead ofincreasing depth limits. Theresulting algorithm called
I TE RA TI VE
iterative lengthening search is explored in Exercise 3.17. It turns out unfortunately that
L EN GT HE NI NG
S EA RC H
iterativelengthening incurssubstantial overheadcomparedtouniform-cost search.
Theideabehindbidirectional searchistoruntwosimultaneous searchesone forwardfrom
the initial state andthe otherbackward from thegoalhoping that thetwosearches meetin
the middle Figure 3.20. The motivation is that bd2 bd2 is much less than bd or in the
figure the areaofthe twosmall circles isless thanthe areaofone bigcircle centered onthe
startandreaching tothegoal.
Bidirectional search is implemented by replacing the goal test with a check to see
whether the frontiers of the two searches intersect; if they do a solution has been found.
It is important to realize that the first such solution found may not be optimal even if the
two searches are both breadth-first; some additional search is required to make sure there
isnt another short-cut across the gap. The check can be done when each node is generated
or selected for expansion and with a hash table will take constant time. For example if a
problem has solution depth d6 and each direction runs breadth-first search one node at a
timethenintheworstcasethetwosearches meetwhentheyhavegenerated allofthenodes
atdepth3. Forb10thismeansatotalof2220nodegenerationscomparedwith1111110
for a standard breadth-first search. Thus the time complexity of bidirectional search using
breadth-first searches in both directions is Obd2. The space complexity is also Obd2.
Wecanreducethisbyroughly halfifoneofthetwosearches is donebyiterative deepening
butatleastoneofthefrontiers mustbekeptinmemorysothat theintersection checkcanbe
done. Thisspacerequirement isthemostsignificant weaknessofbidirectional search.
Section3.4. Uninformed Search Strategies 91
Start Goal
branchfromthestartnodemeetsabranchfromthegoalnode.
Thereduction intimecomplexity makesbidirectional search attractive buthowdowe
search backward? This is not as easy as it sounds. Let the predecessors of a state x be all
P RE DE CE SS OR
thosestatesthathavexasasuccessor. Bidirectional searchrequiresamethodforcomputing
predecessors. Whenalltheactionsinthestatespacearereversible thepredecessors of xare
justitssuccessors. Othercasesmayrequiresubstantial ingenuity.
Considerthequestion ofwhatwemeanbythegoalinsearching backwardfromthe
goal. Forthe8-puzzleandforfindingaroutein Romaniathereisjustonegoalstatesothe
backward search is very much like the forward search. If there are several explicitly listed
goalstatesforexamplethetwodirt-freegoalstatesin Figure3.3thenwecanconstructa
newdummygoalstatewhoseimmediatepredecessors arealltheactualgoalstates. Butifthe
goal isan abstract description such as thegoal that no queen attacks another queen in the
n-queensproblemthenbidirectional searchisdifficulttouse.
Section 3.3.2. This comparison is for tree-search versions. For graph searches the main
differencesarethatdepth-firstsearchiscompleteforfinitestatespacesandthatthespaceand
timecomplexities arebounded bythesizeofthestatespace.
Breadth- Uniform- Depth- Depth- Iterative Bidirectional
Criterion
First Cost First Limited Deepening ifapplicable
Complete? Yesa Yesab No No Yesa Yesad
Time Obd Ob1cid:2 Ccid:2cid:3 Obm Obcid:3 Obd Obd2
Space Obd Ob1cid:2 Ccid:2cid:3 Obm Obcid:3 Obd Obd2
Optimal? Yesc Yes No No Yesc Yescd
Figure3.21 Evaluationoftree-searchstrategies. bisthebranchingfactor; disthedepth
ofthe shallowestsolution; m isthe maximumdepthofthe searchtree; l isthe depthlimit.
Superscriptcaveatsareasfollows: a completeifbisfinite; b completeifstepcosts cid:2for
positivecid:2;coptimalifstepcostsareallidentical;difbothdirectionsusebreadth-firstsearch.
Thissectionshowshowaninformedsearchstrategyonethatusesproblem-specificknowl-
I NF OR ME DS EA RC H
edgebeyondthedefinitionoftheproblemitselfcanfindsolutionsmoreefficientlythancan
anuninformedstrategy.
The general approach we consider is called best-first search. Best-first search is an
B ES T-F IR ST SE AR CH
instance of the general T RE E-S EA RC H or G RA PH-S EA RC H algorithm in which a node is
E VA LU AT IO N selected for expansion based on an evaluation function fn. The evaluation function is
F UN CT IO N
construed as a cost estimate so the node with the lowest evaluation is expanded first. The
implementation of best-first graph search is identical to that for uniform-cost search Fig-
ure3.14exceptfortheuseoff insteadofg toorderthepriority queue.
Thechoice off determines thesearch strategy. Forexample as Exercise 3.21shows
best-first tree search includes depth-first search asaspecial case. Mostbest-first algorithms
H EU RI ST IC includeasacomponent off aheuristicfunctiondenoted hn:
F UN CT IO N
hn estimatedcostofthecheapest pathfromthestateatnode ntoagoalstate.
Notice that hn takes anode as input but unlike gn itdepends only on the state at that
node. Forexamplein Romaniaonemightestimatethecostofthecheapestpathfrom Arad
to Bucharest viathestraight-line distance from Aradto Bucharest.
Heuristic functions are the most common form in which additional knowledge of the
problemisimpartedtothesearchalgorithm. Westudyheuristicsinmoredepthin Section3.6.
Fornowweconsiderthemtobearbitrary nonnegative problem-specificfunctions withone
constraint: ifnisagoalnodethenhn0. Theremainderofthissection coverstwoways
touseheuristic information toguidesearch.
G RE ED YB ES T-F IR ST Greedybest-firstsearch8 triestoexpandthenodethatisclosest tothegoal onthegrounds
S EA RC H
that this is likely to lead to a solution quickly. Thus it evaluates nodes by using just the
heuristic function; thatisfn hn.
Letusseehowthisworksforroute-finding problemsin Romania;weusethestraight-
S TR AI GH T-L IN E line distance heuristic which we will call h . If the goal is Bucharest we need to
D IS TA NC E S LD
know the straight-line distances to Bucharest which are shown in Figure 3.22. For exam-
ple h In Arad366. Notice that the values of h cannot be computed from the
S LD S LD
problem description itself. Moreover it takes a certain amount of experience to know that
h iscorrelated withactualroaddistances andistherefore ausefulheuristic.
S LD
S LD
from Arad to Bucharest. The first node to be expanded from Arad will be Sibiu because it
is closer to Bucharest than either Zerind or Timisoara. The next node to be expanded will
be Fagaras because it is closest. Fagaras in turn generates Bucharest which is the goal. For
this particular problem greedy best-first search using h finds a solution without ever
S LD
usageofthelattertermfollows Pearl1984.
Section3.5. Informed Heuristic Search Strategies 93
Arad 366 Mehadia 241
Bucharest 0 Neamt 234
Craiova 160 Oradea 380
Drobeta 242 Pitesti 100
Eforie 161 Rimnicu Vilcea 193
Fagaras 176 Sibiu 253
Giurgiu 77 Timisoara 329
Hirsova 151 Urziceni 80
Iasi 226 Vaslui 199
Lugoj 244 Zerind 374
Figure3.22 Valuesofh S LDstraight-linedistancesto Bucharest.
expanding a node that is not on the solution path; hence its search cost is minimal. It is
not optimal however: the path via Sibiu and Fagaras to Bucharest is 32 kilometers longer
than the path through Rimnicu Vilcea and Pitesti. This shows why the algorithm is called
greedyat eachstepittriestogetasclosetothegoalasitcan.
Greedy best-first tree search is also incomplete even in a finite state space much like
depth-first search. Consider the problem of getting from Iasi to Fagaras. The heuristic sug-
gests that Neamt be expanded first because it is closest to Fagaras but it is a dead end. The
solution is to go first to Vasluia step that is actually farther from the goal according to
the heuristicand then to continue to Urziceni Bucharest and Fagaras. The algorithm will
never find this solution however because expanding Neamt puts Iasi back into the frontier
Iasiiscloserto Fagarasthan Vasluiisandso Iasiwillbeexpanded again leading toaninfi-
niteloop. Thegraphsearchversion iscompleteinfinitespacesbutnotininfiniteones. The
worst-casetimeandspacecomplexityforthetreeversionis Obmwheremisthemaximum
depth of the search space. With a good heuristic function however the complexity can be
reducedsubstantially. Theamountofthereductiondepends ontheparticularproblemandon
thequalityoftheheuristic.
The most widely known form of best-first search is called A search pronounced A-star
A S EA RC H
search. Itevaluatesnodesbycombininggnthecosttoreachthenodeandhnthecost
togetfromthenodetothegoal:
fn gnhn.
Since gn gives the path cost from the start node to node n and hn is the estimated cost
ofthecheapest pathfrom ntothegoalwehave
fn estimatedcostofthecheapest solution through n.
Thus if we are trying to find the cheapest solution a reasonable thing to try first is the
node with the lowest value of gn hn. It turns out that this strategy is more than just
""
reasonable: providedthattheheuristicfunction hnsatisfiescertainconditions A searchis
both complete and optimal. The algorithm is identical to U NI FO RM-C OS T-S EA RC H except
""
that A usesghinsteadofg.
a The initial state Arad
366
b After expanding Arad Arad
Sibiu Timisoara Zerind
c After expanding Sibiu Arad
Sibiu Timisoara Zerind
Arad Fagaras Oradea Rimnicu Vilcea
d After expanding Fagaras Arad
Sibiu Timisoara Zerind
Arad Fagaras Oradea Rimnicu Vilcea
Sibiu Bucharest
Figure3.23 Stages in a greedybest-first tree search for Bucharestwith the straight-line
distanceheuristich S LD. Nodesarelabeledwiththeirh-values.
Conditionsforoptimality: Admissibilityandconsistency
A DM IS SI BL E The first condition we require for optimality is that hn be an admissible heuristic. An
H EU RI ST IC
admissible heuristic isonethat never overestimates thecost toreach the goal. Because gn
is the actual cost to reach n along the current path and fngnhn we have as an
immediate consequence that fn never overestimates the true cost of a solution along the
currentpaththrough n.
Admissible heuristics are by nature optimistic because they think the cost of solving
the problem is less than it actually is. An obvious example of an admissible heuristic is the
straight-line distance h that we used in getting to Bucharest. Straight-line distance is
S LD
admissible because theshortest pathbetweenanytwopoints isastraight line sothestraight
Section3.5. Informed Heuristic Search Strategies 95
""
linecannot beanoverestimate. In Figure3.24weshowtheprogressofan A treesearchfor
Bucharest. Thevaluesof g arecomputed fromthestepcosts in Figure3.2andthevalues of
h aregivenin Figure3.22. Noticeinparticularthat Bucharestfirstappearsonthefrontier
S LD
at step e but it is not selected forexpansion because its f-cost 450 is higher than that of
Pitesti 417. Anotherwaytosay this isthatthere mightbeasolution through Pitesti whose
costisaslowas417sothealgorithm willnotsettleforasolution thatcosts450.
Asecond slightly strongercondition called consistency orsometimes monotonicity
C ON SI ST EN CY
isrequired only forapplications of A tograph search.9 Aheuristic hnisconsistent if for
M ON OT ON IC IT Y
cid:2
every node n and every successor n of n generated by any action a the estimated cost of
cid:2
reaching the goal from n is no greater than the step cost of getting to n plus the estimated
cid:2
costofreachingthegoalfrom n:
hn cnancid:2 hncid:2 .
T RI AN GL E Thisisaformofthegeneral triangleinequalitywhichstipulatesthateachsideofatriangle
I NE QU AL IT Y
cid:2
cannot be longer than the sum of the other two sides. Here the triangle is formed by n n
andthegoal G closestton. Foranadmissibleheuristic theinequality makesperfect sense:
n
cid:2
if there were a route from n to G via n that was cheaper than hn that would violate the
n
property that hnisalowerboundonthecosttoreach G .
n
Itisfairlyeasytoshow Exercise3.29thateveryconsistentheuristicisalsoadmissible.
Consistency is therefore a stricter requirement than admissibility but one has to work quite
hardtoconcoctheuristicsthatareadmissiblebutnotconsistent. Alltheadmissibleheuristics
we discuss in this chapter are also consistent. Consider for example h . We know that
S LD
the general triangle inequality is satisfied when each side is measured by the straight-line
cid:2 cid:2
distance and that the straight-line distance between n and n is no greater than cnan.
Henceh isaconsistent heuristic.
S LD
Optimalityof A
""
As we mentioned earlier A has the following properties: the tree-search version of A is
optimalifhnisadmissible whilethegraph-search versionisoptimalifhnisconsistent.
We show the second of these two claims since it is more useful. The argument es-
sentially mirrors the argument for the optimality of uniform-cost search with g replaced by
""
fjustasinthe A algorithm itself.
The first step is to establish the following: if hn is consistent then the values of
fn along any path are nondecreasing. The proof follows directly from the definition of
cid:2 cid:2 cid:2
consistency. Suppose n isasuccessor of n; then gngncnanforsome action
aandwehave
fncid:2 gncid:2 hncid:2 gncnancid:2 hncid:2 gnhn fn.
""
The next step is to prove that whenever A selects a node n for expansion the optimal path
to that node has been found. Were this not the case there would have to be another frontier
cid:2
node n on the optimal path from the start node to n by the graph separation property of
a The initial state
Arad
3660366
b After expanding Arad
Arad
Sibiu Timisoara Zerind
393140253 447118329 44975374
c After expanding Sibiu
Arad
Sibiu Timisoara Zerind
447118329 44975374
Arad Fagaras Oradea Rimnicu Vilcea
646280366 415239176671291380 413220193
d After expanding Rimnicu Vilcea
Arad
Sibiu Timisoara Zerind
447118329 44975374
Arad Fagaras Oradea Rimnicu Vilcea
646280366 415239176671291380
Craiova Pitesti Sibiu
526366160417317100 553300253
e After expanding Fagaras
Arad
Sibiu Timisoara Zerind
447118329 44975374
Arad Fagaras Oradea Rimnicu Vilcea
646280366 671291380
Sibiu Bucharest Craiova Pitesti Sibiu
591338253 4504500 526366160 417317100 553300253
f After expanding Pitesti
Arad
Sibiu Timisoara Zerind
447118329 44975374
Arad Fagaras Oradea Rimnicu Vilcea
646280366 671291380
Sibiu Bucharest Craiova Pitesti Sibiu
591338253 4504500 526366160 553300253
Bucharest Craiova Rimnicu Vilcea
4184180 615455160 607414193
Figure3.24 Stagesinan Asearchfor Bucharest.Nodesarelabeledwithf gh. The
hvaluesarethestraight-linedistancesto Bucharesttakenfrom Figure3.22.
Section3.5. Informed Heuristic Search Strategies 97
O
N
Z
A I
F
V
400
T R
P
L
H
M U
B
420
D
E
C
G
Figure3.25 Mapof Romaniashowingcontoursatf 380f 400andf 420with
Aradas the start state. Nodesinside a givencontourhave f-costsless than orequalto the
contourvalue.
cid:2
andwouldhavebeenselectedfirst.
From the two preceding observations it follows that the sequence of nodes expanded
""
by A using G RA PH-S EA RC H is in nondecreasing order of fn. Hence the first goal node
selected forexpansion must be an optimal solution because f is the true cost forgoal nodes
whichhaveh0andalllatergoalnodeswillbeatleastasexpensive.
The fact that f-costs are nondecreasing along any path also means that we can draw
contours in the state space just like the contours in a topographic map. Figure 3.25 shows
C ON TO UR
an example. Inside the contour labeled 400 all nodes have fn less than or equal to 400
""
and so on. Then because A expands the frontier node of lowest f-cost we can see that an
""
A searchfansoutfromthestartnodeaddingnodesinconcentricbandsofincreasingf-cost.
""
With uniform-cost search A search using hn 0 the bands will be circular
around the start state. With more accurate heuristics the bands will stretch toward the goal
""
state and become more narrowly focused around the optimal path. If C is the cost of the
optimalsolutionpaththenwecansaythefollowing:
A expandsallnodeswithfn C .
A mightthenexpandsomeofthenodesrightonthegoalcontourwherefn C
beforeselecting agoalnode.
Completeness requires that there be only finitely many nodes with cost less than orequal to
""
C acondition thatistrueifallstepcostsexceedsomefinitecid:2andifbisfinite.
""
Notice that A expands no nodes with fn C for example Timisoara is not
expanded in Figure 3.24 even though it is a child of the root. Wesay that the subtree below
Timisoaraispruned;becauseh isadmissiblethealgorithmcansafelyignorethissubtree
P RU NI NG S LD
while still guaranteeing optimality. The concept of pruningeliminating possibilities from
consideration withouthavingtoexaminethemisimportant formanyareasof A I.
One final observation is that among optimal algorithms of this typealgorithms that
""
extend search paths from the root and use the same heuristic information A is optimally
O PT IM AL LY efficient for any given consistent heuristic. That is no other optimal algorithm is guaran-
E FF IC IE NT
""
teedtoexpand fewernodes than A except possibly through tie-breaking amongnodeswith
""
fn C . This is because any algorithm that does not expand all nodes with fn C
runstheriskofmissingtheoptimalsolution.
""
That A search iscomplete optimal andoptimally efficientamongallsuchalgorithms
""
israthersatisfying. Unfortunately itdoesnotmeanthat A istheanswertoalloursearching
needs. The catch is that for most problems the number of states within the goal contour
search space is still exponential in the length of the solution. The details of the analysis are
beyondthescopeofthisbookbutthebasicresultsareasfollows. Forproblemswithconstant
stepcosts thegrowthinruntimeasafunction oftheoptimal solution depthdisanalyzed in
terms of the the absolute error or the relative error of the heuristic. The absolute error is
A BS OL UT EE RR OR
defined as Δ h h where h is the actual cost of getting from the root to the goal and
R EL AT IV EE RR OR
therelativeerrorisdefinedas cid:2 h hh .
The complexity results depend very strongly on the assumptions made about the state
space. The simplest model studied is a state space that has a single goal and is essentially a
tree with reversible actions. The 8-puzzle satisfies the first and third of these assumptions.
""
Inthiscase thetimecomplexity of A isexponential inthemaximum absolute error that is
ObΔ. For constant step costs we can write this as Obcid:2d where d is the solution depth.
Foralmostallheuristics inpracticalusetheabsoluteerrorisatleastproportional tothepath
""
cost h so cid:2 is constant or growing and the time complexity is exponential in d. We can
alsoseetheeffectofamoreaccurateheuristic: Obcid:2d Obcid:2dsotheeffectivebranching
factordefinedmoreformallyinthenextsection is bcid:2.
Whenthestate spacehasmanygoalstatesparticularly near-optimal goalstatesthe
searchprocesscanbeledastrayfromtheoptimalpathandthereisanextracostproportional
to the number of goals whose cost is within a factor cid:2 of the optimal cost. Finally in the
general case ofa graph the situation is even worse. There can be exponentially many states
""
with fn C even if the absolute error is bounded by a constant. Forexample consider
aversionofthevacuum worldwheretheagentcanclean upanysquare forunitcostwithout
evenhavingtovisitit: inthatcasesquarescanbecleanedinanyorder. With N initiallydirty
squares there are 2 N states where some subset has been cleaned and all of them are on an
""
optimalsolutionpathandhencesatisfyfn C eveniftheheuristichasanerrorof1.
""
Thecomplexityof A oftenmakesitimpracticaltoinsistonfindinganoptimalsolution.
""
One can use variants of A that find suboptimal solutions quickly or one can sometimes
design heuristics that are more accurate but not strictly admissible. In any case the use of a
goodheuristic stillprovidesenormoussavingscomparedto theuseofanuninformed search.
In Section3.6welookatthequestion ofdesigning goodheuristics.
""
Computation timeisnot however A smaindrawback. Becauseitkeepsallgenerated
""
nodes inmemory asdo all G RA PH-S EA RC H algorithms A usually runs outof space long
Section3.5. Informed Heuristic Search Strategies 99
function R EC UR SI VE-B ES T-F IR ST-S EA RC Hproblemreturnsasolutionorfailure
return R BF Sproblem M AK E-N OD Eproblem.I NI TI AL-S TA TE
function R BF Sproblemnodef limitreturnsasolutionorfailureandanewf-costlimit
ifproblem.G OA L-T ES Tnode.S TA TEthenreturn S OL UT IO Nnode
successors
foreachaction inproblem.A CT IO NSnode.S TA TEdo
add C HI LD-N OD Eproblemnodeaction intosuccessors
ifsuccessors isemptythenreturnfailure
foreachs insuccessors do updatef withvaluefromprevioussearchifany
s.f maxs.g s.h node.f
loopdo
bestthelowestf-valuenodeinsuccessors
ifbest.f f limit thenreturnfailurebest.f
alternativethesecond-lowestf-valueamongsuccessors
resultbest.f R BF Sproblembestminf limitalternative
ifresult cid:7failure thenreturnresult
Figure3.26 Thealgorithmforrecursivebest-firstsearch.
""
before it runs out of time. For this reason A is not practical for many large-scale prob-
lems. There are however algorithms that overcome the space problem without sacrificing
optimality orcompleteness atasmallcostinexecution time. Wediscussthesenext.
""
The simplest way to reduce memory requirements for A is to adapt the idea of iterative
I TE RA TI VE-
D EE PE NI NG deepening to the heuristic search context resulting in the iterative-deepening A I DAal-
A gorithm. Themaindifferencebetween I DA andstandarditerativedeepeningisthatthecutoff
usedisthef-costghratherthanthedepth;ateachiterationthecutoffvalueisthesmall-
""
est f-cost of any node that exceeded the cutoff on the previous iteration. I DA is practical
for many problems with unit step costs and avoids the substantial overhead associated with
keepingasortedqueueofnodes. Unfortunatelyitsuffersfromthesamedifficultieswithreal-
valued costs as does the iterative version of uniform-cost search described in Exercise 3.17.
""
Thissectionbrieflyexaminestwoothermemory-boundedalgorithmscalled R BF Sand M A.
R EC UR SI VE Recursive best-first search R BF S is a simple recursive algorithm that attempts to
B ES T-F IR ST SE AR CH
mimictheoperation ofstandard best-first search but using only linearspace. Thealgorithm
is shown in Figure 3.26. Its structure is similar to that of a recursive depth-first search but
ratherthan continuing indefinitely downthecurrent path ituses thef limit variable tokeep
track of the f-value of the best alternative path available from any ancestor of the current
node. If the current node exceeds this limit the recursion unwinds back to the alternative
path. As the recursion unwinds R BF S replaces the f-value of each node along the path
withabacked-upvaluethebestf-valueofitschildren. Inthisway R BF Sremembersthe
B AC KE D-U PV AL UE
f-value of the best leaf in the forgotten subtree and can therefore decide whether its worth
a After expanding Arad Sibiu
and Rimnicu Vilcea Arad 366
447
Sibiu Timisoara Zerind
393
415
Arad Fagaras Oradea Rimnicu Vilcea
413
Craiova Pitesti Sibiu
b After unwinding back to Sibiu
and expanding Fagaras Arad
366
447
Sibiu Timisoara Zerind
417
Arad Fagaras Oradea Rimnicu Vilcea
Sibiu Bucharest
c After switching back to Rimnicu Vilcea
and expanding Pitesti Arad
366
447
Sibiu Timisoara Zerind
447
Arad Fagaras Oradea Rimnicu Vilcea
447
Craiova Pitesti Sibiu
Bucharest Craiova Rimnicu Vilcea
valueforeachrecursivecallisshownontopofeachcurrentnodeandeverynodeislabeled
withitsf-cost.a Thepathvia Rimnicu Vilceaisfolloweduntilthecurrentbestleaf Pitesti
hasavaluethatisworsethanthebestalternativepath Fagaras. b Therecursionunwinds
andthe bestleaf value of the forgottensubtree417is backedup to Rimnicu Vilcea; then
Fagarasis expandedrevealinga bestleafvalueof 450. c Therecursionunwindsandthe
bestleafvalueoftheforgottensubtree450isbackedupto Fagaras;then Rimnicu Vilceais
expanded.Thistimebecausethebestalternativepaththrough Timisoaracostsatleast447
theexpansioncontinuesto Bucharest.
reexpandingthesubtreeatsomelatertime. Figure3.27showshow R BF Sreaches Bucharest.
""
R BF S is somewhat more efficient than I DA but still suffers from excessive node re-
generation. In the example in Figure 3.27 R BF S follows the path via Rimnicu Vilcea then
Section3.5. Informed Heuristic Search Strategies 101
changes its mind and tries Fagaras and then changes its mind back again. These mind
changes occur because every time the current best path is extended its f-value is likely to
increaseh is usually less optimistic for nodes closer to the goal. When this happens the
second-best path might become the best path so the search has to backtrack to follow it.
""
Eachmind change corresponds to aniteration of I DA and could require manyreexpansions
offorgotten nodestorecreatethebestpathandextenditone morenode.
""
Like A tree search R BF S is an optimal algorithm if the heuristic function hn is
admissible. Its space complexity is linear in the depth of the deepest optimal solution but
its time complexity is rather difficult to characterize: it depends both on the accuracy of the
heuristic function andonhowoftenthebestpathchanges asnodesareexpanded.
""
I DA and R BF S suffer from using too little memory. Between iterations I DA retains
only a single number: the current f-cost limit. R BF S retains more information in memory
butitusesonlylinearspace: evenifmorememorywereavailable R BF Shasnowaytomake
useofit. Becausetheyforgetmostofwhattheyhavedonebothalgorithmsmayendupreex-
pandingthesamestatesmanytimesover. Furthermoretheysufferthepotentiallyexponential
increase incomplexityassociated withredundant pathsingraphssee Section3.3.
It seems sensible therefore to use all available memory. Two algorithms that do this
""
are M A memory-bounded A and S MA simplified M A. S MA iswellsimpler so
M A
""
wewilldescribeit. S MA proceedsjustlike Aexpanding thebestleafuntilmemoryisfull.
S MA
""
Atthispointitcannotaddanewnodetothesearchtreewithoutdropping anoldone. S MA
""
always drops the worst leaf nodethe one with the highest f-value. Like R BF S S MA
then backs up the value of the forgotten node to its parent. In this way the ancestor of a
forgotten subtree knows the quality of the best path in that subtree. With this information
""
S MA regenerates thesubtree onlywhenallotherpaths havebeenshowntolookworsethan
thepathithasforgotten. Anotherwayofsayingthisisthat ifallthedescendants ofanoden
are forgotten then we will not know which way to go from n but we will still have an idea
ofhowworthwhileitistogoanywherefrom n.
Thecompletealgorithmistoocomplicatedtoreproducehere10 butthereisonesubtlety
""
worthmentioning. Wesaidthat S MA expands thebestleafanddeletestheworstleaf. What
if all the leaf nodes have the same f-value? To avoid selecting the same node for deletion
""
and expansion S MA expands the newest best leaf and deletes the oldest worst leaf. These
coincide whenthereisonlyoneleafbutinthatcasethecurrentsearchtreemustbeasingle
pathfromroottoleafthatfillsallofmemory. Iftheleafisnotagoalnodethenevenifitison
anoptimalsolutionpaththatsolutionisnotreachablewiththeavailablememory. Therefore
thenodecanbediscarded exactlyasifithadnosuccessors.
""
S MA is complete if there is any reachable solutionthat is if d the depth of the
shallowest goal node is less than the memory size expressed in nodes. It is optimal if any
optimal solution is reachable; otherwise it returns the best reachable solution. In practical
""
terms S MA isafairlyrobustchoiceforfindingoptimalsolutionsparticularlywhenthestate
space is a graph step costs are not uniform and node generation is expensive compared to
theoverheadofmaintaining thefrontierandtheexploredset.
""
Onveryhardproblemshoweveritwilloftenbethecasethat S MA isforcedtoswitch
backandforthcontinuallyamongmanycandidatesolutionpathsonlyasmallsubsetofwhich
canfitinmemory. Thisresembles the problem of thrashingindisk paging systems. Then
T HR AS HI NG
the extra time required for repeated regeneration of the same nodes means that problems
""
that would be practically solvable by A given unlimited memory become intractable for
""
S MA. That is to say memory limitations can make a problem intractable from the point
of view of computation time. Although no current theory explains the tradeoff between time
and memory it seems that this is an inescapable problem. The only way out is to drop the
optimality requirement.
We have presented several fixed strategiesbreadth-first greedy best-first and so onthat
have been designed by computer scientists. Could anagent learn how to search better? The
M ET AL EV EL ST AT E answerisyesandthemethodrestsonanimportantconceptcalledthemetalevelstatespace.
S PA CE
Eachstateinametalevelstatespacecaptures theinternal computational stateofaprogram
O BJ EC T-L EV EL ST AT E that is searching in an object-level state space such as Romania. For example the internal
S PA CE
""
stateofthe A algorithmconsistsofthecurrentsearchtree. Eachactioninthemetalevelstate
space is acomputation step that alters the internal state; forexample each computation step
""
in A expandsaleafnodeandaddsitssuccessors tothetree. Thus Figure3.24whichshows
asequence of larger and larger search trees can beseen asdepicting apath inthe metalevel
statespacewhereeachstateonthepathisanobject-level searchtree.
Nowthepathin Figure3.24hasfivestepsincludingonesteptheexpansionof Fagaras
that is not especially helpful. Forharder problems there will be many such missteps and a
M ET AL EV EL metalevellearningalgorithmcanlearnfromtheseexperiencestoavoidexploringunpromis-
L EA RN IN G
ing subtrees. Thetechniques used forthis kind of learning are described in Chapter21. The
goal of learning is to minimize the total cost of problem solving trading off computational
expenseandpathcost.
In this section we look at heuristics for the 8-puzzle in order to shed light on the nature of
heuristics ingeneral.
The 8-puzzle was one of the earliest heuristic search problems. As mentioned in Sec-
tion 3.2 the object of the puzzle is toslide the tiles horizontally orvertically into the empty
spaceuntiltheconfiguration matchesthegoalconfiguration Figure3.28.
Theaveragesolutioncostforarandomlygenerated8-puzzle instanceisabout22steps.
The branching factor is about 3. When the empty tile is in the middle four moves are
possible; when it is in a corner two; and when it is along an edge three. This means
that an exhaustive tree search to depth 22 would look at about 322 3.11010 states.
A graph search would cut this down by a factor of about 170000 because only 9!2
181440 distinct states are reachable. See Exercise 3.4. This is a manageable number but
Section3.6. Heuristic Functions 103
Start State Goal State
Figure3.28 Atypicalinstanceofthe8-puzzle.Thesolutionis26stepslong.
the corresponding numberforthe 15-puzzle isroughly 1013so thenext orderof business is
""
to find a good heuristic function. If we want to find the shortest solutions by using A we
needaheuristic function thatneveroverestimates thenumberofstepstothegoal. Thereisa
longhistory ofsuchheuristics forthe15-puzzle; herearetwocommonlyusedcandidates:
h the number of misplaced tiles. For Figure 3.28 all of the eight tiles are out of
1
position so the start state would have h 8. h is an admissible heuristic because it
isclearthatanytilethatisoutofplacemustbemovedatleastonce.
h the sum of the distances of the tiles from their goal positions. Because tiles
2
cannot move along diagonals the distance we will count is the sum of the horizontal
andverticaldistances. Thisissometimescalledthecityblockdistanceor Manhattan
M AN HA TT AN distance. h is also admissible because all any move can do is move one tile one step
D IS TA NC E 2
closertothegoal. Tiles1to8inthestartstategivea Manhattandistance of
h 31222332 18.
2
Asexpected neitheroftheseoverestimates thetruesolution costwhichis26.
""
E FF EC TI VE Onewaytocharacterize thequalityofaheuristicisthe effective branchingfactorb . Ifthe
B RA NC HI NG FA CT OR
""
totalnumberofnodesgeneratedby A foraparticularproblemis N andthesolutiondepthis
""
d then b is the branching factor that a uniform tree of depth d would have to have in order
tocontain N 1nodes. Thus
N 1 1b b 2b d .
""
For example if A finds a solution at depth 5 using 52 nodes then the effective branching
factor is 1.92. Theeffective branching factor can vary across problem instances but usually
it is fairly constant for sufficiently hard problems. The existence of an effective branching
""
factor follows from the result mentioned earlier that the number of nodes expanded by A
""
grows exponentially withsolution depth. Therefore experimental measurements of b on a
smallsetofproblems canprovide agood guide tothe heuristics overall usefulness. Awell-
""
designed heuristic would have a value of b close to 1 allowing fairly large problems to be
solvedatreasonable computational cost.
To test the heuristic functions h and h we generated 1200 random problems with
solution lengths from 2 to 24 100 for each even number and solved them with iterative
""
deepeningsearchandwith A treesearchusingbothh andh . Figure3.29givestheaverage
number of nodes generated by each strategy and the effective branching factor. The results
suggestthath isbetterthanh andisfarbetterthanusingiterativedeepening search. Even
for small problems with d12 A with h is 50000 times more efficient than uninformed
2
iterativedeepening search.
Search Costnodes generated Effective Branching Factor
""
d I DS Ah 1 Ah 2 I DS Ah 1 Ah 2
I TE RA TI VE-D EE PE NI NG-S EA RC H and A algorithmswith h 1 h 2. Data are averagedover
100instancesofthe8-puzzleforeachofvarioussolutionlengthsd.
Onemightaskwhetherh isalwaysbetterthanh . Theansweris Essentially yes. It
iseasytoseefromthedefinitions ofthetwoheuristics that foranynode nh n h n.
""
We thus say that h dominates h . Domination translates directly into efficiency: A using
D OM IN AT IO N 2 1
""
h will never expand more nodes than A using h except possibly for some nodes with
""
fn C . The argument is simple. Recall the observation on page 97 that every node
""
with fn C will surely be expanded. This is the same as saying that every node with
hn C gn will surely be expanded. But because h is at least as big as h for all
""
nodes everynodethat issurely expanded by A search withh willalsosurely beexpanded
2
with h and h might cause other nodes to be expanded as well. Hence it is generally
better to use a heuristic function with higher values provided it is consistent and that the
computation timefortheheuristic isnottoolong.
We have seen that both h misplaced tiles and h Manhattan distance are fairly good
heuristics forthe 8-puzzle and that h isbetter. Howmight one have comeup with h ? Isit
possible foracomputertoinventsuchaheuristicmechanically?
h andh areestimates oftheremaining pathlengthforthe8-puzzle buttheyarealso
perfectlyaccurate pathlengthsfor simplifiedversionsofthepuzzle. Iftherulesofthepuzzle
Section3.6. Heuristic Functions 105
werechangedsothatatilecouldmoveanywhereinsteadofjusttotheadjacentemptysquare
thenh wouldgivetheexactnumberofstepsintheshortestsolution. Similarlyifatilecould
1
moveonesquareinanydirectionevenontoanoccupiedsquarethenh wouldgivetheexact
2
number of steps in the shortest solution. A problem with fewerrestrictions on the actions is
called a relaxed problem. The state-space graph of the relaxed problem is a supergraph of
R EL AX ED PR OB LE M
theoriginalstatespacebecause theremovalofrestrictions createsaddededgesinthegraph.
Because the relaxed problem adds edges to the state space anyoptimal solution in the
original problem is by definition also a solution in the relaxed problem; but the relaxed
problem may have better solutions if the added edges provide short cuts. Hence the cost of
anoptimal solution toarelaxedproblem isanadmissible heuristic fortheoriginal problem.
Furthermore because the derived heuristic is an exact cost for the relaxed problem it must
obeythetriangleinequality andistherefore consistentseepage95.
Ifaproblem definition iswritten down inaformal language itis possible to construct
relaxedproblemsautomatically.11 Forexampleifthe8-puzzle actionsaredescribed as
Atilecanmovefromsquare Atosquare Bif
Aishorizontally orvertically adjacent to B and Bisblank
wecangeneratethreerelaxedproblemsbyremovingoneorbothoftheconditions:
a Atilecanmovefromsquare Atosquare Bif Aisadjacentto B.
b Atilecanmovefromsquare Atosquare Bif Bisblank.
c Atilecanmovefromsquare Atosquare B.
From a we can derive h Manhattan distance. The reasoning is that h would be the
properscoreifwemovedeachtileinturntoitsdestination. Theheuristic derivedfrombis
discussed in Exercise3.31. Fromcwecanderive h misplaced tilesbecause itwouldbe
1
theproperscore iftilescouldmovetotheirintended destination inonestep. Noticethatitis
crucialthattherelaxedproblemsgeneratedbythistechniquecanbesolvedessentiallywithout
searchbecausetherelaxedrulesallowtheproblemtobedecomposedintoeightindependent
subproblems. If the relaxed problem is hard to solve then the values of the corresponding
heuristic willbeexpensivetoobtain.12
Aprogram called A BS OL VE R cangenerate heuristics automatically from problem def-
initions using the relaxed problem method and various other techniques Prieditis 1993.
A BS OL VE R generated a new heuristic for the 8-puzzle that was better than any preexisting
heuristic andfoundthefirstusefulheuristicforthefamous Rubiks Cubepuzzle.
One problem with generating new heuristic functions is that one often fails to get a
single clearly best heuristic. If a collection of admissible heuristics h ...h is available
foraproblem andnoneofthemdominates anyoftheothers whichshould wechoose? Asit
turnsoutweneednotmakeachoice. Wecanhavethebestofallworldsbydefining
hn maxh n...h n.
manipulatedtheconstructionofrelaxedproblemscanbeautomated.Fornowweuse English.
sly.Thusthereisatradeoffbetweenaccuracyandcomputationtimeforheuristicfunctions.
Start State Goal State
Figure3.30 A subproblemofthe 8-puzzleinstance givenin Figure 3.28. Thetask is to
gettiles 1 2 3 and4 intotheircorrectpositionswithout worryingaboutwhathappensto
theothertiles.
This composite heuristic uses whichever function is most accurate on the node in question.
Becausethecomponentheuristicsareadmissible hisadmissible;itisalsoeasytoprovethat
hisconsistent. Furthermore hdominatesallofitscomponent heuristics.
Admissible heuristics canalsobederived fromthesolution costofasubproblemofagiven
S UB PR OB LE M
problem. For example Figure 3.30 shows a subproblem of the 8-puzzle instance in Fig-
ure3.28. Thesubproblem involves gettingtiles1234intotheircorrectpositions. Clearly
the cost of the optimal solution of this subproblem is a lower bound on the cost of the com-
pleteproblem. Itturnsouttobemoreaccuratethan Manhattan distance insomecases.
Theideabehind patterndatabases istostore theseexactsolution costs foreverypos-
P AT TE RN DA TA BA SE
sible subproblem instancein our example every possible configuration of the four tiles
and the blank. The locations of the other four tiles are irrelevant for the purposes of solv-
ing the subproblem but moves of those tiles do count toward the cost. Then we compute
an admissible heuristic h for each complete state encountered during a search simply by
D B
lookingupthecorrespondingsubproblemconfigurationinthedatabase. Thedatabaseitselfis
constructed bysearchingback13 fromthegoalandrecordingthecostofeachnewpatternen-
countered; theexpenseofthissearchisamortized overmany subsequent problem instances.
Thechoiceof1-2-3-4 isfairlyarbitrary; wecouldalsoconstruct databases for5-6-7-8
for2-4-6-8 andsoon. Eachdatabaseyieldsanadmissibleheuristic andtheseheuristics can
be combined as explained earlier by taking the maximum value. A combined heuristic of
thiskindismuchmoreaccuratethanthe Manhattandistance; thenumberofnodesgenerated
whensolvingrandom 15-puzzles canbereducedbyafactorof1000.
One might wonder whether the heuristics obtained from the 1-2-3-4 database and the
5-6-7-8couldbeaddedsincethetwosubproblems seemnottooverlap. Wouldthisstillgive
an admissible heuristic? The answer is no because the solutions of the 1-2-3-4 subproblem
and the 5-6-7-8 subproblem for a given state will almost certainly share some movesit is
available.Thisisanexampleofdynamicprogrammingwhichwediscussfurtherin Chapter17.
Section3.6. Heuristic Functions 107
unlikely that1-2-3-4 can bemovedinto place without touching 5-6-7-8 andvice versa. But
whatifwedont count those moves? Thatis werecord notthetotal costofsolving the1-2-
3-4 subproblem but just the number of moves involving 1-2-3-4. Then it is easy to see that
thesumofthetwocostsisstillalowerboundonthecostofsolvingtheentireproblem. This
D IS JO IN TP AT TE RN is the idea behind disjoint pattern databases. With such databases it is possible to solve
D AT AB AS ES
random 15-puzzles in a few millisecondsthe number of nodes generated is reduced by a
factorof10000compared withtheuseof Manhattan distance. For24-puzzles aspeedup of
roughly afactorofamillioncanbeobtained.
Disjoint pattern databases work for sliding-tile puzzles because the problem can be
dividedupinsuchawaythateachmoveaffectsonlyonesubproblembecause onlyonetile
ismoved atatime. Foraproblem such as Rubiks Cube this kind ofsubdivision isdifficult
because each move affects 8 or 9 of the 26 cubies. More general ways of defining additive
admissible heuristics have been proposed that do apply to Rubiks cube Yang et al. 2008
buttheyhavenotyieldedaheuristicbetterthanthebestnonadditiveheuristicfortheproblem.
A heuristic function hn is supposed to estimate the cost of a solution beginning from the
state at node n. How could an agent construct such a function? One solution was given in
the preceding sectionsnamely to devise relaxed problems for which an optimal solution
can befound easily. Anothersolution isto learn from experience. Experience here means
solvinglotsof8-puzzlesforinstance. Eachoptimalsolutiontoan8-puzzleproblemprovides
examples from which hn can be learned. Each example consists of a state from the solu-
tionpathandtheactualcostofthesolution fromthatpoint. Fromtheseexamples alearning
algorithm canbeusedtoconstruct afunction hnthatcanwithluckpredictsolution costs
forotherstates thatarise during search. Techniques fordoing just thisusing neural nets de-
cisiontreesandothermethodsaredemonstrated in Chapter 18. Thereinforcement learning
methodsdescribed in Chapter21arealsoapplicable.
Inductive learning methods work best when supplied with features of a state that are
F EA TU RE
relevant to predicting the states value rather than with just the raw state description. For
example the feature number of misplaced tiles might be helpful in predicting the actual
distance of a state from the goal. Lets call this feature x n. Wecould take 100 randomly
1
generated 8-puzzle configurations and gather statistics on their actual solution costs. We
might find that when x n is 5 the average solution cost is around 14 and so on. Given
1
thesedatathevalueofx canbeusedtopredicthn. Ofcoursewecanuseseveralfeatures.
1
Asecondfeaturex nmightbenumberofpairsofadjacenttilesthatarenotadjacentinthe
2
goalstate. Howshouldx nandx nbecombinedtopredicthn? Acommonapproach
istousealinearcombination:
hn c x nc x n.
The constants c and c are adjusted to give the best fit to the actual data on solution costs.
Oneexpectsbothc andc tobepositivebecausemisplacedtilesandincorrectadjacentpairs
make the problem harder to solve. Notice that this heuristic does satisfy the condition that
hn0forgoalstatesbutitisnotnecessarily admissibleorconsistent.
This chapter has introduced methods that an agent can use to select actions in environments
thataredeterministic observable staticandcompletelyknown. Insuchcasestheagentcan
construct sequences ofactionsthatachieveitsgoals;thisprocess iscalled search.
Before anagent can start searching forsolutions a goal must beidentified and awell-
definedproblemmustbeformulated.
Aproblem consists of fiveparts: the initial state aset ofactions atransition model
describing the results of those actions a goal test function and a path cost function.
The environment of the problem is represented by a state space. A path through the
statespacefromtheinitialstatetoagoalstateisasolution.
Search algorithms treat states and actions as atomic: theydo not consider anyinternal
structuretheymightpossess.
A general T RE E-S EA RC H algorithm considers all possible paths to find a solution
whereasa G RA PH-S EA RC H algorithm avoids consideration ofredundant paths.
Searchalgorithmsarejudgedonthebasisofcompletenessoptimalitytimecomplex-
ity and space complexity. Complexity depends onb the branching factor inthe state
spaceanddthedepthoftheshallowestsolution.
Uninformed search methods have access only to the problem definition. The basic
algorithmsareasfollows:
Breadth-first search expands the shallowest nodes first; it is complete optimal
forunitstepcosts buthasexponential spacecomplexity.
Uniform-costsearchexpandsthenodewithlowestpathcostgnandisoptimal
forgeneralstepcosts.
Depth-firstsearch expands the deepest unexpanded node first. It is neither com-
plete nor optimal but has linear space complexity. Depth-limited search adds a
depthbound.
Iterative deepening search calls depth-first search with increasing depth limits
untilagoalisfound. Itiscompleteoptimalforunitstepcostshastimecomplexity
comparable tobreadth-first searchandhaslinearspacecomplexity.
Bidirectionalsearchcanenormously reducetimecomplexitybutitisnotalways
applicable andmayrequiretoomuchspace.
Informedsearchmethodsmayhaveaccesstoaheuristicfunctionhnthatestimates
thecostofasolution fromn.
Thegeneric best-firstsearch algorithm selectsanodeforexpansion according to
anevaluationfunction.
Greedybest-first search expands nodes withminimalhn. Itisnot optimal but
isoftenefficient.
4
B EY ON D C LA SS IC AL
S EA RC H
In which we relax the simplifying assumptions of the previous chapter thereby
gettingclosertotherealworld.
ronmentswherethesolutionisasequenceofactions. Inthischapterwelookatwhathappens
whentheseassumptionsarerelaxed. Webeginwithafairlysimplecase: Sections4.1and4.2
coveralgorithms thatperform purely localsearch inthestate space evaluating andmodify-
ingoneormorecurrentstatesratherthansystematically exploringpathsfromaninitialstate.
Thesealgorithms are suitable forproblems inwhich allthat mattersisthe solution state not
thepathcost toreach it. Thefamilyoflocal search algorithms includes methods inspired by
statistical physicssimulatedannealingandevolutionary biology geneticalgorithms.
Then in Sections 4.34.4 we examine what happens when we relax the assumptions
ofdeterminismandobservability. Thekeyideaisthatifanagentcannotpredictexactlywhat
percept it will receive then it will need to consider what to do under each contingency that
its percepts may reveal. With partial observability the agent will also need to keep track of
thestatesitmightbein.
Finally Section 4.5investigates onlinesearch inwhichtheagent isfaced withastate
spacethatisinitially unknownandmustbeexplored.
The search algorithms that we have seen so far are designed to explore search spaces sys-
tematically. Thissystematicity isachieved by keeping one ormore paths in memory and by
recordingwhichalternativeshavebeenexploredateachpointalongthepath. Whenagoalis
foundthepathtothatgoalalsoconstitutesasolutiontotheproblem. Inmanyproblemshow-
ever the path to the goal is irrelevant. Forexample in the 8-queens problem see page 71
whatmattersisthefinalconfiguration ofqueens nottheorderinwhichthey areadded. The
same general property holds for many important applications such as integrated-circuit de-
signfactory-floorlayoutjob-shopschedulingautomaticprogrammingtelecommunications
networkoptimization vehiclerouting andportfolio management.
120
Section4.1. Local Search Algorithmsand Optimization Problems 121
If the path to the goal does not matter we might consider a different class of algo-
rithms ones that do not worry about paths at all. Local search algorithms operate using
L OC AL SE AR CH
a single current node rather than multiple paths and generally move only to neighbors
C UR RE NT NO DE
of that node. Typically the paths followed by the search are not retained. Although local
search algorithms are not systematic they have two key advantages: 1 they use very little
memoryusuallyaconstantamount;and2theycanoftenfindreasonablesolutionsinlarge
orinfinitecontinuous statespacesforwhichsystematic algorithmsareunsuitable.
In addition to finding goals local search algorithms are useful for solving pure op-
O PT IM IZ AT IO N timization problems in which the aim is to find the best state according to an objective
P RO BL EM
O BJ EC TI VE function. Many optimization problems do not fitthe standard search model introduced in
F UN CT IO N
Darwinian evolution could be seen as attempting tooptimize but there isno goal test and
nopathcostforthisproblem.
S TA TE-S PA CE Tounderstand localsearch wefindituseful toconsiderthe state-space landscapeas
L AN DS CA PE
in Figure4.1. Alandscapehasbothlocationdefinedbythestateandelevationdefined
by the value ofthe heuristic cost function orobjective function. Ifelevation corresponds to
cost then the aim is to find the lowest valleya global minimum; if elevation corresponds
G LO BA LM IN IM UM
toanobjective function thentheaim istofindthehighest peaka globalmaximum. You
G LO BA LM AX IM UM
can convert from one to the other just by inserting a minus sign. Local search algorithms
explore this landscape. A complete local search algorithm always finds a goal ifone exists;
anoptimalalgorithm alwaysfindsaglobalminimummaximum.
objective function
global maximum
shoulder
local maximum
flat local maximum
state space
current
state
Figure4.1 Aone-dimensionalstate-spacelandscapeinwhichelevationcorrespondstothe
objectivefunction. The aim is to find the globalmaximum. Hill-climbingsearch modifies
thecurrentstatetotrytoimproveitasshownbythearrow.Thevarioustopographicfeatures
aredefinedinthetext.
function H IL L-C LI MB IN Gproblemreturnsastatethatisalocalmaximum
current M AK E-N OD Eproblem.I NI TI AL-S TA TE
loopdo
neighborahighest-valuedsuccessorofcurrent
ifneighbor.V AL UEcurrent.V AL UEthenreturncurrent.S TA TE
currentneighbor
Figure4.2 Thehill-climbingsearchalgorithmwhichisthemostbasiclocalsearchtech-
nique. At each step the currentnode is replaced by the best neighbor; in this version that
means the neighborwith the highest V AL UE but if a heuristic cost estimate h is used we
wouldfindtheneighborwiththelowesth.
The hill-climbing search algorithm steepest-ascent version is shown in Figure 4.2. It is
H IL LC LI MB IN G
simply a loop that continually moves in the direction of increasing valuethat is uphill. It
S TE EP ES TA SC EN T
terminates when it reaches a peak where no neighbor has a higher value. The algorithm
does not maintain a search tree so the data structure for the current node need only record
the state and the value of the objective function. Hill climbing does not look ahead beyond
the immediate neighbors of thecurrent state. Thisresembles trying tofind thetop of Mount
Everestinathickfogwhilesufferingfromamnesia.
To illustrate hill climbing we will use the 8-queens problem introduced on page 71.
Local search algorithms typically use a complete-state formulation where each state has
generated bymoving asingle queen toanothersquare inthesamecolumn soeachstate has
8756 successors. The heuristic cost function h is the number of pairs of queens that
are attacking each other either directly or indirectly. The global minimum of this function
iszero whichoccurs only atperfect solutions. Figure 4.3a showsastate withh17. The
figure also shows the values of all its successors with the best successors having h12.
Hill-climbingalgorithmstypicallychooserandomlyamongthesetofbestsuccessorsifthere
ismorethanone.
G RE ED YL OC AL Hillclimbingissometimescalledgreedylocalsearchbecauseitgrabsagoodneighbor
S EA RC H
statewithoutthinkingaheadaboutwheretogonext. Althoughgreedisconsideredoneofthe
sevendeadly sinsitturnsoutthatgreedy algorithms often perform quitewell. Hillclimbing
oftenmakesrapidprogresstowardasolutionbecauseitisusuallyquiteeasytoimproveabad
state. For example from the state in Figure 4.3a it takes just five steps to reach the state
in Figure 4.3b which has h1 and is very nearly a solution. Unfortunately hill climbing
oftengetsstuckforthefollowingreasons:
Local maxima: a local maximum is a peak that is higher than each of its neighboring
L OC AL MA XI MU M
states but lower than the global maximum. Hill-climbing algorithms that reach the
vicinity of a local maximum will be drawn upward toward the peak but will then be
stuck with nowhere else to go. Figure 4.1 illustrates the problem schematically. More
Section4.1. Local Search Algorithmsand Optimization Problems 123
a b
Figure4.3 a An8-queensstatewithheuristiccostestimateh17showingthevalueof
hforeachpossiblesuccessorobtainedbymovingaqueenwithinitscolumn.Thebestmoves
aremarked. b Alocalminimuminthe8-queensstatespace;thestatehash1butevery
successorhasahighercost.
concretely thestate in Figure 4.3bisalocal maximumi.e.alocalminimum forthe
costh;everymoveofasinglequeenmakesthesituationworse.
Ridges: a ridge is shown in Figure 4.4. Ridges result in a sequence of local maxima
R ID GE
thatisverydifficultforgreedyalgorithms tonavigate.
Plateaux: a plateau is a flat area of the state-space landscape. It can be a flat local
P LA TE AU
maximum from which no uphill exit exists or a shoulder from which progress is
S HO UL DE R
possible. See Figure4.1. Ahill-climbing searchmightgetlostontheplateau.
Ineachcasethealgorithmreachesapointatwhichnoprogressisbeingmade. Startingfrom
arandomlygenerated8-queensstatesteepest-ascenthillclimbinggetsstuck86ofthetime
solvingonly14ofprobleminstances. Itworksquicklytakingjust4stepsonaveragewhen
itsucceeds and3whenitgetsstucknot badforastatespacewith88 17millionstates.
The algorithm in Figure 4.2 halts if it reaches a plateau where the best successor has
the same value as the current state. Might it not be a good idea to keep goingto allow a
sidewaysmoveinthehopethattheplateauisreallyashoulder asshownin Figure4.1? The
S ID EW AY SM OV E
answerisusuallyyesbutwemusttakecare. Ifwealwaysallowsidewaysmoveswhenthere
are no uphill moves an infinite loop will occur whenever the algorithm reaches a flat local
maximumthatisnotashoulder. Onecommonsolutionistoputalimitonthenumberofcon-
secutive sideways moves allowed. Forexample we could allow up to say 100 consecutive
sideways moves in the 8-queens problem. This raises the percentage of problem instances
solved by hill climbing from 14 to 94. Success comes at a cost: the algorithm averages
roughly 21stepsforeachsuccessful instance and64foreach failure.
Figure4.4 Illustrationofwhyridgescausedifficultiesforhillclimbing.Thegridofstates
darkcirclesissuperimposedonaridgerisingfromlefttorightcreatingasequenceoflocal
maxima that are not directly connected to each other. From each local maximum all the
availableactionspointdownhill.
S TO CH AS TI CH IL L Manyvariantsofhillclimbinghavebeeninvented. Stochastichillclimbingchoosesat
C LI MB IN G
randomfromamongtheuphillmoves;theprobabilityofselectioncanvarywiththesteepness
of the uphill move. This usually converges more slowly than steepest ascent but in some
F IR ST-C HO IC EH IL L state landscapes it finds better solutions. First-choice hill climbing implements stochastic
C LI MB IN G
hillclimbing bygenerating successors randomly until oneisgenerated thatisbetterthanthe
currentstate. Thisisagoodstrategywhenastatehasmanye.g.thousands ofsuccessors.
The hill-climbing algorithms described so far are incompletethey often fail to find
a goal when one exists because they can get stuck on local maxima. Random-restart hill
R AN DO M-R ES TA RT climbingadopts thewell-known adage Ifatfirstyou dont succeed try try again. Itcon-
H IL LC LI MB IN G
ducts a series of hill-climbing searches from randomly generated initial states1 until a goal
is found. It is trivially complete with probability approaching 1 because it will eventually
generate a goal state as the initial state. If each hill-climbing search has a probability p of
success then the expected number of restarts required is 1p. For 8-queens instances with
nosideways movesallowed p 0.14soweneed roughly 7iterations tofindagoal 6fail-
uresand1success. Theexpectednumberofstepsisthecostofonesuccessful iterationplus
1pptimesthecostoffailureorroughly22stepsinall. Whenweallowsidewaysmoves
10.94 1.06iterationsareneededonaverageand 1210.060.9464 25steps.
For8-queens then random-restart hillclimbingisveryeffectiveindeed. Evenforthreemil-
lionqueens theapproach canfindsolutions inunderaminute.2
fixed amount of time and that this can be much more efficient than letting each search continue indefinitely.
Disallowingorlimitingthenumberofsidewaysmovesisanexampleofthisidea.
Section4.1. Local Search Algorithmsand Optimization Problems 125
The success of hill climbing depends very much on the shape of the state-space land-
scape: if there are few local maxima and plateaux random-restart hill climbing will find a
good solution very quickly. On the other hand many real problems have a landscape that
looksmorelikeawidelyscatteredfamilyofbaldingporcupinesonaflatfloorwithminiature
porcupines living on the tip of each porcupine needle ad infinitum. N P-hard problems typi-
callyhaveanexponential numberoflocalmaximatogetstuckon. Despitethisareasonably
goodlocalmaximumcanoftenbefoundafterasmallnumberofrestarts.
Ahill-climbingalgorithmthatnevermakesdownhillmovestowardstateswithlowervalue
or higher cost is guaranteed to be incomplete because it can get stuck on a local maxi-
mum. In contrast a purely random walkthat is moving to a successor chosen uniformly
at random from the set of successorsis complete but extremely inefficient. Therefore it
seemsreasonabletotrytocombinehillclimbingwitharandomwalkinsomewaythatyields
S IM UL AT ED bothefficiencyandcompleteness. Simulatedannealingissuchanalgorithm. Inmetallurgy
A NN EA LI NG
annealing is the process used to temper or harden metals and glass by heating them to a
hightemperature andthengradually coolingthemthusallowingthematerialtoreachalow-
energy crystalline state. To explain simulated annealing we switch our point of view from
hill climbing to gradient descent i.e. minimizing cost and imagine the task of getting a
G RA DI EN TD ES CE NT
ping-pong ball into thedeepest crevice inabumpy surface. Ifwejustlet theball roll itwill
come to rest at a local minimum. If weshake the surface wecan bounce the ball out of the
local minimum. The trick is to shake just hard enough to bounce the ball out of local min-
ima but not hard enough to dislodge it from the global minimum. The simulated-annealing
solution istostartbyshaking hardi.e.atahightemperature andthengradually reduce the
intensity oftheshaking i.e.lowerthetemperature.
Theinnermostloopofthesimulated-annealing algorithm Figure4.5isquitesimilarto
hillclimbing. Insteadofpickingthebestmovehoweveritpicksarandommove. Ifthemove
improvesthesituationitisalwaysaccepted. Otherwisethealgorithmacceptsthemovewith
some probability less than 1. The probability decreases exponentially with the badness of
the movethe amount Δ E by which the evaluation is worsened. The probability also de-
creasesasthetemperature T goesdown: badmovesaremorelikelytobeallowedatthe
startwhen T ishighandtheybecomemoreunlikely as T decreases. Iftheschedule lowers
T slowlyenough thealgorithm willfindaglobaloptimumwithprobability approaching 1.
Simulated annealing was first used extensively to solve V LS I layout problems in the
early1980s. Ithasbeenapplied widelytofactoryscheduling andotherlarge-scale optimiza-
tiontasks. In Exercise4.4youareaskedtocompareitsperformancetothatofrandom-restart
hillclimbingonthe8-queens puzzle.
Keeping just one node in memory might seem to be an extreme reaction to the problem of
L OC AL BE AM memory limitations. The local beam search algorithm3 keeps track of k states rather than
S EA RC H
function S IM UL AT ED-A NN EA LI NGproblemschedulereturnsasolutionstate
inputs:problemaproblem
scheduleamappingfromtimetotemperature
current M AK E-N OD Eproblem.I NI TI AL-S TA TE
fort 1todo
Tschedulet
if T 0thenreturncurrent
nextarandomlyselectedsuccessorofcurrent
Δ Enext.V AL UEcurrent.V AL UE
ifΔ E 0thencurrentnext
elsecurrentnext onlywithprobabilityeΔ ET
Figure4.5 Thesimulatedannealingalgorithmaversionofstochastichillclimbingwhere
somedownhillmovesareallowed. Downhillmovesareacceptedreadilyearlyintheanneal-
ingscheduleandthenlessoftenastimegoeson.Theschedule inputdeterminesthevalueof
thetemperature T asafunctionoftime.
justone. Itbegins withk randomly generated states. Ateach step allthesuccessors ofallk
states aregenerated. Ifanyoneisagoal thealgorithm halts. Otherwise itselects the k best
successors fromthecompletelistandrepeats.
At first sight a local beam search with k states might seem to be nothing more than
running k random restarts in parallel instead of in sequence. In fact the two algorithms
are quite different. In a random-restart search each search process runs independently of
the others. In a local beam search useful information is passed among the parallel search
threads. In effect the states that generate the best successors say to the others Come over
here the grass is greener! The algorithm quickly abandons unfruitful searches and moves
itsresources towherethemostprogress isbeingmade.
In its simplest form local beam search can suffer from a lack of diversity among the
k statesthey canquickly becomeconcentrated inasmallregionofthestate space making
thesearch little morethananexpensive version ofhillclimbing. Avariant called stochastic
S TO CH AS TI CB EA M beam search analogous to stochastic hill climbing helps alleviate this problem. Instead
S EA RC H
of choosing the best k from the the pool of candidate successors stochastic beam search
chooses k successors at random with the probability of choosing a given successor being
an increasing function of its value. Stochastic beam search bears some resemblance to the
process of natural selection whereby the successors offspring of a state organism
populate thenextgeneration according toitsvaluefitness.
G EN ET IC Ageneticalgorithmor G Aisavariantofstochasticbeamsearchinwhichsuccessorstates
A LG OR IT HM
are generated by combining two parent states rather than by modifying a single state. The
analogy tonatural selection isthesameasinstochastic beamsearch exceptthatnowweare
dealingwithsexualratherthanasexualreproduction.
Section4.1. Local Search Algorithmsand Optimization Problems 127
a b c d e
Initial Population Fitness Function Selection Crossover Mutation
Figure4.6 Thegeneticalgorithmillustratedfordigitstringsrepresenting8-queensstates.
The initial population in a is ranked by the fitness function in b resulting in pairs for
matinginc. Theyproduceoffspringindwhicharesubjecttomutationine.
""
Figure4.7 The8-queensstatescorrespondingtothefirsttwoparentsin Figure4.6cand
thefirstoffspringin Figure4.6d.Theshadedcolumnsarelostinthecrossoverstepandthe
unshadedcolumnsareretained.
Like beam searches G As begin with a set of k randomly generated states called the
population. Eachstateorindividualisrepresented asastringoverafinitealphabetmost
P OP UL AT IO N
commonlyastringof0sand1s. Forexamplean8-queensstatemustspecifythepositionsof
I ND IV ID UA L
8queens each inacolumn of 8squares and so requires 8 log 824 bits. Alternatively
2
thestatecouldberepresentedas8digitseachintherangefrom1to8. Wedemonstratelater
that the two encodings behave differently. Figure 4.6a shows a population of four 8-digit
stringsrepresenting 8-queens states.
The production of the next generation of states is shown in Figure 4.6be. In b
each state is rated by the objective function orin G Aterminology the fitnessfunction. A
F IT NE SS FU NC TI ON
fitness function should return higher values for better states so for the 8-queens problem
we use the number of nonattacking pairs of queens which has a value of 28 for a solution.
The values of the four states are 24 23 20 and 11. In this particular variant of the genetic
algorithm the probability of being chosen for reproducing is directly proportional to the
fitnessscoreandthepercentages areshownnexttotherawscores.
In c two pairs are selected at random for reproduction in accordance with the prob-
abilities in b. Notice that one individual is selected twice and one not at all.4 For each
pair to be mated a crossover point is chosen randomly from the positions in the string. In
C RO SS OV ER
Figure4.6thecrossoverpointsareafterthethirddigitin thefirstpairandafterthefifthdigit
inthesecondpair.5
In d the offspring themselves are created by crossing over the parent strings at the
crossoverpoint. Forexamplethefirstchildofthefirstpair getsthefirstthreedigitsfromthe
first parent and the remaining digits from the second parent whereas the second child gets
the first three digits from the second parent and the rest from the first parent. The 8-queens
states involved in this reproduction step are shown in Figure 4.7. The example shows that
whentwoparent statesarequitedifferent thecrossover operation canproduce astatethatis
a long way from either parent state. It is often the case that the population is quite diverse
earlyonintheprocesssocrossoverlikesimulatedannealingfrequentlytakeslargestepsin
the state space early in the search process and smaller steps later on when most individuals
arequitesimilar.
Finally in e each location is subject to random mutation with a small independent
M UT AT IO N
probability. One digit was mutated in the first third and fourth offspring. In the 8-queens
problem this corresponds to choosing a queen at random and moving it to a random square
initscolumn. Figure4.8describes analgorithm thatimplementsallthesesteps.
Like stochastic beam search genetic algorithms combine an uphill tendency with ran-
dom exploration and exchange of information among parallel search threads. The primary
advantage if any of genetic algorithms comes from the crossover operation. Yet it can be
shown mathematically that if the positions of the genetic code are permuted initially in a
random order crossover conveys no advantage. Intuitively the advantage comes from the
abilityofcrossovertocombinelargeblocksoflettersthat haveevolvedindependently toper-
form useful functions thus raising the level of granularity at which the search operates. For
exampleitcouldbethatputtingthefirstthreequeensinpositions 24and6wheretheydo
not attack each other constitutes a useful block that can be combined with other blocks to
construct asolution.
The theory of genetic algorithms explains how this works using the idea of a schema
S CH EM A
which is a substring in which some of the positions can be left unspecified. For example
the schema 246 describes all 8-queens states in which the first three queens are in
positions 2 4 and 6 respectively. Strings that match the schema such as 24613578 are
calledinstancesoftheschema. Itcanbeshownthatiftheaveragefitnessoftheinstances of
I NS TA NC E
aschemaisabovethemeanthenthenumberofinstancesoftheschemawithinthepopulation
willgrowovertime. Clearlythiseffectisunlikelytobesignificantifadjacentbitsaretotally
unrelated to each other because then there will be few contiguous blocks that provide a
consistent benefit. Genetic algorithms work best when schemata correspond to meaningful
componentsofasolution. Forexampleifthestringisarepresentationofanantennathenthe
schematamayrepresentcomponentsoftheantennasuchasreflectorsanddeflectors. Agood
thresholdarediscardedcanbeshowntoconvergefasterthantherandomversion Baumetal.1995.
hasa23chanceofbeinginthemiddleofadigitwhichresultsinanessentiallyarbitrarymutationofthatdigit.
Section4.2. Local Searchin Continuous Spaces 129
function G EN ET IC-A LG OR IT HMpopulation F IT NE SS-F Nreturnsanindividual
inputs:populationasetofindividuals
F IT NE SS-F Nafunctionthatmeasuresthefitnessofanindividual
repeat
new populationemptyset
fori 1to S IZ Epopulationdo
x R AN DO M-S EL EC TI ONpopulation F IT NE SS-F N
y R AN DO M-S EL EC TI ONpopulation F IT NE SS-F N
child R EP RO DU CExy
ifsmallrandomprobabilitythenchild M UT AT Echild
addchild tonew population
populationnew population
untilsomeindividualisfitenoughorenoughtimehaselapsed
returnthebestindividualinpopulationaccordingto F IT NE SS-F N
function R EP RO DU CExyreturnsanindividual
inputs:xyparentindividuals
n L EN GT Hx;crandomnumberfrom1ton
return A PP EN DS UB ST RI NGx1c S UB ST RI NGyc1n
producesonlyoneoffspringnottwo.
componentislikelytobegoodinavarietyofdifferentdesigns. Thissuggeststhatsuccessful
useofgenetic algorithmsrequires carefulengineering oftherepresentation.
Inpracticegeneticalgorithmshavehadawidespreadimpactonoptimizationproblems
such ascircuit layout and job-shop scheduling. Atpresent itisnot clearwhetherthe appeal
ofgeneticalgorithmsarisesfromtheirperformanceorfrom theiræstheticallypleasingorigins
in the theory of evolution. Much work remains to be done to identify the conditions under
whichgeneticalgorithms performwell.
In Chapter 2 we explained the distinction between discrete and continuous environments
pointing out that most real-world environments are continuous. Yet none of the algorithms
wehavedescribed exceptforfirst-choice hillclimbingand simulated annealing canhandle
continuous stateandactionspacesbecausetheyhaveinfinitebranchingfactors. Thissection
provides a very brief introduction to some local search techniques for finding optimal solu-
tions in continuous spaces. Theliterature on this topic is vast; many of the basic techniques
E VO LU TI ON A ND S EA RC H
The theory of evolution was developed in Charles Darwins On the Origin of
Speciesby Meansof Natural Selection1859andindependently by Alfred Russel
Wallace 1858. The central idea is simple: variations occur in reproduction and
will be preserved in successive generations approximately in proportion to their
effectonreproductive fitness.
Darwinstheorywasdevelopedwithnoknowledgeofhowthetraitsoforgan-
isms can be inherited and modified. The probabilistic laws governing these pro-
cesses were first identified by Gregor Mendel 1866 a monk who experimented
withsweetpeas. Muchlater Watsonand Crick1953identifiedthestructureofthe
D NA molecule and its alphabet A GT C adenine guanine thymine cytosine. In
thestandardmodelvariationoccursbothbypointmutationsinthelettersequence
and by crossover in which the D NAof an offspring is generated by combining
longsections of D NAfromeachparent.
Theanalogytolocalsearchalgorithmshasalreadybeendescribed; theprinci-
paldifferencebetweenstochasticbeamsearchandevolutionistheuseofsexualre-
production wherein successors are generated from multiple organisms rather than
just one. The actual mechanisms of evolution are however far richer than most
genetic algorithms allow. For example mutations can involve reversals duplica-
tionsandmovementoflargechunksof D NA;somevirusesborrow D NAfromone
organism andinsert itinanother; and therearetransposable genes thatdonothing
but copy themselves many thousands of times within the genome. There are even
genesthatpoison cellsfrompotential matesthatdonotcarrythegenethereby in-
creasingtheirownchancesofreplication. Mostimportantisthefactthatthegenes
themselves encode the mechanisms whereby the genome is reproduced and trans-
lated into an organism. In genetic algorithms those mechanisms are a separate
programthatisnotrepresented withinthestrings beingmanipulated.
Darwinian evolution may appear inefficient having generated blindly some
yearsbefore Darwinhowevertheotherwisegreat Frenchnaturalist Jean Lamarck
1809 proposed a theory of evolution whereby traits acquired by adaptation dur-
ing an organisms lifetime would be passed on to its offspring. Such a process
would be effective but does not seem to occur in nature. Much later James Bald-
win1896proposedasuperficiallysimilartheory: thatbehaviorlearnedduringan
organismslifetimecouldacceleratetherateofevolution. Unlike Lamarcks Bald-
winstheoryisentirelyconsistentwith Darwinianevolutionbecauseitreliesonse-
lection pressures operating onindividuals that have found local optimaamong the
set of possible behaviors allowed by their genetic makeup. Computer simulations
confirm that the Baldwin effect is real once ordinary evolution has created
organismswhoseinternal performance measurecorrelates withactualfitness.
Section4.2. Local Searchin Continuous Spaces 131
originatedinthe17thcenturyafterthedevelopmentofcalculusby Newtonand Leibniz.6 We
findusesforthesetechniquesatseveralplacesinthebookincludingthechaptersonlearning
vision androbotics.
We begin with an example. Suppose we want to place three new airports anywhere
in Romania such that the sum of squared distances from each city on the map Figure 3.2
to its nearest airport is minimized. The state space is then defined by the coordinates of
the airports: x y x y and x y . This is a six-dimensional space; we also say
that states are defined by six variables. In general states are defined by an n-dimensional
V AR IA BL E
vector of variables x. Moving around in this space corresponds to moving one or more of
the airports on the map. The objective function fx y x y x y is relatively easy to
compute for any particular state once we compute the closest cities. Let C be the set of
i
citieswhoseclosestairportinthecurrentstateisairporti. Thenintheneighborhood ofthe
currentstatewherethe C sremainconstant wehave
i
cid:123 cid:12
fx y x y x y x x 2y y 2 . 4.1
i1c Ci
This expression is correct locally but not globally because the sets C are discontinuous
i
functions ofthestate.
Onewaytoavoidcontinuousproblemsissimplytodiscretizetheneighborhoodofeach
D IS CR ET IZ AT IO N
state. Forexample we can move only one airport at a time in either the x or y direction by
a fixed amount δ. With 6 variables this gives 12 possible successors for each state. We
can then apply any of the local search algorithms described previously. We could also ap-
ply stochastic hill climbing and simulated annealing directly without discretizing the space.
Thesealgorithmschoosesuccessorsrandomlywhichcanbedonebygeneratingrandomvec-
torsoflength δ.
Many methods attempt to use the gradient of the landscape to find a maximum. The
G RA DI EN T
gradientoftheobjectivefunctionisavectorf thatgivesthemagnitudeanddirectionofthe
steepest slope. Forourproblemwehave
cid:13 cid:14
f f f f f f
f .
x y x y x y
Insomecaseswecanfindamaximumbysolvingtheequationf0. Thiscouldbedone
forexampleifwewereplacingjustoneairport;thesolutionisthearithmeticmeanofallthe
cities coordinates. In many cases however this equation cannot be solved in closed form.
For example with three airports the expression for the gradient depends on what cities are
closest to each airport in the current state. This means we can compute the gradient locally
butnotglobally; forexample
cid:12
f
2 x x . 4.2
i c
x
Givenalocallycorrectexpressionforthegradientwecanperformsteepest-ascenthillclimb-
ingbyupdatingthecurrentstateaccording totheformula
x xαfx
where α is a small constant often called the step size. In other cases the objective function
S TE PS IZ E
mightnotbeavailableinadifferentiableformatallforexamplethevalueofaparticularset
of airport locations might be determined by running some large-scale economic simulation
E MP IR IC AL package. In those cases we can calculate a so-called empirical gradient by evaluating the
G RA DI EN T
response to small increments and decrements in each coordinate. Empirical gradient search
isthesameassteepest-ascent hillclimbinginadiscretized versionofthestatespace.
Hidden beneath the phrase α is a small constant lies a huge variety of methods for
adjusting α. The basic problem is that if α is too small too many steps are needed; if α
is too large the search could overshoot the maximum. The technique of line search tries to
L IN ES EA RC H
overcome this dilemma by extending the current gradient directionusually by repeatedly
doublingαuntilf startstodecreaseagain. Thepointatwhichthisoccursbecomesthenew
current state. There are several schools of thought about how the new direction should be
chosenatthispoint.
For many problems the most effective algorithm is the venerable Newton Raphson
N EW TO NR AP HS ON
method. Thisisageneral technique forfindingrootsoffunctionsthat issolvingequations
of the form gx0. It works by computing a new estimate for the root x according to
Newtonsformula
x xgxgcid:2 x.
To find a maximum or minimum of f we need to find x such that the gradient is zero i.e.
fx0. Thus gx in Newtons formula becomes fx and the update equation can
bewritteninmatrixvector formas
x x H1xfx
f
where H x is the Hessian matrix of second derivatives whose elements H are given
H ES SI AN f ij
by 2fx x . For our airport example we can see from Equation 4.2 that H x is
i j f
particularly simple: theoff-diagonal elements are zero and thediagonal elements forairport
i are just twice the number of cities in C . A moments calculation shows that one step of
i
the update moves airport i directly to the centroid of C which is the minimum of the local
i
expression forf from Equation 4.1.7 Forhigh-dimensional problems however computing
then2entriesofthe Hessianandinvertingitmaybeexpensivesomanyapproximateversions
ofthe Newton Raphson methodhavebeendeveloped.
Local search methods suffer from local maxima ridges and plateaux in continuous
statespacesjustasmuchasindiscrete spaces. Randomrestartsandsimulatedannealing can
be used and are often helpful. High-dimensional continuous spaces are however big places
inwhichitiseasytogetlost.
C ON ST RA IN ED Afinaltopicwithwhichapassingacquaintance isusefulisconstrainedoptimization.
O PT IM IZ AT IO N
Anoptimizationproblemisconstrainedifsolutionsmustsatisfysomehardconstraintsonthe
values of the variables. Forexample in ourairport-siting problem wemight constrain sites
directlytotheminimumofthatsurfacewhichisalsotheminimumoff iff isquadratic.
Section4.3. Searchingwith Nondeterministic Actions 133
to be inside Romania and on dry land rather than in the middle of lakes. The difficulty of
constrained optimization problemsdepends onthenatureof theconstraints andtheobjective
L IN EA R function. Thebest-known category isthat of linear programming problems in which con-
P RO GR AM MI NG
straints must be linear inequalities forming a convex set 8 and the objective function is also
C ON VE XS ET
linear. Thetimecomplexityoflinearprogrammingispolynomialinthenumberofvariables.
Linear programming is probably the most widely studied and broadly useful class of
optimization problems. It is a special case of the more general problem of convex opti-
C ON VE X mization which allows the constraint region to be any convex region and the objective to
O PT IM IZ AT IO N
beanyfunction thatisconvexwithintheconstraint region. Undercertainconditions convex
optimization problems are also polynomially solvable and may be feasible in practice with
thousands of variables. Several important problems in machine learning and control theory
canbeformulated asconvexoptimization problemssee Chapter20.
In Chapter3weassumedthattheenvironment isfullyobservable anddeterministic andthat
theagentknowswhattheeffectsofeachactionare. Thereforetheagentcancalculateexactly
which state results from any sequence of actions and always knows which state it is in. Its
percepts provide nonewinformation aftereachaction although ofcourse theytelltheagent
theinitialstate.
Whentheenvironment iseitherpartially observable ornondeterministic orboth per-
ceptsbecomeuseful. Inapartiallyobservableenvironment everypercepthelpsnarrowdown
thesetofpossible states theagent mightbein thus makingiteasierfortheagent toachieve
itsgoals. Whentheenvironmentisnondeterministic perceptstelltheagentwhichofthepos-
sible outcomes ofits actions hasactually occurred. In both cases the future percepts cannot
bedeterminedinadvanceandtheagentsfutureactionswilldependonthosefuturepercepts.
Sothesolutiontoaproblemisnotasequencebutacontingencyplanalsoknownasastrat-
C ON TI NG EN CY PL AN
egy that specifies what to do depending on what percepts are received. In this section we
S TR AT EG Y
examinethecaseofnondeterminism deferring partialobservability to Section4.4.
As an example we use the vacuum world first introduced in Chapter 2 and defined as a
search problem in Section 3.2.1. Recall that the state space has eight states as shown in
the dirt states 7 and 8. If the environment is observable deterministic and completely
known then the problem is trivially solvable by any of the algorithms in Chapter 3 and the
solution isanaction sequence. Forexample iftheinitial state is1 then theaction sequence
Suck Right Suckwillreachagoalstate 8.
oneforwhichthespaceaboveitformsaconvexset;bydefinitionconvexfunctionshavenolocalasopposed
toglobalminima.
Figure4.9 Theeightpossiblestatesofthevacuumworld;states7and8aregoalstates.
Now suppose that we introduce nondeterminism in the form of a powerful but erratic
E RR AT IC VA CU UM vacuumcleaner. Intheerraticvacuumworldthe Suckactionworksasfollows:
W OR LD
When applied to a dirty square the action cleans the square and sometimes cleans up
dirtinanadjacentsquare too.
Whenapplied toacleansquaretheactionsometimesdeposits dirtonthecarpet.9
Toprovideapreciseformulation ofthisproblemweneedtogeneralize thenotionofatran-
sitionmodelfrom Chapter3. Insteadofdefiningthetransition modelbya R ES UL T function
that returns asingle state weuse a R ES UL TS function thatreturns a setofpossible outcome
states. Forexample intheerratic vacuum world the Suck action instate1leads toastatein
theset57thedirtintheright-hand squaremayormaynotbevacuumedup.
Wealsoneed togeneralize thenotion ofasolution totheproblem. Forexample ifwe
start in state 1 there is no single sequence of actions that solves the problem. Instead we
needacontingency plansuchasthefollowing:
Suckif State5then Right Suckelse. 4.3
Thus solutions for nondeterministic problems can contain nested ifthenelse statements;
this means that they are trees rather than sequences. This allows the selection of actions
based oncontingencies arising during execution. Manyproblems in thereal physical world
are contingency problems because exact prediction is impossible. For this reason many
peoplekeeptheireyesopenwhilewalkingaroundordriving.
ownersofmodernefficienthomeapplianceswhocannottakeadvantageofthispedagogicaldevice.
Section4.3. Searchingwith Nondeterministic Actions 135
The next question is how to find contingent solutions to nondeterministic problems. As in
Chapter3webeginbyconstructingsearchtreesbutherethetreeshaveadifferentcharacter.
In adeterministic environment the only branching isintroduced by the agents ownchoices
O RN OD E in each state. We call these nodes O R nodes. In the vacuum world for example at an O R
node the agent chooses Left or Right or Suck. In anondeterministic environment branching
is also introduced by the environments choice of outcome for each action. We call these
A ND NO DE nodes A ND nodes. For example the Suck action in state 1 leads to a state in the set 57
so the agent would need to find a plan for state 5 and for state 7. These two kinds of nodes
A ND OR TR EE
alternate leading toan A ND OR treeasillustrated in Figure4.10.
Asolution foran A ND OR searchproblemisasubtreethat1hasagoalnodeatevery
leaf 2 specifies one action ateach of its O R nodes and 3 includes every outcome branch
at each of its A ND nodes. The solution is shown in bold lines in the figure; it corresponds
to the plan given in Equation 4.3. The plan uses ifthenelse notation to handle the A ND
branchesbutwhentherearemorethantwobranchesatanode itmightbebettertouseacase
1
Suck Right
G OA L
Suck Right Left Suck
Suck Left
L OO P L OO P L OO P G OA L
G OA L L OO P
nodesare O Rnodeswheresomeactionmustbechosen.Atthe A NDnodesshownascircles
everyoutcomemustbehandledasindicatedbythearclinkingtheoutgoingbranches. The
solutionfoundisshowninboldlines.
function A ND-O R-G RA PH-S EA RC Hproblemreturnsa conditional plan or failure
O R-S EA RC Hproblem.I NI TI AL-S TA TEproblem
function O R-S EA RC Hstateproblempathreturnsa conditional plan or failure
ifproblem.G OA L-T ES Tstatethenreturntheemptyplan
ifstate isonpath thenreturnfailure
foreachaction inproblem.A CT IO NSstatedo
plan A ND-S EA RC HR ES UL TSstateactionproblemstate path
ifplan cid:7failure thenreturnaction plan
returnfailure
function A ND-S EA RC Hstatesproblempathreturnsa conditional plan or failure
foreachsiinstates do
plan i O R-S EA RC Hsiproblempath
ifplan failure thenreturnfailure
i
returnifs 1thenplan 1elseifs 2thenplan 2else ...ifsn1thenplan n1elseplan n
environments.Itreturnsaconditionalplanthatreachesagoalstateinallcircumstances.The
notationxlreferstothelistformedbyaddingobjectxtothefrontoflistl.
construct. Modifying the basic problem-solving agent shown in Figure 3.1 to execute con-
tingentsolutionsofthiskindisstraightforward. Onemayalsoconsiderasomewhatdifferent
agentdesign inwhichtheagentcanactbeforeithasfoundaguaranteed plananddealswith
some contingencies only as they arise during execution. Thistype of interleaving of search
I NT ER LE AV IN G
andexecution isalsousefulforexploration problems see Section4.5andforgameplaying
see Chapter5.
key aspect of the algorithm is the way in which it deals with cycles which often arise in
nondeterministic problems e.g. if an action sometimes has no effect or if an unintended
effect can be corrected. If the current state is identical to a state on the path from the root
thenitreturnswithfailure. Thisdoesntmeanthatthereis nosolutionfromthecurrentstate;
it simply means that if there is a noncyclic solution it must be reachable from the earlier
incarnation ofthecurrentstatesothenewincarnation can bediscarded. Withthischeckwe
ensurethatthealgorithmterminatesineveryfinitestatespacebecauseeverypathmustreach
agoal adead end orarepeated state. Notice that the algorithm does notcheck whetherthe
currentstateisarepetitionofastateonsomeotherpathfromtherootwhichisimportantfor
efficiency. Exercise4.5investigates thisissue.
A ND ORgraphscanalsobeexploredbybreadth-firstorbest-firstmethods. Theconcept
of a heuristic function must be modified to estimate the cost of a contingent solution rather
""
than asequence but the notion of admissibility carries over and there is an analog of the A
algorithm forfindingoptimal solutions. Pointers aregiven inthebibliographical notes atthe
endofthechapter.
Section4.3. Searchingwith Nondeterministic Actions 137
1
Suck Right
Right
6
Figure4.12 Partofthesearchgraphfortheslipperyvacuumworldwherewehaveshown
somecyclesexplicitly. Allsolutionsforthisproblemarecyclicplansbecausethereis no
waytomovereliably.
Consider the slippery vacuum world which is identical to the ordinary non-erratic vac-
uumworldexcept thatmovementactionssometimes failleaving theagentinthesameloca-
tion. For example moving Right in state 1 leads to the state set 12. Figure 4.12 shows
part of the search graph; clearly there are no longer any acyclic solutions from state 1 and
C YC LI CS OL UT IO N
A ND-O R-G RA PH-S EA RC H would return with failure. There is however a cyclic solution
whichistokeeptrying Right untilitworks. Wecanexpressthissolutionbyaddingalabelto
L AB EL
denote someportion oftheplanandusing thatlabel laterinstead ofrepeating theplanitself.
Thusourcyclicsolutionis
Suck L : Rightif State5then L else Suck.
A better syntax for the looping part of this plan would be while State5 do Right.
In general a cyclic plan may be considered a solution provided that every leaf is a goal
state and that a leaf is reachable from every point in the plan. The modifications needed
to A ND-O R-G RA PH-S EA RC H arecoveredin Exercise4.6. Thekeyrealization isthataloop
inthestatespacebacktoastate Ltranslates toaloopintheplanbacktothepointwherethe
subplan forstate Lisexecuted.
Giventhedefinitionofacyclicsolutionanagentexecutingsuchasolutionwilleventu-
allyreachthegoalprovidedthateachoutcomeofanondeterministicactioneventuallyoccurs.
Is this condition reasonable? It depends on the reason forthe nondeterminism. If the action
rollsadiethen itsreasonable tosuppose thateventually asixwillberolled. Iftheaction is
toinsertahotelcardkeyintothedoorlockbutitdoesnt workthefirsttimethenperhaps it
willeventuallyworkorperhapsonehasthewrongkeyorthe wrongroom!. Aftersevenor
eighttriesmostpeoplewillassumetheproblem iswiththekeyandwillgobacktothefront
desk to getanew one. Onewayto understand this decision isto saythat the initial problem
formulation observable nondeterministic is abandoned in favor of a different formulation
partially observable deterministic where the failure is attributed to an unobservable prop-
ertyofthekey. Wehavemoretosayonthisissuein Chapter13.
We now turn to the problem of partial observability where the agents percepts do not suf-
fice to pin down the exact state. As noted at the beginning of the previous section if the
agent is in one of several possible states then an action may lead to one of several possible
outcomeseven if the environment is deterministic. The key concept required for solving
partiallyobservableproblemsisthe beliefstaterepresenting theagentscurrentbeliefabout
B EL IE FS TA TE
the possible physical states it might be in given the sequence of actions and percepts up to
that point. Webegin withthe simplest scenario forstudying belief states which iswhen the
agenthasnosensorsatall;thenweaddinpartialsensingaswellasnondeterministic actions.
When the agents percepts provide no information at all we have what is called a sensor-
less problem or sometimes a conformant problem. At first one might think the sensorless
S EN SO RL ES S
agent has no hope ofsolving aproblem if ithas no idea whatstate its in; in fact sensorless
C ON FO RM AN T
problems are quite often solvable. Moreover sensorless agents can be surprisingly useful
primarily because they dont rely on sensors working properly. In manufacturing systems
forexamplemanyingeniousmethodshavebeendevelopedfororientingpartscorrectlyfrom
an unknown initial position by using a sequence of actions with no sensing at all. The high
cost of sensing is another reason to avoid it: for example doctors often prescribe a broad-
spectrum antibiotic rather than using the contingent plan of doing an expensive blood test
thenwaitingfortheresults tocomeback andthenprescribing amorespecificantibiotic and
perhapshospitalization becausetheinfection hasprogressed toofar.
Wecan make a sensorless version of the vacuum world. Assume that the agent knows
the geography of its world but doesnt know its location or the distribution of dirt. In that
caseitsinitialstatecouldbeanyelementoftheset12345678. Nowconsiderwhat
happensifittriestheaction Right. Thiswillcauseittobeinoneofthestates2468the
agentnowhasmoreinformation! Furthermore theactionsequence Right Suckwillalways
endup inone ofthestates 48. Finally thesequence Right Suck Left Suckisguaranteed
to reach the goal state 7 no matter what the start state. We say that the agent can coerce the
C OE RC IO N
worldintostate7.
Tosolvesensorlessproblemswesearchinthespaceofbeliefstatesratherthanphysical
states.10 Notice that in belief-state space the problem is fully observable because the agent
rithmsin Chapter3assearchinginabelief-statespaceofsingletonbeliefstates.
Section4.4. Searchingwith Partial Observations 139
alwaysknowsitsownbeliefstate. Furthermore thesolution ifanyisalwaysasequence of
actions. Thisisbecauseasintheordinaryproblemsof Chapter3theperceptsreceivedafter
eachactionarecompletelypredictabletheyre alwaysempty! Sotherearenocontingencies
toplanfor. Thisistrue eveniftheenvironment isnondeterminstic.
It is instructive to see how the belief-state search problem is constructed. Suppose
theunderlying physical problem P isdefined by A CT IO NS P R ES UL TP G OA L-T ES TPand
S TE P-C OS TP. Thenwecandefinethecorresponding sensorless problem asfollows:
Beliefstates: Theentirebelief-statespacecontainseverypossiblesetofphysicalstates.
If P has N statesthenthesensorless problemhasupto2 N statesalthough manymay
beunreachable fromtheinitialstate.
Initial state: Typically the set of allstates in P although insome cases the agent will
havemoreknowledge thanthis.
Actions: This is slightly tricky. Suppose the agent is in belief state bs s but
A CT IO NS Ps 1 cid:7 A CT IO NS Ps 2;thentheagent isunsure ofwhichactions arelegal.
If we assume that illegal actions have no effect on the environment then it is safe to
taketheunionofalltheactionsinanyofthephysicalstatesinthecurrent beliefstateb:
cid:15
A CT IO NSb A CT IO NS Ps.
sb
Ontheotherhand ifanillegalactionmightbetheendoftheworld itissafertoallow
only the intersection that is the set of actions legal in all the states. For the vacuum
worldeverystatehasthesamelegalactions sobothmethodsgivethesameresult.
Transition model: The agent doesnt know which state in the belief state is the right
one; so as far as it knows it might get to any of the states resulting from applying the
action tooneofthe physical states inthebelief state. Fordeterministic actions theset
ofstatesthatmightbereached is
bcid:2 R ES UL Tba scid:2 : scid:2 R ES UL TPsaands b. 4.4
cid:2
Withdeterministic actions b isneverlargerthan b. Withnondeterminism wehave
bcid:2 R ES UL Tba cid:15scid:2 : scid:2 R ES UL TS Psaands b
R ES UL TS Psa
sb
which may be larger than b as shown in Figure 4.13. The process of generating
cid:2
the new belief state after the action is called the prediction step; the notation b
P RE DI CT IO N
P RE DI CT Pbawillcomeinhandy.
Goaltest: The agent wants a plan that is sure to work which means that a belief state
satisfies the goal only if all the physical states in it satisfy G OA L-T ES TP. The agent
mayaccidentally achievethegoalearlier butitwont knowthatithasdoneso.
Path cost: This is also tricky. If the same action can have different costs in different
states then the cost of taking an action in a given belief state could be one of several
values. Thisgives risetoanew class ofproblems which weexplore in Exercise 4.9.
For now we assume that the cost of an action is the same in all states and so can be
transferred directlyfromtheunderlying physical problem.
1
3
a b
deterministicaction Right. b Predictionforthesamebeliefstateandactionintheslippery
versionofthesensorlessvacuumworld.
world. Thereareonly12reachable beliefstatesoutof 28256possible beliefstates.
Thepreceding definitions enabletheautomaticconstruction ofthebelief-state problem
formulation from the definition of the underlying physical problem. Once this is done we
can apply any of the search algorithms of Chapter 3. In fact we can do a little bit more
than that. In ordinary graph search newly generated states are tested to see if they are
identical toexisting states. Thisworksforbelief states too; forexample in Figure 4.14 the
action sequence Suck Left Suck starting at the initial state reaches the same belief state as
Right Left Suck namely 57. Now consider the belief state reached by Left namely
1357. Obviously this is not identical to 57 but it is a superset. It is easy to prove
Exercise4.8thatifanactionsequenceisasolutionforabeliefstatebitisalsoasolutionfor
anysubset ofb. Hence wecandiscardapathreaching 1357 if57 hasalready been
generated. Conversely if 1357 has already been generated and found to be solvable
thenanysubset suchas57 isguaranteed tobesolvable. Thisextralevelofpruningmay
dramatically improvetheefficiencyofsensorless problem solving.
Evenwiththisimprovementhoweversensorlessproblem-solvingaswehavedescribed
itisseldom feasible inpractice. Thedifficulty isnotsomuchthevastness ofthebelief-state
spaceeven though it is exponentially larger than the underlying physical state space; in
most cases the branching factor and solution length in the belief-state space and physical
state space are not so different. The real difficulty lies with the size of each belief state. For
example the initial belief state forthe 1010 vacuum worldcontains 1002100 oraround
listofstates.
One solution is to represent the belief state by some more compact description. In
English we could say the agent knows Nothing in the initial state; after moving Left we
could say Not in the rightmost column and so on. Chapter 7 explains how to do this in a
formal representation scheme. Another approach is to avoid the standard search algorithms
whichtreatbeliefstatesasblackboxesjustlikeanyotherproblemstate. Insteadwecanlook
Section4.4. Searchingwith Partial Observations 141
L
R
L R
S
S S
L R
L
R
R L L R
L
Figure4.14 Thereachableportionofthebelief-statespaceforthedeterministicsensor-
lessvacuumworld.Eachshadedboxcorrespondstoasinglebeliefstate. Atanygivenpoint
the agentis in a particularbeliefstate butdoesnotknowwhichphysicalstate it is in. The
initial belief state complete ignorance is the top center box. Actions are represented by
labeledlinks. Self-loopsareomittedforclarity.
I NC RE ME NT AL
insidethebeliefstatesanddevelopincrementalbelief-statesearch algorithms thatbuildup
B EL IE F-S TA TE
S EA RC H
the solution one physical state at a time. For example in the sensorless vacuum world the
initial belief state is12345678 and wehave tofindan action sequence that works
inall8states. Wecandothisbyfirstfindingasolution thatworksforstate1;thenwecheck
ifitworksforstate 2;ifnot goback andfindadifferent solution forstate1 andsoon. Just
asan A ND OR search hastofindasolution forevery branch at an A ND node thisalgorithm
hastofindasolution forevery stateinthebelief state; thedifference isthat A ND OR search
can find a different solution for each branch whereas an incremental belief-state search has
tofindonesolutionthatworksforallthestates.
The main advantage of the incremental approach is that it is typically able to detect
failurequicklywhen abeliefstateisunsolvable itisusuallythecasethatasmallsubsetof
thebeliefstateconsisting ofthefirstfewstatesexamined isalsounsolvable. Insomecases
thisleads toaspeedup proportional tothesizeofthebelief states whichmaythemselves be
aslargeasthephysicalstatespaceitself.
Even the most efficient solution algorithm is not of much use when no solutions exist.
Many things just cannot be done without sensing. For example the sensorless 8-puzzle is
impossible. On the other hand a little bit of sensing can go a long way. Forexample every
8-puzzle instanceissolvableifjustonesquareisvisiblethe solution involves movingeach
tileinturnintothevisiblesquareandthenkeeping trackof itslocation.
Forageneralpartiallyobservableproblemwehavetospecifyhowtheenvironmentgenerates
percepts for the agent. Forexample we might define the local-sensing vacuum world to be
oneinwhichtheagenthasapositionsensorandalocaldirtsensorbuthasnosensorcapable
of detecting dirt in other squares. The formal problem specification includes a P ER CE PTs
function that returns the percept received in a given state. If sensing is nondeterministic
thenweusea P ER CE PT S functionthatreturnsasetofpossiblepercepts. Forexampleinthe
local-sensingvacuumworldthe P ER CE PTinstate1is A Dirty. Fullyobservableproblems
are aspecial case in which P ER CE PTssforevery state s whilesensorless problems are
aspecialcaseinwhich P ER CE PTsnull.
Whenobservations are partial itwillusually be the case that several states could have
produced any given percept. For example the percept A Dirty is produced by state 3 as
well as by state 1. Hence given this as the initial percept the initial belief state for the
local-sensing vacuum world will be 13. The A CT IO NS S TE P-C OS T and G OA L-T ES T
areconstructedfromtheunderlying physicalproblemjustasforsensorlessproblemsbutthe
transition model isabitmorecomplicated. Wecan think of transitions from one belief state
tothenextforaparticularactionasoccurring inthreestages asshownin Figure4.15:
Thepredictionstageisthesameasforsensorlessproblems: giventheactionainbelief
statebthepredicted beliefstateisˆb P RE DI CTba.11
The observation prediction stage determines the set of percepts o that could be ob-
servedinthepredicted beliefstate:
P OS SI BL E-P ER CE PT Sˆb o: o P ER CE PTsands ˆb.
The update stage determines for each possible percept the belief state that would
result from the percept. The new belief state b is just the set of states inˆb that could
o
haveproduced thepercept:
b
o
U PD AT Eˆbo s :o P ER CE PTsands ˆb.
Noticethateachupdatedbeliefstateb canbenolargerthanthepredictedbeliefstateˆb;
o
observations can only help reduce uncertainty compared to the sensorless case. More-
over for deterministic sensing the belief states for the different possible percepts will
bedisjoint formingapartition oftheoriginal predicted beliefstate.
Section4.4. Searchingwith Partial Observations 143
B Dirty 2
Right
a
B Clean
4
B Dirty 2
Right 2
b
4
B Clean
4
terministic world Right is applied in the initial belief state resulting in a new belief state
with two possible physicalstates; forthose states the possible perceptsare B Dirty and
B Clean leading to two belief states each of which is a singleton. b In the slippery
world Right is applied in the initial belief state giving a new belief state with four physi-
calstates; forthosestates thepossibleperceptsare A Dirty B Dirty and B Clean
leadingtothreebeliefstatesasshown.
Puttingthesethreestagestogether weobtainthepossible beliefstatesresultingfromagiven
actionandthesubsequent possible percepts:
R ES UL TSba b
o
:b
o
U PD AT EP RE DI CTbaoand
o P OS SI BL E-P ER CE PT SP RE DI CTba. 4.5
Againthenondeterminisminthepartiallyobservableproblemcomesfromtheinability
topredict exactly whichpercept willbereceived afteracting; underlying nondeterminism in
the physical environment may contribute to this inability by enlarging the belief state at the
prediction stage leadingtomorepercepts attheobservation stage.
The preceding section showed how to derive the R ES UL TS function for a nondeterministic
belief-state problem fromanunderlying physicalproblem andthe P ER CE PT function. Given
1
3
Suck Right
A Clean B Dirty B Clean
5
7
Figure4.16 Thefirstlevelofthe A ND OR searchtreeforaprobleminthelocal-sensing
vacuumworld;Suck isthefirststepofthesolution.
such a formulation the A ND OR search algorithm of Figure 4.11 can be applied directly to
derive a solution. Figure 4.16 shows part of the search tree for the local-sensing vacuum
worldassuminganinitialpercept A Dirty. Thesolution istheconditional plan
Suck Rightif Bstate6then Suckelse.
Notice that because we supplied a belief-state problem to the A ND OR search algorithm it
returned a conditional plan that tests the belief state rather than the actual state. This isas it
shouldbe: inapartiallyobservableenvironmenttheagentwontbeabletoexecuteasolution
thatrequires testingtheactualstate.
Asinthecaseofstandard search algorithms applied tosensorless problems the A ND
O R search algorithm treats belief states as black boxes just like any other states. One can
improveonthisbycheckingforpreviouslygeneratedbeliefstatesthataresubsetsorsupersets
of the current state just as for sensorless problems. One can also derive incremental search
algorithms analogous to those described for sensorless problems that provide substantial
speedups overtheblack-box approach.
Thedesign ofaproblem-solving agent forpartially observable environments isquitesimilar
to the simple problem-solving agent in Figure 3.1: the agent formulates a problem calls a
searchalgorithm such as A ND-O R-G RA PH-S EA RC Htosolveitandexecutes thesolution.
There are two main differences. First the solution to a problem will be a conditional plan
rather than a sequence; if the first step is an ifthenelse expression the agent will need to
testtheconditionintheif-partandexecutethethen-partortheelse-partaccordingly. Second
the agent will need to maintain its belief state as it performs actions and receives percepts.
This process resembles the predictionobservationupdate process in Equation 4.5 but is
actuallysimplerbecausetheperceptisgivenbytheenvironmentratherthancalculatedbythe
Section4.4. Searchingwith Partial Observations 145
Suck A Clean Right 2 B Dirty
8
Figure4.17 Twopredictionupdatecyclesofbelief-statemaintenanceinthekindergarten
vacuumworldwithlocalsensing.
agent. Givenaninitialbeliefstatebanactionaandapercept othenewbeliefstateis:
cid:2
b U PD AT EP RE DI CTbao. 4.6
local sensing wherein any square may become dirty at any time unless the agent is actively
cleaning itatthatmoment.12
In partially observable environmentswhich include the vast majority of real-world
environmentsmaintaining ones belief state is a core function of any intelligent system.
This function goes under various names including monitoring filtering and state estima-
M ON IT OR IN G
tion. Equation 4.6 iscalled arecursive state estimator because it computes the new belief
F IL TE RI NG
statefromthepreviousoneratherthanbyexaminingtheentireperceptsequence. Iftheagent
S TA TE ES TI MA TI ON
is not to fall behind the computation has to happen as fast as percepts are coming in. As
R EC UR SI VE
the environment becomes more complex the exact update computation becomes infeasible
and the agent willhave to compute an approximate belief state perhaps focusing on the im-
plications ofthe percept fortheaspects ofthe environment that are ofcurrent interest. Most
work on this problem has been done for stochastic continuous-state environments with the
tools of probability theory as explained in Chapter 15. Here we will show an example in a
discreteenvironment withdetrministic sensorsandnondeterministic actions.
The example concerns a robot with the task of localization: working out where it is
L OC AL IZ AT IO N
given amap of the world and a sequence of percepts and actions. Ourrobot is placed in the
maze-like environment of Figure 4.18. The robot is equipped with four sonar sensors that
tell whether there is an obstaclethe outer wall or a black square in the figurein each of
thefourcompassdirections. Weassumethatthesensors give perfectly correct data andthat
the robot has a correct map of the enviornment. But unfortunately the robots navigational
systemisbrokensowhenitexecutesa Moveactionitmovesrandomlytooneoftheadjacent
squares. Therobotstaskistodetermineitscurrentlocation.
Suppose the robot has just been switched on so it does not know where it is. Thus its
initial belief state b consists of the set of all locations. The the robot receives the percept
a Possiblelocations ofrobotafter E N SW
1
b Possiblelocations ofrobot After E N SW E N S
Figure4.18 Possiblepositionsoftherobotcid:13aafteroneobservation E N SW and
1
bafterasecondobservation E N S.Whensensorsarenoiselessandthetransitionmodel
2
isaccuratetherearenootherpossiblelocationsfortherobotconsistentwiththissequence
oftwoobservations.
N SWmeaningthereareobstaclestothenorthwestandsouthanddoesanupdateusingthe
equation b o U PD AT Ebyielding the4locations shownin Figure4.18a. Youcaninspect
themazetoseethatthosearetheonlyfourlocations thatyieldthepercept N WS.
Nexttherobot executes a Move action buttheresultisnondeterministic. Thenewbe-
liefstateb a P RE DI CTb o Movecontainsallthelocationsthatareonestepawayfromthe
locations in b o. When the second percept N S arrives the robot does U PD AT Eb a N S and
finds that the belief state has collapsed down to the single location shown in Figure 4.18b.
Thatstheonlylocation thatcouldbetheresultof
U PD AT EP RE DI CT UP DA TEb N SW Move N S.
With nondetermnistic actions the P RE DI CT step grows the belief state but the U PD AT E step
shrinks it back downas long as the percepts provide some useful identifying information.
Sometimes the percepts dont help much for localization: If there were one or more long
east-west corridors then a robot could receive a long sequence of N S percepts but never
knowwhereinthecorridors itwas.
Section4.5. Online Search Agentsand Unknown Environments 147
So far we have concentrated on agents that use offline search algorithms. They compute
O FF LI NE SE AR CH
a complete solution before setting foot in the real world and then execute the solution. In
contrastanonlinesearch13agentinterleavescomputationandaction: firstittakesanaction
O NL IN ES EA RC H
then itobserves theenvironment and computes the nextaction. Online search isagoodidea
in dynamic or semidynamic domainsdomains where there is a penalty for sitting around
and computing too long. Online search is also helpful in nondeterministic domains because
it allows the agent to focus its computational efforts on the contingencies that actually arise
rather than those that might happen but probably wont. Of course there is a tradeoff: the
moreanagentplansahead thelessoftenitwillfinditselfupthecreekwithoutapaddle.
Onlinesearchisanecessary ideaforunknownenvironments wheretheagentdoesnot
know what states exist or what its actions do. In this state of ignorance the agent faces an
E XP LO RA TI ON exploration problem and must use its actions as experiments in order to learn enough to
P RO BL EM
makedeliberation worthwhile.
Thecanonical example of online search is a robot that is placed in anew building and
mustexplore ittobuild amapthatitcanuse forgetting from Ato B. Methods forescaping
fromlabyrinthsrequired knowledge foraspiring heroesof antiquityare alsoexamplesof
online search algorithms. Spatial exploration is not the only form of exploration however.
Consider a newborn baby: it has many possible actions but knows the outcomes of none of
them and it has experienced only a few of the possible states that it can reach. The babys
gradualdiscovery ofhowtheworldworksisinpartanonlinesearchprocess.
Anonline search problem mustbesolved byan agent executing actions rather than bypure
computation. We assume a deterministic and fully observable environment Chapter 17 re-
laxestheseassumptions butwestipulate thattheagentknowsonlythefollowing:
A CT IO NSswhichreturnsalistofactions allowedinstate s;
The step-cost function csascid:2 note that this cannot be used until the agent knows
cid:2
thats istheoutcome; and
G OA L-T ES Ts.
Note in particular that the agent cannot determine R ES UL Tsa except by actually being
in s and doing a. For example in the maze problem shown in Figure 4.19 the agent does
not know that going Up from 11 leads to 12; nor having done that does it know that
going Down will take it back to 11. This degree of ignorance can be reduced in some
applicationsfor examplearobotexplorermightknowhowitsmovementactionsworkand
beignorantonlyofthelocations ofobstacles.
astheyarereceivedratherthanwaitingfortheentireinputdatasettobecomeavailable.
2
Figure4.19 Asimplemazeproblem. Theagentstartsat S andmustreach Gbutknows
nothingoftheenvironment.
G
S A
S G
S A
G
a b
Figure4.20 a Twostatespacesthatmightleadanonlinesearchagentintoadeadend.
Anygivenagentwillfailinatleastoneofthesespaces. b Atwo-dimensionalenvironment
that can cause an online search agent to follow an arbitrarily inefficient route to the goal.
Whicheverchoice the agentmakes the adversaryblocksthat route with anotherlong thin
wallsothatthepathfollowedismuchlongerthanthebestpossiblepath.
Finally the agent might have access to an admissible heuristic function hs that es-
timates the distance from the current state to a goal state. For example in Figure 4.19 the
agentmightknowthelocationofthegoalandbeabletousethe Manhattan-distanceheuristic.
Typicallytheagentsobjectiveistoreachagoalstatewhileminimizingcost. Another
possibleobjectiveissimplytoexploretheentireenvironment. Thecostisthetotalpathcost
of the path that the agent actually travels. It is common to compare this cost with the path
cost of the path the agent would follow if it knew the search space in advancethat is the
actual shortest path orshortest complete exploration. Inthelanguage ofonline algorithms
thisiscalledthecompetitiveratio;wewouldlikeittobeassmallaspossible.
C OM PE TI TI VE RA TI O
Section4.5. Online Search Agentsand Unknown Environments 149
Althoughthissounds likeareasonable request itiseasytoseethatthebestachievable
competitive ratio is infinite in some cases. For example if some actions are irreversible
I RR EV ER SI BL E
i.e. they lead to a state from which no action leads back to the previous statethe online
searchmightaccidentally reachadead-endstatefromwhichnogoalstateisreachable. Per-
D EA DE ND
haps the term accidentally is unconvincingafter all there might be an algorithm that
happensnottotakethedead-endpathasitexplores. Ourclaimtobemorepreciseisthatno
algorithm canavoiddeadendsinallstatespaces. Considerthetwodead-end statespacesin
spaces look identical so it must make the same decision in both. Therefore it will fail in
A DV ER SA RY one of them. This isan example of an adversary argumentwecan imagine an adversary
A RG UM EN T
constructing the state space while the agent explores it and putting the goals and dead ends
whereveritchooses.
Deadendsarearealdifficultyforrobotexplorationstaircases rampscliffsone-way
streets andallkindsofnaturalterrainpresentopportunities forirreversibleactions. Tomake
progresswesimplyassumethatthestatespaceissafelyexplorablethatissomegoalstate
S AF EL YE XP LO RA BL E
is reachable from every reachable state. State spaces with reversible actions such as mazes
and8-puzzles canbeviewedasundirected graphsandareclearlysafelyexplorable.
Even in safely explorable environments no bounded competitive ratio can be guaran-
teed if there are paths of unbounded cost. This is easy to show in environments with irre-
versible actions but in fact it remains true for the reversible case as well as Figure 4.20b
shows. Forthisreasonitiscommontodescribetheperformanceofonlinesearchalgorithms
intermsofthesizeoftheentirestatespaceratherthanjust thedepthoftheshallowest goal.
Aftereachaction anonlineagentreceivesapercepttelling itwhatstateithasreached; from
this information it can augment its map of the environment. The current map is used to
decide where to go next. This interleaving of planning and action means that online search
algorithmsarequitedifferentfromtheofflinesearchalgorithmswehaveseenpreviously. For
""
example offline algorithms such as A can expand a node in one part of the space and then
immediately expand a node in another part of the space because node expansion involves
simulated rather than real actions. An online algorithm on the other hand can discover
successors only for a node that it physically occupies. To avoid traveling all the way across
thetreetoexpandthenextnodeitseemsbettertoexpandnodesinalocalorder. Depth-first
searchhasexactlythisproperty becauseexceptwhenbacktracking thenextnodeexpanded
isachildoftheprevious nodeexpanded.
An online depth-first search agent is shown in Figure 4.21. This agent stores its map
in a table R ES UL Tsa that records the state resulting from executing action a in state s.
Whenever an action from the current state has not been explored the agent tries that action.
The difficulty comes when the agent has tried all the actions in a state. In offline depth-first
search the state is simply dropped from the queue; in an online search the agent has to
backtrack physically. Indepth-firstsearchthismeansgoingbacktothestatefromwhichthe
agentmostrecentlyenteredthecurrentstate. Toachievethatthealgorithmkeepsatablethat
function O NL IN E-D FS-A GE NTscid:5returnsanaction
inputs:scid:5aperceptthatidentifiesthecurrentstate
persistent: resultatableindexedbystateandactioninitiallyempty
untriedatablethatlistsforeachstatetheactionsnotyettried
unbacktrackedatablethatlistsforeachstatethebacktracksnotyettried
sathepreviousstateandactioninitiallynull
if G OA L-T ES Tscid:5thenreturnstop
ifscid:5isanewstatenotinuntriedthenuntriedscid:5 AC TI ON Sscid:5
ifs isnotnullthen
resultsascid:5
adds tothefrontofunbacktrackedscid:5
ifuntriedscid:5isemptythen
ifunbacktrackedscid:5isemptythenreturnstop
elseaanactionb suchthatresultscid:5b P OPunbacktrackedscid:5
elsea P OPuntriedscid:5
sscid:5
returna
Figure4.21 Anonlinesearchagentthatusesdepth-firstexploration. Theagentisappli-
cableonlyinstatespacesinwhicheveryactioncanbeundonebysomeotheraction.
lists foreach state the predecessor states to which the agent has not yet backtracked. If the
agenthasrunoutofstatestowhichitcanbacktrack thenits searchiscomplete.
We recommend that the reader trace through the progress of O NL IN E-D FS-A GE NT
when applied to the maze given in Figure 4.19. It is fairly easy to see that the agent will in
theworstcase enduptraversing everylinkinthestatespace exactly twice. Forexploration
this is optimal; for finding a goal on the other hand the agents competitive ratio could be
arbitrarily bad if it goes off on a long excursion when there is a goal right next to the initial
state. Anonlinevariantofiterativedeepeningsolvesthis problem;foranenvironment thatis
auniform treethecompetitive ratioofsuchanagentisasmallconstant.
Because of its method of backtracking O NL IN E-D FS-A GE NT works only in state
spaces where the actions are reversible. There are slightly more complex algorithms that
workingeneralstatespacesbutnosuchalgorithm hasabounded competitiveratio.
Like depth-first search hill-climbing search has the property of locality in its node expan-
sions. In fact because it keeps just one current state in memory hill-climbing search is
already an online search algorithm! Unfortunately it is not very useful in its simplest form
because it leaves the agent sitting at local maxima with nowhere to go. Moreover random
restartscannotbeusedbecausetheagentcannottransport itselftoanewstate.
Instead of random restarts one might consider using a random walk to explore the
R AN DO MW AL K
environment. Arandom walksimply selects atrandom one oftheavailable actions from the
Section4.5. Online Search Agentsand Unknown Environments 151
S G
Figure4.22 Anenvironmentinwhicharandomwalkwilltakeexponentiallymanysteps
tofindthegoal.
current state; preference can be given to actions that have not yet been tried. It is easy to
prove that a random walk will eventually find a goal or complete its exploration provided
thatthespaceisfinite.14 Ontheotherhand theprocess canbeveryslow. Figure4.22shows
an environment in which a random walk will take exponentially many steps to find the goal
because ateachstepbackwardprogressistwiceaslikelyasforwardprogress. Theexample
is contrived of course but there are many real-world state spaces whose topology causes
thesekindsoftrapsforrandomwalks.
Augmenting hill climbing withmemoryrather than randomness turns out to beamore
effective approach. The basic idea is to store a current best estimate Hs of the cost to
reach the goal from each state that has been visited. Hs starts out being just the heuristic
estimate hs and is updated as the agent gains experience in the state space. Figure 4.23
shows a simple example in a one-dimensional state space. In a the agent seems to be
stuck in a flat local minimum at the shaded state. Rather than staying where it is the agent
should follow what seems to bethe best path tothe goal given the current cost estimates for
cid:2
its neighbors. The estimated cost to reach the goal through a neighbor s is the cost to get
cid:2 cid:2 cid:2
to s plus the estimated cost to get to a goal from therethat is csas Hs. In the
exampletherearetwoactionswithestimatedcosts19and12soitseemsbesttomove
right. Now it is clear that the cost estimate of 2 for the shaded state was overly optimistic.
Since the best move cost 1 and led to a state that is at least 2 steps from a goal the shaded
state must be at least 3 steps from agoal so its H should be updated accordingly as shown
in Figure 4.23b. Continuing this process the agent will move back and forth twice more
updating H eachtimeandflattening outthelocalminimumuntilitescapes totheright.
""
Anagent implementing this scheme whichiscalled learning real-time A L RT A is
L RT A
shown in Figure 4.24. Like O NL IN E-D FS-A GE NT it builds a map of the environment in
theresult table. Itupdates thecostestimate forthestate ithas justleft andthen chooses the
apparently best move according to its current cost estimates. One important detail is that
actionsthathavenotyetbeentriedinastatesarealwaysassumedtoleadimmediatelytothe
O PT IM IS MU ND ER goalwiththeleastpossiblecostnamelyhs. Thisoptimismunderuncertaintyencourages
U NC ER TA IN TY
theagenttoexplorenewpossiblypromising paths.
""
An L RT A agentisguaranteedtofindagoalinanyfinitesafelyexplorableenvironment.
""
Unlike Ahoweveritisnotcompleteforinfinitestatespacestherearecaseswhereitcanbe
ledinfinitelyastray. Itcanexploreanenvironmentofnstatesin On2stepsintheworstcase
gridtheprobabilitythatthewalkeverreturnstothestartingpointisonlyabout0.3405 Hughes1995.
a 8 9 2 2 4 3
b 8 9 3 2 4 3
c 8 9 3 4 4 3
d 8 9 5 4 4 3
e 8 9 5 5 4 3
labeledwith Hsthecurrentcostestimatetoreachagoalandeachlinkislabeledwithits
stepcost. Theshadedstatemarksthelocationoftheagentandtheupdatedcostestimatesat
eachiterationarecircled.
function L RT A-A GE NTscid:5returnsanaction
inputs:scid:5aperceptthatidentifiesthecurrentstate
persistent: resultatableindexedbystateandactioninitiallyempty
Hatableofcostestimatesindexedbystateinitiallyempty
sathepreviousstateandactioninitiallynull
if G OA L-T ES Tscid:5thenreturnstop
ifscid:5isanewstatenotin Hthen Hscid:5hscid:5
ifs isnotnull
resultsascid:5
Hs min L RT A-C OS Tsbresultsb H
b A CT IO NSs
aanactionb in A CT IO NSscid:5thatminimizes L RT A-C OS Tscid:5bresultscid:5b H
sscid:5
returna
function L RT A-C OS Tsascid:5 Hreturnsacostestimate
ifscid:5isundefinedthenreturnhs
elsereturncsascid:5 Hscid:5
stateswhichareupdatedastheagentmovesaboutthestatespace.
5
A DV ER SA RI AL S EA RC H
Inwhichweexaminetheproblemsthatarisewhenwetrytoplanaheadinaworld
whereotheragentsareplanning againstus.
actions of other agents and how they affect its own welfare. The unpredictability of these
other agents can introduce contingencies into the agents problem-solving process as dis-
cussedin Chapter4. Inthischapterwecovercompetitiveenvironmentsinwhichtheagents
goalsareinconflictgivingrisetoadversarial search problemsoften knownasgames.
G AM E
Mathematicalgametheoryabranchofeconomicsviewsanymultiagentenvironment
as a game provided that the impact of each agent on the others is significant regardless
of whether the agents are cooperative or competitive.1 In A I the most common games are
ofaratherspecialized kindwhatgametheorists calldeterministic turn-taking two-player
zero-sum games of perfect information such as chess. In our terminology this means
Z ER O-S UM GA ME S
P ER FE CT deterministicfullyobservableenvironmentsinwhichtwoagentsactalternatelyandinwhich
I NF OR MA TI ON
the utility values at the end of the game are always equal and opposite. Forexample if one
playerwinsagameofchess theotherplayernecessarily loses. Itisthisopposition between
theagentsutilityfunctions thatmakesthesituation adversarial.
Games have engaged the intellectual faculties of humanssometimes to an alarming
degreefor as long as civilization has existed. For A I researchers the abstract nature of
games makes them an appealing subject for study. The state of a game is easy to represent
andagentsareusuallyrestrictedtoasmallnumberofactionswhoseoutcomesaredefinedby
precise rules. Physicalgamessuch ascroquet andicehockey havemuchmorecomplicated
descriptions a much larger range of possible actions and rather imprecise rules defining
the legality of actions. With the exception of robot soccer these physical games have not
attracted muchinterest inthe A Icommunity.
161
Games unlike most of the toy problems studied in Chapter 3 are interesting because
they are too hard to solve. For example chess has an average branching factor of about 35
nodesalthough thesearchgraphhasonly about 1040 distinct nodes. Gameslikethereal
worldthereforerequiretheabilitytomake somedecision evenwhencalculating theoptimal
decisionisinfeasible. Gamesalsopenalizeinefficiencyseverely. Whereasanimplementation
""
of A searchthatishalfasefficientwillsimplytaketwiceaslongtoruntocompletionachess
program that is half as efficient in using its available time probably will be beaten into the
ground otherthingsbeingequal. Game-playingresearchhasthereforespawnedanumberof
interesting ideasonhowtomakethebestpossible useoftime.
We begin with a definition of the optimal move and an algorithm for finding it. We
then look at techniques for choosing a good move when time is limited. Pruningallows us
P RU NI NG
toignore portions ofthesearch treethatmakenodifference tothefinalchoice andheuristic
evaluationfunctionsallowustoapproximate thetrueutilityofastatewithoutdoingacom-
plete search. Section 5.5 discusses games such as backgammon that include an element of
I MP ER FE CT chance; wealso discuss bridge which includes elements of imperfect information because
I NF OR MA TI ON
notallcardsarevisible toeachplayer. Finallywelookathowstate-of-the-art game-playing
programsfareagainsthumanopposition andatdirections forfuturedevelopments.
Wefirstconsidergameswithtwoplayerswhomwecall M AXand M INforreasonsthat
willsoonbecomeobvious. M AX movesfirstandthentheytaketurnsmovinguntilthegame
is over. At the end of the game points are awarded to the winning player and penalties are
given to the loser. A game can be formally defined as a kind of search problem with the
followingelements:
S : Theinitialstate whichspecifieshowthegameissetupatthestart.
0
P LA YE Rs: Defineswhichplayerhasthemoveinastate.
A CT IO NSs: Returnsthesetoflegalmovesinastate.
R ES UL Tsa: Thetransitionmodelwhichdefinestheresultofamove.
T ER MI NA LT ES T
T ER MI NA L-T ES Ts: A terminal test which is true when the game is over and false
otherwise. Stateswherethegamehasendedarecalled terminalstates.
T ER MI NA LS TA TE S
U TI LI TYsp: Autilityfunctionalsocalledanobjectivefunctionorpayofffunction
definesthefinalnumericvalueforagamethatendsinterminal statesforaplayerp. In
chesstheoutcomeisawinlossordrawwithvalues 10or 1. Somegameshavea
2
widervarietyofpossibleoutcomes;thepayoffsinbackgammonrangefrom0to192.
Azero-sum game is confusingly defined as one where the total payoff toall players
isthe sameforevery instance ofthegame. Chessiszero-sum because everygamehas
payoffofeither0110or 1 1. Constant-sum wouldhavebeenabetterterm
but zero-sum is traditional and makes sense if you imagine each player is charged an
entryfeeof 1.
2
G AM ET RE E
The initial state A CT IO NS function and R ES UL T function define the game tree for the
gamea tree where the nodes are game states and the edges are moves. Figure 5.1 shows
part of the game tree for tic-tac-toe noughts and crosses. From the initial state M AX has
nine possible moves. Play alternates between M AXs placing an X and M INs placing an O
Section5.2. Optimal Decisions in Games 163
until we reach leaf nodes corresponding to terminal states such that one player has three in
a row or all the squares are filled. The number on each leaf node indicates the utility value
ofthe terminal state from the point ofview of M AX; high values are assumed tobe good for
M AX andbadfor M INwhichishowtheplayersgettheirnames.
For tic-tac-toe the game tree is relatively smallfewer than 9! 362880 terminal
nodes. But for chess there are over 1040 nodes so the game tree is best thought of as a
theoretical construct that we cannot realize in the physical world. But regardless of the size
S EA RC HT RE E
ofthegametreeitis M AXsjobtosearchforagoodmove. Weusethetermsearchtreefora
treethatissuperimposed onthefullgametreeandexamines enoughnodestoallowaplayer
todetermine whatmovetomake.
M AX X
X X X
M IN O
X X X
X X X
X O X O X . . .
M AX X O
X O X X O X O . . .
M IN O X X
. . . . . . . . . . . .
X O X X O X X O X . . .
T ER MI NA L O X O O X X
O X X O X O O
Utility 1 0 1
stateand M AXmovesfirstplacingan Xinanemptysquare.Weshowpartofthetreegiving
alternatingmovesby M IN Oand M AX Xuntilweeventuallyreachterminalstateswhich
canbeassignedutilitiesaccordingtotherulesofthegame.
In a normal search problem the optimal solution would be a sequence of actions leading to
a goal statea terminal state that is a win. In adversarial search M IN has something to say
S TR AT EG Y about it. M AX therefore must find a contingent strategy which specifies M AXs move in
the initial state then M AXs moves in the states resulting from every possible response by
M AX 3 A
a a
a
2
M IN 3 B 2 C 2 D
b b c c d d
b c d
cid:15
turntomoveandthe nodesare M INnodes. Theterminalnodesshowtheutilityvalues
for M AX;theothernodesarelabeledwiththeirminimaxvalues. M AXsbestmoveattheroot
isa 1becauseitleadstothestatewiththehighestminimaxvalueand M INsbestreplyisb 1
becauseitleadstothestatewiththelowestminimaxvalue.
M INthen M AXsmovesinthestates resulting fromeverypossible response by M IN tothose
moves and so on. This is exactly analogous to the A ND OR search algorithm Figure 4.11
with M AX playing theroleof O R and M IN equivalent to A ND. Roughly speaking anoptimal
strategy leads to outcomes at least as good as any other strategy when one is playing an
infallible opponent. Webeginbyshowinghowtofindthisoptimalstrategy.
Even a simple game like tic-tac-toe is too complex for us to draw the entire game tree
ononepagesowewillswitchtothetrivialgamein Figure5.2. Thepossiblemovesfor M AX
at the root node are labeled a 1 a 2 and a 3. The possible replies to a 1 for M IN are b 1 b 2
b 3 and so on. This particular game ends after one move each by M AX and M IN. In game
parlancewesaythatthistreeisonemovedeepconsistingoftwohalf-moveseachofwhich
iscalledaply. Theutilitiesoftheterminalstatesinthisgamerangefrom2to14.
P LY
Given a game tree the optimal strategy can be determined from the minimax value
M IN IM AX VA LU E
of each node which we write as M IN IM AXn. The minimax value of a node is the utility
for M AX of being in the corresponding state assuming that both players play optimally
from there to the end of the game. Obviously the minimax value of a terminal state is just
its utility. Furthermore given a choice M AX prefers to move to a state of maximum value
whereas M IN prefersastateofminimumvalue. Sowehavethefollowing:
M IN IM AXs
""
U TI LI TYs if T ER MI NA L-T ES Ts
""
max a Actionss M IN IM AX RE SU LTsa if P LA YE Rs M AX
min a Actionss M IN IM AX RE SU LTsa if P LA YE Rs M IN
Letusapplythesedefinitionstothegametreein Figure5.2. Theterminalnodesonthebottom
level get their utility values from the games U TI LI TY function. The first M IN node labeled
B has three successor states with values 3 12 and 8 so its minimax value is 3. Similarly
the othertwo M IN nodes have minimaxvalue 2. Theroot node isa M AX node; its successor
states have minimax values 3 2 and2; soithas aminimax value of3. Wecanalso identify
Section5.2. Optimal Decisions in Games 165
M IN IM AX DE CI SI ON
theminimaxdecisionattheroot: actiona 1istheoptimalchoicefor M AXbecauseitleadsto
thestatewiththehighest minimaxvalue.
This definition of optimal play for M AX assumes that M IN also plays optimallyit
maximizestheworst-caseoutcomefor M AX. Whatif M INdoesnotplayoptimally? Thenitis
easytoshow Exercise5.7that M AXwilldoevenbetter. Otherstrategiesagainstsuboptimal
opponents maydobetterthantheminimaxstrategybutthese strategiesnecessarily doworse
againstoptimalopponents.
Theminimaxalgorithm Figure5.3computestheminimaxdecisionfromthecurrent state.
M IN IM AX AL GO RI TH M
Itusesasimplerecursivecomputationoftheminimaxvaluesofeachsuccessorstatedirectly
implementing thedefining equations. Therecursion proceeds allthewaydowntotheleaves
of the tree and then the minimax values are backed up through the tree as the recursion
unwinds. For example in Figure 5.2 the algorithm first recurses down to the three bottom-
leftnodes andusesthe U TI LI TY function onthemtodiscoverthattheirvaluesare312and
8 respectively. Then it takes the minimum of these values 3 and returns it as the backed-
up value of node B. A similar process gives the backed-up values of 2 for C and 2 for D.
Finallywetakethemaximumof32and2togetthebacked-upvalueof3fortherootnode.
The minimax algorithm performs a complete depth-first exploration of the game tree.
If the maximum depth of the tree is m and there are b legal moves at each point then the
timecomplexityoftheminimaxalgorithm is Obm. Thespacecomplexityis Obmforan
algorithm that generates allactions atonce or Omforan algorithm that generates actions
one at a time see page 87. For real games of course the time cost is totally impractical
but this algorithm serves as the basis for the mathematical analysis of games and for more
practical algorithms.
Manypopulargamesallowmorethantwoplayers. Letusexaminehowtoextendtheminimax
idea to multiplayer games. This is straightforward from the technical viewpoint but raises
someinteresting newconceptual issues.
First we need to replace the single value for each node with a vector of values. For
exampleinathree-playergamewithplayers A Band Cavectorcid:16v v v cid:17isassociated
A B C
witheachnode. Forterminalstatesthisvectorgivestheutilityofthestatefromeachplayers
viewpoint. Intwo-playerzero-sumgamesthetwo-elementvectorcanbereducedtoasingle
valuebecausethevaluesarealwaysopposite. Thesimplestwaytoimplementthisistohave
the U TI LI TY function returnavectorofutilities.
Nowwehavetoconsidernonterminalstates. Considerthenodemarked X inthegame
tree shown in Figure 5.4. In that state player C chooses what to do. The two choices lead
toterminal states with utility vectors cid:16v 1v 2v 6cid:17and cid:16v 4v 2v 3cid:17.
A B C A B C
Since6isbiggerthan3 Cshouldchoosethefirstmove. Thismeansthatifstate X isreached
subsequent play will lead to a terminal state with utilities cid:16v 1v 2v 6cid:17. Hence
A B C
thebacked-upvalueof X isthisvector. Thebacked-upvalueofanodenisalwaystheutility
function M IN IM AX-D EC IS IO Nstatereturnsan action
returnargmax
a A CT IO NSs
M IN-V AL UE RE SU LTstatea
function M AX-V AL UEstatereturnsa utility value
if T ER MI NA L-T ES Tstatethenreturn U TI LI TYstate
v
foreacha in A CT IO NSstatedo
v M AXv M IN-V AL UE RE SU LTsa
returnv
function M IN-V AL UEstatereturnsa utility value
if T ER MI NA L-T ES Tstatethenreturn U TI LI TYstate
v
foreacha in A CT IO NSstatedo
v M INv M AX-V AL UE RE SU LTsa
returnv
sponding to the best possible move that is the move that leads to the outcome with the
bestutilityundertheassumptionthattheopponentplaystominimizeutility. Thefunctions
M AX-V AL UE and M IN-V AL UE go throughthe whole game tree all the way to the leaves
to determine the backed-up value of a state. The notation argmax fa computes the
a S
elementaofset S thathasthemaximumvalueoffa.
to move
A 1 2 6
B 1 2 6 1 5 2
C 1 2 6 X 6 1 2 1 5 2 5 4 5
A
1 2 6 4 2 3 6 1 2 7 41 511 1 5 2 7 71 5 4 5
Figure5.4 Thefirstthreepliesofagametreewiththreeplayers A BC. Eachnodeis
labeledwithvaluesfromtheviewpointofeachplayer. Thebestmoveismarkedattheroot.
vector of the successor state with the highest value for the player choosing at n. Anyone
who plays multiplayer games such as Diplomacy quickly becomes aware that much more
isgoing onthan intwo-player games. Multiplayer gamesusually involve alliances whether
A LL IA NC E
formalorinformalamongtheplayers. Alliancesaremadeandbrokenasthegameproceeds.
How are we to understand such behavior? Are alliances a natural consequence of optimal
strategies foreach player in a multiplayer game? It turns out that they can be. Forexample
Section5.3. Alpha Beta Pruning 167
suppose A and B are in weak positions and C is in a stronger position. Then it is often
optimal for both A and B to attack C rather than each other lest C destroy each of them
individually. In this way collaboration emerges from purely selfish behavior. Of course
as soon as C weakens under the joint onslaught the alliance loses its value and either A
or B could violate the agreement. In some cases explicit alliances merely make concrete
what would have happened anyway. In other cases a social stigma attaches to breaking an
alliancesoplayersmustbalancetheimmediateadvantageofbreakinganallianceagainstthe
long-term disadvantage of being perceived as untrustworthy. See Section 17.5 for more on
thesecomplications.
If the game is not zero-sum then collaboration can also occur with just two players.
Supposeforexamplethatthereisaterminalstatewithutilitiescid:16v 1000v 1000cid:17and
A B
that1000 isthehighest possible utility foreachplayer. Thentheoptimalstrategy isforboth
players to do everything possible to reach this statethat is the players will automatically
cooperate toachieveamutuallydesirable goal.
The problem with minimax search is that the number of game states it has to examine is
exponential in the depth of the tree. Unfortunately we cant eliminate the exponent but it
turnsoutwecaneffectivelycutitinhalf. Thetrickisthatitispossibletocomputethecorrect
minimaxdecisionwithoutlookingateverynodeinthegametree. Thatiswecanborrowthe
idea of pruningfrom Chapter 3 to eliminate large parts of the tree from consideration. The
A LP HA BE TA particular technique weexamine iscalled alphabeta pruning. Whenapplied toastandard
P RU NI NG
minimax tree it returns the same move as minimax would but prunes away branches that
cannotpossibly influencethefinaldecision.
Consideragainthetwo-plygametreefrom Figure5.2. Letsgothroughthecalculation
of the optimal decision once more this time paying careful attention to what we know at
eachpoint intheprocess. Thestepsareexplained in Figure5.5. Theoutcome isthatwecan
identify theminimaxdecision withouteverevaluating twooftheleafnodes.
Anotherwaytolookatthisisasasimplification oftheformula for M IN IM AX. Letthe
two unevaluated successors of node C in Figure 5.5 have values x and y. Then the value of
therootnodeisgivenby
M IN IM AXroot maxmin3128min2xymin1452
max3min2xy2
max3z2 wherez min2xy 2
3.
In otherwords thevalue ofthe root and hence the minimax decision are independent of the
valuesoftheprunedleaves xandy.
Alphabeta pruning can be applied to trees of any depth and it is often possible to
prune entire subtrees ratherthanjust leaves. Thegeneral principle isthis: consider anode n
a A b A
3 B 3 B
c
3
A d
3
A
3 3 B 3 3 B 2 C
e 3 14 A f 3 3 A
3 3 B 2 C 14 D 3 3 B 2 C 2 2 D
Figure5.5 Stagesinthecalculationoftheoptimaldecisionforthegametreein Figure5.2.
Ateachpointweshowtherangeofpossiblevaluesforeachnode.a Thefirstleafbelow B
hasthevalue3. Hence Bwhichisa M INnodehasavalueofatmost3.b Thesecondleaf
below B hasavalueof12; M IN wouldavoidthismovesothevalueof B isstillatmost3.
c The third leaf below B has a value of 8; we have seen all Bs successor states so the
value of B is exactly 3. Now we can inferthat the value of the rootis at least 3 because
M AX has a choice worth 3 at the root. d The first leaf below C has the value 2. Hence
C whichisa M IN nodehasavalueofatmost2. Butweknowthat B isworth3so M AX
wouldneverchoose C. Thereforethereisnopointinlookingattheothersuccessorstates
of C. Thisisanexampleofalphabetapruning. e Thefirstleafbelow Dhasthevalue14
so Disworthatmost14. Thisisstillhigherthan M AXsbestalternativei.e.3soweneed
to keep exploring Ds successorstates. Notice also that we nowhave boundson all of the
successorsoftherootsotherootsvalueisalsoatmost14. f Thesecondsuccessorof D
isworth5soagainweneedtokeepexploring. Thethirdsuccessorisworth2sonow Dis
worthexactly2. M AXsdecisionattherootistomoveto Bgivingavalueof3.
somewhereinthetreesee Figure5.6suchthat Playerhasa choice ofmovingtothatnode.
If Playerhasabetterchoicemeitherattheparentnodeofnoratanychoicepointfurtherup
thennwillneverbereached inactualplay. Sooncewehavefoundoutenough aboutnby
examiningsomeofitsdescendants toreachthisconclusion wecanpruneit.
Remember that minimax search is depth-first so at any one time we just have to con-
sider the nodes along a single path in the tree. Alphabeta pruning gets its name from the
followingtwoparametersthatdescribeboundsonthebacked-upvaluesthatappearanywhere
alongthepath:
Section5.3. Alpha Beta Pruning 169
Player
Opponent m
""
""
""
Player
Opponent n
Figure5.6 Thegeneralcaseforalphabetapruning. If misbetterthannfor Playerwe
willnevergettoninplay.
α thevalueofthebesti.e.highest-valuechoicewehavefoundsofaratanychoicepoint
alongthepathfor M AX.
β thevalueofthebesti.e.lowest-valuechoicewehavefoundsofaratanychoicepoint
alongthepathfor M IN.
Alphabeta search updates the values of α and β as it goes along and prunes the remaining
branches at a node i.e. terminates the recursive call as soon as the value of the current
node is known to be worse than the current α or β value for M AX or M IN respectively. The
complete algorithm is given in Figure 5.7. We encourage you to trace its behavior when
appliedtothetreein Figure5.5.
Theeffectiveness ofalphabeta pruning ishighly dependent ontheorderinwhich thestates
areexamined. Forexamplein Figure5.5eandfwecould notpruneanysuccessors of D
at all because the worst successors from the point of view of M IN were generated first. If
thethirdsuccessorof Dhadbeengeneratedfirstwewouldhavebeenabletoprunetheother
two. Thissuggests that it might beworthwhile to try toexamine firstthe successors that are
likelytobebest.
If this can be done2 then it turns out that alphabeta needs to examine only Obm2
nodes to pick the best move instead of Obm for minimax. This means that the effective
branching factor becomes b instead of bfor chess about 6 instead of 35. Put another
way alphabeta can solve a tree roughly twice as deep as minimax in the same amount of
time. If successors are examined in random order rather than best-first the total number of
nodesexaminedwillberoughly Ob3m4formoderateb. Forchessafairlysimpleordering
function such as trying captures first then threats then forward moves and then backward
movesgetsyoutowithinaboutafactorof2ofthebest-case Obm2result.
function A LP HA-B ET A-S EA RC Hstatereturnsanaction
v M AX-V AL UEstate
returntheaction in A CT IO NSstatewithvaluev
function M AX-V AL UEstateαβreturnsa utility value
if T ER MI NA L-T ES Tstatethenreturn U TI LI TYstate
v
foreacha in A CT IO NSstatedo
v M AXv M IN-V AL UE RE SU LTsaαβ
ifv β thenreturnv
α MA Xαv
returnv
function M IN-V AL UEstateαβreturnsa utility value
if T ER MI NA L-T ES Tstatethenreturn U TI LI TYstate
v
foreacha in A CT IO NSstatedo
v M INv M AX-V AL UE RE SU LTsaαβ
ifv αthenreturnv
β MI Nβv
returnv
the M IN IM AX functionsin Figure5.3exceptforthetwolinesineachof M IN-V AL UE and
M AX-V AL UEthatmaintainαandβandthebookkeepingtopasstheseparametersalong.
Addingdynamicmove-orderingschemessuchastryingfirstthemovesthatwerefound
to be best in the past brings us quite close to the theoretical limit. The past could be the
previous moveoften thesame threats remainor itcould come from previous exploration
of the current move. One way to gain information from the current move is with iterative
deepening search. First search 1 ply deep and record the best path of moves. Then search
iterative deepening on an exponential game tree adds only a constant fraction to the total
searchtimewhichcanbemorethanmadeupfrombettermoveordering. Thebestmovesare
oftencalledkillermovesandtotrythemfirstiscalledthekillermoveheuristic.
K IL LE RM OV ES
In Chapter 3 we noted that repeated states in the search tree can cause an exponential
increaseinsearchcost. Inmanygamesrepeatedstatesoccurfrequentlybecauseoftranspo-
sitionsdifferent permutations of the move sequence that end up in the same position. For
T RA NS PO SI TI ON
example if White has one move a that can be answered by Black with b and an unre-
lated movea on the other side of the board that can be answered by b then the sequences
a b a b and a b a b both end up in the same position. It is worthwhile to store
the evaluation of the resulting position in a hash table the first time it is encountered so that
wedont havetorecompute itonsubsequent occurrences. The hashtableofpreviously seen
T RA NS PO SI TI ON positionsistraditionallycalledatranspositiontable;itisessentiallyidenticaltotheexplored
T AB LE
Section5.4. Imperfect Real-Time Decisions 171
listin G RA PH-S EA RC H Section3.3. Usingatransposition tablecanhaveadramaticeffect
sometimesasmuchasdoublingthereachablesearchdepthinchess. Ontheotherhandifwe
areevaluatingamillionnodespersecondatsomepointitisnotpracticaltokeepallofthem
in the transposition table. Various strategies have been used to choose which nodes to keep
andwhichtodiscard.
Theminimaxalgorithmgeneratestheentiregamesearchspacewhereasthealphabetaalgo-
rithm allowsustoprune large partsofit. However alphabeta stillhastosearch alltheway
toterminalstatesforatleastaportionofthesearchspace. Thisdepthisusuallynotpractical
because moves must be made in a reasonable amount of timetypically a few minutes at
most. Claude Shannonspaper Programminga Computerfor Playing Chess1950proposed
insteadthatprogramsshouldcutoffthesearchearlierandapplyaheuristic evaluationfunc-
E VA LU AT IO N tion to states in the search effectively turning nonterminal nodes into terminal leaves. In
F UN CT IO N
otherwordsthesuggestion istoalterminimaxoralphabeta intwoways: replacetheutility
function by a heuristic evaluation function E VA L which estimates the positions utility and
C UT OF FT ES T
replace theterminaltestbya cutofftestthatdecides whentoapply E VA L. Thatgivesusthe
followingforheuristic minimaxforstate sandmaximumdepthd:
H-M IN IM AXsd
""
E VA Ls if C UT OF F-T ES Tsd
""
max a Actionss H-M IN IM AX RE SU LTsad1 if P LA YE Rs M AX
min a Actionss H-M IN IM AX RE SU LTsad1 if P LA YE Rs M IN.
An evaluation function returns an estimate of the expected utility of the game from a given
position just as the heuristic functions of Chapter 3 return an estimate of the distance to
the goal. The idea of an estimator was not new when Shannon proposed it. For centuries
chess players and aficionados of othergames have developed ways ofjudging the value of
a position because humans are even more limited in the amount of search they can do than
are computer programs. It should be clear that the performance of a game-playing program
depends strongly onthequality ofitsevaluation function. Aninaccurate evaluation function
willguideanagenttowardpositions thatturnouttobelost. Howexactlydowedesigngood
evaluation functions?
First the evaluation function should order the terminal states in the same way as the
trueutility function: states thatarewinsmustevaluate betterthan drawswhichinturnmust
be better than losses. Otherwise an agent using the evaluation function might err even if it
can see ahead all the way to the end of the game. Second the computation must not take
too long! The whole point is to search faster. Third for nonterminal states the evaluation
function shouldbestrongly correlated withtheactualchances ofwinning.
Onemightwellwonderaboutthephrasechancesofwinning. Afterallchessisnota
gameofchance: weknowthecurrentstatewithcertaintyandnodiceareinvolved. Butifthe
searchmustbecutoffatnonterminal states thenthealgorithm willnecessarily beuncertain
aboutthefinaloutcomesofthosestates. Thistypeofuncertaintyisinducedbycomputational
rather than informational limitations. Given the limited amount of computation that the
evaluationfunctionisallowedtodoforagivenstatethebestitcandoismakeaguessabout
thefinaloutcome.
Let us make this idea more concrete. Most evaluation functions work by calculating
various features ofthestatefor example inchess wewouldhave features forthenumber
of white pawns black pawns white queens black queens and so on. The features taken
togetherdefinevariouscategoriesorequivalenceclassesofstates: thestatesineachcategory
have the same values for all the features. For example one category contains all two-pawn
vs. one-pawn endgames. Any given category generally speaking will contain some states
that lead to wins some that lead to draws and some that lead to losses. The evaluation
function cannot knowwhichstatesarewhichbutitcanreturnasinglevaluethatreflectsthe
proportion of states with each outcome. Forexample suppose our experience suggests that
72 ofthe states encountered inthetwo-pawns vs. one-pawn category lead toawinutility
1; 20 to a loss 0 and 8 to a draw 12. Then a reasonable evaluation for states in
the category is the expected value: 0.72 10.20 00.08 12 0.76. In
E XP EC TE DV AL UE
principle the expected value canbe determined foreach category resulting inan evaluation
function that works for any state. As with terminal states the evaluation function need not
returnactualexpectedvaluesaslongasthe orderingofthestatesisthesame.
In practice this kind of analysis requires too many categories and hence too much
experience to estimate all the probabilities of winning. Instead most evaluation functions
compute separate numerical contributions from each feature and then combine them to find
the total value. Forexample introductory chess books give an approximate material value
M AT ER IA LV AL UE
foreachpiece: eachpawnisworth1aknightorbishopisworth3arook5andthequeen9.
Otherfeatures suchasgood pawnstructure andkingsafety mightbeworthhalfapawn
say. Thesefeaturevaluesarethensimplyaddeduptoobtaintheevaluation oftheposition.
Asecure advantage equivalent toapawngivesasubstantial likelihood ofwinning and
asecureadvantageequivalenttothreepawnsshouldgivealmostcertainvictoryasillustrated
in Figure5.8a. Mathematically thiskindofevaluation functioniscalledaweightedlinear
W EI GH TE DL IN EA R functionbecauseitcanbeexpressed as
F UN CT IO N
cid:12n
E VA Ls w 1f 1sw 2f 2sw nf ns w if is
i1
whereeach w isaweight andeach f isafeature oftheposition. Forchess the f could be
i i i
thenumbers ofeach kind ofpiece ontheboard andthe w could bethevalues ofthe pieces
i
1forpawn3forbishop etc..
Adding up the values of features seems like a reasonable thing to do but in fact it
involves a strong assumption: that the contribution of each feature is independent of the
values of the other features. Forexample assigning the value 3 to a bishop ignores the fact
that bishops are more powerful in the endgame when they have a lot of space to maneuver.
Section5.4. Imperfect Real-Time Decisions 173
a White to move b White to move
Figure5.8 Twochesspositionsthatdifferonlyinthepositionoftherookatlowerright.
Ina Black has an advantageofa knightand two pawns whichshouldbe enoughto win
thegame. Inb Whitewillcapturethequeengivingitanadvantagethatshouldbestrong
enoughtowin.
Forthisreasoncurrentprogramsforchessandothergamesalsousenonlinearcombinations
offeatures. Forexampleapairofbishopsmightbeworthslightlymorethantwicethevalue
ofasinglebishopandabishopisworthmoreintheendgamethatiswhenthemovenumber
featureishighorthe numberofremainingpiecesfeatureislow.
Theastutereaderwillhavenoticedthatthefeaturesandweightsarenotpartoftherules
ofchess! Theycomefromcenturiesofhumanchess-playingexperience. Ingameswherethis
kind of experience is not available the weights of the evaluation function can be estimated
by the machine learning techniques of Chapter 18. Reassuringly applying these techniques
tochesshasconfirmedthatabishopisindeedworthaboutthreepawns.
The next step is to modify A LP HA-B ET A-S EA RC H so that it will call the heuristic E VA L
function when it is appropriate to cut off the search. We replace the two lines in Figure 5.7
thatmention T ER MI NA L-T ES T withthefollowingline:
if C UT OF F-T ES Tstatedepththenreturn E VA Lstate
Wealsomustarrangeforsomebookkeepingsothatthecurrent depth isincrementedoneach
recursivecall. Themoststraightforwardapproachtocontrollingtheamountofsearchistoset
afixeddepthlimitsothat C UT OF F-T ES Tstatedepthreturnstrue foralldepth greaterthan
somefixeddepthd. Itmustalsoreturn true forallterminalstatesjustas T ER MI NA L-T ES T
did. The depth d is chosen so that a move is selected within the allocated time. A more
robust approach is to apply iterative deepening. See Chapter 3. When time runs out the
program returns the move selected by the deepest completed search. As a bonus iterative
deepening alsohelpswithmoveordering.
These simple approaches can lead to errors due to the approximate nature of the eval-
uation function. Consider again the simple evaluation function for chess based on material
advantage. Suppose the program searches to the depth limit reaching the position in Fig-
ure 5.8b where Black is ahead by a knight and two pawns. It would report this as the
heuristic value of the state thereby declaring that the state is a probable win by Black. But
Whites next move captures Blacks queen with no compensation. Hence the position is
reallywonfor Whitebutthiscanbeseenonlybylooking aheadonemoreply.
Obviouslyamoresophisticatedcutofftestisneeded. Theevaluationfunctionshouldbe
appliedonlytopositions thatarequiescentthatisunlikely toexhibitwildswingsinvalue
Q UI ES CE NC E
in the nearfuture. Inchess forexample positions inwhich favorable captures can be made
arenotquiescentforanevaluation functionthatjustcountsmaterial. Nonquiescent positions
can be expanded further until quiescent positions are reached. This extra search is called a
Q UI ES CE NC E quiescence search; sometimes it is restricted to consider only certain types of moves such
S EA RC H
ascapture movesthatwillquickly resolvetheuncertainties intheposition.
The horizon effect is more difficult to eliminate. It arises when the program is facing
H OR IZ ON EF FE CT
an opponents move that causes serious damage and is ultimately unavoidable but can be
temporarily avoided by delaying tactics. Consider the chess game in Figure 5.9. It is clear
that there isno wayforthe black bishop toescape. Forexample the white rook cancapture
itbymovingtoh1thena1thena2;acaptureatdepth6ply. But Blackdoeshaveasequence
of moves that pushes the capture of the bishop over the horizon. Suppose Black searches
todepth8ply. Mostmovesby Blackwillleadtotheeventualcaptureofthebishop andthus
will be marked as bad moves. But Black will consider checking the white king with the
pawnate4. Thiswillleadtothekingcapturingthepawn. Now Blackwillconsiderchecking
again with the pawn at f5 leading to another pawn capture. That takes up 4 ply and from
there the remaining 4 ply is not enough to capture the bishop. Black thinks that the line of
playhassavedthebishop atthepriceoftwopawnswhenactually allithasdoneispushthe
inevitable captureofthebishopbeyondthehorizonthat Blackcansee.
S IN GU LA R One strategy to mitigate the horizon effect is the singular extension a move that is
E XT EN SI ON
clearly better than all other moves in a given position. Once discovered anywhere in the
treeinthecourseofasearchthissingularmoveisremembered. Whenthesearchreachesthe
normaldepth limitthealgorithm checks toseeifthesingularextension isalegalmove;ifit
is the algorithm allows the moveto be considered. This makes the tree deeper but because
therewillbefewsingularextensions itdoesnotaddmanytotalnodestothetree.
So far we have talked about cutting off search at a certain level and about doing alpha
beta pruning that provably has no effect on the result at least with respect to the heuristic
evaluation values. It is also possible to do forward pruning meaning that some moves at
F OR WA RD PR UN IN G
a given node are pruned immediately without further consideration. Clearly most humans
playing chess consider only a few moves from each position at least consciously. One
approach to forward pruning is beam search: on each ply consider only a beam of the n
B EA MS EA RC H
bestmovesaccording totheevaluation functionratherthanconsidering allpossible moves.
Section5.4. Imperfect Real-Time Decisions 175
1
2
3
4
5
6
7
8
a b c d e f g h
Figure5.9 Thehorizoneffect. With Black tomovethe blackbishopissurelydoomed.
But Blackcanforestallthateventbycheckingthewhitekingwithitspawnsforcingtheking
tocapturethepawns.Thispushestheinevitablelossofthebishopoverthehorizonandthus
thepawnsacrificesareseenbythesearchalgorithmasgoodmovesratherthanbadones.
Unfortunately this approach is rather dangerous because there is no guarantee that the best
movewillnotbeprunedaway.
The P RO BC UT or probabilistic cut algorithm Buro 1995 is a forward-pruning ver-
sionofalphabetasearchthatusesstatisticsgainedfrompriorexperiencetolessenthechance
that the best move will be pruned. Alphabeta search prunes any node that is provably out-
side the current αβ window. P RO BC UT also prunes nodes that are probably outside the
window. It computes this probability by doing a shallow search to compute the backed-up
valuev ofanodeandthenusing pastexperience toestimatehowlikely itisthatascoreof v
atdepthdinthetreewouldbeoutside αβ. Buroappliedthistechniquetohis Othellopro-
gram L OG IS TE LL Oandfoundthataversionofhisprogramwith P RO BC UTbeattheregular
version64ofthetimeevenwhentheregularversionwasgiventwiceasmuchtime.
Combining all the techniques described here results in a program that can play cred-
itablechessorothergames. Letusassumewehaveimplementedanevaluationfunctionfor
chess a reasonable cutoff test with a quiescence search and a large transposition table. Let
usalsoassumethataftermonthsoftediousbit-bashingwecangenerateandevaluatearound
amillionnodespersecondonthelatest P Callowingustosearchroughly 200millionnodes
per move under standard time controls three minutes per move. The branching factor for
chess is about 35 on average and 355 is about 50 million so if we used minimax search
we could look ahead only about fiveplies. Though not incompetent such a program can be
fooledeasilybyanaverage humanchessplayer whocanoccasionally plansixoreightplies
ahead. With alphabeta search we get to about 10 plies which results in an expert level of
play. Section5.8describes additional pruningtechniques thatcanextendtheeffectivesearch
depth to roughly 14 plies. To reach grandmaster status we would need an extensively tuned
evaluation function andalargedatabaseofoptimalopening andendgamemoves.
Somehowitseemslikeoverkillforachessprogramtostartagamebyconsidering atreeofa
billiongamestatesonlytoconclude thatitwillmoveitspawntoe4. Booksdescribing good
playintheopeningandendgameinchesshavebeenavailableforaboutacentury Tattersall
1911. It is not surprising therefore that many game-playing programs use table lookup
ratherthansearchfortheopening andendingofgames.
Forthe openings thecomputer ismostly relying onthe expertise ofhumans. Thebest
advice ofhumanexpertsonhowtoplayeachopening iscopied frombooks andentered into
tables forthecomputers use. However computers canalso gatherstatistics from adatabase
of previously played games to see which opening sequences most often lead to a win. In
theearly movestherearefewchoices andthusmuchexpert commentaryandpastgameson
whichtodraw. Usuallyaftertenmovesweendupinararelyseenposition andtheprogram
mustswitchfromtablelookuptosearch.
Neartheendofthegamethereareagainfewerpossiblepositionsandthusmorechance
to do lookup. But here it is the computer that has the expertise: computer analysis of
endgames goes far beyond anything achieved by humans. A human can tell you the gen-
eral strategy forplaying aking-and-rook-versus-king K RKendgame: reduce the opposing
kings mobility bysqueezing ittowardone edgeoftheboard using yourking toprevent the
opponent fromescaping thesqueeze. Otherendings such asking bishop andknight versus
king K BN Karedifficult tomasterandhave nosuccinct strategy description. Acomputer
ontheotherhandcancompletely solvetheendgamebyproducing apolicywhichisamap-
P OL IC Y
pingfromeverypossiblestatetothebestmoveinthatstate. Thenwecanjustlookupthebest
move rather than recompute it anew. How big will the K BN K lookup table be? It turns out
there are 462 ways that two kings can be placed on the board without being adjacent. After
the kings are placed there are 62 empty squares for the bishop 61 for the knight and two
possible players to move next so there are just 462 6261 2 3494568 possible
positions. Someofthesearecheckmates;markthemassuchinatable. Thendoaretrograde
R ET RO GR AD E
minimax search: reverse the rules of chess to do unmoves rather than moves. Any move by
Whitethatnomatterwhatmove Blackrespondswithendsupinapositionmarkedasawin
must also be a win. Continue this search until all 3494568 positions are resolved as win
lossordrawandyouhaveaninfallible lookuptableforall K BN Kendgames.
Using this technique and a tour de force of optimization tricks Ken Thompson 1986
1996 and Lewis Stiller 1992 1996 solved all chess endgames with up to five pieces and
some with six pieces making them available on the Internet. Stiller discovered one case
whereaforcedmateexistedbutrequired262moves;thiscausedsomeconsternation because
the rules of chess require a capture or pawn move to occur within 50 moves. Laterwork by
Marc Bourzutschky and Yakov Konoval Bourzutschky 2006 solved all pawnless six-piece
andsomeseven-pieceendgames;thereisa K QN KR BNendgamethatwithbestplayrequires
517movesuntilacapture whichthenleadstoamate.
If we could extend the chess endgame tables from 6 pieces to 32 then White would
know onthe opening movewhether itwould be awin loss ordraw. This has not happened
sofarforchessbutithashappenedforcheckers asexplained inthehistoricalnotessection.
Section5.5. Stochastic Games 177
In real life many unpredictable external events can put us into unforeseen situations. Many
games mirror this unpredictability by including a random element such as the throwing of
dice. We call these stochastic games. Backgammon is a typical game that combines luck
S TO CH AS TI CG AM ES
andskill. Dicearerolledatthebeginning ofaplayers turn todetermine thelegalmoves. In
the backgammon position of Figure 5.10 for example White has rolled a 65 and has four
possible moves.
Figure5.10 A typicalbackgammonposition. Thegoalofthegameistomoveallones
piecesofftheboard. Whitemovesclockwisetoward25and Blackmovescounterclockwise
toward0.Apiececanmovetoanypositionunlessmultipleopponentpiecesarethere;ifthere
isoneopponentitiscapturedandmuststartover. Inthepositionshown White hasrolled
65 and must choose among four legal moves: 510511 51119245101016
and5111116wherethenotation5111116meansmoveonepiecefromposition5
to11andthenmoveapiecefrom11to16.
Although Whiteknowswhathisorherownlegalmovesare Whitedoesnotknowwhat
Blackisgoing torollandthus doesnotknow what Blacks legalmoveswillbe. Thatmeans
White cannot construct a standard game tree of the sort we saw in chess and tic-tac-toe. A
C HA NC EN OD ES game tree in backgammon must include chance nodes in addition to M AX and M IN nodes.
Chance nodes are shown as circles in Figure 5.11. The branches leading from each chance
node denote the possible dice rolls; each branch is labeled with the roll and its probability.
Thereare36waystorolltwodiceeachequallylikely;butbecausea65isthesameasa56
thereareonly21distinctrolls. Thesixdoubles11through 66eachhaveaprobability of
136sowesay P11 136. Theother15distinctrollseachhavea118probability.
M AX
C HA NC E . . .
B
... ... ... ...
136 118 118 136
11 12 65 66
M IN . . .
... ... ...
C HA NC E C . . .
... ... ...
136 118 118 136
11 12 65 66
M AX . . .
... ... ...
T ER MI NA L 2 1 1 1 1
Figure5.11 Schematicgametreeforabackgammonposition.
Thenextstepistounderstand howtomakecorrect decisions. Obviously westillwant
to pick the move that leads to the best position. However positions do not have definite
minimaxvalues. Insteadwecanonlycalculatetheexpectedvalueofaposition: theaverage
E XP EC TE DV AL UE
overallpossible outcomesofthechancenodes.
This leads us to generalize the minimax value for deterministic games to an expecti-
E XP EC TI MI NI MA X minimaxvalueforgameswithchance nodes. Terminalnodes and M AX and M IN nodes for
V AL UE
which the dice roll is known work exactly the same way as before. For chance nodes we
compute the expected value which is the sum of the value over all outcomes weighted by
theprobability ofeachchanceaction:
E XP EC TI MI NI MA Xs
""
U TI LI TYs if T ER MI NA L-T ES Ts
""
max a E XP EC TI MI NI MA XR ES UL Tsa if P LA YE Rs M AX
m cid:2in a E XP EC TI MI NI MA XR ES UL Tsa if P LA YE Rs M IN
Pr E XP EC TI MI NI MA XR ES UL Tsr if P LA YE Rs C HA NC E
r
where r represents apossible dicerollorotherchance eventand R ES UL Tsristhesame
stateasswiththeadditional factthattheresultofthedicerollisr.
As with minimax the obvious approximation to make with expectiminimax is to cut the
search offat some point and apply an evaluation function to each leaf. Onemight think that
evaluation functions forgamessuchasbackgammonshould bejustlikeevaluation functions
Section5.5. Stochastic Games 179
forchesstheyjustneedtogivehigherscorestobetterpositions. Butinfactthepresenceof
chance nodes means that one has tobe more careful about what the evaluation values mean.
3 4 to the leaves move a is best; with values 1 20 30 400 move a is best. Hence
the program behaves totally differently if we make a change in the scale of some evaluation
values! It turns out that to avoid this sensitivity the evaluation function must be a positive
lineartransformation oftheprobabilityofwinningfromapositionormoregenerally ofthe
expected utility of the position. This is an important and general property of situations in
whichuncertainty isinvolved andwediscussitfurtherin Chapter16.
M AX
a a a a
C HA NC E 2.1 1.3 21 40.9
.9 .1 .9 .1 .9 .1 .9 .1
M IN 2 3 1 4 20 30 1 400
Figure5.12 Anorder-preservingtransformationonleafvalueschangesthebestmove.
If the program knew in advance all the dice rolls that would occur for the rest of the
gamesolving agamewithdicewouldbejustlikesolving agamewithout dicewhichmini-
maxdoesin Obmtimewherebisthebranchingfactorandmisthemaximumdepthofthe
game tree. Because expectiminimax is also considering all the possible dice-roll sequences
itwilltake Obmnmwherenisthenumberofdistinctrolls.
Evenifthesearchdepthislimitedtosomesmalldepth dtheextracostcomparedwith
that of minimax makes it unrealistic to consider looking ahead very far in most games of
chance. Inbackgammon nis21andb isusually around 20 butinsome situations canbeas
highas4000fordicerollsthataredoubles. Threepliesisprobably allwecouldmanage.
Another way to think about the problem is this: the advantage of alphabeta is that
it ignores future developments that just are not going to happen given best play. Thus it
concentrates on likely occurrences. In games with dice there are no likely sequences of
movesbecause forthosemovestotakeplace thedicewouldfirsthavetocomeouttheright
way to make them legal. This is a general problem whenever uncertainty enters the picture:
the possibilities are multiplied enormously and forming detailed plans of action becomes
pointless becausetheworldprobably willnotplayalong.
It may have occurred to you that something like alphabeta pruning could be applied
to game trees with chance nodes. It turns out that it can. The analysis for M IN and M AX
nodes is unchanged but wecan also prune chance nodes using a bit of ingenuity. Consider
thechance node C in Figure5.11and whathappens toitsvalue asweexamine and evaluate
its children. Is it possible to find an upper bound on the value of C before we have looked
atallitschildren? Recall that thisiswhatalphabeta needs inordertoprune anode andits
subtree. Atfirstsight itmightseem impossible because thevalueof C istheaverage ofits
childrens values and in order to compute the average of a set of numbers we must look at
all the numbers. But ifweput bounds on the possible values of the utility function then we
can arrive atbounds forthe average without looking atevery number. Forexample say that
allutilityvaluesarebetween2and2;thenthevalueofleafnodesisbounded andinturn
wecanplaceanupperboundonthevalueofachancenodewithoutlookingatallitschildren.
M ON TE CA RL O An alternative is to do Monte Carlo simulation to evaluate a position. Start with
S IM UL AT IO N
an alphabeta or other search algorithm. From a start position have the algorithm play
thousands of games against itself using random dice rolls. In the case of backgammon the
resulting win percentage has been shown to be a good approximation of the value of the
position even if the algorithm has an imperfect heuristic and is searching only a few plies
Tesauro1995. Forgameswithdicethistypeofsimulation iscalledarollout.
R OL LO UT
Chess has often been described as war in miniature but it lacks at least one major charac-
teristic of real wars namely partial observability. In the fog of war the existence and
disposition of enemy units is often unknown until revealed by direct contact. As a result
warfareincludestheuseofscoutsandspiestogatherinformationandtheuseofconcealment
and bluff to confuse the enemy. Partially observable games share these characteristics and
arethusqualitatively differentfromthegamesdescribed inthepreceding sections.
Indeterministicpartiallyobservablegamesuncertaintyaboutthestateoftheboardarisesen-
tirelyfromlackofaccesstothechoicesmadebytheopponent. Thisclassincludeschildrens
gamessuchas Battleshipswhereeachplayersshipsareplacedinlocationshiddenfromthe
opponentbutdonotmoveand Strategowherepiecelocationsareknownbutpiecetypesare
hidden. Wewill examine the game of Kriegspiel apartially observable variant of chess in
K RI EG SP IE L
whichpiecescanmovebutarecompletelyinvisible totheopponent.
The rules of Kriegspiel are as follows: White and Black each see a board containing
onlytheirownpieces. Arefereewhocanseeallthepiecesadjudicatesthegameandperiod-
ically makes announcements that are heard by both players. On his turn White proposes to
thereferee anymovethatwouldbelegalifthere werenoblack pieces. Ifthemoveisinfact
not legal because of the black pieces the referee announces illegal. In this case White
maykeepproposing movesuntilalegaloneisfoundand learnsmoreaboutthelocation of
Blacks pieces in the process. Once a legal move is proposed the referee announces one or
Section5.6. Partially Observable Games 181
more of the following: Capture on square Xif there is a capture and Check by D if the
black king is in check where D is the direction of the check and can be one of Knight
Rank File Long diagonal or Short diagonal. In case of discovered check the ref-
eree may make two Check announcements. If Black is checkmated or stalemated the
refereesaysso;otherwise itis Blacksturntomove.
Kriegspielmayseemterrifyinglyimpossiblebuthumansmanageitquitewellandcom-
puter programs are beginning to catch up. It helps to recall the notion of a belief state as
defined in Section 4.4 and illustrated in Figure 4.14the set of all logically possible board
states given the complete history of percepts to date. Initially Whites belief state is a sin-
gleton because Blacks pieces havent moved yet. After White makes a move and Black re-
sponds Whites belief state contains 20positions because Blackhas20replies toany White
move. Keepingtrackofthebeliefstateasthegameprogressesisexactlytheproblemofstate
estimation for which the update step is given in Equation 4.6. We can map Kriegspiel
state estimation directly onto the partially observable nondeterministic framework of Sec-
tion 4.4 if we consider the opponent as the source of nondeterminism; that is the R ES UL TS
of Whitesmovearecomposedfromthepredictable outcome of Whitesownmoveandthe
unpredictable outcomegivenby Blacksreply.3
Given a current belief state White may ask Can I win the game? For a partially
observable game the notion of a strategy is altered; instead of specifying a move to make
foreachpossible movetheopponent mightmakeweneedamoveforeverypossible percept
sequence that might be received. For Kriegspiel a winning strategy or guaranteed check-
G UA RA NT EE D mateisonethat foreachpossible percept sequence leadstoan actual checkmate forevery
C HE CK MA TE
possible board state in the current belief state regardless of how the opponent moves. With
this definition the opponents belief state is irrelevantthe strategy has to work even if the
opponent can see all the pieces. This greatly simplifies the computation. Figure 5.13 shows
part of a guaranteed checkmate for the K RK king and rook against king endgame. In this
case Blackhasjustonepiecetheking soabelief statefor Whitecanbeshowninasingle
boardbymarkingeachpossible position ofthe Blackking.
The general A ND-O R search algorithm can be applied to the belief-state space to find
guaranteed checkmates just as in Section 4.4. The incremental belief-state algorithm men-
tioned inthatsection oftenfindsmidgamecheckmates uptodepth9probably wellbeyond
theabilitiesofhumanplayers.
In addition to guaranteed checkmates Kriegspiel admits an entirely new concept that
P RO BA BI LI ST IC makes no sense in fully observable games: probabilistic checkmate. Such checkmates are
C HE CK MA TE
stillrequiredtoworkineveryboardstateinthebeliefstate;theyareprobabilisticwithrespect
torandomization ofthewinning players moves. Togetthebasic idea consider theproblem
of finding a lone black king using just the white king. Simply by moving randomly the
white king will eventually bump into the black king even if the latter tries to avoid this fate
since Blackcannot keep guessing therightevasivemovesindefinitely. Intheterminology of
probability theory detection occurs with probability 1. The K BN K endgameking bishop
thisissuefornow;Chapters7and8suggestmethodsforcompactlyrepresentingverylargebeliefstates.
4
3
2
1
a b c d
Kc3 ?
O K Illegal
Rc3 ?
O K Check
board. In the initial belief state Blacks king is in one of three possible locations. By a
combination of probing moves the strategy narrows this down to one. Completion of the
checkmateisleftasanexercise.
andknight against kingis woninthis sense; Whitepresents Blackwithaninfiniterandom
sequence of choices for one of which Black will guess incorrectly and reveal his position
leadingtocheckmate. The K BB Kendgameontheotherhandiswonwithprobability1cid:2.
White can force a win only by leaving one of his bishops unprotected for one move. If
Blackhappens tobeintherightplaceandcaptures thebishop amovethatwouldloseifthe
bishopsareprotected thegameisdrawn. Whitecanchoosetomaketheriskymoveatsome
randomlychosenpointinthemiddleofaverylongsequencethusreducingcid:2toanarbitrarily
smallconstant butcannotreduce cid:2tozero.
It is quite rare that a guaranteed or probabilistic checkmate can be found within any
reasonabledepthexceptintheendgame. Sometimesacheckmatestrategyworksforsomeof
theboardstatesinthecurrentbeliefstatebutnotothers. Tryingsuchastrategymaysucceed
A CC ID EN TA L leadingtoanaccidentalcheckmateaccidental inthesensethat Whitecouldnotknowthat
C HE CK MA TE
itwouldbecheckmateif Blackspieceshappentobeintherightplaces. Mostcheckmates
in games between humans are of this accidental nature. This idea leads naturally to the
question ofhow likely itis that agiven strategy willwin whichleads inturn tothe question
ofhowlikelyitisthateachboardstateinthecurrentbeliefstateisthetrueboardstate.
Section5.6. Partially Observable Games 183
Onesfirstinclinationmightbetoproposethatallboardstatesinthecurrentbeliefstate
are equally likelybut this cant be right. Consider for example Whites belief state after
Blacks first move of the game. By definition assuming that Black plays optimally Black
musthaveplayedanoptimalmovesoallboardstatesresultingfromsuboptimalmovesought
tobeassignedzeroprobability. Thisargumentisnotquiterighteitherbecause eachplayers
goal is not just to move pieces tothe right squares but also tominimize the information that
the opponent has about their location. Playing any predictable optimal strategy provides
the opponent with information. Hence optimal play in partially observable games requires
a willingness to play somewhat randomly. This is why restaurant hygiene inspectors do
randominspection visits. Thismeansoccasionally selecting movesthatmayseemintrinsi-
callyweakbuttheygainstrengthfromtheirveryunpredictability becausetheopponentis
unlikely tohaveprepared anydefenseagainstthem.
From these considerations it seems that the probabilities associated with the board
states in the current belief state can only be calculated given an optimal randomized strat-
egy; in turn computing that strategy seems to require knowing the probabilities of the var-
ious states the board might be in. This conundrum can be resolved by adopting the game-
theoretic notion of an equilibrium solution which we pursue further in Chapter 17. An
equilibrium specifies an optimal randomized strategy for each player. Computing equilib-
ria is prohibitively expensive however even for small games and is out of the question for
Kriegspiel. At present the design of effective algorithms for general Kriegspiel play is an
open research topic. Most systems perform bounded-depth lookahead in their own belief-
statespace ignoring theopponents beliefstate. Evaluation functions resemble those forthe
observable gamebutincludeacomponent forthesizeofthebeliefstatesmallerisbetter!
Card games provide many examples of stochastic partial observability where the missing
informationisgeneratedrandomly. Forexampleinmanygamescardsaredealtrandomlyat
the beginning of the game with each player receiving a hand that is not visible to the other
players. Suchgamesincludebridge whisthearts andsome formsofpoker.
Atfirstsightitmightseemthatthesecardgamesarejustlikedicegames: thecardsare
dealtrandomlyanddeterminethemovesavailabletoeachplayerbutallthedicearerolled
atthe beginning! Even though this analogy turns out tobe incorrect itsuggests an effective
algorithm: consider all possible deals of the invisible cards; solve each one as if it were a
fullyobservablegame;andthenchoosethemovethathasthebestoutcomeaveragedoverall
thedeals. Supposethateachdealsoccurswithprobability Ps;thenthemovewewantis
cid:12
argmax Ps M IN IM AX RE SU LTsa. 5.1
a
s
Herewerunexact M IN IM AX ifcomputationally feasible;otherwise werun H-M IN IM AX.
Now in most card games the number of possible deals is rather large. For example
inbridge play each player sees just twocid:20ofcid:21the fourhands; there aretwounseen hands of13
cards each so the number of deals is 26 10400600. Solving even one deal is quite
13
difficult so solving ten million is out of the question. Instead we resort to a Monte Carlo
approximation: instead of adding up all the deals we take a random sample of N deals
wheretheprobability ofdeal sappearing inthesampleisproportional to Ps:
cid:12 N
1
argmax M IN IM AX RE SU LTs ia. 5.2
N
a
i1
Notice that Ps does not appear explicitly in the summation because the samples are al-
ready drawn according to Ps. As N grows large the sum over the random sample tends
totheexact value but evenforfairly small Nsay 100to1000the method gives agood
approximation. Itcanalso beapplied todeterministic gamessuchas Kriegspiel givensome
reasonable estimateof Ps.
Forgameslikewhistandheartswherethereisnobidding orbetting phasebeforeplay
commences each deal will be equally likely and so the values of Ps are all equal. For
bridge play ispreceded byabidding phase inwhicheachteam indicates howmanytricks it
expects towin. Since players bid based on the cards they hold the other players learn more
abouttheprobability ofeachdeal. Takingthisintoaccount indeciding howtoplaythehand
istricky forthereasons mentioned inourdescription of Kriegspiel: players maybidinsuch
awayastominimize theinformation conveyed totheiropponents. Evenso the approach is
quiteeffectiveforbridge asweshowin Section5.7.
The strategy described in Equations 5.1 and 5.2 is sometimes called averaging over
clairvoyance because it assumes that the game will become observable to both players im-
mediately after the first move. Despite its intuitive appeal the strategy can lead one astray.
Considerthefollowingstory:
Day 1: Road A leads to a heap of gold; Road B leads to a fork. Take the left fork and
youllfindabiggerheapofgoldbuttaketherightforkandyoullberunoverbyabus.
Day2: Road A leadsto aheapofgold; Road Bleadstoa fork. Takethe rightforkand
youllfindabiggerheapofgoldbuttaketheleftforkandyoullberunoverbyabus.
Day 3: Road A leads to a heap of gold; Road B leads to a fork. One branch of the
forkleads to a biggerheap of gold but take the wrong forkand youllbe hit by a bus.
Unfortunatelyyoudontknowwhichforkiswhich.
Averagingoverclairvoyanceleadstothefollowingreasoning: on Day1 Bistherightchoice;
on Day2 Bistheright choice; on Day3thesituation isthesameaseither Day1or Day2
so Bmuststillbetherightchoice.
Now we can see how averaging overclairvoyance fails: it does not consider the belief
statethattheagentwillbeinafteracting. Abeliefstateoftotalignoranceisnotdesirablees-
pecially whenonepossibility iscertain death. Becauseitassumesthateveryfuturestatewill
automatically beoneofperfectknowledge theapproach neverselectsactionsthatgatherin-
formationlikethefirstmovein Figure5.13;norwillitchooseactionsthathideinformation
from the opponent or provide information to a partner because it assumes that they already
knowtheinformation; anditwillneverbluffinpoker4 because itassumestheopponent can
B LU FF
seeitscards. In Chapter17 weshowhowtoconstruct algorithms thatdoallthese things by
virtueofsolving thetruepartiallyobservable decisionproblem.
Section5.7. State-of-the-Art Game Programs 185
In 1965 the Russian mathematician Alexander Kronrod called chess the Drosophila of ar-
tificial intelligence. John Mc Carthy disagrees: whereas geneticists use fruit flies to make
discoveries that apply to biology more broadly A I has used chess to do the equivalent of
breeding very fast fruit flies. Perhaps a better analogy is that chess is to A I as Grand Prix
motorracingistothecarindustry: state-of-the-art gameprogramsareblindingly fasthighly
optimized machines that incorporate the latest engineering advances but they arent much
use for doing the shopping or driving off-road. Nonetheless racing and game-playing gen-
erate excitement and a steady stream of innovations that have been adopted by the wider
community. Inthissectionwelookatwhatittakestocomeoutontopinvariousgames.
C HE SS
Chess: I BMs D EE P B LU E chess program now retired is well known for defeating world
champion Garry Kasparov inawidely publicized exhibition match. Deep Blueranonapar-
allel computer with 30 I BM R S6000 processors doing alphabeta search. The unique part
was a configuration of 480 custom V LS I chess processors that performed move generation
andmoveorderingforthelastfewlevelsofthetreeandevaluatedtheleafnodes. Deep Blue
searched up to 30 billion positions per move reaching depth 14 routinely. The key to its
success seemstohavebeen itsability togenerate singular extensions beyond thedepth limit
forsufficiently interesting lines offorcingforced moves. Insomecases thesearch reached a
depth of 40 plies. Theevaluation function had over8000 features many of them describing
highly specific patterns of pieces. An opening book of about 4000 positions was used as
well as a database of 700000 grandmaster games from which consensus recommendations
could beextracted. Thesystem also used alarge endgame database of solved positions con-
taining all positions with five pieces and many with six pieces. This database had the effect
ofsubstantially extending theeffectivesearchdepthallowing Deep Bluetoplayperfectlyin
somecasesevenwhenitwasmanymovesawayfromcheckmate.
Thesuccessof D EE P B LU Ereinforcedthewidelyheldbeliefthatprogressincomputer
game-playing has come primarily from ever-more-powerful hardwarea view encouraged
by I BM. But algorithmic improvements have allowed programs running on standard P Cs
to win World Computer Chess Championships. A variety of pruning heuristics are used to
reducetheeffectivebranchingfactortolessthan3comparedwiththeactualbranchingfactor
ofabout35. Themostimportantoftheseisthenullmoveheuristic whichgenerates agood
N UL LM OV E
lower bound on the value of a position using a shallow search in which the opponent gets
to move twice at the beginning. This lower bound often allows alphabeta pruning without
theexpenseofafull-depth search. Alsoimportant is futilitypruningwhichhelpsdecidein
F UT IL IT YP RU NI NG
advancewhichmoveswillcauseabetacutoffinthesuccessor nodes.
H YD RA can be seen as the successor to D EE P B LU E. H YD RA runs on a 64-processor
cluster with1gigabyte perprocessor andwithcustom hardware intheform of F PG AField
Programmable Gate Arraychips. H YD RAreaches200millionevaluationspersecondabout
the same as Deep Blue but H YD RA reaches 18 plies deep rather than just 14 because of
aggressive useofthenullmoveheuristic andforwardpruning.
R YB KA winnerof the 2008 and 2009 World Computer Chess Championships iscon-
sidered the strongest current computer player. It uses an off-the-shelf 8-core 3.2 G Hz Intel
Xeon processor but little is known about the design of the program. R YB KAs main ad-
vantage appears to be its evaluation function which has been tuned by its main developer
International Master Vasik Rajlichandatleastthreeothergrandmasters.
The most recent matches suggest that the top computer chess programs have pulled
aheadofallhumancontenders. Seethehistorical notesfordetails.
C HE CK ER S
Checkers: Jonathan Schaeffer and colleagues developed C HI NO OK which runs on regular
P Csand uses alphabeta search. Chinook defeated the long-running human champion in an
abbreviatedmatchin1990andsince2007 CH IN OO Khasbeenabletoplayperfectlybyusing
alphabeta searchcombined withadatabase of39trillionendgamepositions.
Othello also called Reversi is probably more popular as a computer game than as a board
O TH EL LO
game. It has a smaller search space than chess usually 5 to 15 legal moves but evaluation
expertisehadtobedevelopedfromscratch. In1997the L OG IS TE LL O program Buro2002
defeatedthehumanworldchampion Takeshi Murakamibysixgamestonone. Itisgenerally
acknowledged thathumansarenomatchforcomputers at Othello.
Backgammon: Section5.5explainedwhytheinclusionofuncertaintyfromdicerollsmakes
B AC KG AM MO N
deep search an expensive luxury. Most work on backgammon has gone into improving the
evaluation function. Gerry Tesauro 1992 combined reinforcement learning with neural
networks to develop a remarkably accurate evaluator that is used with a search to depth 2
or 3. After playing more than a million training games against itself Tesauros program
T D-G AM MO Niscompetitivewithtophumanplayers. Theprogramsopinionsontheopen-
ingmovesofthegamehaveinsomecasesradicallyalteredthe receivedwisdom.
Go is the most popular board game in Asia. Because the board is 19 19 and moves are
G O
allowed into almost every empty square the branching factor starts at 361 which is too
daunting for regular alphabeta search methods. In addition it is difficult to write an eval-
uation function because control of territory is often very unpredictable until the endgame.
Therefore thetopprograms suchas M OG Oavoid alphabeta search andinstead use Monte
Carlorollouts. Thetrickistodecidewhatmovestomakeinthecourseoftherollout. Thereis
noaggressive pruning; allmovesarepossible. The U CTupperconfidence bounds ontrees
method works by making random moves in the first few iterations and over time guiding
thesampling process toprefermovesthathaveledtowinsinprevious samples. Sometricks
are added including knowledge-based rules that suggest particular moves whenever a given
pattern isdetected andlimitedlocal searchtodecide tactical questions. Someprogramsalso
C OM BI NA TO RI AL include special techniques from combinatorial game theory to analyze endgames. These
G AM ET HE OR Y
techniques decompose aposition intosub-positions thatcanbeanalyzed separately andthen
combined Berlekamp and Wolfe 1994; Muller 2003. The optimal solutions obtained in
this way have surprised many professional Go players who thought they had been playing
optimally allalong. Current Goprograms play atthemasterlevelonareduced 99board
butarestillatadvanced amateurlevelonafullboard.
Bridge is a card game of imperfect information: a players cards are hidden from the other
B RI DG E
players. Bridge is also a multiplayer game with four players instead of two although the
Section5.8. Alternative Approaches 187
players are paired into two teams. As in Section 5.6 optimal play in partially observable
gameslikebridgecanincludeelementsofinformationgatheringcommunicationandcareful
weighing of probabilities. Many of these techniques are used in the Bridge Baron program
Smith et al. 1998 which won the 1997 computer bridge championship. While it does
not play optimally Bridge Baron is one of the few successful game-playing systems to use
complexhierarchicalplanssee Chapter11involvinghigh-levelideassuchasfinessingand
squeezingthatarefamiliartobridgeplayers.
The G IBprogram Ginsberg1999wonthe2000computerbridgechampionshipquite
decisivelyusingthe Monte Carlomethod. Sincethenotherwinningprogramshavefollowed
E XP LA NA TI ON-
G IBslead. G IBsmajorinnovation isusingexplanation-based generalization tocompute
B AS ED
G EN ER AL IZ AT IO N
and cache general rules foroptimal play in various standard classes of situations rather than
evaluating each situation individually. For example in a situation where one player has the
cards A-K-Q-J-4-3-2 of one suit and another player has 10-9-8-7-6-5 there are 76 42
ways that the first player can lead from that suit and the second player can follow. But G IB
treats these situations as just two: the first player can lead either a high card or a low card;
theexactcardsplayeddontmatter. Withthisoptimization andafewothers G IBcansolve
a52-card fullyobservable deal exactly inabout asecond. G IBstactical accuracy makesup
foritsinabilitytoreasonaboutinformation. Itfinished12thinafieldof35intheparcontest
involving just play of the hand not bidding at the 1998 human world championship far
exceeding theexpectations ofmanyhumanexperts.
Thereare several reasons why G IB plays atexpert level with Monte Carlosimulation
whereas Kriegspiel programs donot. First G IBs evaluation ofthe fully observable version
ofthegameisexactsearching thefullgametree while Kriegspiel programs relyoninexact
heuristics. But far more important is the fact that in bridge most of the uncertainty in the
partially observable information comesfromtherandomness ofthedealnotfromtheadver-
sarial play of the opponent. Monte Carlo simulation handles randomness well but does not
alwayshandlestrategy wellespecially whenthestrategy involves thevalueofinformation.
Scrabble: Mostpeoplethinkthehardpartabout Scrabbleiscomingupwithgoodwordsbut
S CR AB BL E
giventheofficialdictionary itturnsouttoberathereasytoprogramamovegeneratortofind
the highest-scoring move Gordon 1994. That doesnt mean the game is solved however:
merely taking the top-scoring move each turn results in a good but not expert player. The
problem is that Scrabble is both partially observable and stochastic: you dont know what
letters the other player has or what letters you will draw next. So playing Scrabble well
combines the difficulties of backgammon and bridge. Nevertheless in 2006 the Q UA CK LE
program defeatedtheformerworldchampion David Boys32.
Because calculating optimal decisions in games is intractable in most cases all algorithms
must make some assumptions and approximations. The standard approach based on mini-
maxevaluationfunctions andalphabeta isjustonewaytodothis. Probablybecauseithas
M AX
M IN 99 100
Figure5.14 Atwo-plygametreeforwhichheuristicminimaxmaymakeanerror.
been worked on for so long the standard approach dominates other methods in tournament
play. Some believe that this has caused game playing to become divorced from the main-
stream of A Iresearch: thestandard approach nolongerprovides muchroom fornewinsight
intogeneralquestions ofdecision making. Inthissection welookatthealternatives.
First let us consider heuristic minimax. It selects an optimal move in a given search
tree provided that the leaf node evaluations are exactly correct. In reality evaluations are
usually crude estimates of the value of a position and can be considered to have large errors
associated with them. Figure 5.14 shows a two-ply game tree for which minimax suggests
taking the right-hand branch because 100 99. That is the correct move if the evaluations
are all correct. But of course the evaluation function is only approximate. Suppose that
the evaluation of each node has an error that is independent of other nodes and is randomly
distributed with mean zero and standard deviation of σ. Then when σ 5 the left-hand
branch is actually better 71 of the time and 58 of the time when σ 2. The intuition
behind this is that the right-hand branch has four nodes that are close to 99; if an error in
the evaluation of any one of the four makes the right-hand branch slip below 99 then the
left-hand branchisbetter.
Inrealitycircumstances areactuallyworsethanthisbecausetheerrorintheevaluation
functionisnotindependent. Ifwegetonenodewrongthechancesarehighthatnearbynodes
suggests that in fact it might have a higher true value. We can use an evaluation function
thatreturns aprobability distribution overpossible values butitisdifficulttocombinethese
distributions properly because wewonthaveagoodmodeloftheverystrongdependencies
thatexistbetweenthevaluesofsiblingnodes
Nextweconsiderthesearchalgorithmthatgeneratesthetree. Theaimofanalgorithm
designeristospecifyacomputationthatrunsquicklyandyieldsagoodmove. Thealphabeta
algorithmisdesignednotjusttoselectagoodmovebutalsotocalculateboundsonthevalues
of all the legal moves. Tosee whythis extra information is unnecessary consider aposition
in which there is only one legal move. Alphabeta search still will generate and evaluate a
large search tree telling usthatthe onlymoveisthebest moveand assigning itavalue. But
since we have to make the move anyway knowing the moves value is useless. Similarly if
thereisoneobviouslygoodmoveandseveralmovesthatarelegalbutleadtoaquicklosswe
Section5.9. Summary 189
wouldnotwantalphabetatowastetimedeterminingaprecisevalueforthelonegoodmove.
Bettertojustmakethemovequicklyandsavethetimeforlater. Thisleadstotheideaofthe
utility of a node expansion. A good search algorithm should select node expansions of high
utilitythat is ones that are likely tolead tothe discovery ofa significantly better move. If
there are no node expansions whose utility is higher than their cost in terms of time then
the algorithm should stop searching and make a move. Notice that this works not only for
clear-favorite situations but also forthe case of symmetrical moves forwhich no amount of
searchwillshowthatonemoveisbetterthananother.
This kind of reasoning about what computations to do is called metareasoning rea-
M ET AR EA SO NI NG
soning about reasoning. It applies not just to game playing but to any kind of reasoning
at all. All computations are done in the service of trying to reach better decisions all have
costs andallhave somelikelihood ofresulting inacertain improvement indecision quality.
Alphabeta incorporates thesimplest kindofmetareasoning namely atheorem totheeffect
thatcertainbranches ofthetreecanbeignored withoutloss. Itispossible todomuchbetter.
In Chapter16weseehowtheseideascanbemadepreciseandimplementable.
Finally let us reexamine the nature of search itself. Algorithms for heuristic search
and for game playing generate sequences of concrete states starting from the initial state
and then applying an evaluation function. Clearly this is not how humans play games. In
chessoneoftenhasaparticulargoalinmindforexampletrappingtheopponentsqueen
and can use this goal to selectively generate plausible plans for achieving it. This kind of
goal-directed reasoning or planning sometimes eliminates combinatorial search altogether.
David Wilkins 1980 P AR AD IS E is the only program to have used goal-directed reasoning
successfully in chess: it was capable of solving some chess problems requiring an 18-move
combination. As yet there is no good understanding of how to combine the two kinds of
algorithms into a robust and efficient system although Bridge Baron might be a step in the
right direction. A fully integrated system would be a significant achievement not just for
game-playing research but also for A I research in general because it would be a good basis
forageneralintelligent agent.
We have looked at a variety of games to understand what optimal play means and to under-
standhowtoplaywellinpractice. Themostimportant ideasareasfollows:
A game can be defined by the initial state how the board is set up the legal actions
in each state the result of each action a terminal test which says when the game is
overandautilityfunctionthatapplies toterminalstates.
In two-player zero-sum games with perfect information the minimax algorithm can
selectoptimalmovesbyadepth-firstenumeration ofthegametree.
The alphabeta search algorithm computes the same optimal move as minimax but
achievesmuchgreaterefficiencybyeliminating subtreesthatareprovablyirrelevant.
Usuallyitisnotfeasibletoconsiderthewholegametreeevenwithalphabetasowe
6
C ON ST RA IN T
S AT IS FA CT IO N P RO BL EM S
Inwhichweseehowtreatingstatesasmorethanjustlittleblackboxesleadstothe
inventionofarangeofpowerfulnewsearchmethodsandadeeperunderstanding
ofproblem structureandcomplexity.
Chapters 3 and 4 explored the idea that problems can be solved by searching in a space of
states. Thesestatescanbeevaluated bydomain-specific heuristics andtestedtoseewhether
they are goal states. From the point of view of the search algorithm however each state is
atomicorindivisiblea blackboxwithnointernalstructure.
This chapter describes away to solve a wide variety of problems more efficiently. We
use a factored representation for each state: a set of variables each of which has a value.
A problem is solved when each variable has a value that satisfies all the constraints on the
C ON ST RA IN T
variable. Aproblem described thiswayiscalleda constraintsatisfaction problemor C SP.
S AT IS FA CT IO N
P RO BL EM
C SPsearchalgorithmstakeadvantageofthestructureofstatesandusegeneral-purpose
ratherthanproblem-specific heuristicstoenablethesolutionofcomplexproblems. Themain
ideaistoeliminatelargeportionsofthesearchspaceallatoncebyidentifyingvariablevalue
combinations thatviolatetheconstraints.
Aconstraint satisfaction problemconsists ofthreecomponents X Dand C:
X isasetofvariables X ...X .
Disasetofdomains D ...D oneforeachvariable.
C isasetofconstraints thatspecifyallowablecombinations ofvalues.
Each domain D consists of a set of allowable values v ...v for variable X . Each
i 1 k i
constraint C consistsofapaircid:16scoperelcid:17wherescope isatupleofvariablesthatparticipate
i
intheconstraintandrel isarelationthatdefinesthevaluesthatthosevariablescantakeon. A
relationcanberepresented asanexplicitlistofalltuples ofvaluesthatsatisfytheconstraint
or as an abstract relation that supports two operations: testing if a tuple is a member of the
relation andenumerating themembersoftherelation. Forexample if X and X bothhave
202
Section6.1. Defining Constraint Satisfaction Problems 203
the domain A B then the constraint saying the two variables must have different values
canbewrittenascid:16 X X A BB Acid:17 orascid:16 X X X cid:7 X cid:17.
To solve a C SP we need to define a state space and the notion of a solution. Each
state in a C SP is defined by an assignment of values to some or all of the variables X
A SS IG NM EN T i
v X v .... Anassignment that does not violate anyconstraints iscalled aconsistent
C ON SI ST EN T i j j
C OM PL ET E orlegalassignment. Acompleteassignmentisoneinwhicheveryvariable isassigned and
A SS IG NM EN T
a solution to a C SP is a consistent complete assignment. A partial assignment is one that
S OL UT IO N
P AR TI AL assignsvaluestoonlysomeofthevariables.
A SS IG NM EN T
Suppose that having tired of Romania we are looking at a map of Australia showing each
of its states and territories Figure 6.1a. We are given the task of coloring each region
either red green orblue in such away that no neighboring regions have the samecolor. To
formulatethisasa C SPwedefinethevariables tobetheregions
X W AN TQ NS WV SA T.
The domain of each variable is the set D redgreenblue. The constraints require
i
neighboring regionstohavedistinctcolors. Sincetherearenineplaceswhereregionsborder
therearenineconstraints:
C S A cid:7 W AS A cid:7 N TS A cid:7 Q SA cid:7 N SW SA cid:7 V
W A cid:7 N TN T cid:7 Q Qcid:7 N SW NS W cid:7 V.
Hereweareusingabbreviations; S A cid:7 W Aisashortcutforcid:16 SA WA SA cid:7 W Acid:17where
S A cid:7 W Acanbefullyenumerated inturnas
redgreenredbluegreenredgreenblueblueredbluegreen.
Therearemanypossible solutions tothisproblem suchas
W Ared N T green Qred N SW green V red S Ablue T red .
It can be helpful to visualize a C SP as a constraint graph as shown in Figure 6.1b. The
C ON ST RA IN TG RA PH
nodes ofthegraph correspond tovariables oftheproblem andalink connects anytwovari-
ablesthatparticipate inaconstraint.
Why formulate a problem as a C SP? One reason is that the C SPs yield a natural rep-
resentation for a wide variety of problems; if you already have a C SP-solving system it is
ofteneasiertosolveaproblemusingitthantodesignacustomsolutionusinganothersearch
technique. Inaddition C SPsolverscanbefasterthanstate-space searchers because the C SP
solver canquickly eliminate large swatches ofthe search space. Forexample once wehave
chosen S Ablueinthe Australiaproblemwecanconcludethatnoneofthefiveneighbor-
ingvariablescantakeonthevalueblue. Withouttakingadvantageofconstraintpropagation
a search procedure would have to consider 35243 assignments for the five neighboring
variables; withconstraint propagation weneverhavetoconsider blue asavalue sowehave
only2532assignments tolookatareduction of87.
In regular state-space search we can only ask: is this specific state a goal? No? What
aboutthisone? With C SPsoncewefindoutthatapartialassignmentisnotasolutionwecan
N T
Q
W A
Northern
Territory
Queensland
Western S A N SW
Australia
South
Australia New
South V
Wales
Victoria
T
Tasmania
a b
be viewed as a constraint satisfaction problem C SP. The goal is to assign colors to each
regionso that no neighboringregionshave the same color. b The map-coloringproblem
representedasaconstraintgraph.
immediately discard further refinements of the partial assignment. Furthermore we can see
whytheassignmentisnotasolutionweseewhichvariablesviolateaconstraintsowecan
focus attention on the variables that matter. As a result many problems that are intractable
forregularstate-space searchcanbesolvedquickly whenformulated asa C SP.
Factorieshavetheproblemofschedulingadaysworthofjobssubjecttovariousconstraints.
Inpracticemanyoftheseproblemsaresolvedwith C SPtechniques. Considertheproblemof
schedulingtheassemblyofacar. Thewholejobiscomposedoftasksandwecanmodeleach
task as avariable where the value of each variable isthe time that the task starts expressed
as an integer number of minutes. Constraints can assert that one task must occur before
anotherfor example awheel must beinstalled before the hubcap isput onand that only
so many tasks can go on at once. Constraints can also specify that a task takes a certain
amountoftimetocomplete.
Weconsiderasmallpartofthecarassembly consisting of15 tasks: installaxlesfront
and back affix all four wheels right and left front and back tighten nuts for each wheel
affixhubcaps andinspect thefinalassembly. Wecanrepresent thetaskswith15variables:
X Axle Axle Wheel Wheel Wheel Wheel Nuts
F B R F L F R B L B R F
Nuts Nuts Nuts Cap Cap Cap Cap Inspect.
L F R B L B R F L F R B L B
The value of each variable is the time that the task starts. Next we represent precedence
P RE CE DE NC E constraints between individual tasks. Whenever a task T must occur before task T and
C ON ST RA IN TS 1 2
task T takesduration d tocompleteweaddanarithmeticconstraint oftheform
T d T .
Section6.1. Defining Constraint Satisfaction Problems 205
In our example the axles have to be in place before the wheels are put on and it takes 10
minutestoinstallanaxlesowewrite
Axle 10 Wheel ; Axle 10 Wheel ;
F R F F L F
Axle 10 Wheel ; Axle 10 Wheel .
B R B B L B
Nextwesaythatforeachwheelwemustaffixthewheelwhichtakes1minutethentighten
thenuts2minutesandfinallyattachthehubcap1minutebutnotrepresented yet:
Wheel 1 Nuts ; Nuts 2 Cap ;
R F R F R F R F
Wheel 1 Nuts ; Nuts 2 Cap ;
L F L F L F L F
Wheel 1 Nuts ; Nuts 2 Cap ;
R B R B R B R B
Wheel 1 Nuts ; Nuts 2 Cap .
L B L B L B L B
Supposewehavefourworkerstoinstallwheelsbuttheyhavetoshareonetoolthathelpsput
D IS JU NC TI VE the axle in place. We need a disjunctive constraint to say that Axle and Axle must not
C ON ST RA IN T F B
overlapintime;eitheronecomesfirstortheotherdoes:
Axle 10 Axle or Axle 10 Axle .
F B B F
This looks like a more complicated constraint combining arithmetic and logic. But it still
reducestoasetofpairsofvaluesthat Axle and Axle cantakeon.
F F
We also need to assert that the inspection comes last and takes 3 minutes. For every
variableexcept Inspect weaddaconstraintoftheform Xd Inspect. Finallysuppose
X
thereisarequirement togetthewholeassembly donein30minutes. Wecanachievethatby
limitingthedomainofallvariables:
D 123...27.
i
This particular problem is trivial to solve but C SPs have been applied to job-shop schedul-
ing problems like this with thousands of variables. In some cases there are complicated
constraints that are difficult to specify in the C SP formalism and more advanced planning
techniques areusedasdiscussed in Chapter11.
The simplest kind of C SP involves variables that have discrete finite domains. Map-
D IS CR ET ED OM AI N
coloring problems andscheduling withtimelimitsarebothofthiskind. The8-queens prob-
F IN IT ED OM AI N
lem described in Chapter 3 can also be viewed as a finite-domain C SP where the variables
Q ...Q are the positions of each queen in columns 1...8 and each variable has the
domain D 12345678.
i
Adiscretedomaincanbeinfinitesuchasthesetofintegersorstrings. Ifwedidntput
I NF IN IT E
a deadline on the job-scheduling problem there would be an infinite number of start times
for each variable. With infinite domains it is no longer possible to describe constraints by
C ON ST RA IN T enumerating all allowed combinations of values. Instead a constraint language must be
L AN GU AG E
used that understands constraints such as T d T directly without enumerating the
set of pairs of allowable values for T T . Special solution algorithms which we do not
L IN EA R discuss here exist for linear constraints on integer variablesthat is constraints such as
C ON ST RA IN TS
the one just given in which each variable appears only in linear form. It can be shown that
N ON LI NE AR noalgorithm existsforsolving general nonlinearconstraintsonintegervariables.
C ON ST RA IN TS
C ON TI NU OU S Constraint satisfaction problems with continuous domains are common in the real
D OM AI NS
worldandarewidelystudiedinthefieldofoperations research. Forexamplethescheduling
ofexperiments on the Hubble Space Telescope requires very precise timingofobservations;
the start and finish of each observation and maneuver are continuous-valued variables that
must obey a variety of astronomical precedence and power constraints. The best-known
category of continuous-domain C SPs is that of linear programming problems where con-
straintsmustbelinearequalitiesorinequalities. Linearprogrammingproblemscanbesolved
in time polynomial in the number of variables. Problems with different types of constraints
andobjectivefunctions havealsobeenstudiedquadratic programming second-order conic
programming andsoon.
In addition to examining the types of variables that can appear in C SPs it is useful to
look at the types of constraints. The simplest type is the unary constraint which restricts
U NA RY CO NS TR AI NT
thevalueofasinglevariable. Forexampleinthemap-coloring problem itcouldbethecase
that South Australians wont tolerate the color green; we can express that with the unary
constraint cid:16 SA SA cid:7 greencid:17
A binary constraint relates two variables. For example S A cid:7 N SW is a binary
B IN AR YC ON ST RA IN T
constraint. A binary C SP is one with only binary constraints; it can be represented as a
constraint graph asin Figure6.1b.
We can also describe higher-order constraints such as asserting that the value of Y is
between X and Zwiththeternaryconstraint Between X YZ.
G LO BA L A constraint involving an arbitrary number of variables is called a global constraint.
C ON ST RA IN T
Thenameistraditional butconfusing becauseitneednotinvolveallthevariablesinaprob-
lem. One of the most common global constraints is Alldiff which says that all of the
variables involved in the constraint must have different values. In Sudoku problems see
Section 6.2.6 all variables in a row or column must satisfy an Alldiff constraint. An-
other example isprovided by cryptarithmetic puzzles. See Figure 6.2a. Each letter in a
C RY PT AR IT HM ET IC
cryptarithmetic puzzle represents a different digit. For the case in Figure 6.2a this would
be represented as the global constraint Alldiff F TU WR O. The addition constraints
onthefourcolumnsofthepuzzlecanbewrittenasthefollowingn-aryconstraints:
O O R10 C
10
C W W U 10 C
C T T O10 C
C F
where C C and C areauxiliaryvariablesrepresentingthedigitcarriedoverintothe
tens hundreds or thousands column. These constraints can be represented in a constraint
C ON ST RA IN T hypergraphsuchastheoneshownin Figure6.2b. Ahypergraphconsistsofordinarynodes
H YP ER GR AP H
thecirclesinthefigureandhypernodes thesquares whichrepresent n-aryconstraints.
Alternatively as Exercise 6.6 asks you to prove every finite-domain constraint can be
reducedtoasetofbinaryconstraintsifenoughauxiliaryvariablesareintroducedsowecould
transform any C SPinto one withonly binary constraints; thismakes the algorithms simpler.
Anotherwaytoconvertann-ary C SPtoabinaryoneisthedualgraphtransformation: create
D UA LG RA PH
anewgraphinwhichtherewillbeonevariableforeachconstraint intheoriginal graph and
Section6.1. Defining Constraint Satisfaction Problems 207
T W O F T U W R O
T W O
F O U R
C C C
a b
Figure6.2 a Acryptarithmeticproblem.Eachletterstandsforadistinctdigit;theaimis
tofindasubstitutionofdigitsforletterssuchthattheresultingsumisarithmeticallycorrect
withtheaddedrestrictionthatnoleadingzeroesareallowed. b Theconstrainthypergraph
for the cryptarithmetic problem showing the Alldiff constraint square box at the top as
wellasthecolumnadditionconstraintsfoursquareboxesinthemiddle. Thevariables C
1
C and C representthecarrydigitsforthethreecolumns.
onebinaryconstraintforeachpairofconstraintsintheoriginalgraphthatsharevariables. For
example if the original graph has variables X YZ and constraints cid:16 XY ZC cid:17 and
1
cid:16 XY C cid:17 then the dual graph would have variables C C with the binary constraint
cid:16 XY R cid:17where X Yarethesharedvariablesand R isanewrelationthatdefinesthe
constraint betweenthesharedvariables asspecifiedbythe original C and C .
Therearehowevertworeasonswhywemightpreferaglobalconstraintsuchas Alldiff
rather than a set of binary constraints. First it is easier and less error-prone to write the
problem description using Alldiff. Seconditispossible todesignspecial-purpose inference
algorithmsforglobalconstraintsthatarenotavailableforasetofmoreprimitiveconstraints.
Wedescribetheseinference algorithmsin Section6.2.5.
Theconstraintswehavedescribedsofarhaveallbeenabsoluteconstraints violationof
P RE FE RE NC E which rules out apotential solution. Many real-world C SPsinclude preference constraints
C ON ST RA IN TS
indicating whichsolutions arepreferred. Forexampleina university class-scheduling prob-
lem there are absolute constraints that no professor can teach two classes at the same time.
Butwealsomayallowpreference constraints: Prof.Rmightpreferteaching inthemorning
whereas Prof. N prefers teaching in the afternoon. A schedule that has Prof. R teaching at
2p.m.wouldstillbeanallowablesolutionunless Prof.Rhappenstobethedepartmentchair
butwouldnotbeanoptimalone. Preference constraints canoftenbeencoded ascostsonin-
dividual variable assignmentsfor example assigning an afternoon slot for Prof. R costs
formulation C SPs with preferences can be solved with optimization search methods either
C ON ST RA IN T
path-based or local. We call such a problem a constraint optimization problem or C OP.
O PT IM IZ AT IO N
P RO BL EM
Linearprogramming problemsdothiskindofoptimization.
Inregular state-space search analgorithm can doonly one thing: search. In C SPsthere is a
choice: analgorithmcansearchchooseanewvariableassignmentfromseveralpossibilities
or do a specific type of inference called constraint propagation: using the constraints to
I NF ER EN CE
C ON ST RA IN T reduce the number of legal values for a variable which in turn can reduce the legal values
P RO PA GA TI ON
foranothervariable andsoon. Constraint propagation may beintertwined withsearch orit
maybedoneasapreprocessing step before searchstarts. Sometimesthispreprocessing can
solvethewholeproblemsonosearchisrequired atall.
L OC AL The key idea is local consistency. If we treat each variable as a node in a graph see
C ON SI ST EN CY
sistency ineach part of the graph causes inconsistent values tobe eliminated throughout the
graph. Therearedifferent typesoflocalconsistency whichwenowcoverinturn.
A single variable corresponding to a node in the C SP network is node-consistent if all
N OD EC ON SI ST EN CY
the values in the variables domain satisfy the variables unary constraints. For example
in the variant of the Australia map-coloring problem Figure 6.1 where South Australians
dislike green the variable S A starts with domain redgreenblue and we can make it
node consistent byeliminating greenleaving S Awiththe reduced domain redblue. We
saythatanetworkisnode-consistent ifeveryvariableinthenetworkisnode-consistent.
It is always possible to eliminate all the unary constraints in a C SP by running node
consistency. It is also possible to transform all n-ary constraints into binary ones see Ex-
ercise 6.6. Because of this it is common to define C SP solvers that work with only binary
constraints; wemakethatassumption fortherestofthischapter exceptwherenoted.
A variable in a C SP is arc-consistent if every value in its domain satisfies the variables
A RC CO NS IS TE NC Y
binaryconstraints. Moreformally X isarc-consistent withrespecttoanothervariable X if
i j
forevery value in the current domain D there is some value in the domain D that satisfies
i j
thebinary constraint onthearc X X . Anetworkisarc-consistent ifeveryvariable isarc
i j
consistentwitheveryothervariable. Forexampleconsidertheconstraint Y X2 wherethe
domainofboth X and Y isthesetofdigits. Wecanwritethisconstraint explicitly as
cid:16 XY00112439cid:17 .
To make X arc-consistent with respect to Y we reduce Xs domain to 0123. If we
alsomake Y arc-consistent withrespect to Xthen Ysdomainbecomes 0149 andthe
whole C SPisarc-consistent.
Ontheotherhandarcconsistency candonothingforthe Australiamap-coloring prob-
lem. Considerthefollowinginequality constraint on S AW A:
redgreenredbluegreenredgreenblueblueredbluegreen.
Section6.2. Constraint Propagation: Inferencein C SPs 209
function A C-3cspreturnsfalseifaninconsistencyisfoundandtrueotherwise
inputs:cspabinary C SPwithcomponents X D C
localvariables: queueaqueueofarcsinitiallyallthearcsincsp
whilequeue isnotemptydo
Xi Xj R EM OV E-F IR STqueue
if R EV IS Ecsp Xi Xjthen
ifsizeof Di 0thenreturnfalse
foreach Xk in Xi.N EI GH BO RS-Xjdo
add Xk Xitoqueue
returntrue
function R EV IS Ecsp Xi Xjreturnstrueiffwerevisethedomainof Xi
revisedfalse
foreachx in Dido
ifnovalueyin Dj allowsxytosatisfytheconstraintbetween Xiand Xj then
deletex from Di
revisedtrue
returnrevised
isarc-consistentorsomevariablehasanemptydomainindicatingthatthe C SP cannotbe
solved. Thename A C-3wasusedbythealgorithmsinventor Mackworth1977because
itsthethirdversiondevelopedinthepaper.
No matter what value you choose for S A or for W A there is a valid value for the other
variable. Soapplying arcconsistency hasnoeffectonthedomainsofeithervariable.
The most popular algorithm for arc consistency is called A C-3 see Figure 6.3. To
makeeveryvariablearc-consistent the A C-3algorithmmaintainsaqueueofarcstoconsider.
Actually theorderofconsideration isnotimportant sothedatastructure isreallyaset but
traditioncallsitaqueue. Initiallythequeuecontainsallthearcsinthe C SP.A C-3thenpops
offanarbitraryarc X X fromthequeueandmakes X arc-consistent withrespectto X .
i j i j
If this leaves D unchanged the algorithm just moves on to the next arc. But if this revises
i
D makes the domain smaller then we add to the queue all arcs X X where X is a
i k i k
neighborof X . Weneedtodothatbecausethechangein D mightenablefurtherreductions
i i
in the domains of D even if we have previously considered X . If D is revised down to
k k i
nothing thenweknowthewhole C SPhasnoconsistentsolution and A C-3canimmediately
return failure. Otherwise we keep checking trying to remove values from the domains of
variables until no more arcs are in the queue. At that point we are left with a C SP that is
equivalent to the original C SPthey both have the same solutionsbut the arc-consistent
C SPwillinmostcasesbefastertosearchbecauseitsvariables havesmallerdomains.
The complexity of A C-3 can be analyzed as follows. Assume a C SP with n variables
eachwithdomainsizeatmostdandwithcbinaryconstraints arcs. Eacharc X X can
k i
be inserted in the queue only d times because X has at most d values to delete. Checking
i
consistency ofanarccanbedonein Od2timesoweget Ocd3totalworst-casetime.1
It is possible to extend the notion of arc consistency to handle n-ary rather than just
binary constraints; this is called generalized arc consistency or sometimes hyperarc consis-
G EN ER AL IZ ED AR C tency depending onthe author. A variable X is generalized arc consistent with respect to
C ON SI ST EN T i
ann-aryconstraint ifforeveryvalue v inthedomainof X thereexistsatupleofvaluesthat
i
isamemberoftheconstraint hasallitsvalues takenfromthedomains ofthecorresponding
variables and has its X component equal to v. For example if all variables have the do-
i
main 0123 then to make the variable X consistent with the constraint X Y Z
wewould have to eliminate 2 and 3from the domain of X because the constraint cannot be
satisfiedwhen X is2or3.
Arc consistency can go a long way toward reducing the domains of variables sometimes
finding a solution by reducing every domain to size 1 and sometimes finding that the C SP
cannotbesolvedbyreducingsomedomaintosize0. Butforothernetworksarcconsistency
fails to make enough inferences. Consider the map-coloring problem on Australia but with
onlytwocolorsallowedredandblue. Arcconsistencycandonothingbecauseeveryvariable
isalreadyarcconsistent: eachcanberedwithblueattheotherendofthearcorviceversa.
Butclearlythereisnosolutiontotheproblem: because Western Australia Northern Territory
and South Australiaalltoucheachotherweneedatleastthreecolorsforthemalone.
Arc consistency tightens down the domains unary constraints using the arcs binary
constraints. Tomakeprogress onproblems like mapcoloring weneed astronger notion of
consistency. Path consistency tightens the binary constraints by using implicit constraints
P AT HC ON SI ST EN CY
thatareinferredbylookingattriplesofvariables.
A two-variable set X X is path-consistent with respect to a third variable X if
i j m
foreveryassignment X a X bconsistent withtheconstraints on X X thereis
i j i j
anassignmentto X thatsatisfiestheconstraintson X X and X X . Thisiscalled
m i m m j
path consistency because one can think of it aslooking at apath from X to X with X in
i j m
themiddle.
Letsseehowpathconsistency faresincoloring the Australiamapwithtwocolors. We
willmaketheset W AS Apathconsistentwithrespectto N T. Westartbyenumeratingthe
consistent assignments totheset. Inthis case there areonly two: W A red S A blue
and W A blue S A red. We can see that with both of these assignments N T can be
neither red nor blue because it would conflict with either W A or S A. Because there is no
validchoicefor N Tweeliminatebothassignmentsandweendupwithnovalidassignments
for W AS A. Therefore weknow that there canbenosolution tothisproblem. The P C-2
algorithm Mackworth 1977 achieves path consistency in much the same way that A C-3
achievesarcconsistency. Becauseitissosimilar wedonot showithere.
onaveragecases.See Exercise6.13.
Section6.2. Constraint Propagation: Inferencein C SPs 211
Stronger forms of propagation can be defined with the notion of k-consistency. A C SP is
K-C ON SI ST EN CY
k-consistent if for any set of k 1 variables and for any consistent assignment to those
variables a consistent value can always be assigned to any kth variable. 1-consistency says
that given the empty set we can make any set of one variable consistent: this is what we
called node consistency. 2-consistency is the same as arc consistency. Forbinary constraint
networks 3-consistency isthesameaspathconsistency.
S TR ON GL Y A C SP is strongly k-consistent if it is k-consistent and is also k 1-consistent
K-C ON SI ST EN T
k 2-consistent ... all the way down to 1-consistent. Now suppose we have a C SPwith
n nodes and make it strongly n-consistent i.e. strongly k-consistent for kn. We can
then solve the problem as follows: First we choose a consistent value for X . We are then
1
guaranteed to be able to choose a value for X because the graph is 2-consistent for X
becauseitis3-consistent andsoon. Foreachvariable X weneedonlysearchthroughthed
i
valuesinthedomaintofindavalueconsistent with X 1...X i1. Weareguaranteed tofind
a solution in time On2d. Ofcourse there is no free lunch: any algorithm for establishing
n-consistency must take time exponential in n in the worst case. Worse n-consistency also
requires space thatisexponential in n. Thememoryissue isevenmoreseverethanthetime.
Inpractice determining theappropriate levelofconsistency checking ismostlyanempirical
science. It can be said practitioners commonly compute 2-consistency and less commonly
3-consistency.
Rememberthataglobalconstraintisoneinvolvinganarbitrarynumberofvariablesbutnot
necessarily all variables. Global constraints occur frequently in real problems and can be
handledbyspecial-purpose algorithmsthataremoreefficientthanthegeneral-purpose meth-
ods described so far. Forexample the Alldiff constraint says that all the variables involved
must have distinct values as in the cryptarithmetic problem above and Sudoku puzzles be-
low. One simple form of inconsistency detection for Alldiff constraints works as follows:
ifmvariables areinvolved inthe constraint and iftheyhave npossible distinct values alto-
gether andm nthentheconstraint cannotbesatisfied.
This leads to the following simple algorithm: First remove any variable in the con-
straint that has a singleton domain and delete that variables value from the domains of the
remainingvariables. Repeataslongastherearesingleton variables. Ifatanypointanempty
domainisproducedortherearemorevariablesthandomainvaluesleftthenaninconsistency
hasbeendetected.
Thismethodcandetect theinconsistency intheassignment W Ared N SW red
for Figure 6.1. Notice that the variables S A N T and Q are effectively connected by an
Alldiff constraint because each pair must have two different colors. After applying A C-3
with the partial assignment the domain of each variable is reduced to greenblue. That
is we have three variables and only two colors so the Alldiff constraint is violated. Thus
a simple consistency procedure for a higher-order constraint is sometimes more effective
than applying arc consistency to an equivalent set of binary constraints. There are more
complex inference algorithms for Alldiff see van Hoeve and Katriel 2006 that propagate
moreconstraints butaremorecomputationally expensiveto run.
R ES OU RC E Anotherimportanthigher-orderconstraintisthe resourceconstraintsometimescalled
C ON ST RA IN T
the atmost constraint. For example in a scheduling problem let P ...P denote the
numbers of personnel assigned to each of four tasks. The constraint that no more than 10
personnel are assigned in total is written as Atmost10 P P P P . We can detect an
inconsistency simply by checking the sum of the minimum values of the current domains;
for example if each variable has the domain 3456 the Atmost constraint cannot be
satisfied. Wecanalsoenforceconsistencybydeletingthemaximumvalueofanydomainifit
isnotconsistent withtheminimumvaluesoftheotherdomains. Thusifeachvariableinour
examplehasthedomain23456 thevalues5and6canbedeletedfromeachdomain.
For large resource-limited problems with integer valuessuch as logistical problems
involving moving thousands of people in hundreds of vehiclesit is usually not possible to
representthedomainofeachvariableasalargesetofintegersandgraduallyreducethatsetby
consistency-checking methods. Instead domainsarerepresented byupperandlowerbounds
B OU ND S and are managed by bounds propagation. For example in an airline-scheduling problem
P RO PA GA TI ON
lets suppose there are two flights F and F for which the planes have capacities 165 and
385respectively. Theinitialdomainsforthenumbersofpassengers oneachflightarethen
D 0165 and D 0385.
Now suppose we have the additional constraint that the two flights together must carry 420
people: F F 420. Propagating boundsconstraints wereducethedomainsto
D 35165 and D 255385.
B OU ND S We say that a C SP is bounds consistent if for every variable X and for both the lower-
C ON SI ST EN T
boundandupper-bound valuesof Xthereexistssomevalueof Y thatsatisfiestheconstraint
between X and Y for every variable Y. This kind of bounds propagation is widely used in
practical constraint problems.
Thepopular Sudokupuzzlehasintroducedmillionsofpeopletoconstraintsatisfactionprob-
S UD OK U
lems although they may not recognize it. A Sudoku board consists of 81 squares some of
which are initially filled with digits from 1 to 9. The puzzle is to fill in all the remaining
squaressuchthatnodigitappearstwiceinanyrowcolumnor33boxsee Figure6.4. A
rowcolumnorboxiscalledaunit.
The Sudokupuzzles thatareprintedinnewspapers andpuzzle bookshavetheproperty
that thereisexactly onesolution. Although somecanbetricky tosolve byhand taking tens
ofminutes eventhehardest Sudokuproblemsyieldtoa C SPsolverinlessthan0.1second.
A Sudoku puzzle can beconsidered a C SPwith 81 variables one foreach square. We
usethevariable names A1 through A9 forthetoprowlefttoright downto I1 through I9
forthebottom row. Theemptysquares have the domain 123456789 and the pre-
filled squares have a domain consisting of a single value. In addition there are 27 different
Section6.2. Constraint Propagation: Inferencein C SPs 213
A 3 2 6 A 4 8 3 9 2 1 6 5 7
B 9 3 5 1 B 9 6 7 3 4 5 8 2 1
C 1 8 6 4 C 2 5 1 8 7 6 4 9 3
D 8 1 2 9 D 5 4 8 1 3 2 9 7 6
E 7 8 E 7 2 9 5 6 4 1 3 8
F 6 7 8 2 F 1 3 6 7 9 8 2 4 5
G 2 6 9 5 G 3 7 2 6 8 9 5 1 4
H 8 2 3 9 H 8 1 4 2 5 3 7 6 9
I 5 1 3 I 6 9 5 4 1 7 3 8 2
a b
Figure6.4 a A Sudokupuzzleandbitssolution.
Alldiff constraints: oneforeachrowcolumn andboxof9squares.
Alldiff A1 A2 A3 A4 A5 A6 A7 A8 A9
Alldiff B1 B2 B3 B4 B5 B6 B7 B8 B9
""
Alldiff A1 B1 C1 D1 E1 F1 G1 H1 I1
Alldiff A2 B2 C2 D2 E2 F2 G2 H2 I2
""
Alldiff A1 A2 A3 B1 B2 B3 C1 C2 C3
Alldiff A4 A5 A6 B4 B5 B6 C4 C5 C6
""
Letusseehowfararcconsistencycantakeus. Assumethatthe Alldiff constraintshavebeen
expandedintobinaryconstraintssuchas A1 cid:7 A2sothatwecanapplythe A C-3algorithm
directly. Consider variable E6 from Figure 6.4athe empty square between the 2 and the
8inthemiddlebox. Fromtheconstraintsintheboxwecanremovenotonly2and8butalso
9and3. Thatleaves E6 withadomainof4;inotherwords weknowtheanswerfor E6.
Now consider variable I6the square in the bottom middle box surrounded by 1 3 and 3.
Applyingarcconsistencyinitscolumnweeliminate5624sincewenowknow E6 must
be 4 8 9 and 3. We eliminate 1 by arc consistency with I5 and we are left with only the
value 7 inthe domain of I6. Now there are 8known values in column 6 so arc consistency
caninferthat A6 mustbe1. Inference continues along theselines andeventually A C-3can
solve the entire puzzleall the variables have their domains reduced to a single value as
shownin Figure6.4b.
Of course Sudoku would soon lose its appeal if every puzzle could be solved by a
mechanicalapplicationof A C-3andindeed A C-3worksonlyfortheeasiest Sudokupuzzles.
Slightly harder ones can be solved by P C-2 but at a greater computational cost: there are
255960differentpathconstraintstoconsiderina Sudokupuzzle. Tosolvethehardestpuzzles
andtomakeefficientprogress wewillhavetobemoreclever.
Indeedtheappealof Sudokupuzzlesforthehumansolveristheneedtoberesourceful
in applying more complex inference strategies. Aficionados give them colorful names such
as naked triples. That strategy works as follows: in any unit row column or box find
three squares that each have a domain that contains the same three numbers or a subset of
those numbers. Forexample thethree domains mightbe 18 38 and 138. From
that wedont know which square contains 13 or8 but wedoknow that thethree numbers
must be distributed among the three squares. Therefore we can remove 1 3 and 8 from the
domainsofeveryothersquareintheunit.
It is interesting to note how far we can go without saying much that is specific to Su-
doku. Wedoofcoursehavetosaythatthereare81variablesthattheirdomainsarethedigits
1to9andthatthereare27 Alldiff constraints. Butbeyond that allthestrategiesarc con-
sistency path consistency etc.apply generally to all C SPs not just to Sudoku problems.
Even naked triples is really a strategy for enforcing consistency of Alldiff constraints and
hasnothingtodowith Sudokuperse. Thisisthepowerofthe C SPformalism: foreachnew
problem area we only need to define the problem in terms of constraints; then the general
constraint-solving mechanismscantakeover.
Sudoku problems are designed to be solved by inference over constraints. But many other
C SPs cannot be solved by inference alone; there comes a time when we must search for a
solution. In this section we look at backtracking search algorithms that work on partial as-
signments;inthenextsectionwelookatlocalsearchalgorithmsovercompleteassignments.
We could apply a standard depth-limited search from Chapter 3. A state would be a
partial assignment andanaction wouldbeadding var value totheassignment. Butfora
C SPwith nvariables of domain size d wequickly notice something terrible: the branching
factoratthetoplevelisndbecauseanyofdvaluescanbeassignedtoanyofnvariables. At
the next level the branching factor is n 1d and so on for n levels. We generate a tree
withn!dn leaveseventhoughthereareonly dn possible completeassignments!
Our seemingly reasonable but naive formulation ignores crucial property common to
all C SPs: commutativity. Aproblemiscommutativeiftheorderofapplication ofanygiven
C OM MU TA TI VI TY
set of actions has no effect on the outcome. C SPsare commutative because when assigning
values to variables wereach the samepartial assignment regardless of order. Therefore we
need onlyconsider a single variable ateach node inthe search tree. Forexample attheroot
node of a search tree for coloring the map of Australia we might make a choice between
S Ared S Agreen and S Ablue but we would never choose between S Ared and
W Ablue. Withthisrestriction thenumberofleavesis dnaswewouldhope.
Section6.3. Backtracking Searchfor C SPs 215
function B AC KT RA CK IN G-S EA RC Hcspreturnsasolutionorfailure
return B AC KT RA CKcsp
function B AC KT RA CKassignmentcspreturnsasolutionorfailure
ifassignment iscompletethenreturnassignment
var S EL EC T-U NA SS IG NE D-V AR IA BL Ecsp
foreachvalue in O RD ER-D OM AI N-V AL UE Svarassignmentcspdo
ifvalue isconsistentwithassignment then
addvar valuetoassignment
inferences I NF ER EN CEcspvarvalue
ifinferences cid:7 failure then
addinferences toassignment
result B AC KT RA CKassignmentcsp
ifresult cid:7failure then
returnresult
removevar valueandinferences fromassignment
returnfailure
Figure6.5 Asimplebacktrackingalgorithmforconstraintsatisfactionproblems. Theal-
gorithmismodeledontherecursivedepth-firstsearchof Chapter3.Byvaryingthefunctions
S EL EC T-U NA SS IG NE D-V AR IA BL E and O RD ER-D OM AI N-V AL UE S we can implementthe
general-purposeheuristicsdiscussedinthetext. Thefunction I NF ER EN CEcanoptionallybe
used to impose arc- path- or k-consistency as desired. If a value choice leads to failure
noticedeitherby I NF ER EN CEorby B AC KT RA CKthenvalueassignmentsincludingthose
madeby I NF ER EN CEareremovedfromthecurrentassignmentandanewvalueistried.
B AC KT RA CK IN G The term backtracking search is used for a depth-first search that chooses values for
S EA RC H
one variable atatime and backtracks when avariable has nolegal values left to assign. The
algorithmisshownin Figure6.5. Itrepeatedlychoosesanunassigned variableandthentries
allvaluesinthedomainofthatvariableinturntryingtofindasolution. Ifaninconsistencyis
detectedthen B AC KT RA CKreturnsfailurecausingthepreviouscalltotryanothervalue. Part
of the search tree forthe Australia problem is shown in Figure 6.6 where wehave assigned
variables in the order W AN TQ.... Because the representation of C SPs is standardized
there is no need to supply B AC KT RA CK IN G-S EA RC H with a domain-specific initial state
actionfunction transition modelorgoaltest.
Noticethat B AC KT RA CK IN G-S EA RC H keepsonlyasinglerepresentation ofastateand
altersthatrepresentation ratherthancreating newonesasdescribed onpage87.
In Chapter 3 we improved the poor performance of uninformed search algorithms by
supplying them with domain-specific heuristic functions derived from ourknowledge of the
problem. Itturnsoutthatwecansolve C SPsefficientlywithoutsuchdomain-specific knowl-
edge. Instead we can add some sophistication to the unspecified functions in Figure 6.5
usingthemtoaddressthefollowingquestions:
1. Which variable should be assigned next S EL EC T-U NA SS IG NE D-V AR IA BL E and in
whatordershoulditsvaluesbetried O RD ER-D OM AI N-V AL UE S?
W Ared W Agreen W Ablue
W Ared W Ared
N Tgreen N Tblue
W Ared W Ared
N Tgreen N Tgreen
Qred Qblue
Figure6.6 Partofthesearchtreeforthemap-coloringproblemin Figure6.1.
2. Whatinferences should beperformed ateachstepinthesearch I NF ER EN CE?
3. Whenthesearcharrivesatanassignmentthatviolatesaconstraintcanthesearchavoid
repeating thisfailure?
Thesubsections thatfollowanswereachofthesequestions inturn.
Thebacktracking algorithm containstheline
var S EL EC T-U NA SS IG NE D-V AR IA BL Ecsp .
Thesimpleststrategyfor S EL EC T-U NA SS IG NE D-V AR IA BL E istochoosethenextunassigned
variableinorder X X .... Thisstaticvariable ordering seldomresultsinthemosteffi-
cientsearch. Forexampleaftertheassignmentsfor W Ared and N T greenin Figure6.6
thereisonlyonepossiblevaluefor S Asoitmakessensetoassign S Ablue nextratherthan
assigning Q. Infactafter S Aisassignedthechoicesfor Q NS Wand V areallforced. This
intuitiveideachoosingthevariablewiththefewestlegalvaluesiscalledtheminimum-
M IN IM UM- remaining-values M RVheuristic. Italsohasbeencalledthemostconstrainedvariableor
R EM AI NI NG-V AL UE S
fail-first heuristic thelatterbecause itpicksavariablethatismostlikelytocauseafailure
soon thereby pruning the search tree. If some variable X has no legal values left the M RV
heuristicwillselect X andfailurewillbedetectedimmediatelyavoiding pointlesssearches
through other variables. The M RV heuristic usually performs better than a random or static
orderingsometimesbyafactorof1000ormorealthoughtheresultsvarywidelydepending
ontheproblem.
The M RVheuristic doesnt help atallinchoosing thefirstregion tocolorin Australia
becauseinitiallyeveryregionhasthreelegalcolors. Inthiscasethedegreeheuristiccomes
D EG RE EH EU RI ST IC
in handy. It attempts to reduce the branching factor on future choices by selecting the vari-
able that is involved in the largest number of constraints on other unassigned variables. In
except for T which has degree 0. In fact once S A is chosen applying the degree heuris-
tic solves the problem without any false stepsyou can choose any consistent color at each
choice point and still arrive at a solution with no backtracking. The minimum-remaining-
Section6.3. Backtracking Searchfor C SPs 217
values heuristic isusually amorepowerful guide but the degree heuristic canbe useful as a
tie-breaker.
Once a variable has been selected the algorithm must decide on the order in which to
L EA ST-
examine itsvalues. Forthis the least-constraining-value heuristic can beeffective insome
C ON ST RA IN IN G-
V AL UE
cases. It prefers the value that rules out the fewest choices for the neighboring variables in
the constraint graph. For example suppose that in Figure 6.1 we have generated the partial
assignment with W Ared and N T green and that our next choice is for Q. Blue would
be a bad choice because it eliminates the last legal value left for Qs neighbor S A. The
least-constraining-value heuristic therefore prefers red to blue. In general the heuristic is
tryingtoleavethemaximumflexibilityforsubsequentvariableassignments. Ofcourseifwe
are trying to find all the solutions to a problem not just the first one then the ordering does
not matter because we have to consider every value anyway. The same holds if there are no
solutions totheproblem.
Whyshould variable selection be fail-first but value selection be fail-last? It turns out
that for a wide variety of problems a variable ordering that chooses a variable with the
minimumnumberofremainingvalueshelpsminimizethenumberofnodesinthesearchtree
by pruning larger parts of the tree earlier. Forvalue ordering the trick is that we only need
onesolution;thereforeitmakessensetolookforthemostlikelyvaluesfirst. Ifwewantedto
enumerateallsolutions ratherthanjustfindonethenvalue orderingwouldbeirrelevant.
So far we have seen how A C-3 and other algorithms can infer reductions in the domain of
variables before webeginthesearch. Butinference canbeevenmorepowerfulinthecourse
of a search: every time we make a choice of a value for a variable we have a brand-new
opportunity toinfernewdomainreductions ontheneighboring variables.
F OR WA RD One of the simplest forms of inference is called forward checking. Whenever a vari-
C HE CK IN G
able X isassigned the forward-checking process establishes arc consistency forit: foreach
unassigned variable Y that is connected to X by a constraint delete from Ys domain any
value that is inconsistent withthe value chosen for X. Because forward checking only does
arcconsistencyinferencesthereisnoreasontodoforward checkingifwehavealreadydone
arcconsistency asapreprocessing step.
ward checking. There are two important points to notice about this example. First notice
that after W Ared and Qgreen are assigned the domains of N T and S A are reduced
to a single value; we have eliminated branching on these variables altogether by propagat-
ing information from W A and Q. A second point to notice is that after V blue the do-
main of S A is empty. Hence forward checking has detected that the partial assignment
W Ared Qgreen V blue is inconsistent with the constraints of the problem and
thealgorithm willtherefore backtrack immediately.
For many problems the search will be more effective if we combine the M RV heuris-
tic with forward checking. Consider Figure 6.7 after assigning W Ared. Intuitively it
seems that that assignment constrains itsneighbors N T and S A soweshould handle those
W A N T Q N SW V S A T
Initial domains R G B R G B R G B R G B R G B R G B R G B
After W Ared R G B R G B R G B R G B G B R G B
After Qgreen R B G R B R G B B R G B
After Vblue R B G R B R G B
is assigned first; then forward checking deletes red from the domains of the neighboring
variables N T and S A. After Qgreen is assigned green is deleted from the domainsof
N TS Aand N SW. After V blue isassignedblue isdeletedfromthedomainsof N SW
and S Aleaving S Awithnolegalvalues.
variables next and then all the other variables will fall into place. Thats exactly what hap-
pens with M RV:N T and S Ahave two values soone of them ischosen first then the other
then Q N SW and V in order. Finally T still has three values and any one of them works.
Wecanviewforwardchecking asanefficientwaytoincrementally compute theinformation
thatthe M RVheuristic needstodoitsjob.
Althoughforwardcheckingdetectsmanyinconsistencies itdoesnotdetectallofthem.
The problem is that it makes the current variable arc-consistent but doesnt look ahead and
makealltheothervariablesarc-consistent. Forexampleconsiderthethirdrowof Figure6.7.
Itshowsthatwhen W Aisred and Qisgreenboth N T and S Aareforcedtobeblue. Forward
checking does notlook farenough ahead tonotice that thisis aninconsistency: N T and S A
areadjacentandsocannothavethesamevalue.
M AI NT AI NI NG AR C The algorithm called M AC for Maintaining Arc Consistency M AC detects this
C ON SI ST EN CY MA C
inconsistency. Afteravariable X
i
isassignedavaluethe I NF ER EN CE procedurecalls A C-3
but instead of a queue of all arcs in the C SP we start with only the arcs X X for all
j i
X thatareunassigned variables thatareneighbors of X . Fromthere A C-3doesconstraint
j i
propagation intheusualwayandifanyvariablehasitsdomainreducedtotheemptysetthe
call to A C-3 fails and we know to backtrack immediately. We can see that M AC is strictly
morepowerfulthanforwardcheckingbecauseforwardcheckingdoesthesamethingas M AC
on the initial arcs in M ACs queue; but unlike M AC forward checking does not recursively
propagate constraints whenchanges aremadetothedomainsofvariables.
The B AC KT RA CK IN G-S EA RC H algorithm in Figure6.5hasaverysimple policy forwhatto
do when a branch of the search fails: back up to the preceding variable and try a different
C HR ON OL OG IC AL value for it. This is called chronological backtracking because the most recent decision
B AC KT RA CK IN G
pointisrevisited. Inthissubsection weconsiderbetterpossibilities.
Consider what happens when we apply simple backtracking in Figure 6.1 with a fixed
variable ordering Q N SW V T S A W A N T. Suppose we have generated the partial
assignment Qred N SW green V blue T red. When we try the next variable
S A we see that every value violates a constraint. We back up to T and try a new color for
Section6.3. Backtracking Searchfor C SPs 219
Tasmania! Obviouslythisissillyrecoloring Tasmaniacannotpossiblyresolvetheproblem
with South Australia.
Amoreintelligent approach tobacktracking istobacktrack toavariable thatmightfix
the problema variable that was responsible for making one of the possible values of S A
impossible. To do this we will keep track of a set of assignments that are in conflict with
somevaluefor S A. Thesetinthiscase Qred N SW green V blueiscalledthe
conflict set for S A. The backjumpingmethod backtracks to the most recent assignment in
C ON FL IC TS ET
the conflict set; in this case backjumping would jump over Tasmania and try a new value
B AC KJ UM PI NG
for V. This method is easily implemented by a modification to B AC KT RA CK such that it
accumulates the conflict set while checking for a legal value to assign. If no legal value is
found the algorithm should return the most recent element of the conflict setalong with the
failureindicator.
The sharp-eyed reader will have noticed that forward checking can supply the conflict
setwithnoextrawork: wheneverforwardchecking basedonan assignment Xxdeletes a
value from Ys domain it should add Xx to Ys conflict set. If the last value is deleted
from Ys domain then the assignments in the conflict set of Y are added to the conflict set
of X. Thenwhenwegetto Yweknowimmediately wheretobacktrack ifneeded.
The eagle-eyed reader will have noticed something odd: backjumping occurs when
every value in a domain is in conflict with the current assignment; but forward checking
detects this event and prevents the search from ever reaching such a node! In fact it can be
shownthateverybranchprunedbybackjumping isalsoprunedbyforwardchecking. Hence
simple backjumping is redundant in a forward-checking search or indeed in a search that
usesstrongerconsistency checking suchas M AC.
Despite the observations of the preceding paragraph the idea behind backjumping re-
mainsagoodone: tobacktrackbasedonthereasonsforfailure. Backjumpingnoticesfailure
whenavariablesdomainbecomesemptybutinmanycasesabranchisdoomedlongbefore
this occurs. Consider again the partial assignment W Ared N SW red which from
ourearlierdiscussion isinconsistent. Supposewetry T red nextandthenassign N TQ
V SA. Weknowthatnoassignment canworkfortheselastfourvariables soeventually we
runoutofvaluestotryat N T. Nowthequestioniswheretobacktrack? Backjumpingcannot
work because N T does have values consistent with the preceding assigned variables N T
doesnt have a complete conflict set of preceding variables that caused it to fail. We know
howeverthatthefourvariables N TQ Vand S Atakentogether failedbecauseofasetof
preceding variables which must be those variables that directly conflict with the four. This
leads toadeepernotion oftheconflict setforavariable such as N T: itisthat setofpreced-
ing variables that caused N T together with any subsequent variables to have no consistent
solution. In this case the set is W A and N SW so the algorithm should backtrack to N SW
andskipover Tasmania. Abackjumping algorithm thatusesconflictsets definedinthisway
C ON FL IC T-D IR EC TE D iscalledconflict-directed backjumping.
B AC KJ UM PI NG
We must now explain how these new conflict sets are computed. The method is in
fact quite simple. The terminal failure of a branch of the search always occurs because a
variables domain becomes empty; that variable has a standard conflict set. In our example
S A fails and its conflict set is say W AN TQ. We backjump to Q and Q absorbs
the conflict set from S A minus Q itself of course into its own direct conflict set which is
N TN SW; the new conflict set is W AN TN SW. That is there is no solution from
Q onward given the preceding assignment to W AN TN SW. Therefore we backtrack
to N T the most recent of these. N T absorbs W AN TN SW N T into its own
direct conflict set W A giving W AN SW as stated in the previous paragraph. Now
the algorithm backjumps to N SW as wewould hope. Tosummarize: let X be the current
j
variable and let conf X be its conflict set. If every possible value for X fails backjump
j j
tothemostrecentvariable X inconf X andset
i j
conf X conf X conf X X .
i i j i
When we reach a contradiction backjumping can tell us how far to back up so we dont
waste time changing variables that wont fix the problem. But we would also like to avoid
running into the same problem again. When the search arrives at a contradiction we know
C ON ST RA IN T thatsomesubsetoftheconflictsetisresponsiblefortheproblem. Constraintlearningisthe
L EA RN IN G
ideaoffindingaminimumsetofvariablesfromtheconflictsetthatcausestheproblem. This
set of variables along with their corresponding values is called a no-good. Wethen record
N O-G OO D
the no-good either byadding anew constraint tothe C SPorby keeping aseparate cache of
no-goods.
For example consider the state W A red N T green Q blue in the bottom
row of Figure 6.6. Forward checking can tell us this state is a no-good because there is no
validassignmentto S A. Inthisparticularcaserecordingtheno-goodwouldnothelpbecause
once we prune this branch from the search tree we will never encounter this combination
again. Butsupposethatthesearchtreein Figure6.6wereactuallypartofalargersearchtree
that started by first assigning values for V and T. Then it would be worthwhile to record
W A red N T green Q blue as a no-good because we are going to run into the
sameproblemagainforeachpossible setofassignments to V and T.
No-goods can be effectively used by forward checking orby backjumping. Constraint
learning is one of the most important techniques used by modern C SP solvers to achieve
efficiencyoncomplexproblems.
Localsearchalgorithmssee Section4.1turnouttobeeffectiveinsolvingmany C SPs. They
use a complete-state formulation: the initial state assigns a value to every variable and the
searchchangesthevalueofonevariableatatime. Forexampleinthe8-queensproblemsee
each step moves a single queen to a new position in its column. Typically the initial guess
violatesseveralconstraints. Thepointoflocalsearchistoeliminatetheviolatedconstraints.2
Inchoosing anewvalueforavariable themostobvious heuristic istoselect thevalue
that results in the minimum number of conflicts with other variablesthe min-conflicts
M IN-C ON FL IC TS
forhillclimbingandsimulatedannealingcanbeappliedtooptimizetheobjectivefunction.
Section6.4. Local Searchfor C SPs 221
function M IN-C ON FL IC TScspmax stepsreturnsasolutionorfailure
inputs:cspaconstraintsatisfactionproblem
max stepsthenumberofstepsallowedbeforegivingup
currentaninitialcompleteassignmentforcsp
fori 1tomax steps do
ifcurrent isasolutionforcsp thenreturncurrent
vararandomlychosenconflictedvariablefromcsp.V AR IA BL ES
valuethevaluev forvar thatminimizes C ON FL IC TSvarvcurrentcsp
setvarvalue incurrent
returnfailure
Figure6.8 The M IN-C ON FL IC TSalgorithmforsolving C SPsbylocalsearch. Theinitial
state may be chosen randomlyor by a greedy assignmentprocess that choosesa minimal-
conflict value for each variable in turn. The C ON FL IC TS function counts the number of
constraintsviolatedbyaparticularvaluegiventherestofthecurrentassignment.
1
0
stage a queen is chosen for reassignment in its column. The number of conflicts in this
case the number of attacking queens is shown in each square. The algorithm moves the
queentothemin-conflictssquarebreakingtiesrandomly.
heuristic. Thealgorithm isshownin Figure6.8anditsapplication toan8-queens problem is
diagrammedin Figure6.9.
Min-conflicts is surprisingly effective for many C SPs. Amazingly on the n-queens
problem if you dont count the initial placement of queens the run time of min-conflicts is
roughly independent of problem size. It solves even the million-queens problem in an aver-
age of 50 steps after the initial assignment. This remarkable observation was the stimulus
leading to a great deal of research in the 1990s on local search and the distinction between
easy and hard problems which we take up in Chapter 7. Roughly speaking n-queens is
easy for local search because solutions are densely distributed throughout the state space.
Min-conflicts also works well for hard problems. Forexample it has been used to schedule
observations forthe Hubble Space Telescope reducing the timetaken toschedule aweekof
observations fromthreeweeks! toaround10minutes.
Allthelocalsearchtechniquesfrom Section4.1arecandidatesforapplicationto C SPs
and some of those have proved especially effective. The landscape of a C SPunder the min-
conflicts heuristic usually has a series of plateaux. There may be millions of variable as-
signments that are only one conflict away from a solution. Plateau searchallowing side-
waysmovestoanother state withthesamescorecan helplocal search finditswayoffthis
plateau. This wandering on the plateau can be directed with tabu search: keeping a small
listofrecentlyvisitedstatesandforbidding thealgorithm toreturntothosestates. Simulated
annealing canalsobeusedtoescapefromplateaux.
C ON ST RA IN T Anothertechniquecalledconstraintweightingcanhelpconcentratethesearchonthe
W EI GH TI NG
important constraints. Eachconstraint isgivenanumeric weight W initially all1. Ateach
i
stepofthesearchthealgorithmchoosesavariablevalue pairtochangethatwillresultinthe
lowesttotalweightofallviolatedconstraints. Theweightsarethenadjustedbyincrementing
theweightofeachconstraintthatisviolatedbythecurrentassignment. Thishastwobenefits:
it adds topography to plateaux making sure that it is possible to improve from the current
stateanditalsoovertimeaddsweighttotheconstraints thatareprovingdifficulttosolve.
Another advantage of local search is that it can be used in an online setting when the
problem changes. This is particularly important in scheduling problems. A weeks airline
schedule may involve thousands of flights and tens of thousands of personnel assignments
butbadweatheratoneairportcanrenderthescheduleinfeasible. Wewouldliketorepairthe
schedule with a minimum number of changes. This can be easily done with a local search
algorithm starting from the current schedule. A backtracking search with the new set of
constraints usually requires much more time and might find a solution with many changes
fromthecurrentschedule.
In this section we examine ways in which the structure of the problem as represented by
theconstraint graph can beused tofindsolutions quickly. Mostofthe approaches herealso
applytootherproblemsbesides C SPssuchasprobabilistic reasoning. Afteralltheonlyway
wecan possibly hope todeal withthe realworld istodecompose itintomany subproblems.
Lookingagainattheconstraintgraphfor Australia Figure6.1brepeatedas Figure6.12a
onefactstandsout: Tasmaniaisnotconnectedtothemainland.3 Intuitively itisobviousthat
I ND EP EN DE NT coloring Tasmania and coloring the mainland are independentsubproblemsany solution
S UB PR OB LE MS
for the mainland combined with any solution for Tasmania yields a solution for the whole
C ON NE CT ED map. Independence can be ascertained simply by finding connected components of the
C OM PO NE NT
constraint graph. Each component corresponds to a subproblem C SP . If assignment S is
cid:22 cid:22 i i
a solution of C SP then S is a solution of C SP . Why is this important? Consider
i i i i i
the following: suppose each C SP has c variables from the total of n variables where c is
i
a constant. Then there are nc subproblems each of which takes at most dc work to solve
itsnearestmainlandneighbortoavoidtheimpressionthatitmightbepartofthatstate.
Section6.5. The Structureof Problems 223
where d is the size of the domain. Hence the total work is Odcnc which is linear in n;
without the decomposition the total work is Odn which is exponential in n. Lets make
thismoreconcrete: dividinga Boolean C SPwith80variables intofoursubproblems reduces
theworst-casesolution timefromthelifetimeoftheuniversedowntolessthanasecond.
Completely independent subproblems are delicious then but rare. Fortunately some
other graph structures are also easy to solve. Forexample a constraint graph isa tree when
anytwovariablesareconnectedbyonlyonepath. Weshowthatanytree-structured C SPcan
besolved intimelinear inthenumberofvariables.4 Thekeyisanewnotion ofconsistency
D IR EC TE DA RC calleddirectedarcconsistencyor D AC.A CS Pisdefinedtobedirectedarc-consistentunder
C ON SI ST EN CY
an ordering of variables X X ...X if and only if every X is arc-consistent with each
X forj i.
j
To solve a tree-structured C SP first pick any variable to be the root of the tree and
chooseanorderingofthevariablessuchthateachvariableappearsafteritsparentinthetree.
Such an ordering is called a topological sort. Figure 6.10a shows a sample tree and b
T OP OL OG IC AL SO RT
showsonepossibleordering. Anytreewithnnodeshasn1arcssowecanmakethisgraph
directed arc-consistent in On steps each of which must compare up to d possible domain
values fortwo variables foratotal time of Ond2. Once we have a directed arc-consistent
graph we can just march down the list of variables and choose any remaining value. Since
eachlinkfromaparenttoitschildisarcconsistentweknowthatforanyvaluewechoosefor
theparent there willbeavalidvalue lefttochoose forthechild. Thatmeanswewonthave
to backtrack; we can move linearly through the variables. Thecomplete algorithm is shown
in Figure6.11.
A E
B D A B C D E F
C F
a b
Figure6.10 a Theconstraintgraphofatree-structured C SP.b Alinearorderingofthe
variablesconsistentwiththetreewith Aastheroot. Thisisknownasatopologicalsortof
thevariables.
Nowthatwehaveanefficientalgorithmfortreeswecanconsiderwhethermoregeneral
constraint graphs can be reduced to trees somehow. There are two primary ways to do this
onebasedonremovingnodesandonebasedoncollapsing nodes together.
The first approach involves assigning values to some variables so that the remaining
variables form a tree. Consider the constraint graph for Australia shown again in Fig-
ure 6.12a. If we could delete South Australia the graph would become a tree as in b.
Fortunately we can do this in the graph not the continent by fixing a value for S A and
function T RE E-C SP-S OL VE Rcspreturnsasolutionorfailure
inputs: cspa C SPwithcomponents X D C
nnumberofvariablesin X
assignmentanemptyassignment
rootanyvariablein X
X T OP OL OG IC AL SO RT Xroot
forj n downto2do
M AK E-A RC-C ON SI ST EN TP AR EN TXj Xj
ifitcannotbemadeconsistentthenreturnfailure
fori 1ton do
assignment Xianyconsistentvaluefrom Di
ifthereisnoconsistentvaluethenreturnfailure
returnassignment
Figure6.11 The T RE E-C SP-S OL VE R algorithmforsolvingtree-structured C SPs. Ifthe
C SPhasasolutionwewillfinditinlineartime;ifnotwewilldetectacontradiction.
N T N T
Q Q
W A W A
S A N SW N SW
V V
T T
a b
Figure6.12 a Theoriginalconstraintgraphfrom Figure6.1. b The constraintgraph
aftertheremovalof S A.
deleting from the domains of the other variables any values that are inconsistent with the
valuechosen for S A.
Now any solution for the C SP after S A and its constraints are removed will be con-
sistent with the value chosen for S A. This works for binary C SPs; the situation is more
complicated withhigher-order constraints. Therefore wecan solve theremaining treewith
the algorithm given above and thus solve the whole problem. Ofcourse in the general case
asopposed tomapcoloring thevaluechosenfor S Acouldbethewrongone sowewould
needtotryeachpossible value. Thegeneralalgorithm isasfollows:
Section6.5. The Structureof Problems 225
1. Chooseasubset S ofthe C SPsvariables suchthattheconstraint graphbecomesatree
afterremovalof S. S iscalledacyclecutset.
C YC LE CU TS ET
2. Foreachpossible assignment tothevariables in S thatsatisfiesallconstraints on S
a remove from thedomains oftheremaining variables anyvalues that are inconsis-
tentwiththeassignment for Sand
b Iftheremaining C SPhasasolution returnittogetherwiththeassignment for S.
Ifthecyclecutsethassizecthenthetotalruntimeis Odcncd2: wehavetotryeach
of the dc combinations of values for the variables in S and for each combination we must
solveatreeproblemofsize nc. Ifthegraphisnearlyatreethen cwillbesmallandthe
savingsoverstraightbacktracking willbehuge. Intheworstcasehoweverccanbeaslarge
asn2. Finding the smallest cycle cutset is N P-hard but several efficient approximation
C UT SE T algorithms are known. The overall algorithmic approach is called cutset conditioning; it
C ON DI TI ON IN G
comesupagainin Chapter14whereitisusedforreasoning aboutprobabilities.
T RE E The second approach is based on constructing a tree decomposition of the constraint
D EC OM PO SI TI ON
graphintoasetofconnectedsubproblems. Eachsubproblemissolvedindependently andthe
resulting solutions are then combined. Likemostdivide-and-conquer algorithms this works
well if no subproblem is too large. Figure 6.13 shows a tree decomposition of the map-
coloring problem into five subproblems. A tree decomposition must satisfy the following
threerequirements:
Everyvariableintheoriginalproblem appearsinatleastoneofthesubproblems.
Iftwovariablesareconnectedbyaconstraint intheoriginalproblemtheymustappear
togetheralongwiththeconstraint inatleastoneofthesubproblems.
Ifavariableappearsintwosubproblemsinthetreeitmustappearineverysubproblem
alongthepathconnecting thosesubproblems.
The first two conditions ensure that all the variables and constraints are represented in the
decomposition. Thethird condition seemsrathertechnical butsimply reflectstheconstraint
that any given variable must have the same value in every subproblem in which it appears;
thelinksjoining subproblems inthetreeenforcethisconstraint. Forexample S Aappears in
all four of the connected subproblems in Figure 6.13. You can verify from Figure 6.12 that
thisdecomposition makessense.
Wesolve eachsubproblem independently; ifanyone hasnosolution weknow theen-
tireproblemhasnosolution. Ifwecansolveallthesubproblemsthenweattempttoconstruct
aglobalsolutionasfollows. Firstwevieweachsubproblem asamega-variablewhosedo-
main isthe set of all solutions forthe subproblem. Forexample the leftmost subproblem in
Figure6.13isamap-coloring problemwiththreevariablesandhencehassixsolutionsone
is W A red S A blue N T green. Then we solve the constraints connecting the
subproblems using the efficient algorithm for trees given earlier. The constraints between
subproblems simply insistthatthesubproblem solutions agreeontheirshared variables. For
examplegiventhesolution W A red S A blue N T greenforthefirstsubproblem
theonlyconsistentsolutionforthenextsubproblemis S A blue N T green Q red.
A given constraint graph admits many tree decompositions; in choosing a decompo-
sition the aim is to make the subproblems as small as possible. The tree width of a tree
T RE EW ID TH
N T
N T
Q
W A
S A
S A
Q
S A N SW
S A N SW
T
V
Figure6.13 Atreedecompositionoftheconstraintgraphin Figure6.12a.
decomposition of a graph is one less than the size of the largest subproblem; the tree width
ofthegraphitselfisdefinedtobetheminimumtreewidthamongallitstreedecompositions.
Ifagraph has treewidth w andwearegiven thecorresponding tree decomposition then the
problem can be solved in Ondw1 time. Hence C SPs with constraint graphs of bounded
tree width are solvable in polynomial time. Unfortunately finding the decomposition with
minimaltreewidthis N P-hardbutthereareheuristic methodsthatworkwellinpractice.
Sofar wehave looked atthestructure oftheconstraint graph. There canbeimportant
structureinthevaluesofvariablesaswell. Considerthemap-coloringproblemwithncolors.
Foreveryconsistent solution there isactually aset of n!solutions formed bypermuting the
colornames. Forexampleonthe Australiamapweknowthat W AN Tand S Amustallhave
different colors but there are 3! 6 ways to assign the three colors to these three regions.
This is called value symmetry. We would like to reduce the search space by a factor of
V AL UE SY MM ET RY
S YM ME TR Y-
n!by breaking the symmetry. Wedo this by introducing a symmetry-breaking constraint.
B RE AK IN G
C ON ST RA IN T
Forourexample we might impose an arbitrary ordering constraint N T S A W A that
requires the three values tobeinalphabetical order. Thisconstraint ensures that only one of
then!solutions ispossible: N T blue S A green W A red.
For map coloring it was easy to find a constraint that eliminates the symmetry and
in general it is possible to find constraints that eliminate all but one symmetric solution in
polynomial time but it is N P-hard to eliminate all symmetry among intermediate sets of
values during search. In practice breaking value symmetry has proved to be important and
effectiveonawiderangeofproblems.
7
L OG IC AL A GE NT S
Inwhichwedesignagentsthatcanformrepresentationsofacomplexworldusea
processofinference toderivenewrepresentations abouttheworldandusethese
newrepresentations todeduce whattodo.
Humans it seems know things; and what they know helps them do things. These are
not empty statements. They make strong claims about how the intelligence of humans is
achievednot by purely reflex mechanisms but by processes of reasoning that operate on
R EA SO NI NG
internal representations of knowledge. In A I this approach to intelligence is embodied in
R EP RE SE NT AT IO N
K NO WL ED GE-B AS ED knowledge-basedagents.
A GE NT S
Theproblem-solvingagentsof Chapters3and4knowthingsbutonlyinaverylimited
inflexiblesense. Forexample thetransition modelforthe8-puzzleknowledge ofwhatthe
actions dois hidden inside the domain-specific code of the R ES UL T function. It can be
used to predict the outcome of actions but not to deduce that two tiles cannot occupy the
samespaceorthatstateswithoddparitycannotbereachedfromstateswithevenparity. The
atomic representations used by problem-solving agents are also very limiting. In a partially
observable environment an agents only choice for representing what it knows about the
currentstateistolistallpossibleconcretestatesahopelessprospectinlargeenvironments.
ables; this is a step in the right direction enabling some parts of the agent to work in a
domain-independent way and allowing for more efficient algorithms. In this chapter and
those that follow we take this step to its logical conclusion so to speakwe develop logic
L OG IC
as a general class of representations to support knowledge-based agents. Such agents can
combineandrecombineinformationtosuitmyriadpurposes. Oftenthisprocesscanbequite
far removed from the needs of the momentas when a mathematician proves a theorem or
anastronomercalculatestheearthslifeexpectancy. Knowledge-basedagentscanacceptnew
tasksintheformofexplicitlydescribedgoals;theycanachievecompetencequicklybybeing
toldorlearning newknowledge about theenvironment; andtheycanadapttochanges inthe
environment byupdating therelevantknowledge.
We begin in Section 7.1 with the overall agent design. Section 7.2 introduces a sim-
plenewenvironment thewumpusworld andillustrates theoperation ofaknowledge-based
agentwithoutgoingintoanytechnicaldetail. Thenweexplainthegeneralprinciplesoflogic
234
Section7.1. Knowledge-Based Agents 235
in Section 7.3 and the specifics of propositional logic in Section 7.4. While less expressive
than first-order logic Chapter 8 propositional logic illustrates all the basic concepts of
logic; it also comes with well-developed inference technologies which we describe in sec-
tions7.5and7.6. Finally Section7.7combinestheconceptofknowledge-based agentswith
thetechnology ofpropositional logictobuildsomesimpleagentsforthewumpusworld.
Thecentralcomponent ofaknowledge-based agentisitsknowledgebaseor K B.Aknowl-
K NO WL ED GE BA SE
edge base is a set of sentences. Here sentence is used as a technical term. It is related
S EN TE NC E
but not identical to the sentences of English and other natural languages. Each sentence is
K NO WL ED GE
expressed in a language called a knowledge representation language and represents some
R EP RE SE NT AT IO N
L AN GU AG E
assertion about theworld. Sometimeswedignify asentence withthenameaxiom whenthe
A XI OM
sentence istakenasgivenwithoutbeingderivedfromothersentences.
There must be a way to add new sentences to the knowledge base and a way to query
what is known. The standard names for these operations are T EL L and A SK respectively.
Bothoperations mayinvolve inferencethat isderiving newsentences fromold. Inference
I NF ER EN CE
mustobeytherequirementthatwhenone A SKsaquestionoftheknowledgebasetheanswer
should follow from whathasbeen toldor T EL Ledtotheknowledge basepreviously. Later
in this chapter we will be more precise about the crucial word follow. Fornow take it to
meanthattheinferenceprocess shouldnotmakethingsupasitgoesalong.
Figure7.1showstheoutlineofaknowledge-based agentprogram. Likeallouragents
ittakesaperceptasinputandreturnsanaction. Theagentmaintainsaknowledgebase K B
B AC KG RO UN D whichmayinitially containsomebackgroundknowledge.
K NO WL ED GE
Each time the agent program is called it does three things. First it T EL Ls the knowl-
edge base what it perceives. Second it A SKs the knowledge base what action it should
perform. In the process of answering this query extensive reasoning may be done about
the current state of the world about the outcomes of possible action sequences and so on.
Thirdtheagentprogram T EL Lstheknowledgebasewhichactionwaschosen andtheagent
executestheaction.
Thedetails oftherepresentation language arehidden insidethreefunctions thatimple-
menttheinterface between thesensors andactuators ononesideandthecorerepresentation
and reasoning system on the other. M AK E-P ER CE PT-S EN TE NC E constructs a sentence as-
sertingthattheagentperceivedthegivenperceptatthegiventime. M AK E-A CT IO N-Q UE RY
constructs a sentence that asks what action should be done at the current time. Finally
M AK E-A CT IO N-S EN TE NC E constructs a sentence asserting that the chosen action was ex-
ecuted. The details of the inference mechanisms are hidden inside T EL L and A SK. Later
sections willrevealthesedetails.
Theagentin Figure7.1appearsquitesimilartotheagentswithinternalstatedescribed
in Chapter 2. Because of the definitions of T EL L and A SK however the knowledge-based
agent is not an arbitrary program for calculating actions. It is amenable to a description at
function K B-A GE NTperceptreturnsanaction
persistent: K Baknowledgebase
tacounterinitially0indicatingtime
T EL LK BM AK E-P ER CE PT-S EN TE NC Eperceptt
action A SK KB MA KE-A CT IO N-Q UE RYt
T EL LK BM AK E-A CT IO N-S EN TE NC Eactiont
tt 1
returnaction
Figure7.1 Agenericknowledge-basedagent.Givenapercepttheagentaddsthepercept
toitsknowledgebaseaskstheknowledgebaseforthebestactionandtellstheknowledge
basethatithasinfacttakenthataction.
the knowledge level where we need specify only what the agent knows and what its goals
K NO WL ED GE LE VE L
are in order to fix its behavior. For example an automated taxi might have the goal of
taking a passenger from San Francisco to Marin County and might know that the Golden
Gate Bridge is the only link between the two locations. Then we can expect it to cross the
Golden Gate Bridgebecauseitknowsthatthatwillachieveitsgoal. Noticethatthisanalysis
I MP LE ME NT AT IO N isindependent ofhowthetaxiworksattheimplementationlevel. Itdoesnt matterwhether
L EV EL
itsgeographicalknowledgeisimplementedaslinkedlistsorpixelmapsorwhetheritreasons
by manipulating strings of symbols stored in registers or by propagating noisy signals in a
networkofneurons.
A knowledge-based agent can be built simply by T EL Ling it what it needs to know.
Starting with an empty knowledge base the agent designer can T EL L sentences one by one
until the agent knows how to operate in its environment. This is called the declarative ap-
D EC LA RA TI VE
proach to system building. In contrast the procedural approach encodes desired behaviors
directly asprogram code. Inthe1970s and1980s advocates ofthetwoapproaches engaged
inheateddebates. Wenowunderstandthatasuccessfulagentoftencombinesbothdeclarative
andprocedural elements initsdesign andthatdeclarative knowledge canoften becompiled
intomoreefficientprocedural code.
We can also provide a knowledge-based agent with mechanisms that allow it to learn
for itself. These mechanisms which are discussed in Chapter 18 create general knowledge
abouttheenvironment fromaseriesofpercepts. Alearning agentcanbefullyautonomous.
Inthissection wedescribe anenvironment inwhichknowledge-based agents canshowtheir
worth. Thewumpusworldisacaveconsistingofroomsconnectedbypassageways. Lurking
W UM PU SW OR LD
somewhere in the cave isthe terrible wumpus a beast that eats anyone who enters its room.
Thewumpuscanbeshotbyanagentbuttheagenthasonlyonearrow. Someroomscontain
Section7.2. The Wumpus World 237
bottomless pitsthatwilltrapanyone whowanders intothese roomsexcept forthewumpus
which is too big to fall in. The only mitigating feature of this bleak environment is the
possibility of finding a heap of gold. Although the wumpus world is rather tame by modern
computergamestandards itillustrates someimportant pointsaboutintelligence.
A sample wumpus world is shown in Figure 7.2. The precise definition of the task
environment isgivenassuggested in Section2.3bythe P EA Sdescription:
Performance measure: 1000 for climbing out of the cave with the gold 1000 for
falling into a pit or being eaten by the wumpus 1 for each action taken and 10 for
usingupthearrow. Thegameendseitherwhentheagentdiesorwhentheagentclimbs
outofthecave.
Environment: A 44 grid of rooms. The agent always starts in the square labeled
11 facing to the right. The locations of the gold and the wumpus are chosen ran-
domly with a uniform distribution from the squares other than the start square. In
addition eachsquareotherthanthestartcanbeapitwithprobability 0.2.
Actuators: The agent can move Forward Turn Left by 90 or Turn Right by 90 . The
agent dies a miserable death if it enters a square containing apit ora live wumpus. It
issafe albeit smelly toenter asquare with adead wumpus. Ifan agent tries to move
forward and bumps into awall then the agent does notmove. Theaction Grabcan be
used to pick up the gold if it is in the same square as the agent. The action Shoot can
beusedtofireanarrowinastraight lineinthedirection theagentisfacing. Thearrow
continues untiliteitherhitsandhence kills thewumpusorhits awall. Theagent has
only one arrow so only the first Shoot action has any effect. Finally the action Climb
canbeusedtoclimboutofthecavebutonlyfromsquare11.
Sensors: Theagenthasfivesensors eachofwhichgivesasinglebitofinformation:
Inthe square containing the wumpusand in thedirectly not diagonally adjacent
squares theagentwillperceivea Stench.
Inthesquares directlyadjacent toapittheagentwillperceivea Breeze.
Inthesquarewherethegoldistheagentwillperceivea Glitter.
Whenanagentwalksintoawallitwillperceivea Bump.
When the wumpus is killed it emits a woeful Scream that can be perceived any-
whereinthecave.
The percepts will be given to the agent program in the form of a list of five symbols;
forexample ifthereisastenchandabreeze butnoglitter bumporscreamtheagent
programwillget Stench Breeze None None None.
We can characterize the wumpus environment along the various dimensions given in Chap-
ter2. Clearlyitisdiscretestaticandsingle-agent. Thewumpusdoesntmovefortunately.
It is sequential because rewards may come only after many actions are taken. It is partially
observable because some aspects of the state are not directly perceivable: the agents lo-
cation the wumpuss state of health and the availability of an arrow. As for the locations
of the pits and the wumpus: we could treat them as unobserved parts of the state that hap-
pentobeimmutablein whichcase thetransition modelfortheenvironment iscompletely
Breeze
Gold
S TA RT
Figure7.2 Atypicalwumpusworld.Theagentisinthebottomleftcornerfacingright.
known;orwecouldsaythatthetransitionmodelitselfisunknownbecausetheagentdoesnt
know which Forward actions are fatalin which case discovering the locations of pits and
wumpuscompletestheagentsknowledge ofthetransition model.
Foran agent in the environment the main challenge is its initial ignorance of the con-
figurationoftheenvironment; overcomingthisignorance seemstorequirelogicalreasoning.
Inmostinstancesofthewumpusworlditispossiblefortheagenttoretrievethegoldsafely.
Occasionally theagentmustchoosebetweengoinghomeempty-handedandriskingdeathto
find the gold. About 21 of the environments are utterly unfair because the gold is in a pit
orsurrounded bypits.
Let us watch a knowledge-based wumpus agent exploring the environment shown in
downsymbolsinagridasin Figures7.3and7.4.
The agents initial knowledge base contains the rules of the environment as described
previously; inparticular itknowsthat itisin11 and that 11is asafe square; wedenote
thatwithan Aand O Krespectively insquare11.
Thefirstpercept is None None None None None from which the agent can con-
clude that its neighboring squares 12 and 21 are free of dangersthey are O K. Fig-
ure7.3ashowstheagentsstateofknowledgeatthispoint.
A cautious agent will move only into a square that it knows to be O K. Let us suppose
theagentdecidestomoveforwardto21. Theagentperceivesabreezedenotedby Bin
21sotheremustbeapitinaneighboringsquare. Thepitcannotbein11bytherulesof
the game so there must be apitin 22 or31 orboth. The notation P? in Figure 7.3b
indicates apossible pitinthosesquares. Atthispoint thereisonlyoneknownsquare thatis
O Kandthathasnotyetbeenvisited. Sotheprudentagentwillturnaround gobackto11
andthenproceed to12.
The agent perceives a stench in 12 resulting in the state of knowledge shown in
Section7.2. The Wumpus World 239
14 24 34 44 A Agent 14 24 34 44
B Breeze
G Glitter Gold
O K Safe square
13 23 33 43 P Pit 13 23 33 43
S Stench
V Visited
W Wumpus
12 22 32 42 12 22 32 42
P?
O K O K
11 21 31 41 11 21 31 41
A P?
A
V B
O K O K O K O K
a b
uation after percept None None None None None. b After one move with percept
None Breeze None None None.
14 24 34 44 A Agent 14 24 34 44
P?
B Breeze
G Glitter Gold
O K Safe square
13 W! 23 33 43 P Pit 13 W! 23 A 33 P? 43
S Stench
S G
V Visited
B
W Wumpus
12 22 32 42 12 22 32 42
A S
S V V
O K O K O K O K
11 21 31 41 11 21 31 41
B P! B P!
V V V V
O K O K O K O K
a b
with percept Stench None None None None. b After the fifth move with percept
Stench Breeze Glitter None None.
wumpus cannot be in 11 by the rules of the game and it cannot be in 22 or the agent
would have detected a stench when it was in 21. Therefore the agent can infer that the
wumpusisin13. Thenotation W!indicates thisinference. Moreover thelack ofabreeze
in12impliesthatthereisnopitin22. Yettheagenthasalready inferredthattheremust
be a pit in either 22 or 31 so this means it must be in 31. This is a fairly difficult
inference because it combines knowledge gained at different times in different places and
reliesonthelackofapercepttomakeonecrucialstep.
Theagenthasnowprovedtoitselfthatthereisneitherapitnorawumpusin22soit
is O Ktomovethere. Wedonotshowtheagentsstateofknowledgeat22;wejustassume
that the agent turns and moves to 23 giving us Figure 7.4b. In 23 the agent detects a
glitter soitshouldgrabthegoldandthenreturnhome.
Note that in each case for which the agent draws a conclusion from the available in-
formation that conclusion is guaranteed tobecorrect iftheavailable information iscorrect.
This is a fundamental property of logical reasoning. In the rest of this chapter we describe
howtobuildlogicalagentsthatcanrepresentinformationanddrawconclusionssuchasthose
described inthepreceding paragraphs.
This section summarizes the fundamental concepts of logical representation and reasoning.
These beautiful ideas are independent of any of logics particular forms. We therefore post-
pone the technical details of those forms until the next section using instead the familiar
exampleofordinary arithmetic.
In Section 7.1 we said that knowledge bases consist of sentences. These sentences
areexpressed according tothe syntax oftherepresentation language whichspecifies allthe
S YN TA X
sentences that are well formed. The notion of syntax is clear enough in ordinary arithmetic:
xy 4isawell-formed sentence whereasx4y isnot.
Alogicmustalsodefinethesemanticsormeaningofsentences. Thesemanticsdefines
S EM AN TI CS
the truth of each sentence with respect to each possible world. Forexample the semantics
T RU TH
for arithmetic specifies that the sentence x y4 is true in a world where x is 2 and y
P OS SI BL EW OR LD
is 2 but false in a world where x is 1 and y is1. In standard logics every sentence must be
eithertrueorfalseineachpossible worldthere isnoinbetween.1
When we need to be precise we use the term model in place of possible world.
M OD EL
Whereaspossibleworldsmightbethoughtofaspotentiallyrealenvironmentsthattheagent
might or might not be in models are mathematical abstractions each of which simply fixes
thetruthorfalsehoodofeveryrelevantsentence. Informallywemaythinkofapossibleworld
asforexamplehavingxmenandywomensittingatatableplayingbridgeandthesentence
xy4 is true when there are four people in total. Formally the possible models are just
allpossibleassignmentsofrealnumberstothevariables xandy. Eachsuchassignmentfixes
thetruthofanysentence ofarithmetic whosevariables are xandy. Ifasentence αistruein
model m we say that m satisfies α or sometimes m is a model of α. We use the notation
S AT IS FA CT IO N
Mαtomeanthesetofallmodelsofα.
Nowthat wehave a notion of truth we are ready to talk about logical reasoning. This
involves the relation of logical entailment between sentencesthe idea that a sentence fol-
E NT AI LM EN T
lowslogically fromanothersentence. Inmathematicalnotation wewrite
α β
Section7.3. Logic 241
K B 1 1 Bree 2ze P 3 IT α 1 1 Bre 2eze 3 K B 1 1 Bree 2ze P 3 IT 1 1 Bree 2ze 3 2
1
a b
Figure7.5 Possiblemodelsforthepresenceofpitsinsquares1222and31. The
K B correspondingto theobservationsofnothingin 11anda breezein21isshownby
the solid line. a Dotted line shows modelsof α no pit in 12. b Dotted line shows
1
modelsofα nopitin22.
2
tomeanthatthesentenceαentailsthesentenceβ. Theformaldefinitionofentailmentisthis:
α β ifandonlyifineverymodelinwhichαistrueβ isalsotrue. Usingthenotationjust
introduced wecanwrite
α β ifandonlyif Mα Mβ.
Notethedirection ofthehere: ifα βthenαisastronger assertionthanβ: itrulesout
more possible worlds. The relation of entailment is familiar from arithmetic; we are happy
with the idea that the sentence x 0 entails the sentence xy 0. Obviously in any model
wherexiszeroitisthecasethatxy iszeroregardlessofthevalueof y.
Wecanapplythesamekindofanalysistothewumpus-worldreasoning examplegiven
in the preceding section. Consider the situation in Figure 7.3b: the agent has detected
nothing in11andabreezein21. Thesepercepts combined withtheagents knowledge
of the rules of the wumpus world constitute the K B. The agent is interested among other
things in whether the adjacent squares 12 22 and 31 contain pits. Each of the three
squaresmightormightnotcontainapitsoforthepurposes ofthisexamplethereare238
possible models. Theseeightmodelsareshownin Figure7.5.2
The K B can be thought of as a set of sentences or as a single sentence that asserts all
the individual sentences. The K B is false in models that contradict what the agent knows
for example the K B is false in any model in which 12 contains a pit because there is
no breeze in11. Thereare in fact just three models inwhich the K Bis true and these are
oftrue andfalsetothesentencesthereisapitin12etc. Modelsinthemathematicalsensedonotneedto
haveorribleairywumpusesinthem.
shownsurroundedbyasolidlinein Figure7.5. Nowletusconsidertwopossibleconclusions:
α Thereisnopitin12.
1
α Thereisnopitin22.
2
Wehavesurrounded themodelsof α andα withdotted linesin Figures7.5aand 7.5b
respectively. Byinspection weseethefollowing:
ineverymodelinwhich K B istrue α isalsotrue.
1
Hence K B α : thereisnopitin12. Wecanalsoseethat
1
insomemodelsinwhich K B istrue α isfalse.
2
Hence K B cid:7 α : theagentcannotconcludethatthereisnopitin22. Norcanitconclude
2
thatthere isapitin22.3
Theprecedingexamplenotonlyillustratesentailmentbutalsoshowshowthedefinition
of entailment can be applied to derive conclusionsthat is to carry out logical inference.
L OG IC AL IN FE RE NC E
The inference algorithm illustrated in Figure 7.5 is called model checking because it enu-
M OD EL CH EC KI NG
meratesallpossible modelstocheckthat αistrueinallmodelsinwhich K B istrue thatis
that M KB Mα.
Inunderstanding entailmentandinferenceitmighthelptothinkofthesetofallconse-
quences of K B asahaystack andofαasaneedle. Entailmentisliketheneedlebeing inthe
haystack;inferenceislikefindingit. Thisdistinctionisembodiedinsomeformalnotation: if
aninferencealgorithm icanderiveαfrom K Bwewrite
K B cid:20 α
i
whichispronounced αisderivedfrom K B byioriderives αfrom K B.
An inference algorithm that derives only entailed sentences is called sound or truth-
S OU ND
preserving. Soundness is a highly desirable property. An unsound inference procedure es-
T RU TH-P RE SE RV IN G
sentiallymakesthingsupasitgoesalongitannouncesthediscoveryofnonexistentneedles.
Itiseasytoseethatmodelchecking whenitisapplicable4 isasoundprocedure.
The property of completeness is also desirable: an inference algorithm is complete if
C OM PL ET EN ES S
it can derive any sentence that is entailed. For real haystacks which are finite in extent
it seems obvious that a systematic examination can always decide whether the needle is in
thehaystack. Formanyknowledge bases however thehaystack ofconsequences isinfinite
and completeness becomes an important issue.5 Fortunately there are complete inference
procedures forlogicsthataresufficiently expressivetohandle manyknowledgebases.
We have described a reasoning process whose conclusions are guaranteed to be true
in any world in which the premises are true; in particular if K B is true in the real world
thenanysentence αderived from K B byasoundinference procedure isalsotrueinthereal
world. Sowhileaninferenceprocessoperatesonsyntaxinternal physicalconfigurations
such as bits in registers or patterns of electrical blips in brainsthe process corresponds
arithmeticontheotherhandthespaceofmodelsisinfinite: evenifwerestrictourselvestotheintegersthere
areinfinitelymanypairsofvaluesforxandyinthesentencexy4.
Section7.4. Propositional Logic: A Very Simple Logic 243
Sentences Sentence
Entails
Follows
Semantics Semantics
Representation
World
Aspects of the Aspect of the
real world real world
Figure7.6 Sentencesarephysicalconfigurationsoftheagentandreasoningisaprocess
of constructing new physical configurations from old ones. Logical reasoning should en-
surethatthenewconfigurationsrepresentaspectsoftheworldthatactuallyfollowfromthe
aspectsthattheoldconfigurationsrepresent.
to the real-world relationship whereby some aspect of the real world is the case6 by virtue
of other aspects of the real world being the case. This correspondence between world and
representation isillustrated in Figure7.6.
The final issue to consider is groundingthe connection between logical reasoning
G RO UN DI NG
processes andthe real environment inwhich theagent exists. Inparticular how do weknow
that K B is true in the real world? After all K B is just syntax inside the agents head.
This is a philosophical question about which many many books have been written. See
Chapter26. Asimpleansweristhattheagentssensors createtheconnection. Forexample
our wumpus-world agent has a smell sensor. The agent program creates a suitable sentence
whenever there is a smell. Then whenever that sentence is in the knowledge base it is
true in the real world. Thus the meaning and truth of percept sentences are defined by the
processesofsensingandsentenceconstruction thatproducethem. Whatabouttherestofthe
agents knowledge such as its belief that wumpuses cause smells in adjacent squares? This
is not a direct representation of a single percept but a general rulederived perhaps from
perceptual experience but not identical to a statement of that experience. General rules like
this are produced by a sentence construction process called learning which is the subject
of Part V. Learning is fallible. It could be the case that wumpuses cause smells except on
February29inleapyearswhichiswhentheytaketheirbaths. Thus K B maynotbetruein
therealworldbutwithgoodlearning procedures thereisreasonforoptimism.
P RO PO SI TI ON AL Wenowpresentasimplebutpowerfullogiccalled propositional logic. Wecoverthesyntax
L OG IC
of propositional logic and its semanticsthe way in which the truth of sentences is deter-
mined. Then we look at entailmentthe relation between a sentence and another sentence
thatfollows from itandseehow thisleads toasimple algorithm forlogical inference. Ev-
erything takesplace ofcourse inthewumpusworld.
The syntax of propositional logic defines the allowable sentences. The atomic sentences
A TO MI CS EN TE NC ES
P RO PO SI TI ON consist of a single proposition symbol. Each such symbol stands for a proposition that can
S YM BO L
be true or false. We use symbols that start with an uppercase letter and may contain other
letters or subscripts for example: P Q R W and North. The names are arbitrary but
13
are often chosen to have some mnemonic valuewe use W to stand for the proposition
13
that the wumpus is in 13. Remember that symbols such as W are atomic i.e. W 1
13
and3arenotmeaningfulpartsofthesymbol. Therearetwoproposition symbolswithfixed
meanings: True is the always-true proposition and False is the always-false proposition.
C OM PL EX Complex sentences are constructed from simpler sentences using parentheses and logical
S EN TE NC ES
L OG IC AL connectives. Therearefiveconnectives incommonuse:
C ON NE CT IV ES
not. A sentence such as W is called the negation of W . A literal is either an
N EG AT IO N 13 13
atomicsentenceapositiveliteraloranegatedatomicsentence anegative literal.
L IT ER AL
and. A sentence whose main connective is such as W P is called a con-
13 31
junction;itspartsaretheconjuncts. Thelookslikean Afor And.
C ON JU NC TI ON
or. Asentenceusingsuchas W P W isadisjunctionofthedisjuncts
D IS JU NC TI ON 13 31 22
W P and W . Historically the comes fromthe Latinvelwhich means
13 31 22
or. Formostpeople itiseasiertoremember asanupside-down .
implies. Asentencesuchas W P W iscalledanimplicationorcon-
I MP LI CA TI ON 13 31 22
ditional. Itspremiseorantecedentis W P anditsconclusionorconsequent
P RE MI SE 13 31
is W . Implications arealsoknownas rulesorifthenstatements. Theimplication
C ON CL US IO N 22
symbolissometimeswritteninotherbooksas or.
R UL ES
if and only if. The sentence W W is a biconditional. Some other books
B IC ON DI TI ON AL 13 22
writethisas.
Sentence Atomic Sentence Complex Sentence
Atomic Sentence True False P Q R ...
Complex Sentence Sentence Sentence
Sentence
Sentence Sentence
Sentence Sentence
Sentence Sentence
Sentence Sentence
O PE RA TO RP RE CE DE NC E :
alongwithoperatorprecedencesfromhighesttolowest.
Section7.4. Propositional Logic: A Very Simple Logic 245
Figure7.7givesaformalgrammarofpropositional logic; seepage1060 ifyouarenot
familiar with the B NF notation. The B NF grammar by itself is ambiguous; a sentence with
severaloperatorscanbeparsedbythegrammarinmultipleways. Toeliminatetheambiguity
wedefineaprecedenceforeachoperator. Thenotoperatorhasthehighestprecedence
which means that in the sentence A B the binds most tightly giving us the equivalent
of A Bratherthan A B. Thenotationforordinaryarithmeticisthesame: 24
is2not6. Whenindoubtuseparentheses tomakesureoftherightinterpretation. Square
brackets meanthesamething asparentheses; thechoice ofsquare brackets orparentheses is
solelytomakeiteasierforahumantoreadasentence.
Having specified the syntax of propositional logic we now specify its semantics. The se-
mantics defines the rules for determining the truth of a sentence with respect to a particular
model. Inpropositional logic amodelsimply fixesthetruthvaluetrue orfalseforev-
T RU TH VA LU E
eryproposition symbol. Forexampleifthesentencesinthe knowledgebasemakeuseofthe
proposition symbols P P and P thenonepossible modelis
12 22 31
m P false P false P true.
With three proposition symbols there are 238 possible modelsexactly those depicted
in Figure 7.5. Notice however that the models are purely mathematical objects with no
necessary connection to wumpus worlds. P is just asymbol; itmight mean there is apit
12
in12or Imin Paristodayandtomorrow.
The semantics for propositional logic must specify how to compute the truth value of
any sentence given a model. This is done recursively. All sentences are constructed from
atomicsentences andthefiveconnectives; therefore weneed tospecify how tocompute the
truthofatomicsentences andhowtocomputethetruthofsentences formedwitheachofthe
fiveconnectives. Atomicsentences areeasy:
True istrueineverymodeland False isfalseineverymodel.
The truth value of every other proposition symbol must be specified directly in the
model. Forexampleinthemodelm givenearlier P isfalse.
Forcomplexsentences wehave fiverules whichhold foranysubsentences P and Qinany
modelmhereiffmeansifandonlyif:
P istrueiff P isfalseinm.
P Qistrueiffboth P and Qaretruein m.
P Qistrueiffeither P or Qistrueinm.
P Qistrueunless P istrueand Qisfalseinm.
P Qistrueiff P and Qarebothtrueorbothfalsein m.
The rules can also be expressed with truth tables that specify the truth value of a complex
T RU TH TA BL E
sentence foreach possible assignment oftruth values toits components. Truthtables forthe
fiveconnectives aregiven in Figure7.8. Fromthese tables thetruth value ofanysentence s
canbecomputedwithrespecttoanymodelmbyasimplerecursiveevaluation. Forexample
P Q P P Q P Q P Q P Q
false false true false false true true
false true true false true true false
true false false false true false false
true true false true true true true
Figure7.8 Truthtablesforthefivelogicalconnectives. Tousethetabletocomputefor
examplethevalueof P Qwhen P istrueand Qisfalsefirstlookontheleftfortherow
where P istrueand Qisfalsethethirdrow.Thenlookinthatrowunderthe P Qcolumn
toseetheresult: true.
the sentence P P P evaluated in m gives true false truetrue
12 22 31 1
truetrue. Exercise7.3asksyoutowritethealgorithm P L-T RU E?smwhichcomputes
thetruthvalueofapropositional logicsentence sinamodelm.
Thetruth tables forand or andnot areinclose accord withourintuitions about
the Englishwords. Themainpointofpossibleconfusionisthat P Qistruewhen P istrue
or Q is true or both. A different connective called exclusive or xor for short yields
false when both disjuncts are true.7 There is no consensus on the symbol for exclusive or;
somechoicesare orcid:7or.
Thetruth table for maynot quite fitones intuitive understanding of P implies Q
orif P then Q. Foronethingpropositionallogicdoesnotrequireanyrelationofcausation
orrelevancebetween P and Q. Thesentence5isoddimplies Tokyoisthecapitalof Japan
is a true sentence of propositional logic under the normal interpretation even though it is
a decidedly odd sentence of English. Another point of confusion is that any implication is
true whenever its antecedent is false. Forexample 5 is even implies Sam is smart is true
regardless of whether Sam is smart. This seems bizarre but it makes sense if you think of
P Qassaying If P istruethen Iamclaimingthat Qistrue. Otherwise Iammaking
noclaim. Theonlywayforthissentence tobefalseisif P istruebut Qisfalse.
The biconditional P Q is true whenever both P Q and Q P are true. In
Englishthisisoftenwrittenas P ifandonlyif Q. Manyoftherulesofthewumpusworld
are best written using . Forexample a square is breezy if a neighboring square has a pit
andasquareisbreezy onlyifaneighboring squarehasapit. Soweneedabiconditional
B P P
11 12 21
where B meansthatthereisabreezein11.
11
Nowthatwehavedefinedthesemanticsforpropositionallogicwecanconstructaknowledge
base for the wumpus world. We focus first on the immutable aspects of the wumpus world
leaving the mutable aspects for a later section. Fornow weneed the following symbols for
eachxylocation:
Section7.4. Propositional Logic: A Very Simple Logic 247
P istrueifthereisapitinxy.
xy
W istrueifthereisawumpusin xydeadoralive.
xy
B istrueiftheagentperceives abreezein xy.
xy
S istrueiftheagentperceivesastenchin xy.
xy
The sentences we write will suffice to derive P there is no pit in 12 as was done
12
informally in Section7.3. Welabeleachsentence R sothatwecanrefertothem:
i
Thereisnopitin11:
R : P .
A square is breezy if and only if there is a pit in a neighboring square. This has to be
statedforeachsquare; fornowweincludejusttherelevant squares:
R : B P P .
R : B P P P .
The preceding sentences are true in all wumpus worlds. Now we include the breeze
perceptsforthefirsttwosquaresvisitedinthespecificworldtheagentisinleadingup
tothesituation in Figure7.3b.
R : B .
R : B .
Our goal now is to decide whether K B α for some sentence α. For example is P
12
entailedbyour K B? Ourfirstalgorithm forinference isamodel-checking approachthatisa
direct implementation of the definition of entailment: enumerate the models and check that
α is true in every model in which K B is true. Models are assignments of true or false to
every proposition symbol. Returning to our wumpus-world example the relevant proposi-
tion symbols are B B P P P P and P . With seven symbols there are
11 21 11 12 21 22 31
27128 possible models; in three of these K B is true Figure 7.9. In those three models
P istruehencethereisnopitin12. Ontheotherhand P istrueintwoofthethree
12 22
modelsandfalseinonesowecannotyettellwhetherthereisapitin22.
Figure7.9reproduces inamoreprecise form thereasoning illustrated in Figure7.5. A
generalalgorithmfordecidingentailmentinpropositionallogicisshownin Figure7.10. Like
the B AC KT RA CK IN G-S EA RC H algorithm on page 215 T T-E NT AI LS? performs a recursive
enumeration of afinite space of assignments to symbols. Thealgorithm is soundbecause it
implements directly thedefinition ofentailment and completebecause itworksforany K B
andαandalwaysterminatesthere areonlyfinitelymanymodelsto examine.
Of course finitely many is not always the same as few. If K B and α contain n
symbols in all then there are 2n models. Thus the time complexity of the algorithm is
O2n. Thespacecomplexityisonly Onbecausetheenumeration isdepth-first. Laterin
this chapter weshow algorithms that are much more efficient in many cases. Unfortunately
propositional entailment isco-N P-complete i.e.probably noeasierthan N P-completesee
Appendix A so every known inference algorithm for propositional logic has a worst-case
complexity thatisexponential inthesizeoftheinput.
B 11 B 21 P 11 P 12 P 21 P 22 P 31 R 1 R 2 R 3 R 4 R 5 K B
false false false false false false false true true true true false false
false false false false false false true true true false true false false
. . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . .
false true false false false false false true true false true true false
false true false false false false true true true true true true true
false true false false false true false true true true true true true
false true false false false true true true true true true true true
false true false false true false false true false false true true false
. . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . .
true true true true true true true false true true false true false
Figure7.9 Atruthtableconstructedfortheknowledgebasegiveninthetext. K Bistrue
if R through R aretruewhichoccursinjust3ofthe128rowstheonesunderlinedinthe
right-handcolumn.Inall3rows P 12isfalsesothereisnopitin12.Ontheotherhand
theremightormightnotbeapitin22.
function T T-E NT AI LS?K Bαreturnstrue orfalse
inputs:K Btheknowledgebaseasentenceinpropositionallogic
αthequeryasentenceinpropositionallogic
symbolsalistofthepropositionsymbolsin K B andα
return T T-C HE CK-A LL KBαsymbols
function T T-C HE CK-A LL KBαsymbolsmodelreturnstrue orfalse
if E MP TY?symbolsthen
if P L-T RU E?K Bmodelthenreturn P L-T RU E?αmodel
elsereturntrue when K Bisfalsealwaysreturntrue
elsedo
P FI RS Tsymbols
rest R ES Tsymbols
return T T-C HE CK-A LL KBαrestmodel P true
and
T T-C HE CK-A LL KBαrestmodel P false
T Tstandsfortruthtable. P L-T RU E? returnstrueifasentenceholdswithinamodel. The
variablemodelrepresentsapartialmodelanassignmenttosomeofthesymbols. Thekey-
wordandisusedhereasalogicaloperationonitstwoargumentsreturningtrue orfalse.
Section7.5. Propositional Theorem Proving 249
αβ β α commutativity of
αβ β α commutativity of
αβγ αβγ associativity of
αβγ αβγ associativity of
α α double-negation elimination
α β β α contraposition
α β αβ implication elimination
α β α ββ α biconditional elimination
αβ αβ De Morgan
αβ αβ De Morgan
αβ γ αβαγ distributivity of over
αβ γ αβαγ distributivity of over
Figure7.11 Standardlogicalequivalences. Thesymbolsα β andγ standforarbitrary
sentencesofpropositionallogic.
Sofarwehaveshownhowtodetermineentailmentbymodelchecking: enumeratingmodels
and showing that the sentence must hold in all models. In this section weshow how entail-
mentcanbedonebytheoremprovingapplying rulesofinferencedirectlytothesentences
T HE OR EM PR OV IN G
inourknowledgebasetoconstructaproofofthedesiredsentencewithoutconsultingmodels.
Ifthenumberofmodelsislargebutthelengthoftheproofisshortthentheoremprovingcan
bemoreefficientthanmodelchecking.
Before we plunge into the details of theorem-proving algorithms we will need some
L OG IC AL additional concepts related toentailment. Thefirstconcept islogical equivalence: twosen-
E QU IV AL EN CE
tences α and β are logically equivalent if they are true in the same set of models. We write
this as α β. Forexample wecan easily show using truth tables that P Q and Q P
are logically equivalent; other equivalences are shown in Figure 7.11. These equivalences
play much the same role in logic as arithmetic identities do in ordinary mathematics. An
alternative definition of equivalence is asfollows: any two sentences α and β are equivalent
onlyifeachofthementailstheother:
α β ifandonlyif α β andβ α.
Thesecondconceptwewillneedisvalidity. Asentenceisvalidifitistrueinallmodels. For
V AL ID IT Y
examplethesentence P P isvalid. Validsentencesarealsoknownastautologiesthey
T AU TO LO GY
are necessarily true. Because the sentence True is true in all models every valid sentence
is logically equivalent to True. What good are valid sentences? From our definition of
D ED UC TI ON entailment wecanderivethedeductiontheoremwhichwasknowntotheancient Greeks:
T HE OR EM
Foranysentences αandβα β ifandonlyifthesentenceα βisvalid.
Exercise7.5asksforaproof. Hence wecandecide if α β bychecking thatα βis
trueineverymodelwhichisessentiallywhattheinference algorithmin Figure7.10does
orby proving that α β isequivalent to True. Conversely the deduction theorem states
thateveryvalidimplication sentence describes alegitimateinference.
The final concept we will need is satisfiability. A sentence is satisfiable if it is true
S AT IS FI AB IL IT Y
in orsatisfied by some model. Forexample the knowledge base given earlier R R
R R R is satisfiable because there are three models in which it is true as shown
in Figure 7.9. Satisfiability can be checked by enumerating the possible models until one is
found that satisfies the sentence. The problem of determining the satisfiability of sentences
inpropositional logicthe S ATproblemwas thefirstproblem proved tobe N P-complete.
S AT
Many problems in computer science are really satisfiability problems. For example all the
constraint satisfaction problems in Chapter 6 ask whether the constraints are satisfiable by
someassignment.
Validity and satisfiability are of course connected: α is valid iff α is unsatisfiable;
contrapositively αissatisfiable iffαisnotvalid. Wealsohavethefollowingusefulresult:
α β ifandonlyifthesentence αβisunsatisfiable.
Proving β from α by checking the unsatisfiability of α β corresponds exactly to the
R ED UC TI OA D standard mathematical proof technique of reductio ad absurdum literally reduction to an
A BS UR DU M
absurdthing. Itisalsocalledproofbyrefutationorproofbycontradiction. Oneassumesa
R EF UT AT IO N
sentenceβtobefalseandshowsthatthisleadstoacontradiction withknownaxiomsα. This
C ON TR AD IC TI ON
contradiction isexactlywhatismeantbysayingthatthesentence αβisunsatisfiable.
Thissectioncoversinferencerulesthatcanbeappliedtoderiveaproofachainofconclu-
I NF ER EN CE RU LE S
sions that leads tothe desired goal. Thebest-known rule is called Modus Ponens Latin for
P RO OF
modethataffirmsandiswritten
M OD US PO NE NS
α β α
.
β
The notation means that whenever any sentences of the form α β and α are given then
thesentenceβcanbeinferred. Forexampleif Wumpus Ahead Wumpus Alive Shoot
and Wumpus Ahead Wumpus Alivearegiventhen Shoot canbeinferred.
Anotherusefulinferenceruleis And-Eliminationwhichsaysthatfromaconjunction
A ND-E LI MI NA TI ON
anyoftheconjuncts canbeinferred:
αβ
.
α
Forexamplefrom Wumpus Ahead Wumpus Alive Wumpus Alive canbeinferred.
By considering the possible truth values of α and β one can show easily that Modus
Ponens and And-Elimination are sound once and for all. These rules can then be used in
any particular instances where they apply generating sound inferences without the need for
enumerating models.
Allofthelogicalequivalences in Figure7.11canbeusedasinferencerules. Forexam-
pletheequivalence forbiconditional elimination yields thetwoinference rules
α β α ββ α
and .
α ββ α α β
Section7.5. Propositional Theorem Proving 251
Notall inference rules work inboth directions like this. Forexample wecannot run Modus
Ponensintheoppositedirection toobtain α β andαfromβ.
Letusseehowtheseinferencerulesandequivalencescanbeusedinthewumpusworld.
We start with the knowledge base containing R through R and show how to prove P
thatisthereisnopitin12. Firstweapplybiconditional elimination to R toobtain
2
R : B P P P P B .
Thenweapply And-Elimination to R toobtain
6
R : P P B .
Logicalequivalence forcontrapositives gives
R : B P P .
Nowwecanapply Modus Ponenswith R andthepercept R i.e.B toobtain
R : P P .
Finallyweapply De Morgansrulegivingtheconclusion
R : P P .
Thatisneither12nor21contains apit.
Wefoundthisproofbyhandbutwecanapplyanyofthesearchalgorithmsin Chapter3
tofindasequence ofstepsthatconstitutes aproof. Wejustneedtodefineaproofproblemas
follows:
I NI TI AL S TA TE: theinitialknowledgebase.
A CT IO NS: the set of actions consists of all the inference rules applied to all the sen-
tencesthatmatchthetophalfoftheinferencerule.
R ES UL T: theresultofanactionistoaddthesentenceinthebottomhalfoftheinference
rule.
G OA L: thegoalisastatethatcontainsthesentence wearetryingtoprove.
Thus searching for proofs is an alternative to enumerating models. In many practical cases
findingaproofcanbemoreefficient becausetheproofcanignoreirrelevantpropositions no
matter how manyof them there are. Forexample the proof given earlier leading to P
12
P does not mention the propositions B P P or P . They can be ignored
21 21 11 22 31
becausethegoalproposition P appearsonlyinsentence R ;theotherpropositions in R
12 2 2
appearonlyin R and R ;so R R and R havenobearingontheproof. Thesamewould
holdevenifweaddedamillionmoresentencestotheknowledgebase;thesimpletruth-table
algorithmontheotherhandwouldbeoverwhelmedbytheexponentialexplosionofmodels.
One final property of logical systems is monotonicity which says that the set of en-
M ON OT ON IC IT Y
tailed sentences can only increase as information is added to the knowledge base.8 Forany
sentences αandβ
if K B α then K B β α.
soning:changingonesmind.Theyarediscussedin Section12.6.
Forexamplesupposetheknowledgebasecontainstheadditionalassertionβstatingthatthere
areexactlyeightpitsintheworld. Thisknowledgemighthelptheagentdrawadditionalcon-
clusions but it cannot invalidate any conclusion α already inferredsuch as the conclusion
thatthereisnopitin12. Monotonicitymeansthatinferencerulescanbeappliedwhenever
suitable premises are found in the knowledge basethe conclusion of the rule must follow
regardless ofwhatelseisintheknowledge base.
Wehave argued that the inference rules covered so farare sound but wehave not discussed
the question of completeness for the inference algorithms that use them. Search algorithms
such as iterative deepening search page 89 are complete in the sense that they will find
any reachable goal but if the available inference rules are inadequate then the goal is not
reachableno proof existsthatuses onlythose inference rules. Forexample ifweremoved
the biconditional elimination rule the proof in the preceding section would not go through.
The current section introduces a single inference rule resolution that yields a complete
inference algorithm whencoupledwithanycompletesearchalgorithm.
Webeginbyusingasimpleversion oftheresolution ruleinthewumpusworld. Letus
consider the steps leading up to Figure 7.4a: the agent returns from 21 to 11 and then
goes to 12 where it perceives a stench but no breeze. We add the following facts to the
knowledgebase:
R : B .
R : B P P P .
By the same process that led to R earlier we can now derive the absence of pits in 22
10
and13rememberthat11isalreadyknowntobepitless:
R : P .
R : P .
We can also apply biconditional elimination to R followed by Modus Ponens with R to
obtainthefactthatthereisapitin1122or31:
R : P P P .
Now comes the firstapplication of the resolution rule: the literal P in R resolves with
22 13
theliteral P in R togivetheresolvent
R ES OL VE NT 22 15
R : P P .
In English;iftheresapitinoneof1122and31anditsnotin22thenitsin11
or31. Similarlytheliteral P in R resolveswiththeliteral P in R togive
11 1 11 16
R : P .
In English: if theres a pit in 11 or 31 and its not in 11 then its in 31. These last
twoinference stepsareexamplesoftheunitresolution inference rule
U NI TR ES OL UT IO N
cid:3 cid:3 m
""
cid:3 1cid:3 i1cid:3 i1cid:3
k
C OM PL EM EN TA RY where each cid:3 is a literal and cid:3 and m are complementary literals i.e. one is the negation
L IT ER AL S i
Section7.5. Propositional Theorem Proving 253
of the other. Thus the unit resolution rule takes a clausea disjunction of literalsand a
C LA US E
literalandproduces anewclause. Notethatasingleliteral canbeviewedasadisjunction of
oneliteral alsoknownasaunitclause.
U NI TC LA US E
Theunitresolution rulecanbegeneralized tothefull resolution rule
R ES OL UT IO N
cid:3 cid:3 m m
""
cid:3 1cid:3 i1cid:3 i1cid:3
k
m 1m j1m j1m
n
where cid:3 and m are complementary literals. Thissays that resolution takes twoclauses and
i j
produces a new clause containing all the literals of the two original clauses except the two
complementary literals. Forexamplewehave
P P P P
11 31 11 22
.
P P
31 22
Thereisonemoretechnical aspectoftheresolution rule: theresulting clauseshould contain
onlyonecopyofeachliteral.9 Theremovalofmultiplecopies ofliterals iscalled factoring.
F AC TO RI NG
Forexampleifweresolve A Bwith A Bweobtain A Awhichisreducedto
just A.
Thesoundnessoftheresolutionrulecanbeseeneasilybyconsidering theliteralcid:3 that
i
is complementary to literal m in the other clause. If cid:3 is true then m is false and hence
j i j
m 1m j1m j1m
n
mustbetrue because m 1m
n
isgiven. Ifcid:3
i
is
falsethencid:3 1cid:3 i1cid:3 i1cid:3
k
mustbetruebecause cid:3 1cid:3
k
isgiven. Now
cid:3 iseithertrueorfalsesooneorotheroftheseconclusions holdsexactly astheresolution
i
rulestates.
Whatis moresurprising about theresolution rule is that itforms the basis forafamily
ofcomplete inference procedures. Aresolution-based theorem prover can foranysentences
α and β in propositional logic decide whether α β. The next two subsections explain
howresolution accomplishes this.
Conjunctivenormalform
Theresolution ruleapplies onlytoclauses that isdisjunctions ofliterals soitwouldseem
to be relevant only to knowledge bases and queries consisting of clauses. How then can
it lead to a complete inference procedure for all of propositional logic? The answer is that
every sentence of propositional logic is logically equivalent to a conjunction of clauses. A
C ON JU NC TI VE sentence expressed as a conjunction of clauses is said to be in conjunctive normal form or
N OR MA LF OR M
C NF see Figure 7.14. We now describe a procedure for converting to C NF. We illustrate
the procedure by converting the sentence B P P into C NF. The steps are as
11 12 21
follows:
1. Eliminatereplacing α β withα ββ α.
B P P P P B .
11 12 21 12 21 11
2. Eliminatereplacing α β withαβ:
B P P P P B .
11 12 21 12 21 11
clausesmakestheresolutionrulemuchcleaneratthecostofintroducingadditionalnotation.
3. C NFrequires to appear only in literals so we move inwards by repeated appli-
cationofthefollowingequivalences from Figure7.11:
α α double-negation elimination
αβ αβ De Morgan
αβ αβ De Morgan
Intheexamplewerequirejustoneapplication ofthelastrule:
B P P P P B .
11 12 21 12 21 11
4. Now we have a sentence containing nested and operators applied to literals. We
applythedistributivity lawfrom Figure7.11distributing overwhereverpossible.
B P P P B P B .
11 12 21 12 11 21 11
The original sentence is now in C NFas aconjunction of three clauses. It is much harder to
readbutitcanbeusedasinputtoaresolution procedure.
Aresolution algorithm
Inference procedures based on resolution work by using the principle of proof bycontradic-
tion introduced on page 250. That is to show that K B α we show that K B α is
unsatisfiable. Wedothisbyproving acontradiction.
A resolution algorithm is shown in Figure 7.12. First K B α is converted into
C NF. Then the resolution rule is applied to the resulting clauses. Each pair that contains
complementary literals is resolved to produce a new clause which is added to the set if it is
notalreadypresent. Theprocesscontinues untiloneoftwothingshappens:
therearenonewclauses thatcanbeadded inwhichcase K B doesnotentailα;or
twoclausesresolvetoyieldtheemptyclauseinwhichcase K B entailsα.
Theemptyclauseadisjunctionofnodisjunctsisequivalentto False becauseadisjunction
is true only if at least one of its disjuncts is true. Another way to see that an empty clause
represents acontradiction istoobservethatitarisesonly fromresolving twocomplementary
unitclausessuchas P and P.
Wecanapplytheresolutionproceduretoaverysimpleinferenceinthewumpusworld.
Whentheagentisin11thereisnobreeze sotherecanbenopitsinneighboring squares.
Therelevantknowledgebaseis
K B R R B P P B
and we wish to prove α which is say P . When we convert K B α into C NF we
12
obtain the clauses shown at the top of Figure 7.13. The second row of the figure shows
clauses obtained by resolving pairs in the firstrow. Then when P isresolved with P
12 12
we obtain the empty clause shown as a small square. Inspection of Figure 7.13 reveals that
manyresolutionstepsarepointless. Forexampletheclause B B P isequivalent
11 11 12
to True P which is equivalent to True. Deducing that True is true is not very helpful.
12
Therefore anyclauseinwhichtwocomplementary literalsappearcanbediscarded.
Section7.5. Propositional Theorem Proving 255
function P L-R ES OL UT IO NK Bαreturnstrue orfalse
inputs:K Btheknowledgebaseasentenceinpropositionallogic
αthequeryasentenceinpropositionallogic
clausesthesetofclausesinthe C NFrepresentationof K B α
new
loopdo
foreachpairofclauses Ci Cj inclauses do
resolvents P L-R ES OL VE Ci Cj
ifresolvents containstheemptyclausethenreturntrue
newnew resolvents
ifnew clauses thenreturnfalse
clausesclausesnew
P L-R ES OL VEreturnsthesetofallpossibleclausesobtainedbyresolvingitstwoinputs.
P B B P P P B B P
21 11 11 12 21 12 11 11 12
B P B P P P B P B P P P P P
11 12 11 12 21 21 11 21 11 12 21 12 21 12
Figure7.13 Partialapplicationof P L-R ES OL UT IO Ntoasimpleinferenceinthewumpus
world. P 12 isshowntofollowfromthefirstfourclausesinthetoprow.
Completenessofresolution
Toconclude ourdiscussion of resolution wenow show why P L-R ES OL UT IO N iscomplete.
R ES OL UT IO N Todothisweintroducetheresolutionclosure R CSofasetofclauses Swhichistheset
C LO SU RE
of all clauses derivable by repeated application of the resolution rule to clauses in S ortheir
derivatives. The resolution closure is what P L-R ES OL UT IO N computes as the final value of
thevariableclauses. Itiseasytoseethat R CSmustbefinitebecausethereareonlyfinitely
manydistinctclausesthatcanbeconstructed outofthesymbols P ...P thatappearin S.
Noticethatthiswouldnotbetruewithoutthefactoring stepthatremovesmultiplecopiesof
literals. Hence P L-R ES OL UT IO N alwaysterminates.
The completeness theorem for resolution in propositional logic is called the ground
G RO UN D
resolution theorem:
R ES OL UT IO N
T HE OR EM
If a set of clauses is unsatisfiable then the resolution closure of those clauses
containstheemptyclause.
This theorem is proved by demonstrating its contrapositive: if the closure R CS does not
contain the empty clause then S is satisfiable. In fact we can construct a model for S with
suitable truthvaluesfor P ...P . Theconstruction procedure isasfollows:
Forifrom1tok
Ifaclausein R CScontainstheliteral P andallitsotherliteralsarefalseunder
i
theassignment chosenfor P 1...P i1thenassignfalse to P i.
Otherwiseassign true to P .
i
This assignment to P ...P is a model of S. To see this assume the oppositethat at
some stage i in the sequence assigning symbol P causes some clause C to become false.
i
Forthis tohappen itmust bethe case that all the other literals in C mustalready have been
falsifiedbyassignments to P 1...P i1. Thus C mustnowlooklikeeitherfalse false
false P orlikefalsefalsefalse P . Ifjustoneofthesetwoisin R CSthen
i i
the algorithm will assign the appropriate truth value to P to make C true so C can only be
i
falsifiedifbothoftheseclausesarein R CS. Nowsince R CSisclosedunderresolution
itwillcontaintheresolventofthesetwoclausesandthatresolventwillhaveallofitsliterals
already falsified by the assignments to P 1...P i1. This contradicts our assumption that
thefirstfalsifiedclauseappearsatstage i. Hencewehaveprovedthattheconstruction never
falsifies a clause in R CS; that is it produces a model of R CS and thus a model of S
itselfsince S iscontained in R CS.
Thecompletenessofresolutionmakesitaveryimportantinferencemethod. Inmanypractical
situations however the full power of resolution is not needed. Some real-world knowledge
bases satisfy certain restrictions on the form of sentences they contain which enables them
touseamorerestricted andefficientinference algorithm.
One such restricted form is the definite clause which is a disjunction of literals of
D EF IN IT EC LA US E
whichexactlyoneispositive. Forexampletheclause L Breeze B isadefinite
11 11
clause whereas B P P isnot.
11 12 21
Slightlymoregeneral isthe Hornclause whichisadisjunction ofliterals ofwhich at
H OR NC LA US E
mostone is positive. Soall definite clauses are Horn clauses as areclauses with no positive
literals;thesearecalledgoalclauses. Hornclausesareclosedunderresolution: ifyouresolve
G OA LC LA US ES
two Hornclauses yougetbacka Hornclause.
Knowledgebasescontaining onlydefiniteclausesareinteresting forthreereasons:
1. Every definite clause can be written as an implication whose premise is a conjunction
ofpositiveliteralsandwhoseconclusionisasinglepositiveliteral. See Exercise7.13.
For example the definite clause L Breeze B can be written as the im-
11 11
plication L Breeze B . In the implication form the sentence is easier to
11 11
understand: itsaysthatiftheagentisin11andthereisabreezethen11isbreezy.
In Horn form the premise is called the body and the conclusion is called the head. A
B OD Y
sentence consisting of a single positive literal such as L is called a fact. It too can
H EA D 11
bewritteninimplication formas True L butitissimplertowritejust L .
F AC T 11 11
Section7.5. Propositional Theorem Proving 257
C NF Sentence Clause
1
Clausen
Clause Literal
1
Literalm
Literal Symbol Symbol
Symbol P Q R ...
Horn Clause Form Definite Clause Form Goal Clause Form
Definite Clause Form Symbol Symbol Symbol
Goal Clause Form Symbol Symbol False
Figure7.14 Agrammarforconjunctivenormalform Hornclausesanddefiniteclauses.
Aclausesuchas A B C isstilladefiniteclausewhenitiswrittenas A BC
butonlytheformerisconsideredthecanonicalformfordefiniteclauses. Onemoreclassis
thek-C NFsentencewhichisa C NFsentencewhereeachclausehasatmostkliterals.
2. Inferencewith Hornclausescanbedonethroughtheforward-chainingandbackward-
F OR WA RD-C HA IN IN G
B AC KW AR D- chaining algorithms which we explain next. Both of these algorithms are natural
C HA IN IN G
in that the inference steps are obvious and easy for humans to follow. This type of
inferenceisthebasisforlogicprogramming whichisdiscussed in Chapter9.
3. Deciding entailment with Hornclauses can be done in timethat is linear inthe size of
theknowledge baseapleasant surprise.
The forward-chaining algorithm P L-F C-E NT AI LS?K Bq determines if a single proposi-
tion symbol qthe queryis entailed by a knowledge base of definite clauses. It begins
from knownfacts positive literals intheknowledge base. Ifallthepremises ofanimplica-
tion are known then its conclusion is added to the set of known facts. For example if L
11
and Breeze areknownand L Breeze B isintheknowledge base then B can
11 11 11
beadded. Thisprocesscontinues untilthequery q isaddedoruntilnofurtherinferences can
bemade. Thedetailedalgorithm isshownin Figure7.15;themainpointtorememberisthat
itrunsinlineartime.
The best way to understand the algorithm is through an example and a picture. Fig-
ure 7.16a shows a simple knowledge base of Horn clauses with A and B as known facts.
ter 4. In A ND OR graphs multiple links joined by an arc indicate a conjunctionevery
link must be provedwhile multiple links without an arc indicate a disjunctionany link
canbeproved. Itiseasytoseehowforwardchaining worksinthegraph. Theknownleaves
here A and B are set and inference propagates up the graph as far as possible. Wher-
ever a conjunction appears the propagation waits until all the conjuncts are known before
proceeding. Thereaderisencouraged toworkthroughtheexampleindetail.
function P L-F C-E NT AI LS?K Bqreturnstrue orfalse
inputs:K Btheknowledgebaseasetofpropositionaldefiniteclauses
qthequeryapropositionsymbol
countatablewherecountcisthenumberofsymbolsincspremise
inferredatablewhereinferredsisinitiallyfalse forallsymbols
agendaaqueueofsymbolsinitiallysymbolsknowntobetruein K B
whileagenda isnotemptydo
p P OPagenda
ifp q thenreturntrue
ifinferredpfalse then
inferredptrue
foreachclausec in K B wherep isinc.P RE MI SEdo
decrementcountc
ifcountc0thenaddc.C ON CL US IO Ntoagenda
returnfalse
Figure7.15 Theforward-chainingalgorithmforpropositionallogic. Theagenda keeps
trackofsymbolsknownto betruebutnotyetprocessed. The count tablekeepstrackof
howmanypremisesofeachimplicationareasyetunknown.Wheneveranewsymbolpfrom
theagendaisprocessedthecountisreducedbyoneforeachimplicationinwhosepremise
p appearseasily identified in constanttime with appropriateindexing. If a countreaches
zero all the premises of the implication are known so its conclusion can be added to the
agenda.Finallyweneedtokeeptrackofwhichsymbolshavebeenprocessed;asymbolthat
isalreadyinthesetofinferredsymbolsneednotbeaddedtotheagendaagain. Thisavoids
redundantworkandpreventsloopscausedbyimplicationssuchas P Qand Q P.
Itiseasytoseethatforward chaining issound: everyinference isessentially anappli-
cationof Modus Ponens. Forwardchaining isalsocomplete: everyentailed atomicsentence
willbederived. Theeasiest waytoseethis istoconsider the finalstate oftheinferred table
after the algorithm reaches a fixed point where no new inferences are possible. The table
F IX ED PO IN T
contains true for each symbol inferred during the process and false for all other symbols.
Wecanviewthetableasalogicalmodel;moreovereverydefiniteclauseintheoriginal K Bis
trueinthismodel. Toseethisassumetheoppositenamelythatsomeclausea ...a b
is false in the model. Then a ...a must be true in the model and b must be false in
the model. But this contradicts ourassumption that the algorithm has reached a fixed point!
Wecanconcludethereforethatthesetofatomicsentences inferredatthefixedpointdefines
a model of the original K B. Furthermore any atomic sentence q that is entailed by the K B
must be true in all its models and in this model in particular. Hence every entailed atomic
sentence q mustbeinferred bythealgorithm.
Forwardchainingisanexampleofthegeneralconceptof data-drivenreasoningthat
D AT A-D RI VE N
isreasoninginwhichthefocusofattention startswiththeknowndata. Itcanbeusedwithin
an agent to derive conclusions from incoming percepts often without a specific query in
mind. Forexample thewumpusagent might T EL L itspercepts totheknowledge baseusing
Section7.6. Effective Propositional Model Checking 259
Q
P Q
L M P P
B L M
A P L M
A B L
L
A
B
A B
a b
Figure7.16 a Asetof Hornclauses. b Thecorresponding A ND ORgraph.
anincremental forward-chaining algorithm inwhichnewfactscanbeaddedtotheagendato
initiate newinferences. Inhumans acertain amount ofdata-driven reasoning occurs asnew
information arrives. Forexampleif Iamindoorsandhearrainstartingtofallitmightoccur
tomethatthepicnicwillbecanceled. Yetitwillprobablynotoccurtomethattheseventeenth
petalonthelargestroseinmyneighborsgardenwillgetwet;humanskeepforwardchaining
undercarefulcontrol lesttheybeswampedwithirrelevant consequences.
The backward-chaining algorithm as its name suggests works backward from the
query. Ifthe query q is known to be true then no work is needed. Otherwise the algorithm
finds those implications inthe knowledge base whose conclusion is q. If all the premises of
one of those implications can be proved true by backward chaining then q is true. When
applied to the query Q in Figure 7.16 it works back downthe graph until it reaches a set of
knownfacts Aand Bthatformsthebasisforaproof. Thealgorithm isessentially identical
to the A ND-O R-G RA PH-S EA RC H algorithm in Figure 4.11. As with forward chaining an
efficientimplementation runsinlineartime.
G OA L-D IR EC TE D Backward chaining is a form of goal-directed reasoning. It is useful for answering
R EA SO NI NG
specificquestionssuchas Whatshall Idonow? and Wherearemykeys? Oftenthecost
ofbackward chaining is muchlessthanlinearinthesizeoftheknowledge base because the
processtouches onlyrelevantfacts.
In this section we describe two families of efficient algorithms for general propositional
inference based on model checking: One approach based on backtracking search and one
onlocalhill-climbing search. Thesealgorithms arepartof thetechnology ofpropositional
logic. Thissectioncanbeskimmedonafirstreadingofthechapter.
Thealgorithmswedescribeareforcheckingsatisfiability: the S ATproblem. Asnoted
earlier testing entailment α β can be done by testing unsatisfiability of α β. We
have already noted the connection between finding a satisfying model fora logical sentence
andfindingasolutionforaconstraintsatisfactionproblemsoitisperhapsnotsurprisingthat
the two families of algorithms closely resemble the backtracking algorithms of Section 6.3
and the local search algorithms of Section 6.4. They are however extremely important in
theirownrightbecausesomanycombinatorialproblemsincomputersciencecanbereduced
to checking the satisfiability of a propositional sentence. Any improvement in satisfiability
algorithms hashugeconsequences forourabilitytohandlecomplexityingeneral.
D AV IS PU TN AM Thefirstalgorithm weconsider isoften called the Davis Putnamalgorithm afterthesem-
A LG OR IT HM
inal paper by Martin Davis and Hilary Putnam 1960. The algorithm is in fact the version
described by Davis Logemann and Loveland 1962 so we will call it D PL L after the ini-
tials of all four authors. D PL L takes as input a sentence in conjunctive normal forma set
of clauses. Like B AC KT RA CK IN G-S EA RC H and T T-E NT AI LS? it is essentially a recursive
depth-first enumeration ofpossible models. Itembodies threeimprovements overthesimple
schemeof T T-E NT AI LS?:
Early termination: The algorithm detects whether the sentence must be true or false
even with a partially completed model. A clause is true if any literal is true even if
the other literals do not yet have truth values; hence the sentence as a whole could be
judged true even before the model is complete. For example the sentence A B
A Cistrue if Aistrue regardless of thevalues of B and C. Similarly asentence
isfalseifanyclauseisfalsewhichoccurswheneachofitsliteralsisfalse. Againthis
can occurlong before the model iscomplete. Early termination avoids examination of
entiresubtrees inthesearchspace.
Pure symbol heuristic: A puresymbol is a symbol that always appears with the same
P UR ES YM BO L
sign in all clauses. For example in the three clauses A B B C and
C A the symbol A is pure because only the positive literal appears B is pure
because only the negative literal appears and C is impure. It is easy to see that if
a sentence has a model then it has a model with the pure symbols assigned so as to
make their literals true because doing so can never make a clause false. Note that in
determining the purity of a symbol the algorithm can ignore clauses that are already
known to be true in the model constructed so far. For example if the model contains
Bfalse then the clause B Cis already true and in the remaining clauses C
appearsonlyasapositiveliteral;therefore C becomespure.
Unit clause heuristic: A unit clause was defined earlier as a clause with just one lit-
eral. In the context of D PL L it also means clauses in which all literals but one are
already assigned false by the model. For example if the model contains Btrue
then B C simplifies to C which is a unit clause. Obviously for this clause
to be true C must be set to false. The unit clause heuristic assigns all such symbols
before branching on the remainder. Oneimportant consequence ofthe heuristic is that
Section7.6. Effective Propositional Model Checking 261
function D PL L-S AT IS FI AB LE?sreturnstrue orfalse
inputs:sasentenceinpropositionallogic
clausesthesetofclausesinthe C NFrepresentationofs
symbolsalistofthepropositionsymbolsins
return D PL Lclausessymbols
function D PL Lclausessymbolsmodelreturnstrue orfalse
ifeveryclauseinclauses istrueinmodel thenreturntrue
ifsomeclauseinclauses isfalseinmodel thenreturnfalse
Pvalue F IN D-P UR E-S YM BO Lsymbolsclausesmodel
if P isnon-nullthenreturn D PL Lclausessymbols Pmodel Pvalue
Pvalue F IN D-U NI T-C LA US Eclausesmodel
if P isnon-nullthenreturn D PL Lclausessymbols Pmodel Pvalue
P FI RS Tsymbols;rest R ES Tsymbols
return D PL Lclausesrestmodel Ptrueor
D PL Lclausesrestmodel Pfalse
Figure7.17 The D PL Lalgorithmforcheckingsatisfiabilityofasentenceinpropositional
logic. The ideas behind F IN D-P UR E-S YM BO L and F IN D-U NI T-C LA US E are described in
the text; each returnsa symbolor null and the truth value to assign to that symbol. Like
T T-E NT AI LS?D PL Loperatesoverpartialmodels.
any attempt toprove by refutation a literal that is already in the knowledge base will
succeed immediately Exercise 7.23. Notice also that assigning one unit clause can
create another unit clausefor example when C is set to false C A becomes a
unit clause causing true to be assigned to A. This cascade of forced assignments
is called unitpropagation. It resembles the process of forward chaining with definite
U NI TP RO PA GA TI ON
clauses and indeed if the C NF expression contains only definite clauses then D PL L
essentially replicates forwardchaining. See Exercise7.24.
The D PL L algorithm is shown in Figure 7.17 which gives the the essential skeleton of the
searchprocess.
What Figure 7.17 does not show are the tricks that enable S AT solvers to scale up to
large problems. It is interesting that most of these tricks are in fact rather general and we
haveseenthembeforeinotherguises:
1. Componentanalysis asseenwith Tasmaniain C SPs: As D PL L assigns truthvalues
tovariables thesetofclauses maybecomeseparated intodisjoint subsets calledcom-
ponentsthatsharenounassigned variables. Givenanefficientwaytodetectwhenthis
occursasolvercangainconsiderablespeedbyworkingoneachcomponentseparately.
2. Variable and value ordering as seen in Section 6.3.1 for C SPs: Our simple imple-
mentation of D PL L uses an arbitrary variable ordering and always tries the value true
before false. The degree heuristic see page 216 suggests choosing the variable that
appearsmostfrequently overallremainingclauses.
3. Intelligent backtracking as seen in Section 6.3 for C SPs: Many problems that can-
not be solved in hours of run time with chronological backtracking can be solved in
seconds with intelligent backtracking that backs up all the way to the relevant point of
conflict. All S AT solvers that do intelligent backtracking use some form of conflict
clause learning to record conflicts so that they wont be repeated later in the search.
Usuallyalimited-size setofconflictsiskeptandrarelyusedonesaredropped.
4. Randomrestartsasseenonpage124forhill-climbing: Sometimesarunappearsnot
to be making progress. In this case we can start over from the top of the search tree
rather than trying to continue. After restarting different random choices in variable
andvalueselection aremade. Clausesthatarelearnedinthefirstrunareretainedafter
the restart and can help prune the search space. Restarting does not guarantee that a
solutionwillbefoundfaster butitdoesreducethevariance onthetimetosolution.
5. Clever indexing as seen in many algorithms: The speedup methods used in D PL L
itself aswell asthe tricks used in modern solvers require fast indexing ofsuch things
as the set of clauses in which variable X appears as a positive literal. This task is
i
complicated by the fact that the algorithms are interested only in the clauses that have
not yet been satisfied by previous assignments to variables so the indexing structures
mustbeupdateddynamically asthecomputation proceeds.
Withtheseenhancements modernsolverscanhandleproblemswithtensofmillionsofvari-
ables. They have revolutionized areas such as hardware verification and security protocol
verification whichpreviously required laborious hand-guided proofs.
Wehaveseenseverallocalsearchalgorithmssofarinthisbookincluding H IL L-C LI MB IN G
page 122 and S IM UL AT ED-A NN EA LI NG page 126. These algorithms can be applied di-
rectly to satisfiability problems provided that we choose the right evaluation function. Be-
cause thegoalistofindanassignment that satisfieseveryclause anevaluation function that
counts the number of unsatisfied clauses will do the job. In fact this is exactly the measure
usedbythe M IN-C ON FL IC TS algorithmfor C SPspage221. Allthesealgorithmstakesteps
in the space of complete assignments flipping the truth value of one symbol at a time. The
space usually contains many local minima to escape from which various forms of random-
ness are required. In recent years there has been a great deal of experimentation to find a
goodbalancebetweengreediness andrandomness.
Oneofthesimplestandmosteffectivealgorithmstoemergefromallthisworkiscalled
W AL KS AT Figure 7.18. On every iteration the algorithm picks an unsatisfied clause and
picks a symbol in the clause to flip. It chooses randomly between two ways to pick which
symboltoflip: 1amin-conflicts stepthatminimizesthenumberofunsatisfied clausesin
thenewstateand2arandom walkstepthatpicksthesymbolrandomly.
When W AL KS AT returns a model the input sentence is indeed satisfiable but when
it returns failure there are two possible causes: either the sentence is unsatisfiable or we
need togive the algorithm more time. If weset max flips and p 0 W AL KS AT will
eventually return a model if one exists because the random-walk steps will eventually hit
Section7.6. Effective Propositional Model Checking 263
function W AL KS ATclausespmax flipsreturnsasatisfyingmodelorfailure
inputs:clausesasetofclausesinpropositionallogic
ptheprobabilityofchoosingtodoarandomwalkmovetypicallyaround0.5
max flipsnumberofflipsallowedbeforegivingup
modelarandomassignmentoftruefalse tothesymbolsinclauses
fori 1tomax flips do
ifmodel satisfiesclauses thenreturnmodel
clausearandomlyselectedclausefromclauses thatisfalseinmodel
withprobabilitypflipthevalueinmodel ofarandomlyselectedsymbolfromclause
elseflipwhicheversymbolinclause maximizesthenumberofsatisfiedclauses
returnfailure
thevaluesofvariables.Manyversionsofthealgorithmexist.
upon the solution. Alas if max flips is infinity and the sentence is unsatisfiable then the
algorithm neverterminates!
Forthisreason W AL KS AT ismostuseful whenweexpect asolution toexistforex-
ampletheproblemsdiscussedin Chapters3and6usuallyhavesolutions. Ontheotherhand
W AL KS AT cannot always detect unsatisfiability which is required for deciding entailment.
For example an agent cannot reliably use W AL KS AT to prove that a square is safe in the
wumpusworld. Instead itcansay Ithoughtaboutitforanhourandcouldntcomeupwith
apossible worldinwhichthesquare isntsafe. Thismaybeagoodempirical indicator that
thesquareissafebutitscertainly notaproof.
Some S AT problems are harder than others. Easy problems can be solved by any old algo-
rithm but because weknow that S ATis N P-complete atleast someproblem instances must
requireexponential runtime. In Chapter6wesawsomesurprisingdiscoveries aboutcertain
kindsofproblems. Forexamplethe n-queensproblemthought tobequitetrickyforback-
tracking search algorithmsturned outtobetrivially easy forlocal search methods such as
min-conflicts. This is because solutions are very densely distributed in the space of assign-
ments and anyinitial assignment isguaranteed tohaveasolution nearby. Thus n-queens is
easybecause itisunderconstrained.
U ND ER CO NS TR AI NE D
When we look at satisfiability problems in conjunctive normal form an undercon-
strained problem is one with relatively few clauses constraining the variables. For example
hereisarandomlygenerated 3-C NFsentence withfivesymbols andfiveclauses:
D BC BA CC B E
E D BB E C.
Sixteen of the 32 possible assignments are models of this sentence so on average it would
take just two random guesses to find a model. This is an easy satisfiability problem as are
most such underconstrained problems. On the other hand an overconstrained problem has
manyclauses relativetothenumberofvariablesandislikelytohavenosolutions.
To go beyond these basic intuitions we must define exactly how random sentences
are generated. The notation C NF mn denotes a k-C NF sentence with m clauses and n
k
symbols where the clauses are chosen uniformly independently and without replacement
fromamongallclauseswithkdifferentliteralswhicharepositiveornegativeatrandom. A
symbolmaynotappeartwiceinaclausenormayaclauseappeartwiceinasentence.
Given a source of random sentences we can measure the probability of satisfiability.
3
and 3 literals per clause as a function of the clausesymbol ratio mn. As we expect for
small mn the probability of satisfiability is close to 1 and at large mn the probability
is close to 0. The probability drops fairly sharply around mn4.3. Empirically we find
that the cliff stays in roughly the same place for k3and gets sharper and sharper as n
S AT IS FI AB IL IT Y increases. Theoretically the satisfiability threshold conjecture says that for every k 3
T HR ES HO LD
C ON JE CT UR E
thereisathreshold ratio r suchthatasngoestoinfinitytheprobability that C NF nrn
k k
is satisfiable becomes 1 for all values of r below the threshold and 0 for all values above.
Theconjecture remainsunproven.
1
0.8
0.6
0.4
0.2
0
elbaifsitas P
800
600
400
200
0
Clausesymbol ratio mn
emitnu R
D PL L
Walk S AT
Clausesymbol ratio mn
a b
Figure7.19 a Graphshowingtheprobabilitythatarandom3-C NFsentencewithn50
symbolsissatisfiableasafunctionoftheclausesymbolratiomn.b Graphofthemedian
runtimemeasuredinnumberofrecursivecallsto D PL Lagoodproxyonrandom3-C NF
sentences.Themostdifficultproblemshaveaclausesymbolratioofabout4.3.
Nowthatwehaveagood ideawherethesatisfiable andunsatisfiable problems are the
next question is where are the hard problems? It turns out that they are also often at the
threshold value. Figure7.19b showsthat 50-symbol problems atthe threshold value of4.3
are about 20 times more difficult to solve than those at a ratio of 3.3. The underconstrained
problems are easiest to solve because it is so easy to guess a solution; the overconstrained
problemsarenotaseasyastheunderconstrained butstillaremucheasierthantheonesright
atthethreshold.
Section7.7. Agents Basedon Propositional Logic 265
In this section webring together what we have learned so far in order to construct wumpus
worldagentsthatusepropositionallogic. Thefirststepistoenabletheagenttodeducetothe
extent possible thestate oftheworldgiven itspercept history. Thisrequires writing downa
completelogicalmodeloftheeffectsofactions. Wealsoshowhowtheagentcankeeptrackof
the world efficiently without going back into the percept history for each inference. Finally
we show how the agent can use logical inference to construct plans that are guaranteed to
achieveitsgoals.
As stated at the beginning of the chapter a logical agent operates by deducing what to do
from a knowledge base of sentences about the world. The knowledge base is composed of
axiomsgeneral knowledge about how the world worksand percept sentences obtained
fromtheagentsexperienceinaparticularworld. Inthissectionwefocusontheproblemof
deducing thecurrentstateofthewumpusworldwheream Iis thatsquaresafeandsoon.
Webegancollecting axioms in Section 7.4.3. Theagent knows thatthe starting square
containsnopit P andnowumpus W . Furthermore foreachsquare itknowsthat
11 11
thesquareisbreezyifandonlyifaneighboringsquarehasapit;andasquareissmellyifand
onlyifaneighboring squarehasawumpus. Thusweinclude alargecollection ofsentences
ofthefollowingform:
B P P
11 12 21
S W W
11 12 21
""
Theagentalsoknowsthatthereisexactlyonewumpus. Thisisexpressed intwoparts. First
wehavetosaythatthereisatleastonewumpus:
W W W W .
11 12 43 44
Then wehave tosaythat there is atmostonewumpus. Foreach pairoflocations weadd a
sentence sayingthatatleastoneofthemmustbewumpus-free:
W W
11 12
W W
11 13
""
W W .
43 44
So far so good. Now lets consider the agents percepts. If there is currently a stench one
mightsuppose thataproposition Stench shouldbeaddedtotheknowledgebase. Thisisnot
quiterighthowever: iftherewasnostenchattheprevioustimestepthen Stench wouldal-
readybeassertedandthenewassertionwouldsimplyresult inacontradiction. Theproblem
issolvedwhenwerealizethataperceptassertssomething onlyaboutthecurrenttime. Thus
ifthetimestepassupplied to M AK E-P ER CE PT-S EN TE NC E in Figure7.1is4thenweadd
Stench4 totheknowledge base ratherthan Stenchneatlyavoiding anycontradiction with
Stench3. Thesamegoesforthebreeze bumpglitter andscream percepts.
Theideaofassociating propositions withtimesteps extends toanyaspect oftheworld
thatchangesovertime. Forexampletheinitialknowledgebaseincludes L0 theagentisin
11
square11attime0aswellas Facing East0 Have Arrow0and Wumpus Alive0. Weuse
theword fluentfrom the Latin fluens flowing toreferanaspect of theworld thatchanges.
F LU EN T
Fluentisasynonymforstatevariableinthesensedescribedinthediscussionoffactored
representations in Section 2.4.7 on page 57. Symbols associated with permanent aspects of
A TE MP OR AL theworlddonotneedatimesuperscript andaresometimescalled atemporalvariables.
V AR IA BL E
We can connect stench and breeze percepts directly to the properties of the squares
where they areexperienced through the location fluentas follows.10 Forany timestep t and
anysquare xyweassert
Lt Breezet B
xy xy
Lt Stencht S .
xy xy
Now of course we need axioms that allow the agent to keep track of fluents such as Lt .
xy
These fluents change as the result of actions taken by the agent so in the terminology of
logicalsentences.
First we need proposition symbols for the occurrences of actions. As with percepts
thesesymbolsareindexedbytime;thus Forward0meansthattheagentexecutesthe Forward
action at time 0. Byconvention the percept fora given time step happens first followed by
theactionforthattimestepfollowedbyatransition tothe nexttimestep.
To describe how the world changes we can try writing effect axioms that specify the
E FF EC TA XI OM
outcomeofanactionatthenexttimestep. Forexampleiftheagentisatlocation11facing
east at time0 and goes Forward the result isthat the agent is in square 21 and no longer
isin11:
L0 Facing East0 Forward0 L1 L1 . 7.1
11 21 11
We would need one such sentence for each possible time step for each of the 16 squares
andeachofthefourorientations. Wewouldalsoneedsimilarsentencesfortheotheractions:
Grab Shoot Climb Turn Leftand Turn Right.
Let us suppose that the agent does decide to move Forward at time 0 and asserts this
fact into its knowledge base. Given the effect axiom in Equation 7.1 combined with the
initial assertions about the state at time 0 the agent can now deduce that it is in 21. That
is A SK KB L1 true. Sofar so good. Unfortunately the news elsewhere isless good:
21
if we A SK KB Have Arrow1 the answer is false that is the agent cannot prove it still
has the arrow; norcan it prove it doesnt have it! Theinformation has been lost because the
effectaxiom fails tostatewhatremains unchanged astheresultofanaction. Theneed todo
this gives rise to the frame problem.11 One possible solution to the frame problem would
F RA ME PR OB LE M
withrespecttowhichmotionismeasured. Italsohasananalogytotheframesofamovie inwhichnormally
mostofthebackgroundstaysconstantwhilechangesoccurintheforeground.
Section7.7. Agents Basedon Propositional Logic 267
be to add frame axioms explicitly asserting all the propositions that remain the same. For
F RA ME AX IO M
exampleforeachtimetwewouldhave
Forwardt Have Arrowt Have Arrowt1
Forwardt Wumpus Alivet Wumpus Alivet1
""
where we explicitly mention every proposition that stays unchanged from time t to time
t 1 under the action Forward. Although the agent now knows that it still has the arrow
aftermovingforwardandthatthewumpushasnt diedorcomebacktolifetheproliferation
of frame axioms seems remarkably inefficient. In a world with m different actions and n
fluents the set of frame axioms will be of size Omn. This specific manifestation of the
R EP RE SE NT AT IO NA L frame problem is sometimes called the representational frame problem. Historically the
F RA ME PR OB LE M
problemwasasignificantonefor A Iresearchers; weexploreitfurtherinthenotesattheend
ofthechapter.
Therepresentational frameproblemissignificantbecausetherealworldhasverymany
fluents to put it mildly. Fortunately for us humans each action typically changes no more
than some small number k of those fluentsthe world exhibits locality. Solving the repre-
L OC AL IT Y
sentational frameproblem requires definingthetransition modelwithasetofaxiomsofsize
I NF ER EN TI AL FR AM E Omk rather than size Omn. There is also an inferential frame problem: the problem
P RO BL EM
ofprojecting forwardtheresultsofa tstepplanofactionintime Oktratherthan Ont.
The solution to the problem involves changing ones focus from writing axioms about
actions towriting axioms about fluents. Thus foreach fluent F wewillhavean axiom that
definesthetruthvalueof Ft1intermsoffluentsincluding F itselfattimetandtheactions
thatmayhaveoccurredattimet. Nowthetruthvalueof Ft1 canbesetinoneoftwoways:
eithertheactionattimetcauses F tobetrueatt1or F wasalreadytrueattimetandthe
actionattimetdoesnotcauseittobefalse. Anaxiomofthisformiscalledasuccessor-state
S UC CE SS OR-S TA TE axiomandhasthisschema:
A XI OM
Ft1 Action Causes Ft Ft Action Causes Not Ft.
One of the simplest successor-state axioms is the one for Have Arrow. Because there is no
actionforreloading the Action Causes Ft partgoesawayandweareleftwith
Have Arrowt1 Have Arrowt Shoott. 7.2
For the agents location the successor-state axioms are more elaborate. For example Lt1
11
is true if either a the agent moved Forward from 12 when facing south or from 21
whenfacingwest;orb Lt wasalreadytrueandtheactiondidnotcausemovementeither
11
because the action wasnot Forward orbecause theaction bumped into awall. Written out
inpropositional logicthisbecomes
Lt1 Lt Forwardt Bumpt1
11 11
Lt Southt Forwardt 7.3
12
Lt Westt Forwardt.
21
Exercise7.26asksyoutowriteoutaxiomsfortheremaining wumpusworldfluents.
Givenacompletesetofsuccessor-state axiomsandtheotheraxiomslistedatthebegin-
ningofthissection theagentwillbeableto A SK andansweranyanswerablequestionabout
thecurrentstateoftheworld. Forexamplein Section7.2theinitialsequenceofperceptsand
actionsis
Stench0 Breeze0 Glitter0 Bump0 Scream0 ; Forward0
Stench1 Breeze1 Glitter1 Bump1 Scream1 ; Turn Right1
Stench2 Breeze2 Glitter2 Bump2 Scream2 ; Turn Right2
Stench3 Breeze3 Glitter3 Bump3 Scream3 ; Forward3
Stench4 Breeze4 Glitter4 Bump4 Scream4 ; Turn Right4
Stench5 Breeze5 Glitter5 Bump5 Scream5 ; Forward5
Stench6 Breeze6 Glitter6 Bump6 Scream6
At this point we have A SK KB L6 true so the agent knows where it is. Moreover
12
A SK KB W 13true and A SK KB P 31truesotheagenthasfoundthewumpusand
oneofthepits. Themostimportantquestionfortheagentiswhetherasquareis O Ktomove
into that is the square contains no pit nor live wumpus. Its convenient to add axioms for
thishavingtheform
O Kt P W Wumpus Alivet.
xy xy xy
Finally A SK KB OK6 true so the square 22 is O K to move into. In fact given a
22
soundandcompleteinferencealgorithmsuchas D PL Ltheagentcanansweranyanswerable
questionaboutwhichsquaresare O Kandcandosoinjustafewmilliseconds forsmall-to-
mediumwumpusworlds.
Solving the representational and inferential frame problems is a big step forward but
apernicious problem remains: weneed toconfirm that allthenecessary preconditions ofan
actionholdforittohaveitsintendedeffect. Wesaidthatthe Forward actionmovestheagent
aheadunlessthereisawallinthewaybuttherearemanyotherunusualexceptionsthatcould
causetheactiontofail: theagentmighttripandfallbestrickenwithaheartattackbecarried
Q UA LI FI CA TI ON awaybygiant bats etc. Specifying allthese exceptions iscalled thequalification problem.
P RO BL EM
There is no complete solution within logic; system designers have to use good judgment in
deciding how detailed they want to be in specifying their model and what details they want
toleaveout. Wewillseein Chapter13thatprobability theoryallowsustosummarizeallthe
exceptions withoutexplicitly namingthem.
Theabilitytodeducevariousaspectsofthestateoftheworldcanbecombinedfairlystraight-
forwardly withconditionaction rulesandwithproblem-solving algorithms from Chapters3
and4toproduceahybridagentforthewumpusworld. Figure7.20showsonepossibleway
H YB RI DA GE NT
to do this. The agent program maintains and updates a knowledge base as well as a current
plan. The initial knowledge base contains the atemporal axiomsthose that dont depend
on t such as the axiom relating the breeziness of squares to the presence of pits. At each
timestepthenewperceptsentenceisaddedalongwithalltheaxiomsthatdependontsuch
Section7.7. Agents Basedon Propositional Logic 269
asthesuccessor-state axioms. Thenextsectionexplainswhytheagentdoesnt needaxioms
for future time steps. Then the agent uses logical inference by A SKing questions of the
knowledgebasetoworkoutwhichsquares aresafeandwhichhaveyettobevisited.
Themainbodyoftheagentprogramconstructsaplanbasedonadecreasingpriorityof
goals. Firstifthereisaglitter theprogram constructs a plantograbthegoldfollowaroute
back tothe initial location and climb out of the cave. Otherwise ifthere isno current plan
the program plans a route to the closest safe square that it has not visited yet making sure
""
the route goes through only safe squares. Route planning is done with A search not with
A SK. Iftherearenosafesquarestoexplorethenextstepifthe agentstillhasanarrowis
totry to makea safe square byshooting at one ofthe possible wumpus locations. These are
determined by asking where A SK KB W xy is falsethat is where it is not known that
there is not awumpus. The function P LA N-S HO T not shown uses P LA N-R OU TE to plan a
sequence ofactions thatwillline upthis shot. Ifthis fails the program looks forasquare to
explore that isnot provably unsafethat is asquare forwhich A SK KB OKt xyreturns
false. Ifthereisnosuchsquare thenthemissionisimpossibleandtheagentretreatsto11
andclimbsoutofthecave.
The agent program in Figure 7.20 works quite well but it has one major weakness: as time
goesbythecomputationalexpenseinvolvedinthecallsto A SKgoesupandup. Thishappens
mainlybecausetherequiredinferenceshavetogobackfurtherandfurtherintimeandinvolve
more and more proposition symbols. Obviously this is unsustainablewe cannot have an
agent whose timetoprocess each percept grows inproportion to the length ofits life! What
wereallyneedisaconstant updatetimethatisindependent oft. Theobviousansweristo
saveorcachetheresultsofinference sothattheinferenceprocessatthenexttimestepcan
C AC HI NG
buildontheresultsofearlierstepsinstead ofhavingtostartagainfromscratch.
As we saw in Section 4.4 the past history of percepts and all their ramifications can
bereplaced bythe beliefstatethat issomerepresentation ofthesetofallpossible current
statesoftheworld.12 Theprocessofupdating thebeliefstateasnewpercepts arriveiscalled
state estimation. Whereas in Section 4.4 the belief state was an explicit list of states here
we can use a logical sentence involving the proposition symbols associated with the current
timestepaswellastheatemporal symbols. Forexamplethe logicalsentence
Wumpus Alive1 L1 B P P 7.4
21 21 31 22
represents the set of all states at time 1 in which the wumpus is alive the agent is at 21
thatsquareisbreezy andthereisapitin 31or22orboth.
Maintaining an exact belief state as a logical formula turns out not to be easy. If there
arenfluentsymbolsfortimetthenthereare2npossiblestatesthatisassignmentsoftruth
valuestothosesymbols. Nowthesetofbeliefstatesisthepowersetsetofallsubsetsofthe
set of physical states. There are 2n physical states hence 22n belief states. Even if weused
the most compact possible encoding of logical formulas with each belief state represented
increasinglyexpensiveasthehistorygetslonger.
function H YB RI D-W UM PU S-A GE NTperceptreturnsanaction
inputs:perceptaliststenchbreezeglitterbumpscream
persistent: K Baknowledgebaseinitiallytheatemporalwumpusphysics
tacounterinitially0indicatingtime
plananactionsequenceinitiallyempty
T EL LK BM AK E-P ER CE PT-S EN TE NC Eperceptt
T EL Lthe K B thetemporalphysicssentencesfortimet
safexy : A SK KB OKt xy true
t
if A SK KB Glitter true then
plan Grab P LA N-R OU TEcurrent11safe Climb
ifplan isemptythen
unvisitedxy : A SK KB Lt xcid:3 y false forall tcid:5 t
plan P LA N-R OU TEcurrentunvisitedsafesafe
t
ifplan isemptyand A SK KB Have Arrow true then
possible wumpusxy : A SK KB Wxy false
plan P LA N-S HO Tcurrentpossible wumpussafe
ifplan isemptythen nochoicebuttotakearisk
not unsafexy : A SK KB OKt xy false
plan P LA N-R OU TEcurrentunvisitednot unsafesafe
ifplan isemptythen
plan P LA N-R OU TEcurrent11safe Climb
action P OPplan
T EL LK BM AK E-A CT IO N-S EN TE NC Eactiont
tt 1
returnaction
function P LA N-R OU TEcurrentgoalsallowedreturnsanactionsequence
inputs:currenttheagentscurrentposition
goalsasetofsquares;trytoplanaroutetooneofthem
allowedasetofsquaresthatcanformpartoftheroute
problem R OU TE-P RO BL EMcurrentgoalsallowed
return A-G RA PH-S EA RC Hproblem
Figure7.20 Ahybridagentprogramforthewumpusworld.Itusesapropositionalknowl-
edgebase to inferthestate ofthe world anda combinationof problem-solvingsearch and
domain-specificcodetodecidewhatactionstotake.
by a unique binary number we would need numbers with log 22n 2n bits to label the
2
currentbeliefstate. Thatisexactstateestimationmayrequirelogicalformulaswhosesizeis
exponential inthenumberofsymbols.
Onevery common and natural scheme for approximate state estimation is to represent
beliefstatesasconjunctionsofliteralsthatis1-C NFformulas. Todothistheagentprogram
simply tries to prove Xt and Xt for each symbol Xt as well as each atemporal symbol
whose truth value is not yet known given the belief state at t 1. The conjunction of
Section7.7. Agents Basedon Propositional Logic 271
conservative approximation to the exact wiggly belief state shaded region with dashed
outline.Eachpossibleworldisshownasacircle;theshadedonesareconsistentwithallthe
percepts.
provableliteralsbecomesthenewbeliefstateandthepreviousbeliefstateisdiscarded.
Itis important to understand that this scheme maylose some information as timegoes
along. Forexample if the sentence in Equation 7.4 were the true belief state then neither
P nor P would be provable individually and neither would appear in the 1-C NF belief
31 22
state. Exercise 7.27 explores one possible solution to this problem. On the other hand
because every literal in the 1-C NF belief state is proved from the previous belief state and
the initial belief state is a true assertion we know that entire 1-C NF belief state must be
true. Thus thesetofpossible statesrepresented bythe1-C NFbeliefstateincludes allstates
that are in fact possible given the full percept history. As illustrated in Figure 7.21 the 1-
C ON SE RV AT IV E C NFbeliefstateactsasasimpleouterenvelopeorconservativeapproximationaroundthe
A PP RO XI MA TI ON
exact belief state. We see this idea of conservative approximations to complicated sets as a
recurring themeinmanyareasof A I.
Theagentin Figure7.20useslogicalinference todetermine whichsquaresaresafebutuses
""
A search to make plans. In this section we show how to make plans by logical inference.
Thebasicideaisverysimple:
1. Constructasentence thatincludes
a Init0acollection ofassertions abouttheinitialstate;
b Transition1...Transitiont the successor-state axioms for all possible actions
ateachtimeuptosomemaximumtimet;
c theassertion thatthegoalisachieved attimet: Have Goldt Climbed Outt.
2. Presentthewholesentence toa S ATsolver. Ifthesolverfindsasatisfying model then
the goal is achievable; if the sentence is unsatisfiable then the planning problem is
impossible.
3. Assuming a model is found extract from the model those variables that represent ac-
tionsandareassigned true. Togethertheyrepresent aplantoachievethegoals.
A propositional planning procedure S AT PL AN is shown in Figure 7.22. It implements the
basic idea just given with one twist. Because the agent does not know how many steps it
will take to reach the goal the algorithm tries each possible number of steps t up to some
maximumconceivableplanlength T . Inthiswayitisguaranteedtofindtheshortestplan
max
if one exists. Because of the way S AT PL AN searches for a solution this approach cannot
be used in a partially observable environment; S AT PL AN would just set the unobservable
variables tothevaluesitneedstocreateasolution.
function S AT PL ANinit transition goal T maxreturnssolutionorfailure
inputs:init transition goalconstituteadescriptionoftheproblem
T anupperlimitforplanlength
max
fort 0to T do
max
cnf T RA NS LA TE-T O-S ATinit transition goalt
model S AT-S OL VE Rcnf
ifmodel isnotnullthen
return E XT RA CT-S OL UT IO Nmodel
returnfailure
sentenceinwhichthegoalisassertedtoholdatafixedtimesteptandaxiomsareincluded
foreachtimestepuptot. Ifthesatisfiabilityalgorithmfindsamodelthenaplanisextracted
by looking at those proposition symbols that refer to actions and are assigned true in the
model.Ifnomodelexiststhentheprocessisrepeatedwiththegoalmovedonesteplater.
The key step in using S AT PL AN is the construction of the knowledge base. It might
seem on casual inspection that the wumpus world axioms in Section 7.7.1 suffice forsteps
1aand1babove. Thereishoweverasignificantdifference betweentherequirements for
entailment as tested by A SKand those forsatisfiability. Consider forexample theagents
location initially 11 and suppose the agents unambitious goal isto be in21 attime 1.
Theinitialknowledgebasecontains L0 andthegoalis L1 . Using A SKwecanprove L1
11 21 21
if Forward0 is asserted and reassuringly we cannot prove L1 if say Shoot0 is asserted
21
instead. Now S AT PL AN will find the plan Forward0; so far so good. Unfortunately
S AT PL ANalsofindstheplan Shoot0. Howcouldthisbe? Tofindoutweinspectthemodel
that S AT PL AN constructs: it includes the assignment L0 that is the agent can be in 21
21
attime1bybeingthereattime0andshooting. Onemightask Didntwesaytheagentisin
11attime0? Yeswedidbutwedidnttelltheagentthatitcantbeintwoplacesatonce!
For entailment L0 is unknown and cannot therefore be used in a proof; for satisfiability
21
Section7.7. Agents Basedon Propositional Logic 273
on the other hand L0 is unknown and can therefore be set to whatever value helps to
21
makethegoaltrue. Forthisreason S AT PL AN isagooddebuggingtoolforknowledgebases
because it reveals places where knowledge is missing. Inthis particular case wecan fixthe
knowledgebasebyassertingthatateachtimesteptheagentisinexactlyonelocationusing
acollection ofsentences similartothoseusedtoasserttheexistence ofexactlyonewumpus.
Alternativelywecanassert L0 foralllocationsotherthan11;thesuccessor-stateaxiom
xy
for location takes care of subsequent time steps. The same fixes also work to make sure the
agenthasonlyoneorientation.
S AT PL AN has more surprises in store however. The first is that it finds models with
impossibleactionssuchasshootingwithnoarrow. Tounderstandwhyweneedtolookmore
carefullyatwhatthesuccessor-state axiomssuchas Equation7.3sayaboutactionswhose
preconditionsarenotsatisfied. Theaxiomsdopredictcorrectlythatnothingwillhappenwhen
suchanaction isexecuted see Exercise10.14 buttheydonotsaythattheactioncannot be
P RE CO ND IT IO N executed! Toavoid generating plans withillegal actions wemustadd precondition axioms
A XI OM S
statingthatanactionoccurrencerequiresthepreconditionstobesatisfied.13 Forexamplewe
needtosayforeachtimetthat
Shoott Have Arrowt .
This ensures that if a plan selects the Shoot action at any time it must be the case that the
agenthasanarrowatthattime.
S AT PL ANssecondsurpriseisthecreationofplanswithmultiplesimultaneousactions.
For example it may come up with a model in which both Forward0 and Shoot0 are true
A CT IO NE XC LU SI ON whichisnotallowed. Toeliminate thisproblem weintroduce action exclusion axioms: for
A XI OM
everypairofactions At and At weaddtheaxiom
i j
At At .
i j
Itmight bepointed out that walking forward and shooting at the same timeis not so hard to
do whereas say shooting and grabbing atthesametimeis ratherimpractical. Byimposing
action exclusion axioms only on pairs of actions that really do interfere with each other we
canallowforplansthatincludemultiplesimultaneousactionsandbecause S AT PL ANfinds
theshortestlegalplanwecanbesurethatitwilltakeadvantage ofthiscapability.
To summarize S AT PL AN finds models for a sentence containing the initial state the
goal the successor-state axioms the precondition axioms and the action exclusion axioms.
It can be shown that this collection of axioms is sufficient in the sense that there are no
longer any spurious solutions. Any model satisfying the propositional sentence will be a
valid plan for the original problem. Modern S AT-solving technology makes the approach
quite practical. Forexample a D PL L-stylesolverhasno difficulty in generating the11-step
solution forthewumpusworldinstance shownin Figure7.2.
Thissectionhasdescribedadeclarativeapproachtoagentconstruction: theagentworks
byacombination ofasserting sentences intheknowledge baseandperforming logical infer-
ence. This approach has some weaknesses hidden in phrases such as for each time t and
thesuccessor-stateaxioms.
for each square xy. For any practical agent these phrases have to be implemented by
codethatgenerates instances ofthegeneral sentence schemaautomatically forinsertion into
theknowledge base. Forawumpusworldofreasonable sizeone comparable toasmallish
computer gamewe might need a 100100 board and 1000 time steps leading to knowl-
edge bases withtens orhundreds ofmillions ofsentences. Notonly does thisbecome rather
impractical but it also illustrates a deeper problem: we know something about the wum-
pus worldnamely that the physics works the same way across all squares and all time
stepsthat we cannot express directly in the language of propositional logic. To solve this
problem we need a more expressive language one in which phrases like for each time t
and for each square xy can be written in a natural way. First-order logic described in
canbedescribed inabouttensentences ratherthantenmillionortentrillion.
We have introduced knowledge-based agents and have shown how to define a logic with
whichsuchagentscanreasonabouttheworld. Themainpoints areasfollows:
Intelligent agentsneedknowledge abouttheworldinordertoreachgooddecisions.
Knowledge is contained in agents in the form of sentences in a knowledge represen-
tationlanguagethatarestoredinaknowledgebase.
A knowledge-based agent is composed of a knowledge base and an inference mecha-
nism. Itoperates bystoring sentences abouttheworldinits knowledge base using the
inference mechanism toinfernew sentences and using these sentences todecide what
actiontotake.
A representation language is defined by its syntax which specifies the structure of
sentencesanditssemanticswhichdefinesthetruthofeachsentenceineachpossible
worldormodel.
The relationship of entailment between sentences is crucial to our understanding of
reasoning. A sentence α entails another sentence β if β is true in all worlds where
α is true. Equivalent definitions include the validity of the sentence α β and the
unsatisfiabilityofthesentence αβ.
Inferenceistheprocessofderivingnewsentencesfromoldones. Soundinferencealgo-
rithmsderiveonlysentencesthatareentailed; completealgorithmsderiveallsentences
thatareentailed.
Propositionallogicisasimplelanguageconsistingofpropositionsymbolsandlogical
connectives. Itcanhandlepropositionsthatareknowntrueknownfalseorcompletely
unknown.
The set of possible models given a fixed propositional vocabulary is finite so en-
tailment can be checked by enumerating models. Efficient model-checking inference
algorithms for propositional logic include backtracking and local search methods and
canoftensolvelargeproblemsquickly.
8
F IR ST-O RD ER L OG IC
Inwhichwenoticethattheworldisblessedwithmanyobjectssomeofwhichare
relatedtootherobjects andinwhichweendeavor toreasonaboutthem.
In Chapter7weshowedhowaknowledge-based agentcouldrepresenttheworldinwhichit
operates and deduce what actions totake. Weused propositional logic as ourrepresentation
language because it sufficed to illustrate the basic concepts of logic and knowledge-based
agents. Unfortunately propositional logic is too puny a language to represent knowledge
of complex environments in a concise way. In this chapter we examine first-order logic1
F IR ST-O RD ER LO GI C
which is sufficiently expressive to represent a good deal of our commonsense knowledge.
It also either subsumes or forms the foundation of many other representation languages and
hasbeenstudied intensively formanydecades. Webeginin Section8.1withadiscussion of
representationlanguagesingeneral;Section8.2coversthesyntaxandsemanticsoffirst-order
logic;Sections8.3and8.4illustrate theuseoffirst-order logicforsimplerepresentations.
In this section we discuss the nature of representation languages. Ourdiscussion motivates
thedevelopmentoffirst-orderlogicamuchmoreexpressive languagethanthepropositional
logicintroduced in Chapter7. Welookatpropositional logicandatotherkindsoflanguages
to understand what works and what fails. Our discussion will be cursory compressing cen-
turiesofthought trialanderrorintoafewparagraphs.
Programming languages such as C or Java or Lisp are by far the largest class of
formal languages in common use. Programs themselves represent in a direct sense only
computational processes. Data structures within programs can represent facts; for example
aprogram could usea 44array torepresent the contents ofthe wumpusworld. Thus the
programminglanguagestatement World22 Pit isafairlynaturalwaytoassertthatthere
is apit in square 22. Such representations might be considered ad hoc; database systems
were developed precisely to provide a more general domain-independent way to store and
285
retrieve facts. What programming languages lack is any general mechanism for deriving
factsfromotherfacts;eachupdatetoadatastructureisdonebyadomain-specificprocedure
whosedetails arederived bytheprogrammer fromhisorherownknowledge ofthedomain.
Thisproceduralapproachcanbecontrastedwiththedeclarativenatureofpropositionallogic
inwhichknowledgeandinferenceareseparateandinferenceisentirelydomainindependent.
A second drawback of data structures in programs and of databases for that matter
is the lack of any easy way to say for example There is a pit in 22 or 31 or If the
wumpusisin11thenheisnotin22. Programscanstoreasinglevalueforeachvariable
andsomesystemsallowthevaluetobeunknownbuttheylacktheexpressivenessrequired
tohandle partialinformation.
Propositional logic is a declarative language because its semantics is based on a truth
relation between sentences and possible worlds. It also has sufficient expressive power to
deal withpartial information using disjunction and negation. Propositional logic hasathird
property that is desirable in representation languages namely compositionality. In a com-
C OM PO SI TI ON AL IT Y
positional language the meaning of asentence is a function of the meaning of its parts. For
example the meaning of S S is related to the meanings of S and S . It
14 12 14 12
wouldbeverystrange if S meantthatthere isastench insquare 14and S meant
14 12
thatthereisastenchinsquare12but S S meantthat Franceand Polanddrew11
14 12
in last weeks ice hockey qualifying match. Clearly noncompositionality makes life much
moredifficultforthereasoning system.
As we saw in Chapter 7 however propositional logic lacks the expressive power to
concisely describe anenvironment withmanyobjects. Forexample wewereforced towrite
aseparate ruleaboutbreezes andpitsforeachsquare suchas
B P P .
11 12 21
In Englishontheotherhanditseemseasyenoughtosayonceandforall Squaresadjacent
topitsarebreezy. Thesyntaxandsemanticsof Englishsomehowmakeitpossibletodescribe
theenvironment concisely.
Natural languages such as English or Spanish are very expressive indeed. We managed to
writealmostthiswholebookinnatural language withonlyoccasional lapsesintootherlan-
guages including logic mathematics and the language of diagrams. There is a long tradi-
tioninlinguisticsandthephilosophyoflanguagethatviewsnaturallanguageasadeclarative
knowledge representation language. If we could uncover the rules for natural language we
could use it in representation and reasoning systems and gain the benefit of the billions of
pagesthathavebeenwritteninnaturallanguage.
Themodernviewofnaturallanguageisthatitservesaasamediumforcommunication
ratherthanpurerepresentation. Whenaspeakerpoints andsays Look! thelistenercomes
to know that say Superman has finally appeared over the rooftops. Yet we would not want
to say that the sentence Look! represents that fact. Rather the meaning of the sentence
depends both on the sentence itself and on the context in which the sentence was spoken.
Clearly one could not store a sentence such as Look! in a knowledge base and expect to
Section8.1. Representation Revisited 287
recover its meaning without also storing a representation of the contextwhich raises the
question of how the context itself can be represented. Natural languages also suffer from
ambiguityaproblemforarepresentation language. As Pinker1995putsit: Whenpeople
A MB IG UI TY
thinkaboutspringsurelytheyarenotconfusedastowhethertheyarethinkingaboutaseason
or something that goes boingand if one word can correspond to two thoughts thoughts
cantbewords.
The famous Sapir Whorf hypothesis claims that our understanding of the world is
strongly influenced bythelanguage wespeak. Whorf1956wrote Wecutnature uporga-
nizeitintoconcepts andascribe significances aswedo largely because weareparties toan
agreement to organize it this wayan agreement that holds throughout our speech commu-
nity and is codified in the patterns of our language. It is certainly true that different speech
communities divide uptheworlddifferently. The Frenchhavetwowordschaise andfau-
teuil for a concept that English speakers cover with one: chair. But English speakers
caneasilyrecognizethecategoryfauteuilandgiveitanameroughlyopen-armchairso
does language really make a difference? Whorf relied mainly on intuition and speculation
but in the intervening years we actually have real data from anthropological psychological
andneurological studies.
Forexamplecanyourememberwhichofthefollowingtwophrasesformedtheopening
of Section8.1?
Inthissection wediscussthenatureofrepresentation languages ...
Thissectioncoversthetopicofknowledge representation languages ...
Wanner 1974 did a similar experiment and found that subjects made the right choice at
chance levelabout 50 of the timebut remembered the content of what they read with
betterthan90 accuracy. Thissuggests thatpeople process thewordstoform somekindof
nonverbal representation.
More interesting is the case in which a concept is completely absent in a language.
Speakers of the Australian aboriginal language Guugu Yimithirr have no words for relative
directions such as front back right or left. Instead they use absolute directions saying
for example the equivalent of I have a pain in my north arm. This difference in language
makes a difference in behavior: Guugu Yimithirr speakers are better at navigating in open
terrain while Englishspeakers arebetteratplacingtheforktotherightoftheplate.
Language also seems to influence thought through seemingly arbitrary grammatical
features such as the gender of nouns. For example bridge is masculine in Spanish and
feminine in German. Boroditsky 2003 asked subjects to choose English adjectives to de-
scribe a photograph of a particular bridge. Spanish speakers chose big dangerous strong
andtoweringwhereas Germanspeakerschosebeautifulelegantfragileandslender. Words
can serve as anchor points that affect how weperceive the world. Loftus and Palmer1974
showed experimental subjects a movie of an auto accident. Subjects who were asked How
fast were the cars going when they contacted each other? reported an average of 32 mph
whilesubjects whowereaskedthequestion withthewordsmashed insteadofcontacted
reported 41mphforthesamecarsinthesamemovie.
Inafirst-orderlogicreasoningsystemthatuses C NFwecanseethatthelinguisticform
A B and A B are the same because we can look inside the system and see
that the two sentences are stored as the same canonical C NFform. Can wedo that with the
human brain? Until recently the answer was no but now it is maybe. Mitchell et al.
2008 put subjects in an f M RI functional magnetic resonance imaging machine showed
themwordssuchasceleryandimagedtheirbrains. Theresearcherswerethenabletotrain
acomputerprogramtopredictfromabrainimagewhatwordthesubjecthadbeenpresented
with. Given two choices e.g. celery or airplane the system predicts correctly 77 of
the time. The system can even predict at above-chance levels for words it has never seen
an f M RI image of before by considering the images of related words and for people it has
never seen before proving that f M RI reveals some level of common representation across
people. This type of work is still in its infancy but f M RI and other imaging technology
such as intracranial electrophysiology Sahin et al. 2009 promises to give us much more
concrete ideasofwhathumanknowledge representations are like.
From the viewpoint of formal logic representing the same knowledge in two different
ways makes absolutely no difference; the same facts will be derivable from either represen-
tation. Inpractice however onerepresentation mightrequire fewerstepstoderiveaconclu-
sion meaning that a reasoner with limited resources could get to the conclusion using one
representation but not the other. For nondeductive tasks such as learning from experience
outcomes are necessarily dependent on the form of the representations used. We show in
ofwhichareconsistentwithallthedatathemostcommonwayofbreakingthetieistochoose
themostsuccincttheoryandthatdependsonthelanguageusedtorepresenttheories. Thus
theinfluenceoflanguage onthoughtisunavoidable foranyagentthatdoeslearning.
Wecan adopt the foundation of propositional logica declarative compositional semantics
that is context-independent and unambiguousand build a more expressive logic on that
foundation borrowing representational ideas fromnatural language whileavoiding itsdraw-
backs. Whenwelookatthesyntaxofnaturallanguagethemostobviouselementsarenouns
and noun phrases that referto objects squares pits wumpuses and verbs and verbphrases
O BJ EC T
that refer to relations among objects is breezy is adjacent to shoots. Some of these rela-
R EL AT IO N
tions are functionsrelations in which there is only one value for a given input. It is
F UN CT IO N
easytostartlistingexamplesofobjects relations andfunctions:
Objects: peoplehousesnumberstheories Ronald Mc Donaldcolorsbaseballgames
warscenturies ...
Relations: thesecanbeunaryrelations or propertiessuchasredround bogus prime
P RO PE RT Y
multistoried ...ormoregeneral n-aryrelations suchasbrotherofbiggerthaninside
partofhascoloroccurred afterownscomesbetween ...
Functions: fatherofbestfriendthirdinningofonemore thanbeginning of...
Indeed almost any assertion can be thought of as referring toobjects and properties orrela-
tions. Someexamplesfollow:
Section8.1. Representation Revisited 289
Oneplustwoequalsthree.
Objects: one two three one plus two; Relation: equals; Function: plus. One plus
two is a name for the object that is obtained by applying the function plus to the
objectsoneandtwo. Threeisanothernameforthisobject.
Squaresneighboring thewumpusaresmelly.
Objects: wumpussquares; Property: smelly;Relation: neighboring.
Evil King Johnruled Englandin1200.
Objects: John England1200;Relation: ruled;Properties: evilking.
Thelanguageoffirst-orderlogicwhosesyntaxandsemanticswedefineinthenextsection
isbuiltaroundobjectsandrelations. Ithasbeensoimportanttomathematicsphilosophyand
artificial intelligence precisely because those fieldsand indeed much of everyday human
existencecan beusefully thought ofasdealing withobjects andtherelations amongthem.
First-order logic can also express facts about some orall of theobjects inthe universe. This
enables one to represent general laws or rules such as the statement Squares neighboring
thewumpusaresmelly.
Theprimarydifference betweenpropositional andfirst-orderlogicliesinthe ontologi-
O NT OL OG IC AL calcommitmentmadebyeachlanguagethatiswhatitassumesaboutthenatureofreality.
C OM MI TM EN T
Mathematically this commitmentisexpressed through thenature oftheformal modelswith
respect to which the truth of sentences is defined. Forexample propositional logic assumes
that there are facts that either hold or do not hold in the world. Each fact can be in one
of two states: true or false and each model assigns true or false to each proposition sym-
bol see Section 7.4.2.2 First-order logic assumes more; namely that the world consists of
objects with certain relations among them that do or do not hold. The formal models are
correspondingly morecomplicatedthanthoseforpropositional logic. Special-purpose logics
make still further ontological commitments; forexample temporal logic assumes that facts
T EM PO RA LL OG IC
hold at particular times and that those times which may be points or intervals are ordered.
Thus special-purpose logics givecertain kinds ofobjects andtheaxioms about themfirst
class status within the logic rather than simply defining them within the knowledge base.
H IG HE R-O RD ER Higher-order logic views the relations and functions referred to by first-order logic as ob-
L OG IC
jectsinthemselves. Thisallowsonetomakeassertionsaboutallrelationsforexampleone
couldwishtodefinewhatitmeansforarelationtobetransitive. Unlikemostspecial-purpose
logics higher-order logic is strictly more expressive than first-order logic in the sense that
somesentences ofhigher-order logiccannot beexpressed by anyfinitenumberoffirst-order
logicsentences.
E PI ST EM OL OG IC AL A logic can also be characterized by its epistemological commitmentsthe possible
C OM MI TM EN T
states of knowledge that it allows with respect to each fact. In both propositional and first-
orderlogic asentence represents afactand theagent eitherbelieves thesentence tobetrue
believes it to be false or has no opinion. These logics therefore have three possible states
ofknowledge regarding anysentence. Systemsusing probability theoryontheotherhand
alargecitymightbetrueinourworldonlytodegree0.6infuzzylogic.
can have any degree of belief ranging from 0 total disbelief to 1 total belief.3 For ex-
ample a probabilistic wumpus-world agent might believe that the wumpus is in 13 with
probability 0.75. The ontological and epistemological commitments of five different logics
aresummarizedin Figure8.1.
Language Ontological Commitment Epistemological Commitment
Whatexistsintheworld Whatanagentbelievesaboutfacts
Propositionallogic facts truefalseunknown
First-orderlogic factsobjectsrelations truefalseunknown
Temporallogic factsobjectsrelationstimes truefalseunknown
Probabilitytheory facts degreeofbelief01
Fuzzylogic factswithdegreeoftruth01 knownintervalvalue
Figure8.1 Formallanguagesandtheirontologicalandepistemologicalcommitments.
Inthenextsectionwewilllaunchintothedetailsoffirst-orderlogic. Justasastudentof
physicsrequiressomefamiliaritywithmathematicsastudentof A Imustdevelopatalentfor
workingwithlogicalnotation. Ontheotherhanditisalsoimportantnottogettooconcerned
with the specifics of logical notationafter all there are dozens of different versions. The
mainthingstokeepholdofarehowthelanguage facilitates concise representations andhow
itssemanticsleadstosoundreasoning procedures.
We begin this section by specifying more precisely the way in which the possible worlds
of first-order logic reflect the ontological commitment to objects and relations. Then we
introduce thevariouselementsofthelanguage explaining theirsemantics aswegoalong.
Recall from Chapter 7 that the models of a logical language are the formal structures that
constitute the possible worlds under consideration. Each model links the vocabulary of the
logical sentences to elements of the possible world so that the truth of any sentence can
be determined. Thus models for propositional logic link proposition symbols to predefined
truth values. Models forfirst-order logic are much moreinteresting. First they have objects
inthem! Thedomainofamodelisthesetofobjectsordomainelementsitcontains. Thedo-
D OM AI N
mainisrequiredtobenonemptyeverypossibleworldmustcontainatleastoneobject. See
D OM AI NE LE ME NT S
Exercise 8.7 for a discussion of empty worlds. Mathematically speaking it doesnt matter
whattheseobjects areall thatmatters is howmanythereareineachparticularmodelbut
for pedagogical purposes well use a concrete example. Figure 8.2 shows a model with five
Indeedsomefuzzysystemsallowuncertaintydegreeofbeliefaboutdegreesoftruth.
Section8.2. Syntaxand Semanticsof First-Order Logic 291
objects: Richardthe Lionheart Kingof Englandfrom1189to1199;hisyoungerbrother the
evil King Johnwhoruledfrom1199to1215;theleftlegsof Richardand John;andacrown.
The objects in the model may be related in various ways. In the figure Richard and
John are brothers. Formally speaking a relation is just the set of tuples of objects that are
T UP LE
related. Atuple isacollection ofobjects arranged inafixedorderandiswrittenwithangle
brackets surrounding theobjects. Thusthebrotherhood relationinthismodelistheset
cid:16 Richard the Lionheart King Johncid:17 cid:16 King John Richardthe Lionheartcid:17. 8.1
Herewehavenamedtheobjectsin Englishbutyoumayifyouwishmentallysubstitutethe
picturesforthenames. Thecrownison King Johnsheadsotheonheadrelationcontains
just one tuple cid:16thecrown King Johncid:17. The brother and on head relations are binary
relationsthat is they relate pairs of objects. The model also contains unary relations or
properties: thepersonpropertyistrueofboth Richardand John;thekingpropertyistrue
onlyof Johnpresumably because Richardisdeadatthispoint;andthecrownpropertyis
trueonlyofthecrown.
Certain kinds of relationships are best considered as functions in that a given object
must berelated to exactly one object in this way. Forexample each person has one left leg
sothemodelhasaunaryleftlegfunction thatincludes thefollowingmappings:
cid:16 Richardthe Lionheartcid:17 Richardsleftleg
8.2
cid:16 King Johncid:17 Johnsleftleg.
Strictly speaking models in first-order logic require total functions that is there must be a
T OT AL FU NC TI ON S
valueforeveryinputtuple. Thus thecrownmusthavealeftlegandsomusteachoftheleft
legs. Thereisatechnicalsolutiontothisawkwardprobleminvolvinganadditionalinvisible
crown
on head
brother
person person
king
brother
R J
""
left leg left leg
indicatedbylabelsontheobjectsandoneunaryfunctionleft-leg.
object that is the left leg of everything that has no left leg including itself. Fortunately as
long as one makes no assertions about the left legs of things that have no left legs these
technicalities areofnoimport.
So far we have described the elements that populate models for first-order logic. The
other essential part of a model is the link between those elements and the vocabulary of the
logicalsentences whichweexplainnext.
We turn now to the syntax of first-order logic. The impatient reader can obtain a complete
description fromtheformalgrammarin Figure8.3.
Thebasic syntactic elements of first-order logic are the symbols that stand forobjects
relations and functions. The symbols therefore come in three kinds: constant symbols
C ON ST AN TS YM BO L
which stand for objects; predicate symbols which stand for relations; and function sym-
P RE DI CA TE SY MB OL
bols whichstand forfunctions. Weadopt theconvention thatthese symbols willbeginwith
F UN CT IO NS YM BO L
uppercase letters. For example we might use the constant symbols Richard and John; the
predicate symbols Brother On Head Person King and Crown; and the function symbol
Left Leg. As with proposition symbols the choice of names is entirely up to the user. Each
predicate andfunction symbolcomeswithanaritythatfixesthenumberofarguments.
A RI TY
Asin propositional logic every model must provide the information required to deter-
mine if any given sentence is true or false. Thus in addition to its objects relations and
functions each model includes an interpretation that specifies exactly which objects rela-
I NT ER PR ET AT IO N
tions and functions are referred to by the constant predicate and function symbols. One
possibleinterpretation forourexamplewhichalogicianwouldcalltheintendedinterpre-
I NT EN DE D tationisasfollows:
I NT ER PR ET AT IO N
Richard refersto Richardthe Lionheartand John referstotheevil King John.
Brother refers to the brotherhood relation that is the set of tuples of objects given in
Equation 8.1; On Head refers to theon head relation that holds between the crown
and King John;Person Kingand Crown refertothesetsofobjects thatarepersons
kingsandcrowns.
Left Leg referstotheleftlegfunction thatisthemappinggiven in Equation8.2.
There are many other possible interpretations of course. For example one interpretation
maps Richard to the crown and John to King Johns left leg. There are five objects in
the model so there are 25 possible interpretations just for the constant symbols Richard
and John. Notice that not all the objects need have a namefor example the intended
interpretation does not name the crown or the legs. It is also possible for an object to have
several names; there is an interpretation under which both Richard and John refer to the
crown.4 If you find this possibility confusing remember that in propositional logic it is
perfectly possible tohaveamodelinwhich Cloudy and Sunny arebothtrue; itisthejobof
theknowledgebasetoruleoutmodelsthatareinconsistent withourknowledge.
Section8.2. Syntaxand Semanticsof First-Order Logic 293
Sentence Atomic Sentence Complex Sentence
Atomic Sentence Predicate Predicate Term... Term Term
Complex Sentence Sentence Sentence
Sentence
Sentence Sentence
Sentence Sentence
Sentence Sentence
Sentence Sentence
Quantifier Variable... Sentence
Term Function Term...
Constant
Variable
Quantifier
Constant A X John
1
Variable a x s
Predicate True False After Loves Raining
Function Mother Left Leg
O PE RA TO RP RE CE DE NC E :
Figure8.3 The syntax of first-orderlogic with equality specified in Backus Naurform
seepage1060ifyouarenotfamiliarwiththisnotation.Operatorprecedencesarespecified
from highest to lowest. The precedence of quantifiers is such that a quantifier holds over
everythingtotherightofit.
R J R J R J R J R J R J
. . . . . . . . .
Figure8.4 Somemembersofthesetofallmodelsforalanguagewithtwoconstantsym-
bols Rand Jandonebinaryrelationsymbol.Theinterpretationofeachconstantsymbolis
shownbyagrayarrow.Withineachmodeltherelatedobjectsareconnectedbyarrows.
Insummaryamodelinfirst-orderlogicconsistsofasetofobjectsandaninterpretation
that maps constant symbols to objects predicate symbols to relations on those objects and
function symbols to functions on those objects. Just as with propositional logic entailment
validity and so on are defined in terms of all possible models. To get an idea of what the
set ofallpossible models looks like see Figure 8.4. Itshows that models vary inhow many
objects they containfrom one up to infinityand in the way the constant symbols map
to objects. If there are two constant symbols and one object then both symbols must refer
to the same object; but this can still happen even with more objects. When there are more
objects thanconstant symbols someoftheobjects willhavenonames. Becausethenumber
of possible models is unbounded checking entailment by the enumeration of all possible
modelsisnotfeasibleforfirst-orderlogicunlikepropositional logic. Evenifthenumberof
objects is restricted the number of combinations can be very large. See Exercise 8.5. For
theexamplein Figure8.4thereare137506194466 models withsixorfewerobjects.
Atermisalogicalexpression thatreferstoanobject. Constantsymbolsarethereforeterms
T ER M
but itisnotalways convenient tohave adistinct symbol tonameevery object. Forexample
in English we might use the expression King Johns left leg rather than giving a name
to his leg. This is what function symbols are for: instead of using a constant symbol we
use Left Leg John. In the general case a complex term is formed by a function symbol
followedbyaparenthesized listoftermsasargumentstothe functionsymbol. Itisimportant
to remember that acomplex term is just a complicated kind of name. It is not a subroutine
call that returns a value. There is no Left Leg subroutine that takes a person as input and
returnsaleg. Wecanreasonaboutleftlegse.g.statingthegeneralrulethateveryonehasone
and then deducing that John must have one without ever providing a definition of Left Leg.
Thisissomethingthatcannotbedonewithsubroutines inprogramming languages.5
The formal semantics of terms is straightforward. Consider a term ft ...t . The
function symbol f refers to somefunction inthe modelcall it F; theargument termsrefer
to objects in the domain call them d ...d ; and the term as a whole refers to the object
that is the value of the function F applied to d ...d . Forexample suppose the Left Leg
functionsymbolreferstothefunctionshownin Equation8.2and John refersto King John
then Left Leg John refers to King Johns left leg. In this way the interpretation fixes the
referentofeveryterm.
Now that we have both terms for referring to objects and predicate symbols for referring to
relations we can put them together to make atomic sentences that state facts. An atomic
example thefunctionthatsquaresitsargumentcanbewrittenasλx xxandcanbeappliedtoarguments
justlikeanyotherfunctionsymbol. Aλ-expressioncanalsobedefinedandusedasapredicatesymbol. See
Chapter22.Thelambdaoperatorin Lispplaysexactlythesamerole.Noticethattheuseofλinthiswaydoes
notincreasetheformalexpressivepoweroffirst-orderlogicbecauseanysentencethatincludesaλ-expression
canberewrittenbyplugginginitsargumentstoyieldanequivalentsentence.
Section8.2. Syntaxand Semanticsof First-Order Logic 295
sentence or atom for short is formed from a predicate symbol optionally followed by a
A TO MI CS EN TE NC E
parenthesized listoftermssuchas
A TO M
Brother Richard John.
This states under the intended interpretation given earlier that Richard the Lionheart is the
brotherof King John.6 Atomicsentences canhavecomplextermsasarguments. Thus
Married Father Richard Mother John
states that Richard the Lionhearts father is married to King Johns mother again under a
suitable interpretation.
Anatomic sentence is true in agiven model if the relation referred to by the predicate
symbolholdsamongtheobjectsreferredtobythearguments.
We can use logical connectives to construct more complex sentences with the same syntax
andsemanticsasinpropositional calculus. Herearefoursentences thataretrueinthemodel
of Figure8.2underourintendedinterpretation:
Brother Left Leg Richard John
Brother Richard John Brother John Richard
King Richard King John
King Richard King John.
Once we have a logic that allows objects it is only natural to want to express properties of
entire collections of objects instead of enumerating the objects by name. Quantifierslet us
Q UA NT IF IE R
dothis. First-orderlogiccontains twostandard quantifiers calleduniversal andexistential.
Universalquantification
Recall the difficulty we had in Chapter 7 with the expression of general rules in proposi-
tional logic. Rules such as Squares neighboring the wumpus are smelly and All kings
are persons are the bread and butter of first-order logic. We deal with the first of these in
Section8.3. Thesecondrule Allkingsarepersonsiswritteninfirst-orderlogicas
x Kingx Personx.
is usually pronounced Forall .... Remember that the upside-down A stands for all.
Thusthesentence says Forallxifxisaking thenxisaperson. Thesymbolxiscalled
a variable. By convention variables are lowercase letters. A variable is a term all by itself
V AR IA BL E
and as such can also serve as the argument of a functionfor example Left Legx. Aterm
withnovariables iscalledagroundterm.
G RO UN DT ER M
Intuitively the sentence x P where P is any logical expression says that P is true
forevery object x. More precisely x P istrue in agiven model if P istrue inall possible
E XT EN DE D extendedinterpretationsconstructedfromtheinterpretationgiveninthemodelwhereeach
I NT ER PR ET AT IO N
extended interpretation specifiesadomainelementtowhich xrefers.
Thissoundscomplicatedbutitisreallyjustacarefulwayofstatingtheintuitivemean-
ing of universal quantification. Consider the model shown in Figure 8.2 and the intended
interpretation thatgoeswithit. Wecanextendtheinterpretation infiveways:
x Richardthe Lionheart
x King John
x Richardsleftleg
x Johnsleftleg
x thecrown.
Theuniversallyquantifiedsentence x Kingx Personxistrueintheoriginalmodel
if the sentence Kingx Personx is true under each of the five extended interpreta-
tions. Thatistheuniversally quantifiedsentence isequivalent toasserting thefollowingfive
sentences:
Richardthe Lionheartisaking Richardthe Lionheartisaperson.
King Johnisaking King Johnisaperson.
Richardsleftlegisaking Richardsleftlegisaperson.
Johnsleftlegisaking Johnsleftlegisaperson.
Thecrownisaking thecrownisaperson.
Let us look carefully at this set of assertions. Since in our model King John is the only
king the second sentence asserts that he is a person as we would hope. But what about
the other four sentences which appear to make claims about legs and crowns? Is that part
of the meaning of All kings are persons? In fact the other four assertions are true in the
model but make no claim whatsoever about the personhood qualifications of legs crowns
orindeed Richard. Thisisbecause noneofthese objects isaking. Lookingatthetruthtable
for Figure 7.8 on page 246 we see that the implication is true whenever its premise is
falseregardless ofthetruthoftheconclusion. Thusbyasserting theuniversally quantified
sentence which is equivalent to asserting a whole list of individual implications we end
up asserting the conclusion of the rule just for those objects for whom the premise is true
and saying nothing at all about those individuals for whom the premise is false. Thus the
truth-table definition of turns out to be perfect for writing general rules with universal
quantifiers.
Acommonmistakemadefrequently evenbydiligent readers whohavereadthispara-
graphseveraltimesistouseconjunction instead ofimplication. Thesentence
x Kingx Personx
wouldbeequivalent toasserting
Richardthe Lionheartisaking Richardthe Lionheartisaperson
King Johnisaking King Johnisaperson
Richardsleftlegisaking Richardsleftlegisaperson
andsoon. Obviously thisdoesnotcapturewhatwewant.
Section8.2. Syntaxand Semanticsof First-Order Logic 297
Existentialquantification
Universalquantificationmakesstatementsabouteveryobject. Similarlywecanmakeastate-
ment about some object in the universe without naming it by using anexistential quantifier.
Tosayforexamplethat King Johnhasacrownonhisheadwewrite
x Crownx On Headx John.
xispronounced Thereexistsan xsuchthat...or Forsome x....
Intuitively the sentence x P says that P is true for at least one object x. More
precisely x P is true in a given model if P is true in at least one extended interpretation
thatassignsxtoadomainelement. Thatisatleastoneofthefollowingistrue:
Richardthe Lionheartisacrown Richardthe Lionheartison Johnshead;
King Johnisacrown King Johnison Johnshead;
Richardsleftlegisacrown Richardsleftlegison Johnshead;
Johnsleftlegisacrown Johnsleftlegison Johnshead;
Thecrownisacrownthecrownison Johnshead.
The fifth assertion is true in the model so the original existentially quantified sentence is
true in the model. Notice that by ourdefinition the sentence would also be true in a model
in which King John was wearing two crowns. This is entirely consistent with the original
sentence King Johnhasacrownonhishead. 7
Justasappearstobethenaturalconnectivetousewithisthenaturalconnective
to use with . Using as the main connective with led to an overly strong statement in
theexample inthe previous section; using withusually leads toavery weakstatement
indeed. Considerthefollowingsentence:
x Crownx On Headx John.
On the surface this might look like a reasonable rendition of our sentence. Applying the
semantics weseethatthesentence saysthatatleastoneofthefollowingassertions istrue:
Richardthe Lionheartisacrown Richardthe Lionheart ison Johnshead;
King Johnisacrown King Johnison Johns head;
Richardsleftlegisacrown Richardsleftlegison Johnshead;
andsoon. Nowanimplicationistrueifbothpremiseandconclusionaretrueorifitspremise
is false. So if Richard the Lionheart is not a crown then the first assertion is true and the
existential is satisfied. So an existentially quantified implication sentence is true whenever
anyobjectfailstosatisfythepremise;hencesuchsentences reallydonotsaymuchatall.
Nestedquantifiers
We will often want to express more complex sentences using multiple quantifiers. The sim-
plestcaseiswherethequantifiers areofthesametype. Forexample Brothersaresiblings
canbewrittenas
x y Brotherxy Siblingxy.
Thesamemeaningcanbeexpressedusingequalitystatements.
Consecutive quantifiers of the same type can be written as one quantifier with several vari-
ables. Forexampletosaythatsiblinghood isasymmetricrelationship wecanwrite
xy Siblingxy Siblingyx.
In other cases we will have mixtures. Everybody loves somebody means that for every
person thereissomeonethatpersonloves:
x y Lovesxy.
Ontheotherhandtosay Thereissomeonewhoislovedbyeveryonewewrite
y x Lovesxy.
Theorderofquantification isthereforeveryimportant. Itbecomesclearerifweinsertparen-
theses. xy Lovesxy says that everyone has a particular property namely the prop-
erty that they love someone. On the other hand y x Lovesxy says that someone in
theworldhasaparticularproperty namelythepropertyofbeinglovedbyeverybody.
Someconfusion can arise when two quantifiers are used with the same variable name.
Considerthesentence
x Crownxx Brother Richardx.
Here the x in Brother Richardx is existentially quantified. The rule is that the variable
belongs to the innermost quantifier that mentions it; then it will not be subject to any other
quantification. Another way to think of it is this: x Brother Richardx is a sentence
about Richardthathehasabrother notabout x;soputtingaxoutsideithasnoeffect. It
couldequallywellhavebeenwrittenz Brother Richardz. Becausethiscanbeasource
ofconfusion wewillalwaysusedifferentvariablenameswithnestedquantifiers.
Connectionsbetweenand
Thetwoquantifiers areactually intimately connected witheachother through negation. As-
serting that everyone dislikes parsnips is the same as asserting there does not exist someone
wholikesthemandviceversa:
x Likesx Parsnips isequivalent to x Likesx Parsnips.
Wecangoonestepfurther: Everyonelikesicecreammeansthatthereisnoonewhodoes
notlikeicecream:
x Likesx Ice Cream isequivalent to x Likesx Ice Cream.
Becauseisreallyaconjunctionovertheuniverseofobjectsand isadisjunction itshould
notbesurprising that theyobey De Morgans rules. The De Morgan rules forquantified and
unquantified sentences areasfollows:
x P x P P Q P Q
x P x P P Q P Q
x P x P P Q P Q
x P x P P Q P Q.
Thus we do not really need both and just as wedo not really need both and . Still
readability ismoreimportant thanparsimony sowewillkeepbothofthequantifiers.
Section8.2. Syntaxand Semanticsof First-Order Logic 299
First-order logic includes onemorewaytomakeatomic sentences otherthan using apredi-
cateandtermsasdescribedearlier. Wecanusethe equalitysymboltosignifythattwoterms
E QU AL IT YS YM BO L
refertothesameobject. Forexample
Father John Henry
saysthattheobjectreferred toby Father Johnandtheobjectreferred toby Henry arethe
same. Because an interpretation fixes the referent of any term determining the truth of an
equalitysentenceissimplyamatterofseeingthatthereferentsofthetwotermsarethesame
object.
Theequalitysymbolcanbeusedtostatefactsaboutagivenfunctionaswejustdidfor
the Father symbol. Itcanalsobeusedwithnegationtoinsistthattwotermsarenotthesame
object. Tosaythat Richardhasatleasttwobrothers wewouldwrite
xy Brotherx Richard Brothery Richardxy.
Thesentence
xy Brotherx Richard Brothery Richard
doesnothavetheintendedmeaning. Inparticular itistrue inthemodelof Figure8.2where
Richardhasonlyonebrother. Toseethisconsidertheextended interpretation inwhichboth
x and y are assigned to King John. The addition of xy rules out such models. The
notation x cid:7 y issometimesusedasanabbreviation forxy.
Continuing the example from the previous section suppose that webelieve that Richard has
twobrothers Johnand Geoffrey.8 Canwecapturethisstateofaffairsbyasserting
Brother John Richard Brother Geoffrey Richard? 8.3
Not quite. First this assertion is true in a model where Richard has only one brother
we need to add John cid:7 Geoffrey. Second the sentence doesnt rule out models in which
Richard has manymore brothers besides John and Geoffrey. Thus thecorrect translation of
Richards brothersare Johnand Geoffreyisasfollows:
Brother John Richard Brother Geoffrey Richard John cid:7 Geoffrey
x Brotherx Richard x John x Geoffrey.
For many purposes this seems much more cumbersome than the corresponding natural-
language expression. As a consequence humans may make mistakes in translating their
knowledge into first-order logic resulting in unintuitive behaviors from logical reasoning
systems that use the knowledge. Can we devise a semantics that allows a more straightfor-
wardlogicalexpression?
Oneproposalthatisverypopularindatabasesystemsworksasfollows. Firstweinsist
that every constant symbol refer to a distinct objectthe so-called unique-names assump-
U NI QU E-N AM ES tion. Second we assume that atomic sentences not known to be true are in fact falsethe
A SS UM PT IO N
C LO SE D-W OR LD closed-world assumption. Finally we invoke domain closure meaning that each model
A SS UM PT IO N
D OM AI NC LO SU RE 8 Actuallyhehadfourtheothersbeing Williamand Henry.
R J R J R J R J R J
R R R R . . . R
J J J J J
Figure8.5 Somemembersofthesetofallmodelsforalanguagewithtwoconstantsym-
bols Rand Jandonebinaryrelationsymbolunderdatabasesemantics. Theinterpretation
oftheconstantsymbolsisfixedandthereisadistinctobjectforeachconstantsymbol.
contains no more domain elements than those named by the constant symbols. Under the
D AT AB AS E resulting semantics which we call database semantics to distinguish it from the standard
S EM AN TI CS
semantics of first-order logic the sentence Equation 8.3 does indeed state that Richards
two brothers are John and Geoffrey. Database semantics is also used in logic programming
systemsasexplained in Section9.4.5.
It is instructive to consider the set of all possible models under database semantics for
the same case as shown in Figure 8.4. Figure 8.5 shows some of the models ranging from
the model with no tuples satisfying the relation to the model with all tuples satisfying the
relation. With two objects there are four possible two-element tuples so there are 2416
different subsets oftuples that cansatisfy therelation. Thus there are16possible modelsin
allalotfewerthantheinfinitelymanymodelsforthestandardfirst-ordersemantics. Onthe
otherhandthedatabasesemantics requiresdefiniteknowledgeofwhattheworldcontains.
This example brings up an important point: there is no one correct semantics for
logic. The usefulness of any proposed semantics depends on how concise and intuitive it
makes the expression of the kinds of knowledge we want to write down and on how easy
andnatural itistodevelop thecorresponding rules ofinference. Database semantics ismost
useful when we are certain about the identity of all the objects described in the knowledge
base and whenwehave allthe facts athand; inother cases itisquite awkward. Forthe rest
ofthischapterweassumethestandardsemanticswhilenotinginstancesinwhichthischoice
leadstocumbersomeexpressions.
Nowthatwehavedefinedanexpressivelogicallanguageitistimetolearnhowtouseit. The
bestwaytodothisisthroughexamples. Wehaveseensomesimplesentencesillustrating the
variousaspectsoflogicalsyntax;inthissection weprovidemoresystematicrepresentations
of some simple domains. In knowledge representation a domain is just some part of the
D OM AI N
worldaboutwhichwewishtoexpresssomeknowledge.
Webegin withabriefdescription ofthe T EL LA SK interface forfirst-orderknowledge
bases. Then we look at the domains of family relationships numbers sets and lists and at
Section8.3. Using First-Order Logic 301
thewumpusworld. Thenextsectioncontainsamoresubstantialexampleelectroniccircuits
and Chapter12coverseverything intheuniverse.
Sentencesareaddedtoaknowledgebaseusing T EL Lexactlyasinpropositional logic. Such
sentences are called assertions. Forexample wecan assert that John is aking Richard is a
A SS ER TI ON
person andallkingsarepersons:
T EL LK B King John.
T EL LK B Person Richard.
T EL LK B x Kingx Personx.
Wecanaskquestions oftheknowledgebaseusing A SK. Forexample
A SK KB King John
Q UE RY
returnstrue. Questionsaskedwith A SKarecalledqueriesorgoals. Generallyspeakingany
query thatislogically entailed bytheknowledge baseshould beanswered affirmatively. For
G OA L
examplegiventhetwopreceding assertions thequery
A SK KB Person John
shouldalsoreturn true. Wecanaskquantifiedqueries suchas
A SK KB x Personx.
The answer is true but this is perhaps not as helpful as we would like. It is rather like
answering Can you tell me the time? with Yes. If we want to know what value of x
makesthesentence truewewillneedadifferentfunction A SK VA RSwhichwecallwith
A SK VA RS KB Personx
andwhichyieldsastream ofanswers. Inthiscasetherewillbetwoanswers: x Johnand
S UB ST IT UT IO N
x Richard. Suchanansweriscalledasubstitutionorbindinglist. A SK VA RS isusually
reserved forknowledge bases consisting solely of Horn clauses because in such knowledge
B IN DI NG LI ST
bases every way of making the query true will bind the variables to specific values. That is
notthecasewithfirst-orderlogic; if K B hasbeentold King John King Richardthen
thereisnobinding toxforthequery x Kingxeventhoughthequeryistrue.
Thefirstexampleweconsideristhedomainoffamilyrelationships orkinship. Thisdomain
includes facts such as Elizabeth is the mother of Charles and Charles is the father of
Williamandrulessuchas Onesgrandmotheristhemotherofonesparent.
Clearlytheobjectsinourdomainarepeople. Wehavetwounarypredicates Male and
Female. Kinship relationsparenthood brotherhood marriage andsoonarerepresented
by binary predicates: Parent Sibling Brother Sister Child Daughter Son Spouse
Wife Husband Grandparent Grandchild Cousin Aunt and Uncle. We use functions
for Mother and Father because every person has exactly one of each of these at least
according tonaturesdesign.
We can go through each function and predicate writing down what we know in terms
oftheothersymbols. Forexample onesmotherisonesfemaleparent:
mc Mothercm Femalem Parentmc.
Oneshusband isonesmalespouse:
wh Husbandhw Maleh Spousehw.
Maleandfemalearedisjoint categories:
x Malex Femalex.
Parentandchildareinverserelations:
pc Parentpc Childcp.
Agrandparent isaparentofonesparent:
gc Grandparentgc p Parentgp Parentpc.
Asibling isanotherchildofonesparents:
xy Siblingxy x cid:7 yp Parentpx Parentpy.
Wecouldgoonforseveralmorepageslikethisand Exercise8.14asksyoutodojustthat.
Eachofthesesentencescanbeviewedasanaxiomofthekinshipdomainasexplained
in Section 7.1. Axioms are commonly associated with purely mathematical domainswe
willseesomeaxiomsfornumbersshortlybuttheyareneeded inalldomains. Theyprovide
the basic factual information from which useful conclusions can be derived. Our kinship
axioms are also definitions; they have theform xy Pxy .... Theaxioms define
D EF IN IT IO N
the Mother functionandthe Husband Male Parent Grandparentand Sibling predicates
intermsofotherpredicates. Ourdefinitions bottom outat abasicsetofpredicates Child
Spouse and Female in terms of which the others are ultimately defined. This is a natural
way in which to build up the representation of a domain and it is analogous to the way in
whichsoftwarepackages arebuiltupbysuccessive definitions ofsubroutines fromprimitive
library functions. Notice that there is not necessarily a unique set of primitive predicates;
wecould equally wellhave used Parent Spouseand Male. Insomedomains asweshow
thereisnoclearly identifiablebasicset.
Notalllogicalsentencesaboutadomainareaxioms. Somearetheoremsthatisthey
T HE OR EM
areentailedbytheaxioms. Forexampleconsidertheassertionthatsiblinghoodissymmetric:
xy Siblingxy Siblingyx.
Is this an axiom ora theorem? In fact it is a theorem that follows logically from the axiom
thatdefinessiblinghood. Ifwe A SK theknowledge basethissentence itshouldreturn true.
From a purely logical point of view a knowledge base need contain only axioms and
no theorems because the theorems do not increase the set of conclusions that follow from
the knowledge base. From a practical point of view theorems are essential to reduce the
computational costofderiving newsentences. Without themareasoning system hastostart
fromfirstprincipleseverytimeratherlikeaphysicisthavingtorederivetherulesofcalculus
foreverynewproblem.
Section8.3. Using First-Order Logic 303
Not all axioms are definitions. Some provide more general information about certain
predicates without constituting a definition. Indeed some predicates have no complete defi-
nition because we do not know enough to characterize them fully. For example there is no
obviousdefinitivewaytocompletethesentence
x Personx ...
Fortunately first-order logic allows us to make use of the Person predicate without com-
pletelydefiningit. Instead wecanwritepartialspecifications ofproperties thateveryperson
hasandproperties thatmakesomething aperson:
x Personx ...
x ... Personx.
Axioms can also be just plain facts such as Male Jim and Spouse Jim Laura.
Such facts form the descriptions of specific problem instances enabling specific questions
to be answered. The answers to these questions will then be theorems that follow from
the axioms. Often one finds that the expected answers are not forthcomingfor example
from Spouse Jim Lauraoneexpectsunderthelawsofmanycountriestobeabletoinfer
Spouse George Laura;butthisdoesnotfollowfromtheaxiomsgivenearlierevenafter
weadd Jim cid:7 George assuggested in Section8.2.8. Thisisasignthatanaxiomismissing.
Exercise8.8asksthereadertosupplyit.
Numbers are perhaps the most vivid example of how a large theory can be built up from
a tiny kernel of axioms. We describe here the theory of natural numbers or non-negative
N AT UR AL NU MB ER S
integers. We need a predicate Nat Num that will be true of natural numbers; we need one
constant symbol 0; and we need one function symbol S successor. The Peano axioms
P EA NO AX IO MS
definenaturalnumbersandaddition.9 Naturalnumbersaredefinedrecursively:
Nat Num0.
n Nat Numn Nat Num Sn.
That is 0 is a natural number and for every object n if n is a natural number then Sn is
a natural number. So the natural numbers are 0 S0 S S0 and so on. After reading
Section 8.2.8 you will notice that these axioms allow for other natural numbers besides the
usualones;see Exercise8.12. Wealsoneedaxiomstoconstrain thesuccessorfunction:
n 0 cid:7 Sn.
mn m cid:7 n Smcid:7 Sn.
Nowwecandefineadditionintermsofthesuccessorfunction:
m Nat Numm 0m m.
mn Nat Numm Nat Numn Smn Smn.
The first of these axioms says that adding 0 to any natural number m gives m itself. Notice
theuseofthebinaryfunctionsymbolinthetermm0;inordinary mathematics the
termwouldbewritten m0usinginfixnotation. Thenotationwehaveusedforfirst-order
I NF IX
thanoffirst-orderlogic.Theimportanceofthisdistinctionisexplainedin Chapter9.
logiciscalledprefix. Tomakeoursentencesaboutnumberseasiertoreadweallowtheuse
P RE FI X
ofinfixnotation. Wecanalsowrite Snasn1sothesecond axiombecomes
mn Nat Numm Nat Numn m1n mn1.
Thisaxiomreduces additiontorepeated application ofthesuccessorfunction.
The use of infix notation is an example of syntactic sugar that is an extension to or
S YN TA CT IC SU GA R
abbreviation of the standard syntax that does not change the semantics. Any sentence that
usessugarcanbedesugaredtoproduceanequivalentsentenceinordinaryfirst-orderlogic.
Once we have addition it is straightforward to define multiplication as repeated addi-
tion exponentiation asrepeated multiplication integer division and remainders primenum-
bers and so on. Thus the whole of numbertheory including cryptography can be built up
fromoneconstant onefunction onepredicate andfouraxioms.
The domain of sets is also fundamental to mathematics as well as to commonsense
S ET
reasoning. Infact itispossible todefinenumbertheory in termsofsettheory. Wewantto
beable torepresent individual sets including the empty set. Weneed awaytobuild up sets
by adding an element to a set or taking the union or intersection of two sets. We will want
to know whether an element is a member of a set and we will want to distinguish sets from
objectsthatarenotsets.
Wewilluse the normal vocabulary of set theory as syntactic sugar. The empty set is a
constant written as . There is one unary predicate Set which is true of sets. The binary
predicates are xs x is a memberof set sand s s set s is a subset not necessarily
proper of set s . The binary functions are s s the intersection of two sets s s
the union of two sets and xs the set resulting from adjoining element x to set s. One
possible setofaxiomsisasfollows:
1. Theonlysetsaretheemptysetandthosemadebyadjoining something toaset:
s Sets sxs Sets sxs .
2. The empty set has no elements adjoined into it. In other words there is no way to
decomposeintoasmallersetandanelement:
xs xs.
3. Adjoininganelementalreadyinthesethasnoeffect:
xs xs sxs.
4. The only members of a set are the elements that were adjoined into it. We express
this recursively saying that x is a member of s if and only if s is equal to some set s
2
adjoinedwithsomeelementywhereeither y isthesameasxorxisamemberofs :
2
xs xs ys sys xyxs .
5. Asetisasubset ofanothersetifandonly ifallofthefirstsets membersaremembers
ofthesecondset:
s s s s x xs xs .
6. Twosetsareequalifandonlyifeachisasubsetoftheother:
s s s s s s s s .
Section8.3. Using First-Order Logic 305
7. Anobjectisintheintersection oftwosetsifandonlyifitisamemberofbothsets:
xs s xs s xs xs .
8. Anobjectisintheunionoftwosetsifandonlyifitisamemberofeitherset:
xs s xs s xs xs .
Lists are similar to sets. The differences are that lists are ordered and the same element can
L IS T
appearmorethanonceinalist. Wecanusethevocabularyof Lispforlists: Nil istheconstant
list with no elements; Cons Append First and Rest are functions; and Find is the pred-
icate that does for lists what Member does for sets. List? is a predicate that is true only of
lists. Aswithsetsitiscommontousesyntacticsugarinlogicalsentencesinvolvinglists. The
empty list is. Theterm Consxy where y isanonempty list iswritten xy. Theterm
Consx Nil i.e. the list containing the element x is written as x. A list of several ele-
mentssuchas A BCcorrespondstothenestedterm Cons A Cons B Cons C Nil.
Exercise8.16asksyoutowriteouttheaxiomsforlists.
Some propositional logic axioms for the wumpus world were given in Chapter 7. The first-
orderaxioms inthis section aremuch moreconcise capturing inanatural wayexactly what
wewanttosay.
Recall that the wumpus agent receives a percept vector with fiveelements. The corre-
spondingfirst-ordersentencestoredintheknowledgebasemustincludeboththeperceptand
thetimeatwhichitoccurred; otherwisetheagentwillgetconfused aboutwhenitsawwhat.
Weuseintegersfortimesteps. Atypicalperceptsentence wouldbe
Percept Stench Breeze Glitter None None5.
Here Percept isabinary predicate and Stench andsoonareconstants placedinalist. The
actionsinthewumpusworldcanberepresented bylogicalterms:
Turn Right Turn Left Forward Shoot Grab Climb .
Todetermine whichisbesttheagentprogram executesthequery
A SK VA RSa Best Actiona5
which returns a binding list such as a Grab. The agent program can then return Grab as
the action to take. The raw percept data implies certain facts about the current state. For
example:
tsgmc Percepts Breezegmct Breezet
tsbmc Perceptsb Glittermct Glittert
andsoon. Theserulesexhibitatrivialformofthereasoning processcalledperceptionwhich
westudyindepthin Chapter24. Noticethequantificationovertimet. Inpropositionallogic
wewouldneedcopiesofeachsentence foreachtimestep.
Simplereflexbehaviorcanalsobeimplemented byquantifiedimplication sentences.
Forexamplewehave
t Glittert Best Action Grabt.
Giventheperceptandrulesfromthepreceding paragraphs thiswouldyieldthedesiredcon-
clusion Best Action Grab5that is Grab istherightthingtodo.
We have represented the agents inputs and outputs; now it is time to represent the
environment itself. Let us begin with objects. Obvious candidates are squares pits and the
wumpus. Wecouldnameeachsquare Square andsoonbutthenthefactthat Square
12 12
and Square are adjacent would have to be an extra fact and we would need one such
13
factforeach pairofsquares. Itisbettertouseacomplex term inwhich therowandcolumn
appearasintegers; forexamplewecansimplyusethelistterm 12. Adjacencyofanytwo
squarescanbedefinedas
xyab Adjacentxyab
x ay b1y b1y bx a1x a1.
We could name each pit but this would be inappropriate for a different reason: there is no
reason to distinguish among pits.10 It is simpler to use a unary predicate Pit that is true of
squares containing pits. Finally since there is exactly one wumpus a constant Wumpus is
justasgoodasaunarypredicateandperhapsmoredignifiedfromthewumpussviewpoint.
The agents location changes over time so we write At Agentst to mean that the
agentisatsquaresattimet. Wecanfixthewumpusslocationwitht At Wumpus22t.
Wecanthensaythatobjectscanonlybeatonelocation atatime:
xs s t Atxs t Atxs t s s .
Given its current location the agent can infer properties of the square from properties of its
current percept. For example if the agent is at a square and perceives a breeze then that
squareisbreezy:
st At Agentst Breezet Breezys.
Itisusefultoknowthatasquareisbreezybecauseweknowthatthepitscannotmoveabout.
Noticethat Breezy hasnotimeargument.
Havingdiscovered whichplacesarebreezyorsmellyandveryimportant notbreezy
ornotsmellytheagentcandeducewherethepitsareandwherethewumpusis. Whereas
propositionallogicnecessitatesaseparateaxiomforeachsquaresee R and R onpage247
andwouldneedadifferentsetofaxiomsforeachgeographicallayoutoftheworldfirst-order
logicjustneedsoneaxiom:
s Breezys r Adjacentrs Pitr. 8.4
Similarly in first-order logic wecan quantify overtime so weneed just one successor-state
axiom for each predicate rather than a different copy for each time step. For example the
axiomforthearrow Equation7.2onpage267becomes
t Have Arrowt1 Have Arrowt Action Shoott.
From these two example sentences we can see that the first-order logic formulation is no
less concise than the original English-language description given in Chapter 7. The reader
ornithologistwishingtostudymigrationpatternssurvivalratesandsoondoesnameeachbirdbymeansofa
ringonitslegbecauseindividualbirdsmustbetracked.
Section8.4. Knowledge Engineering in First-Order Logic 307
is invited to construct analogous axioms for the agents location and orientation; in these
cases the axioms quantify over both space and time. As in the case of propositional state
estimationanagentcanuselogicalinferencewithaxiomsofthiskindtokeeptrackofaspects
oftheworldthatarenotdirectlyobserved. Chapter10goesintomoredepthonthesubjectof
first-ordersuccessor-state axiomsandtheirusesforconstructing plans.
The preceding section illustrated the use of first-order logic to represent knowledge in three
simpledomains. Thissectiondescribesthegeneralprocessofknowledge-baseconstruction
K NO WL ED GE aprocesscalledknowledgeengineering. Aknowledgeengineerissomeonewhoinvestigates
E NG IN EE RI NG
aparticular domain learns whatconcepts areimportant inthatdomain andcreates aformal
representation of the objects and relations in the domain. We illustrate the knowledge engi-
neering process in anelectronic circuit domain that should already befairly familiar so that
wecanconcentrate ontherepresentational issues involved. Theapproach wetakeissuitable
for developing special-purpose knowledge bases whose domain is carefully circumscribed
and whose range of queries is known in advance. General-purpose knowledge bases which
cover a broad range of human knowledge and are intended to support tasks such as natural
language understanding arediscussed in Chapter12.
Knowledge engineering projects vary widely in content scope and difficulty but all such
projects includethefollowingsteps:
1. Identify the task. The knowledge engineer must delineate the range of questions that
the knowledge base will support and the kinds of facts that will be available for each
specific problem instance. Forexample does the wumpus knowledge base need to be
able to choose actions or is it required to answer questions only about the contents
of the environment? Will the sensor facts include the current location? The task will
determinewhatknowledgemustberepresentedinordertoconnectprobleminstancesto
answers. Thisstepisanalogoustothe P EA Sprocessfordesigning agentsin Chapter2.
2. Assembletherelevant knowledge. Theknowledge engineer might already beanexpert
in the domain or might need to work with real experts to extract what they knowa
K NO WL ED GE process called knowledgeacquisition. Atthisstage theknowledge isnotrepresented
A CQ UI SI TI ON
formally. Theideaistounderstand thescope oftheknowledge base asdetermined by
thetaskandtounderstand howthedomainactuallyworks.
For the wumpus world which is defined by an artificial set of rules the relevant
knowledge is easy to identify. Notice however that the definition of adjacency was
not supplied explicitly in the wumpus-world rules. For real domains the issue of
relevance can be quite difficultfor example a system for simulating V LS I designs
mightormightnotneedtotakeintoaccountstraycapacitances andskineffects.
3. Decide on a vocabulary of predicates functions and constants. That is translate the
important domain-level concepts intologic-level names. Thisinvolves manyquestions
of knowledge-engineering style. Like programming style this can have a significant
impact on the eventual success of the project. Forexample should pits be represented
by objects or by a unary predicate on squares? Should the agents orientation be a
function or a predicate? Should the wumpuss location depend on time? Once the
choices have been made the result is a vocabulary that is known as the ontology of
O NT OL OG Y
the domain. The word ontology means a particular theory of the nature of being or
existence. Theontology determines what kinds of things exist but does not determine
theirspecificproperties andinterrelationships.
4. Encode general knowledge about the domain. The knowledge engineer writes down
the axioms for all the vocabulary terms. This pins down to the extent possible the
meaningoftheterms enabling theexpert tocheck thecontent. Oftenthisstepreveals
misconceptions orgaps in the vocabulary that mustbe fixedby returning tostep 3 and
iteratingthrough theprocess.
5. Encode a description of the specific problem instance. If the ontology is well thought
out this step will be easy. It will involve writing simple atomic sentences about in-
stances of concepts that are already part of the ontology. Fora logical agent problem
instancesaresuppliedbythesensorswhereasadisembodied knowledgebaseissup-
plied with additional sentences in the same way that traditional programs are supplied
withinputdata.
6. Pose queries to the inference procedure and get answers. This is where the reward is:
wecanlettheinference procedure operate ontheaxiomsandproblem-specific factsto
derive the facts we are interested in knowing. Thus we avoid the need for writing an
application-specific solution algorithm.
7. Debug the knowledge base. Alas the answers to queries will seldom be correct on
the first try. More precisely the answers will be correct for the knowledge base as
written assuming that the inference procedure is sound but they will not be the ones
thattheuserisexpecting. Forexampleifanaxiomismissingsomequerieswillnotbe
answerable from the knowledge base. A considerable debugging process could ensue.
Missingaxiomsoraxiomsthataretooweakcanbeeasily identifiedbynoticing places
where the chain of reasoning stops unexpectedly. For example if the knowledge base
includesadiagnostic rulesee Exercise8.13forfindingthewumpus
s Smellys Adjacent Home Wumpuss
instead of the biconditional then the agent will never be able to prove the absence of
wumpuses. Incorrect axioms can be identified because they are false statements about
theworld. Forexamplethesentence
x Num Of Legsx4 Mammalx
is false for reptiles amphibians and more importantly tables. The falsehood of this
sentencecanbedeterminedindependentlyoftherestoftheknowledgebase. Incontrast
Section8.4. Knowledge Engineering in First-Order Logic 309
atypicalerrorinaprogramlookslikethis:
offset position 1.
Itisimpossibletotellwhetherthisstatementiscorrectwithoutlookingattherestofthe
program to see whether for example offsetis used to refer to the current position
or to one beyond the current position or whether the value of positionis changed
byanotherstatementandsooffsetshouldalsobechanged again.
To understand this seven-step process better we now apply it to an extended examplethe
domainofelectronic circuits.
Wewilldevelopanontologyandknowledgebasethatallowustoreasonaboutdigitalcircuits
ofthekindshownin Figure8.6. Wefollowtheseven-stepprocessforknowledgeengineering.
Identifythetask
There are many reasoning tasks associated with digital circuits. At the highest level one
analyzes the circuits functionality. For example does the circuit in Figure 8.6 actually add
properly? If all the inputs are high what is the output of gate A2? Questions about the
circuits structure are also interesting. For example what are all the gates connected to the
firstinput terminal? Doesthe circuit contain feedback loops? Thesewillbeourtasks inthis
section. There are more detailed levels of analysis including those related to timing delays
circuit area power consumption production cost and so on. Each of these levels would
requireadditional knowledge.
Assembletherelevantknowledge
What do we know about digital circuits? Forour purposes they are composed of wires and
gates. Signals flow along wires to the input terminals of gates and each gate produces a
C
1
1
2
A O 1 2
1
Figure8.6 Adigitalcircuit C1purportingtobeaone-bitfulladder. Thefirsttwoinputs
arethetwobitstobeaddedandthethirdinputisacarrybit. Thefirstoutputisthesumand
thesecondoutputisacarrybitforthenextadder. Thecircuitcontainstwo X ORgatestwo
A NDgatesandone O Rgate.
signal on theoutput terminal thatflowsalong another wire. Todetermine whatthese signals
will be we need to know how the gates transform their input signals. There are four types
ofgates: A ND ORand X ORgateshave twoinput terminals and N OTgates have one. All
gateshaveoneoutputterminal. Circuits likegateshaveinputandoutput terminals.
Toreason about functionality and connectivity we do not need to talk about the wires
themselves the paths they take or the junctions where they come together. All that matters
is the connections between terminalswe can say that one output terminal is connected to
anotherinputterminalwithouthavingtosaywhatactuallyconnectsthem. Otherfactorssuch
asthesizeshape colororcostofthevariouscomponents areirrelevant toouranalysis.
Ifourpurpose weresomething otherthan verifying designs atthegatelevel theontol-
ogywouldbedifferent. Forexampleifwewereinterested in debugging faultycircuits then
it would probably be a good idea to include the wires in the ontology because a faulty wire
cancorruptthesignalflowingalongit. Forresolvingtiming faultswewouldneedtoinclude
gate delays. If we were interested in designing a product that would be profitable then the
costofthecircuitanditsspeedrelativetootherproductsonthemarketwouldbeimportant.
Decideonavocabulary
Wenowknowthatwewanttotalkaboutcircuitsterminalssignalsandgates. Thenextstep
istochoose functions predicates and constants torepresent them. First weneed tobeable
to distinguish gates from each other and from other objects. Each gate is represented as an
objectnamedbyaconstant aboutwhichweassertthatitisagatewithsay Gate X . The
1
behavior of each gate is determined by its type: one of the constants A ND OR X OR or
N OT. Because a gate has exactly one type a function is appropriate: Type X X OR.
1
Circuitslikegatesareidentifiedbyapredicate: Circuit C .
1
Nextweconsiderterminalswhichareidentifiedbythepredicate Terminalx. Agate
orcircuitcanhaveoneormoreinputterminalsandoneormore outputterminals. Weusethe
function In1 X to denote the first input terminal for gate X . A similar function Out is
usedforoutputterminals. Thefunction Aritycijsaysthatcircuitchasiinputandj out-
putterminals. Theconnectivitybetweengatescanberepresentedbyapredicate Connected
whichtakestwoterminalsasarguments asin Connected Out1 X In1 X .
Finallyweneedtoknowwhetherasignalisonoroff. Onepossibility istouseaunary
predicate Ont which is true when the signal at a terminal is on. This makes it a little
difficult however to pose questions such as What are all the possible values of the signals
attheoutputterminalsofcircuit C ? Wethereforeintroduce asobjectstwosignalvalues 1
1
and0andafunction Signaltthatdenotes thesignalvaluefortheterminal t.
Encodegeneralknowledgeofthedomain
Onesignthatwehaveagoodontology isthatwerequireonlyafewgeneralruleswhichcan
bestatedclearlyandconcisely. Thesearealltheaxiomswewillneed:
1. Iftwoterminalsareconnected thentheyhavethesamesignal:
t t Terminalt Terminalt Connectedt t
Signalt Signalt .
Section8.4. Knowledge Engineering in First-Order Logic 311
2. Thesignalateveryterminaliseither1or0:
t Terminalt Signalt1 Signalt0 .
3. Connectediscommutative:
t t Connectedt t Connectedt t .
4. Therearefourtypesofgates:
g Gategk Typeg k A ND k O Rk X ORk N OT .
5. An A NDgatesoutputis0ifandonlyifanyofitsinputsis0:
g Gateg Typeg A ND
Signal Out1g0 n Signal Inng0.
6. An O Rgatesoutputis1ifandonlyifanyofitsinputsis1:
g Gateg Typeg O R
Signal Out1g1 n Signal Inng1.
7. An X ORgatesoutputis1ifandonlyifitsinputsaredifferent:
g Gateg Typeg X OR
Signal Out1g1 Signal In1g cid:7 Signal In2g.
8. A NO Tgatesoutputisdifferentfromitsinput:
g Gateg Typeg N OT
Signal Out1g cid:7 Signal In1g .
9. Thegatesexceptfor N OThavetwoinputsandoneoutput.
g Gateg Typeg N OT Arityg11 .
g Gategk Typegk A ND k O Rk X OR
Arityg21
10. Acircuithasterminals uptoitsinputandoutputarity andnothing beyonditsarity:
cij Circuitc Aritycij
n n i Terminal Incnn i Incn Nothing
n n j Terminal Outcnn j Outcn Nothing
11. Gatesterminals signals gatetypesand Nothing arealldistinct.
gt Gateg Terminalt
g cid:7 t cid:7 1 cid:7 0 cid:7 OR cid:7 A ND cid:7 X OR cid:7 N OT cid:7 Nothing .
12. Gatesarecircuits.
g Gateg Circuitg
Encodethespecificprobleminstance
Thecircuitshownin Figure8.6isencodedascircuit C withthefollowingdescription. First
1
wecategorize thecircuitanditscomponent gates:
Circuit C Arity C 32
Gate X Type X X OR
Gate X Type X X OR
Gate A Type A A ND
Gate A Type A A ND
Gate O Type O O R .
Thenweshowtheconnections betweenthem:
Connected Out1 X In1 X Connected In1 C In1 X
Connected Out1 X In2 A Connected In1 C In1 A
Connected Out1 A In1 O Connected In2 C In2 X
Connected Out1 A In2 O Connected In2 C In2 A
Connected Out1 X Out1 C Connected In3 C In2 X
Connected Out1 O Out2 C Connected In3 C In1 A .
Posequeriestotheinferenceprocedure
Whatcombinations ofinputs wouldcause thefirstoutput of C thesumbittobe0andthe
1
secondoutputof C thecarrybittobe1?
1
i i i Signal In1 C i Signal In2 C i Signal In3 C i
Signal Out1 C 0 Signal Out2 C 1.
The answers are substitutions forthe variables i i and i such that the resulting sentence
isentailed bytheknowledge base. A SK VA RS willgiveusthreesuchsubstitutions:
i 1 i 1 i 0 i 1 i 0 i 1 i 0 i 1 i 1.
Whatarethepossible setsofvaluesofalltheterminalsfortheaddercircuit?
i i i o o Signal In1 C i Signal In2 C i
Signal In3 C i Signal Out1 C o Signal Out2 C o .
This final query will return a complete inputoutput table for the device which can be used
to check that it does in fact add its inputs correctly. This is a simple example of circuit
C IR CU IT verification. We can also use the definition of the circuit to build larger digital systems for
V ER IF IC AT IO N
whichthesamekindofverification procedure canbecarriedout. See Exercise8.26. Many
domainsareamenabletothesamekindofstructuredknowledge-base developmentinwhich
morecomplexconcepts aredefinedontopofsimplerconcepts.
Debugtheknowledgebase
Wecanperturbtheknowledgebaseinvariouswaystoseewhatkindsoferroneousbehaviors
emerge. For example suppose we fail to read Section 8.2.8 and hence forget to assert that
the input cases 000 and 110. Wecan pinpoint the problem byasking forthe outputs ofeach
gate. Forexamplewecanask
i i o Signal In1 C i Signal In2 C i Signal Out1 X
which reveals that nooutputs are knownat X forthe input cases 10 and01. Then welook
1
attheaxiomfor X ORgatesasappliedto X :
1
Signal Out1 X 1 Signal In1 X cid:7 Signal In2 X .
Iftheinputsareknowntobesay1and0thenthisreducesto
Signal Out1 X 1 1 cid:7 0.
1
Nowthe problem is apparent: the system isunable to infer that Signal Out1 X 1 so
1
weneedtotellitthat1 cid:70.
9
I NF ER EN CE I N
F IR ST-O RD ER L OG IC
In which we define effective procedures for answering questions posed in first-
orderlogic.
Chapter7showedhowsoundandcompleteinferencecanbeachievedforpropositionallogic.
In this chapter we extend those results to obtain algorithms that can answer any answer-
ablequestion statedinfirst-orderlogic. Section9.1introduces inference rulesforquantifiers
andshowshowtoreducefirst-orderinference topropositional inference albeitatpotentially
great expense. Section 9.2 describes the idea of unification showing how it can be used
to construct inference rules that work directly with first-order sentences. We then discuss
three major families of first-order inference algorithms. Forward chaining and its applica-
tionstodeductivedatabasesandproductionsystemsarecoveredin Section9.3;backward
chainingandlogic programmingsystems aredeveloped in Section 9.4. Forwardandback-
ward chaining can be very efficient but are applicable only to knowledge bases that can
be expressed as sets of Horn clauses. General first-order sentences require resolution-based
theoremprovingwhichisdescribed in Section9.5.
This section and the next introduce the ideas underlying modern logical inference systems.
Webegin with some simple inference rules that can be applied to sentences with quantifiers
toobtain sentences without quantifiers. These rules lead naturally tothe idea that first-order
inference can be done by converting the knowledge base to propositional logic and using
propositional inference which we already know how to do. The next section points out an
obviousshortcut leadingtoinference methodsthatmanipulate first-ordersentences directly.
Let us begin with universal quantifiers. Suppose our knowledge base contains the standard
folkloric axiomstatingthatallgreedykingsareevil:
x Kingx Greedyx Evilx.
322
Section9.1. Propositional vs.First-Order Inference 323
Thenitseemsquitepermissible toinferanyofthefollowing sentences:
King John Greedy John Evil John
King Richard Greedy Richard Evil Richard
King Father John Greedy Father John Evil Father John.
.
.
.
U NI VE RS AL The rule of Universal Instantiation U I for short says that we can infer any sentence ob-
I NS TA NT IA TI ON
tained by substituting a ground term a term without variables for the variable.1 To write
G RO UN DT ER M
outtheinference ruleformally weusethenotion of substitutionsintroduced in Section8.3.
Let S UB STθαdenote theresult ofapplying thesubstitution θ tothesentence α. Thenthe
ruleiswritten
v α
S UB STvgα
for any variable v and ground term g. For example the three sentences given earlier are
obtained withthesubstitutions x Johnx Richardandx Father John.
E XI ST EN TI AL In the rule for Existential Instantiation the variable is replaced by a single new con-
I NS TA NT IA TI ON
stantsymbol. Theformalstatementisasfollows: foranysentence αvariablevandconstant
symbolk thatdoesnotappearelsewhereintheknowledgebase
v α
.
S UB STvkα
Forexamplefromthesentence
x Crownx On Headx John
wecaninferthesentence
Crown C On Head C John
as long as C does not appear elsewhere in the knowledge base. Basically the existential
1
sentence saysthereissomeobjectsatisfying acondition andapplying theexistential instan-
tiation rule just gives a name to that object. Of course that name must not already belong
toanotherobject. Mathematics provides anice example: suppose wediscoverthatthere isa
numberthatisalittlebiggerthan2.71828andthatsatisfiestheequationdxydyxy forx.
Wecan give this number a name such as e but it would be a mistake to give itthe name of
an existing object such as π. In logic the new name is called a Skolem constant. Existen-
S KO LE MC ON ST AN T
tial Instantiation isaspecial caseofamoregeneral process called skolemization which we
coverin Section9.5.
Whereas Universal Instantiation can be applied many times to produce many different
consequences Existential Instantiation can be applied once and then the existentially quan-
tified sentence can be discarded. Forexample weno longer need x Killx Victimonce
wehaveaddedthesentence Kill Murderer Victim. Strictlyspeaking thenewknowledge
I NF ER EN TI AL baseisnotlogically equivalent totheoldbutitcanbeshowntobeinferentially equivalent
E QU IV AL EN CE
inthesensethatitissatisfiableexactlywhentheoriginal knowledgebaseissatisfiable.
The substitution replaces a variable with a term a piece of syntax to produce a new sentence whereas an
interpretationmapsavariabletoanobjectinthedomain.
Once we have rules for inferring nonquantified sentences from quantified sentences it be-
comes possible to reduce first-order inference to propositional inference. In this section we
givethemainideas;thedetailsaregivenin Section9.5.
The first idea is that just as an existentially quantified sentence can be replaced by
one instantiation a universally quantified sentence can be replaced by the set of all possible
instantiations. Forexample supposeourknowledgebasecontainsjustthesentences
x Kingx Greedyx Evilx
King John
9.1
Greedy John
Brother Richard John.
Then weapply U I to the first sentence using all possible ground-term substitutions from the
vocabulary oftheknowledge baseinthiscase x Johnandx Richard. Weobtain
King John Greedy John Evil John
King Richard Greedy Richard Evil Richard
and we discard the universally quantified sentence. Now the knowledge base is essentially
propositional if we view the ground atomic sentences King John Greedy John and
so onas proposition symbols. Therefore we can apply any of the complete propositional
algorithms in Chapter7toobtainconclusions suchas Evil John.
This technique of propositionalization can be made completely general as we show
in Section 9.5; that is every first-order knowledge base and query can be propositionalized
in such a way that entailment is preserved. Thus we have a complete decision procedure
for entailment ... or perhaps not. There is a problem: when the knowledge base includes
a function symbol the set of possible ground-term substitutions is infinite! For example if
the knowledge base mentions the Father symbol then infinitely many nested terms such as
Father Father Father Johncanbeconstructed. Ourpropositional algorithms willhave
difficultywithaninfinitelylargesetofsentences.
Fortunately there is a famous theorem due to Jacques Herbrand 1930 to the effect
thatifasentence isentailed bytheoriginal first-order knowledge base thenthere isaproof
involving justafinite subset ofthepropositionalized knowledge base. Sinceanysuch subset
has a maximum depth of nesting among its ground terms we can find the subset by first
generatingalltheinstantiations withconstantsymbols Richard and Johnthenalltermsof
depth1 Father Richardand Father Johnthenalltermsofdepth2andsoonuntilwe
areabletoconstruct apropositional proofoftheentailedsentence.
We have sketched an approach to first-order inference via propositionalization that is
completethat is any entailed sentence can be proved. This is a major achievement given
that the space of possible models is infinite. On the other hand we do not know until the
proof isdone that thesentence isentailed! Whathappens whenthesentence isnot entailed?
Canwetell? Well forfirst-order logic itturns outthat wecannot. Ourproof procedure can
goonandon generating moreandmoredeeply nested terms butwewillnotknowwhether
itisstuckinahopeless looporwhethertheproofisjustabout topopout. Thisisverymuch
Section9.2. Unificationand Lifting 325
likethehaltingproblemfor Turingmachines. Alan Turing1936and Alonzo Church1936
bothproved inratherdifferent ways theinevitability of thisstateofaffairs. Thequestion of
entailmentforfirst-orderlogicissemidecidablethatisalgorithmsexistthatsayyestoevery
entailed sentence butnoalgorithm existsthatalsosaysnotoeverynonentailed sentence.
The preceding section described the understanding of first-order inference that existed up
to the early 1960s. The sharp-eyed reader and certainly the computational logicians of the
early1960swillhavenoticedthatthepropositionalization approachisratherinefficient. For
example given the query Evilx and the knowledge base in Equation 9.1 it seems per-
versetogenerate sentences suchas King Richard Greedy Richard Evil Richard.
Indeed theinference of Evil Johnfromthesentences
x Kingx Greedyx Evilx
King John
Greedy John
seems completely obvious to a human being. We now show how to make it completely
obvioustoacomputer.
Theinferencethat Johnisevilthatisthatx Johnsolvesthequery Evilxworkslike
this: to use the rule that greedy kings are evil find some x such that x is a king and x is
greedy and then infer that this x is evil. More generally if there is some substitution θ that
makes each of the conjuncts of the premise of the implication identical to sentences already
intheknowledgebasethenwecanasserttheconclusion oftheimplication afterapplying θ.
Inthiscasethesubstitution θx Johnachievesthataim.
We can actually make the inference step do even more work. Suppose that instead of
knowing Greedy Johnweknowthateveryoneisgreedy:
y Greedyy. 9.2
Then we would still like to be able to conclude that Evil John because we know that
John is a king given and John is greedy because everyone is greedy. What we need for
this to work is to find a substitution both for the variables in the implication sentence and
for the variables in the sentences that are in the knowledge base. In this case applying the
substitutionx Johny Johntotheimplicationpremises Kingxand Greedyxandthe
knowledge-base sentences King John and Greedyy will make them identical. Thus we
caninfertheconclusion oftheimplication.
This inference process can be captured as a single inference rule that we call Gener-
G EN ER AL IZ ED alized Modus Ponens:2 For atomic sentences p p cid:2 and q where there is a substitution θ
M OD US PO NE NS i i
cid:2
suchthat S UB STθp
i
S UB STθp iforalli
p cid:2 p cid:2 ... p cid:2 p p ...p q
.
S UB STθq
cid:2
Therearen1premisestothisrule: thenatomicsentencesp andtheoneimplication. The
i
conclusion istheresultofapplying thesubstitution θ totheconsequent q. Forourexample:
cid:2
p is King John p is Kingx
cid:2
p is Greedyy p is Greedyx
θ isx Johny John q is Evilx
S UB STθqis Evil John.
Itiseasytoshowthat Generalized Modus Ponensisasoundinferencerule. Firstweobserve
that for any sentence p whose variables are assumed to be universally quantified and for
anysubstitution θ
p S UB STθp
holds by Universal Instantiation. It holds in particular for a θ that satisfies the conditions of
cid:2 cid:2
the Generalized Modus Ponensrule. Thusfrom p ...p wecaninfer
S UB STθp 1cid:2 ...S UB STθp ncid:2
andfromtheimplication p ...p q wecaninfer
S UB STθp 1...S UB STθp n S UB STθq.
cid:2
Now θ in Generalized Modus Ponens is defined so that S UB STθp
i
S UB STθp i for
all i; therefore the first of these two sentences matches the premise of the second exactly.
Hence S UB STθqfollowsby Modus Ponens.
Generalized Modus Ponens is a lifted version of Modus Ponensit raises Modus Po-
L IF TI NG
nens from ground variable-free propositional logic to first-order logic. We will see in the
rest of this chapter that we can develop lifted versions of the forward chaining backward
chaining and resolution algorithms introduced in Chapter 7. The key advantage of lifted
inference rules over propositionalization is that they make only those substitutions that are
required toallowparticularinferences toproceed.
Lifted inference rules require finding substitutions that make different logical expressions
look identical. This process is called unification and is a key component of all first-order
U NI FI CA TI ON
U NI FI ER
inference algorithms. The U NI FY algorithm takes two sentences and returns a unifier for
themifoneexists:
U NI FYpqθ where S UB STθp S UB STθq.
Let us look at some examples of how U NI FY should behave. Suppose we have a query
Ask Vars Knows Johnx: whom does John know? Answers to this query can be found
andthepremiseoftheimplicationneedmatchonlyuptoasubstitutionratherthanexactly. Ontheotherhand
Modus Ponensallowsanysentenceαasthepremiseratherthanjustaconjunctionofatomicsentences.
Section9.2. Unificationand Lifting 327
byfindingallsentencesintheknowledgebasethatunifywith Knows Johnx. Herearethe
resultsofunificationwithfourdifferentsentences thatmightbeintheknowledgebase:
U NI FY Knows Johnx Knows John Jane x Jane
U NI FY Knows Johnx Knowsy Bill x Billy John
U NI FY Knows Johnx Knowsy Mothery y Johnx Mother John
U NI FY Knows Johnx Knowsx Elizabeth fail .
The last unification fails because x cannot take on the values John and Elizabeth at the
same time. Now remember that Knowsx Elizabeth means Everyone knows Elizabeth
so we should be able to infer that John knows Elizabeth. The problem arises only because
the two sentences happen to use the same variable name x. The problem can be avoided
S TA ND AR DI ZI NG by standardizing apart one of the two sentences being unified which means renaming its
A PA RT
variables to avoid name clashes. Forexample we can rename xin Knowsx Elizabeth to
x anewvariablenamewithoutchanging itsmeaning. Nowtheunification willwork:
17
U NI FY Knows Johnx Knowsx 17 Elizabeth x Elizabethx 17 John.
Exercise9.12delvesfurtherintotheneedforstandardizing apart.
There is one more complication: we said that U NI FY should return a substitution
that makes the two arguments look the same. But there could be more than one such uni-
fier. For example U NI FY Knows Johnx Knowsyz could return y Johnxz or
y Johnx Johnz John. The first unifier gives Knows Johnz as the result of unifi-
cation whereas thesecond gives Knows John John. Thesecond result could beobtained
from the first by an additional substitution z John; we say that the first unifier is more
generalthanthesecond because itplaces fewerrestrictions onthe valuesofthevariables. It
M OS TG EN ER AL turnsoutthatforeveryunifiablepairofexpressionsthereisasinglemostgeneralunifieror
U NI FI ER
M GUthatisunique uptorenaming andsubstitution ofvariables. Forexample x John
andy Johnareconsideredequivalentasarex Johny Johnandx Johnyx. In
thiscaseitisy Johnxz.
Analgorithm forcomputing mostgeneral unifiers isshownin Figure 9.1. Theprocess
issimple: recursivelyexplorethetwoexpressions simultaneously sidebysidebuilding up
aunifieralongthewaybutfailingiftwocorresponding pointsinthestructuresdonotmatch.
There is one expensive step: when matching a variable against a complex term one must
checkwhetherthevariableitselfoccursinsidetheterm;if itdoesthematchfailsbecauseno
consistent unifiercanbeconstructed. Forexample Sxcant unify with S Sx. Thisso-
called occur checkmakes thecomplexity oftheentire algorithm quadratic inthesize ofthe
O CC UR CH EC K
expressions being unified. Some systems including all logic programming systems simply
omittheoccurcheck andsometimes makeunsound inferences asaresult; othersystems use
morecomplexalgorithms withlinear-time complexity.
Underlying the T EL L and A SK functions used to inform and interrogate a knowledge base
are the more primitive S TO RE and F ET CH functions. S TO REs stores a sentence s into the
knowledge base and F ET CHq returns all unifiers such that the query q unifies with some
function U NI FYxyθreturnsasubstitutiontomakex andy identical
inputs:xavariableconstantlistorcompoundexpression
yavariableconstantlistorcompoundexpression
θthesubstitutionbuiltupsofaroptionaldefaultstoempty
ifθfailurethenreturnfailure
elseifx y thenreturnθ
elseif V AR IA BL E?xthenreturn U NI FY-V ARxyθ
elseif V AR IA BL E?ythenreturn U NI FY-V ARyxθ
elseif C OM PO UN D?xand C OM PO UN D?ythen
return U NI FYx.A RG Sy.A RG SU NI FYx.O Py.O Pθ
elseif L IS T?xand L IS T?ythen
return U NI FYx.R ES Ty.R ES TU NI FYx.F IR STy.F IR STθ
elsereturnfailure
function U NI FY-V ARvarxθreturnsasubstitution
ifvarval θthenreturn U NI FYvalxθ
elseifxval θthenreturn U NI FYvarvalθ
elseif O CC UR-C HE CK?varxthenreturnfailure
elsereturnaddvarxtoθ
Figure9.1 Theunificationalgorithm. Thealgorithmworksbycomparing thestructures
oftheinputselementbyelement. Thesubstitutionθ thatistheargumentto U NI FY isbuilt
upalongthewayandisusedtomakesurethatlatercomparisonsareconsistentwithbindings
thatwereestablishedearlier. Inacompoundexpressionsuchas F ABthe O Pfieldpicks
outthefunctionsymbol F andthe A RG Sfieldpicksouttheargumentlist A B.
sentence in the knowledge base. The problem we used to illustrate unificationfinding all
factsthatunifywith Knows Johnxisaninstanceof F ET CHing.
The simplest way to implement S TO RE and F ET CH is to keep all the facts in one long
list and unify each query against every element of the list. Such a process is inefficient but
it works and its all you need to understand the rest of the chapter. The remainder of this
sectionoutlines waystomakeretrievalmoreefficient;itcanbeskippedonfirstreading.
We can make F ET CH more efficient by ensuring that unifications are attempted only
with sentences that have some chance of unifying. For example there is no point in trying
tounify Knows Johnxwith Brother Richard John. Wecanavoidsuchunificationsby
indexing the facts in the knowledge base. A simple scheme called predicate indexing puts
I ND EX IN G
P RE DI CA TE all the Knows facts in one bucket and all the Brother facts in another. The buckets can be
I ND EX IN G
storedinahashtableforefficientaccess.
Predicate indexing is useful when there are many predicate symbols but only a few
clauses for each symbol. Sometimes however a predicate has many clauses. For example
suppose that the tax authorities want to keep track of who employs whom using a predi-
cate Employsxy. This would be a very large bucket with perhaps millions of employers
Section9.2. Unificationand Lifting 329
Employsxy Employsxy
Employsx Richard Employs I BMy Employsx John Employsxx Employs Johny
Employs I BM Richard Employs John John
a b
Figure9.2 a Thesubsumptionlatticewhoselowestnodeis Employs I BM Richard.
b Thesubsumptionlatticeforthesentence Employs John John.
and tens of millions of employees. Answering a query such as Employsx Richard with
predicate indexingwouldrequirescanning theentirebucket.
Forthis particular query it would help if facts were indexed both by predicate and by
secondargument perhaps usingacombined hashtablekey. Thenwecouldsimplyconstruct
the key from the query and retrieve exactly those facts that unify with the query. For other
queries such as Employs I BMy we would need to have indexed the facts by combining
the predicate with the first argument. Therefore facts can be stored under multiple index
keysrendering theminstantly accessible tovariousqueriesthattheymightunifywith.
Givenasentence tobestored itispossible toconstruct indices forallpossible queries
thatunifywithit. Forthefact Employs I BM Richardthequeriesare
Employs I BM Richard Does I BMemploy Richard?
Employsx Richard Whoemploys Richard?
Employs I BMy Whomdoes I BMemploy?
Employsxy Whoemployswhom?
S UB SU MP TI ON These queries form a subsumption lattice as shown in Figure 9.2a. The lattice has some
L AT TI CE
interesting properties. For example the child of any node in the lattice is obtained from its
parent by a single substitution; and the highest common descendant of any two nodes is
the result of applying their mostgeneral unifier. Theportion ofthe lattice above anyground
factcanbeconstructedsystematically Exercise9.5. Asentencewithrepeatedconstantshas
a slightly different lattice as shown in Figure 9.2b. Function symbols and variables in the
sentences tobestoredintroduce stillmoreinteresting latticestructures.
The scheme we have described works very well whenever the lattice contains a small
number of nodes. For a predicate with n arguments however the lattice contains O2n
nodes. If function symbols are allowed the number of nodes is also exponential in the size
ofthetermsinthesentencetobestored. Thiscanleadtoahugenumberofindices. Atsome
point the benefits of indexing are outweighed by the costs of storing and maintaining all
the indices. Wecan respond by adopting afixedpolicy such as maintaining indices only on
keyscomposedofapredicatepluseachargumentorbyusinganadaptivepolicythatcreates
indices to meet the demands of the kinds of queries being asked. For most A I systems the
number of facts to be stored is small enough that efficient indexing is considered a solved
problem. For commercial databases where facts number in the billions the problem has
beenthesubjectofintensivestudyandtechnology development..
A forward-chaining algorithm for propositional definite clauses was given in Section 7.5.
The idea is simple: start with the atomic sentences in the knowledge base and apply Modus
Ponens in the forward direction adding new atomic sentences until no further inferences
can be made. Here we explain how the algorithm is applied to first-order definite clauses.
Definiteclausessuchas Situation Response areespeciallyusefulforsystemsthatmake
inferences in response to newlyarrived information. Manysystems can bedefined this way
andforwardchainingcanbeimplementedveryefficiently.
First-order definite clauses closely resemble propositional definite clauses page 256: they
aredisjunctions ofliterals ofwhich exactly oneispositive. Adefiniteclause eitherisatomic
or is an implication whose antecedent is a conjunction of positive literals and whose conse-
quentisasinglepositiveliteral. Thefollowingarefirst-orderdefiniteclauses:
Kingx Greedyx Evilx.
King John.
Greedyy.
Unlike propositional literals first-order literals can include variables in which case those
variables are assumed to be universally quantified. Typically we omit universal quantifiers
when writing definite clauses. Not every knowledge base can be converted into a set of
definite clauses because of the single-positive-literal restriction but many can. Consider the
followingproblem:
The law says thatit is a crime foran American to sell weaponsto hostile nations. The
country Nonoanenemyof Americahassomemissilesandallofitsmissilesweresold
toitby Colonel Westwhois American.
Wewillprovethat Westisacriminal. Firstwewillrepresentthesefactsasfirst-orderdefinite
clauses. Thenextsection showshowtheforward-chaining algorithm solvestheproblem.
...itisacrimeforan Americantosellweaponstohostilenations:
Americanx Weapony Sellsxyz Hostilez Criminalx. 9.3
Nono...hassomemissiles. Thesentencex Owns Nonox Missilexistransformed
intotwodefiniteclausesby Existential Instantiation introducing anewconstant M :
1
Owns Nono M 9.4
1
M. issile M 9.5
1
Allofitsmissilesweresoldtoitby Colonel West:
Missilex Owns Nonox Sells Westx Nono. 9.6
Wewillalsoneedtoknowthatmissilesareweapons:
Missilex Weaponx 9.7
Section9.3. Forward Chaining 331
andwemustknowthatanenemyof Americacountsashostile:
Enemyx America Hostilex. 9.8
Westwhois American...:
American West. 9.9
Thecountry Nonoanenemyof America ...:
Enemy Nono America. 9.10
This knowledge base contains no function symbols and is therefore an instance of the class
of Datalog knowledge bases. Datalog is a language that is restricted to first-order definite
D AT AL OG
clauses withnofunction symbols. Datalog getsitsnamebecause itcanrepresent thetypeof
statements typically made in relational databases. We will see that the absence of function
symbolsmakesinference mucheasier.
Thefirstforward-chaining algorithmweconsiderisasimple oneshownin Figure9.3. Start-
ing from the known facts it triggers all the rules whose premises are satisfied adding their
conclusions to the known facts. The process repeats until the query is answered assuming
that just one answer is required or no new facts are added. Notice that a fact is not new
if it is just a renaming of a known fact. One sentence is a renaming of another if they
R EN AM IN G
are identical except for the names of the variables. For example Likesx Ice Cream and
Likesy Ice Cream arerenamings ofeach otherbecause theydiffer only in thechoice of x
ory;theirmeaningsareidentical: everyonelikesicecream.
We use our crime problem to illustrate how F OL-F C-A SK works. The implication
sentences are9.39.69.7and9.8. Twoiterations arerequired:
Onthefirstiteration rule9.3hasunsatisfiedpremises.
Rule9.6issatisfiedwithx M and Sells West M Nonoisadded.
Rule9.7issatisfiedwithx M and Weapon M isadded.
Rule9.8issatisfiedwithx Nonoand Hostile Nonoisadded.
On the second iteration rule 9.3 is satisfied with x Westy M z Nono and
1
Criminal Westisadded.
at this point because every sentence that could be concluded by forward chaining is already
containedexplicitlyinthe K B.Suchaknowledgebaseiscalledafixedpointoftheinference
process. Fixedpointsreachedbyforwardchainingwithfirst-orderdefiniteclausesaresimilar
tothoseforpropositional forwardchaining page258;the principal difference isthatafirst-
orderfixedpointcaninclude universally quantifiedatomicsentences.
F OL-F C-A SK iseasy to analyze. First itis soundbecause every inference is just an
applicationof Generalized Modus Ponenswhichissound. Seconditiscompletefordefinite
clause knowledge bases; that is it answers every query whose answers are entailed by any
knowledgebaseofdefiniteclauses. For Datalogknowledgebaseswhichcontainnofunction
symbols the proof of completeness is fairly easy. We begin by counting the number of
function F OL-F C-A SK KBαreturnsasubstitutionorfalse
inputs:K Btheknowledgebaseasetoffirst-orderdefiniteclauses
αthequeryanatomicsentence
localvariables: newthenewsentencesinferredoneachiteration
repeatuntilnew isempty
new
foreachrule in K B do
p 1... pn q S TA ND AR DI ZE-V AR IA BL ESrule
foreachθsuchthat S UB STθp
1
... pn S UB STθp 1cid:5 ... p ncid:5
forsomepcid:5...pcid:5 in K B
qcid:5 SU BS Tθq
ifqcid:5 doesnotunifywithsomesentencealreadyin K B ornew then
addqcid:5tonew
φ UN IF Yqcid:5α
ifφisnotfail thenreturnφ
addnew to K B
returnfalse
rithm. Oneachiterationitaddsto K B alltheatomicsentencesthatcanbeinferredinone
step fromthe implicationsentencesandtheatomicsentencesalreadyin K B. Thefunction
S TA ND AR DI ZE-V AR IA BL ES replacesallvariablesinitsargumentswithnewonesthathave
notbeenusedbefore.
Criminal West
Weapon M Sells West M Nono Hostile Nono
American West Missile M Owns Nono M Enemy Nono America
Figure9.4 Theprooftreegeneratedbyforwardchainingonthecrimeexample.Theinitial
factsappearatthe bottomlevel factsinferredon the firstiterationin the middlelevel and
factsinferredontheseconditerationatthetoplevel.
possible factsthatcanbeadded whichdetermines themaximumnumberofiterations. Letk
bethemaximumaritynumberofargumentsofanypredicate pbethenumberofpredicates
and n be the number of constant symbols. Clearly there can be no more than pnk distinct
groundfactssoafterthismanyiterationsthealgorithmmusthavereachedafixedpoint. Then
wecanmakeanargumentverysimilartotheproofofcompletenessforpropositionalforward
Section9.3. Forward Chaining 333
chaining. See page 258. The details of how to make the transition from propositional to
first-ordercompleteness aregivenfortheresolution algorithm in Section9.5.
For general definite clauses with function symbols F OL-F C-A SK can generate in-
finitely many new facts so we need to be more careful. Forthe case in which an answer to
the query sentence q is entailed we must appeal to Herbrands theorem to establish that the
algorithm will find a proof. See Section 9.5 for the resolution case. If the query has no
answer the algorithm could fail to terminate in some cases. For example if the knowledge
baseincludes the Peanoaxioms
Nat Num0
n Nat Numn Nat Num Sn
then forward chaining adds Nat Num S0 Nat Num S S0 Nat Num S SS0
and so on. This problem is unavoidable in general. As with general first-order logic entail-
mentwithdefiniteclauses issemidecidable.
The forward-chaining algorithm in Figure 9.3 is designed for ease of understanding rather
than for efficiency of operation. There are three possible sources of inefficiency. First the
inner loop of the algorithm involves finding all possible unifiers such that the premise of
arule unifies with asuitable set offacts in the knowledge base. This is often called pattern
matching and can be very expensive. Second the algorithm rechecks every rule on every
P AT TE RN MA TC HI NG
iteration to see whether its premises are satisfied even ifvery few additions are made to the
knowledge base on each iteration. Finally the algorithm might generate many facts that are
irrelevant tothegoal. Weaddresseachoftheseissuesinturn.
Matchingrulesagainstknownfacts
Theproblemofmatchingthepremiseofaruleagainstthefactsintheknowledgebasemight
seemsimpleenough. Forexamplesuppose wewanttoapplytherule
Missilex Weaponx.
Thenweneedtofindallthefactsthatunifywith Missilex;inasuitablyindexedknowledge
basethiscanbedoneinconstant timeperfact. Nowconsider arulesuchas
Missilex Owns Nonox Sells Westx Nono.
Againwecanfindalltheobjects ownedby Nonoinconstant timeperobject; then foreach
object wecould check whether it isa missile. If the knowledge base contains manyobjects
ownedby Nonoandveryfewmissileshoweveritwouldbebettertofindallthemissilesfirst
C ON JU NC T and then check whether they are owned by Nono. This is the conjunct ordering problem:
O RD ER IN G
findanorderingtosolvetheconjuncts oftherulepremisesothatthetotalcostisminimized.
It turns out that finding the optimal ordering is N P-hard but good heuristics are available.
Forexample the minimum-remaining-values M RVheuristic used for C SPsin Chapter6
would suggest ordering the conjuncts to look formissiles first if fewer missiles than objects
areownedby Nono.
N T Diffwant Diffwasa
Q
Diffntq Diffntsa
W A
Diffqnsw Diffqsa
Diffnswv Diffnswsa
S A N SW
Diffvsa Colorable
V Diff Red Blue Diff Red Green
Diff Green Red Diff Green Blue
Diff Blue Red Diff Blue Green
T
a b
Figure9.5 a Constraintgraphforcoloringthemapof Australia. b Themap-coloring
C SPexpressedasasingledefiniteclause.Eachmapregionisrepresentedasavariablewhose
valuecanbeoneoftheconstants Red Green or Blue.
The connection between pattern matching and constraint satisfaction is actually very
close. We can view each conjunct as a constraint on the variables that it containsfor ex-
ample Missilex is a unary constraint on x. Extending this idea we can express every
finite-domain C SP as a single definite clause together with some associated ground facts.
Considerthemap-coloringproblemfrom Figure6.1shownagainin Figure9.5a. Anequiv-
alentformulation asasingledefiniteclauseisgivenin Figure9.5b. Clearly theconclusion
Colorablecanbeinferred onlyifthe C SPhasasolution. Because C SPsingeneralinclude
3-S AT problems as special cases wecan conclude that matching a definite clause against a
setoffactsis N P-hard.
Itmightseemratherdepressingthatforwardchaininghasan N P-hardmatchingproblem
initsinnerloop. Therearethreewaystocheerourselvesup:
We can remind ourselves that most rules in real-world knowledge bases are small and
simple like the rules in our crime example rather than large and complex like the
C SP formulation in Figure 9.5. It is common in the database world to assume that
both the sizes of rules and the arities of predicates are bounded by a constant and to
worry only about data complexitythat is the complexity of inference as a function
D AT AC OM PL EX IT Y
of the number of ground facts in the knowledge base. It is easy to show that the data
complexityofforwardchaining ispolynomial.
We can consider subclasses of rules for which matching is efficient. Essentially every
Datalog clause can be viewed as defining a C SP so matching will be tractable just
whenthecorresponding C SPistractable. Chapter6describesseveraltractablefamilies
of C SPs. For example if the constraint graph the graph whose nodes are variables
and whose links are constraints forms a tree then the C SP can be solved in linear
time. Exactlythesameresultholdsforrulematching. Forinstanceifweremove South
Section9.3. Forward Chaining 335
Australiafromthemapin Figure9.5theresultingclauseis
Diffwant Diffntq Diffqnsw Diffnswv Colorable
whichcorresponds tothe reduced C SPshown in Figure 6.12 onpage 224. Algorithms
forsolvingtree-structured C SPscanbeapplieddirectlytotheproblemofrulematching.
We can try to to eliminate redundant rule-matching attempts in the forward-chaining
algorithm asdescribed next.
Incrementalforwardchaining
Whenweshowedhow forward chaining worksonthecrimeexample wecheated; inpartic-
ular we omitted some of the rule matching done by the algorithm shown in Figure 9.3. For
exampleontheseconditeration therule
Missilex Weaponx
matchesagainst Missile M again andofcourse theconclusion Weapon M isalready
known so nothing happens. Such redundant rule matching can be avoided if we make the
following observation: Every new fact inferred on iteration t must be derived from at least
one new fact inferred on iteration t 1. This is true because any inference that does not
requireanewfactfromiteration t1couldhavebeendoneatiteration t1already.
This observation leads naturally to an incremental forward-chaining algorithm where
atiterationtwecheckaruleonlyifitspremiseincludesaconjunct p thatunifieswithafact
i
pcid:2 newlyinferredatiteration t1. Therule-matching stepthenfixesp tomatchwithpcid:2 but
i i i
allows the other conjuncts of the rule to match with facts from any previous iteration. This
algorithmgeneratesexactlythesamefactsateachiterationasthealgorithmin Figure9.3but
ismuchmoreefficient.
With suitable indexing it is easy to identify all the rules that can be triggered by any
givenfactandindeedmanyrealsystemsoperateinanupdatemodewhereinforwardchain-
ing occurs in response to each new fact that is T EL Led to the system. Inferences cascade
through thesetofrulesuntilthefixedpointisreached andthentheprocess beginsagainfor
thenextnewfact.
Typicallyonlyasmallfractionoftherulesintheknowledgebaseareactuallytriggered
by the addition of a given fact. This means that a great deal of redundant work is done in
repeatedly constructing partial matches that have some unsatisfied premises. Our crime ex-
ampleisrathertoosmalltoshowthiseffectivelybutnoticethatapartialmatchisconstructed
onthefirstiterationbetweentherule
Americanx Weapony Sellsxyz Hostilez Criminalx
andthefact American West. Thispartialmatchisthendiscardedandrebuiltonthesecond
iteration when the rule succeeds. It would be better to retain and gradually complete the
partialmatchesasnewfactsarriveratherthandiscarding them.
The rete algorithm3 was the first to address this problem. The algorithm preprocesses
R ET E
the set of rules in the knowledge base to construct a sort of dataflow network in which each
node is a literal from a rule premise. Variable bindings flow through the network and are
filtered out when they fail to match a literal. If two literals in a rule share a variablefor
example Sellsxyz Hostilez in the crime examplethen the bindings from each
literal are filtered through an equality node. A variable binding reaching a node for an n-
ary literal such as Sellsxyz might have towait forbindings forthe other variables tobe
established before the process can continue. At any given point the state of a rete network
captures allthepartialmatchesoftherulesavoiding agreatdealofrecomputation.
Rete networks and various improvements thereon have been a key component of so-
P RO DU CT IO N called production systems which were among the earliest forward-chaining systems in
S YS TE M
widespread use.4 The X CO N system originally called R1; Mc Dermott 1982 was built
withaproduction-system architecture. X CO Ncontainedseveralthousand rulesfordesigning
configurationsofcomputercomponentsforcustomersofthe Digital Equipment Corporation.
It was one of the first clear commercial successes in the emerging field of expert systems.
Manyothersimilarsystemshavebeenbuiltwiththesameunderlying technology whichhas
beenimplemented inthegeneral-purpose language O PS-5.
C OG NI TI VE Productionsystemsarealsopopularincognitivearchitecturesthat ismodelsofhu-
A RC HI TE CT UR ES
manreasoningsuch as A CT Anderson 1983and S OA R Lairdetal.1987. Insuchsys-
tems the working memory of the system models human short-term memory and the pro-
ductionsarepartoflong-termmemory. Oneachcycleofoperation productions arematched
againsttheworkingmemoryoffacts. Aproduction whoseconditions aresatisfiedcanaddor
delete facts in working memory. In contrast to the typical situation in databases production
systems often have many rules and relatively few facts. With suitably optimized matching
technology somemodernsystemscanoperateinrealtimewithtensofmillionsofrules.
Irrelevant facts
The final source of inefficiency in forward chaining appears to be intrinsic to the approach
andalsoarisesinthepropositional context. Forwardchainingmakesallallowableinferences
basedontheknownfactseveniftheyareirrelevanttothegoalathand. Inourcrimeexample
therewerenorulescapableofdrawingirrelevantconclusions sothelackofdirectednesswas
notaproblem. Inothercasese.g.ifmanyrulesdescribetheeatinghabitsof Americansand
thepricesofmissiles F OL-F C-A SK willgenerate manyirrelevant conclusions.
One way to avoid drawing irrelevant conclusions is to use backward chaining as de-
scribedin Section9.4. Anothersolutionistorestrictforwardchainingtoaselectedsubsetof
rules as in P L-F C-E NT AI LS? page 258. A third approach has emerged in the field of de-
D ED UC TI VE ductive databases which are large-scale databases like relational databases but which use
D AT AB AS ES
forwardchainingasthestandardinferencetoolratherthan S QLqueries. Theideaistorewrite
the rule set using information from the goal so that only relevant variable bindingsthose
belongingtoaso-calledmagicsetareconsideredduringforwardinference. Forexampleif
M AG IC SE T
thegoalis Criminal Westtherulethatconcludes Criminalxwillberewrittentoinclude
anextraconjunctthatconstrains thevalueof x:
Magicx Americanx Weapony Sellsxyz Hostilez Criminalx.
Section9.4. Backward Chaining 337
The fact Magic West is also added to the K B. In this way even if the knowledge base
contains factsaboutmillionsof Americans only Colonel Westwillbeconsidered during the
forward inference process. The complete process for defining magic sets and rewriting the
knowledge base is too complex to go into here but the basic idea is to perform a sort of
generic backward inference from the goal in order to work out which variable bindings
need to be constrained. The magic sets approach can therefore be thought of as a kind of
hybridbetweenforwardinference andbackwardpreprocessing.
The second major family of logical inference algorithms uses the backward chaining ap-
proach introduced in Section7.5fordefiniteclauses. These algorithms workbackward from
the goal chaining through rules to find known facts that support the proof. We describe
thebasicalgorithm andthenwedescribehowitisusedinlogicprogrammingwhichisthe
mostwidelyusedformofautomatedreasoning. Wealsoseethatbackwardchaininghassome
disadvantages compared withforward chaining and welook atwaysto overcome them. Fi-
nallywelookatthecloseconnectionbetweenlogicprogrammingandconstraintsatisfaction
problems.
goalwillbe proved iftheknowledge base contains aclause of the form lhs goal where
lhs left-handsideisalistofconjuncts. Anatomicfactlike American Westisconsidered
asaclause whoselhs istheemptylist. Nowaquery thatcontains variables mightbeproved
in multiple ways. For example the query Personx could be proved with the substitution
G EN ER AT OR
x Johnaswellaswithx Richard. Soweimplement F OL-B C-A SK asagenerator
afunction thatreturnsmultipletimeseachtimegivingone possible result.
Backward chaining is a kind of A ND OR searchthe O R part because the goal query
canbeprovedbyanyruleintheknowledgebaseandthe A NDpartbecausealltheconjuncts
inthelhs ofaclausemustbeproved. F OL-B C-O R worksbyfetchingallclauses thatmight
unify with the goal standardizing the variables in the clause to be brand-new variables and
then if the rhs of the clause does indeed unify with the goal proving every conjunct in the
lhs using F OL-B C-A ND. That function in turn works by proving each of the conjuncts in
turn keeping track oftheaccumulated substitution aswego. Figure 9.7isthe proof tree for
deriving Criminal Westfromsentences 9.3through9.10.
Backward chaining as we have written it is clearly a depth-first search algorithm.
This means that its space requirements are linear in the size of the proof neglecting for
now the space required to accumulate the solutions. It also means that backward chaining
unlikeforwardchainingsuffersfromproblemswithrepeatedstatesandincompleteness. We
will discuss these problems and some potential solutions but first we show how backward
chaining isusedinlogicprogrammingsystems.
function F OL-B C-A SK KBqueryreturnsageneratorofsubstitutions
return F OL-B C-O RK Bquery
generator F OL-B C-O RK Bgoalθyieldsasubstitution
foreachrulelhs rhsin F ET CH-R UL ES-F OR-G OA LK Bgoaldo
lhsrhs S TA ND AR DI ZE-V AR IA BL ESlhsrhs
foreachθcid:5in F OL-B C-A ND KBlhs U NI FYrhsgoalθdo
yieldθcid:5
generator F OL-B C-A ND KBgoalsθyieldsasubstitution
ifθ failure thenreturn
elseif L EN GT Hgoals0thenyieldθ
elsedo
firstrest F IR STgoals R ES Tgoals
foreachθcid:5in F OL-B C-O RK B S UB STθfirstθdo
foreachθcid:5cid:5in F OL-B C-A ND KBrestθcid:5do
yieldθcid:5cid:5
Figure9.6 Asimplebackward-chainingalgorithmforfirst-orderknowledgebases.
Criminal West
American West Weapony Sells West M z Hostile Nono
1
z Nono
Missiley Missile M Owns Nono M Enemy Nono America
y M 1
Figure9.7 Prooftreeconstructedbybackwardchainingtoprovethat Westisacriminal.
Thetreeshouldbereaddepthfirstlefttoright.Toprove Criminal Westwehavetoprove
the four conjuncts below it. Some of these are in the knowledge base and others require
furtherbackward chaining. Bindings for each successful unification are shown next to the
correspondingsubgoal.Notethatonceonesubgoalinaconjunctionsucceedsitssubstitution
isappliedtosubsequentsubgoals.Thusbythetime F OL-B C-A SKgetstothelastconjunct
originally Hostilezzisalreadyboundto Nono.
Section9.4. Backward Chaining 339
Logic programming is a technology that comes fairly close to embodying the declarative
idealdescribedin Chapter7: thatsystemsshouldbeconstructed byexpressingknowledgein
aformallanguageandthatproblemsshouldbesolvedbyrunninginferenceprocessesonthat
knowledge. Theidealissummedupin Robert Kowalskisequation
Algorithm Logic Control .
Prolog isthemostwidelyused logicprogramming language. Itisused primarily asarapid-
P RO LO G
prototypinglanguageandforsymbol-manipulationtaskssuchaswritingcompilers Van Roy
1990 and parsing natural language Pereira and Warren 1980. Many expert systems have
beenwrittenin Prologforlegalmedical financialandotherdomains.
Prolog programs are sets of definite clauses written in a notation somewhat different
from standard first-order logic. Prologuses uppercase letters forvariables and lowercase for
constantsthe opposite ofourconvention forlogic. Commasseparate conjuncts inaclause
and theclause iswrittenbackwards from whatweareused to; instead of A B C in
Prologwehave C :- A B.Hereisatypicalexample:
criminal X :- american X weapon Y sells X YZ hostile Z.
The notation E L denotes a list whose first element is E and whose rest is L. Here is a
Prolog program for append X YZwhich succeeds if list Z is the result of appending
lists Xand Y:
append Y Y.
append A XY AZ :- append X YZ.
In English we can read these clauses as 1 appending an empty list with a list Y produces
the same list Y and 2 A Zis the result of appending A X onto Y provided that Z is
the result ofappending Xonto Y.Inmost high-level languages wecan writeasimilar recur-
sive function that describes how to append two lists. The Prolog definition is actually much
more powerful however because it describes a relation that holds among three arguments
rather than a function computed from two arguments. For example we can ask the query
append X Y12: whattwo lists can be appended to give 12? Weget back the
solutions
X Y12;
X1 Y2;
X12 Y
The execution of Prolog programs is done through depth-first backward chaining where
clauses are tried inthe order inwhich theyare written in the knowledge base. Someaspects
of Prologfalloutsidestandard logicalinference:
Prolog uses the database semantics of Section 8.2.8 rather than first-order semantics
andthisisapparent initstreatmentofequality andnegationsee Section9.4.5.
Thereisasetofbuilt-in functions forarithmetic. Literalsusingthesefunction symbols
are proved by executing code rather than doing further inference. For example the
goal Xis43succeedswith Xboundto7. Ontheotherhandthegoal5is X Y
failsbecausethebuilt-in functions donotdoarbitrary equationsolving.5
Therearebuilt-inpredicatesthathavesideeffectswhenexecuted. Theseincludeinput
output predicates and the assertretractpredicates formodifying theknowledge
base. Suchpredicateshavenocounterpartinlogicandcanproduceconfusingresults
forexampleiffactsareassertedinabranchoftheprooftreethateventually fails.
Theoccurcheckisomittedfrom Prologsunificationalgorithm. Thismeansthatsome
unsoundinferences canbemade;thesearealmostneveraprobleminpractice.
Prologusesdepth-first backward-chaining search withnochecks forinfiniterecursion.
Thismakes itvery fast when given theright setof axioms but incomplete when given
thewrongones.
Prologsdesignrepresentsacompromisebetweendeclarativenessandexecutionefficiency
inasmuchasefficiencywasunderstood atthetime Prologwasdesigned.
The execution of a Prolog program can happen in two modes: interpreted and compiled.
Interpretation essentially amounts to running the F OL-B C-A SK algorithm from Figure 9.6
with the program as the knowledge base. We say essentially because Prolog interpreters
containavarietyofimprovementsdesigned tomaximizespeed. Hereweconsideronlytwo.
First our implementation had to explicitly manage the iteration over possible results
generated by each of the subfunctions. Prolog interpreters have a global data structure
a stack of choice points to keep track of the multiple possibilities that we considered in
C HO IC EP OI NT
F OL-B C-O R. This global stack is more efficient and it makes debugging easier because
thedebuggercanmoveupanddownthestack.
Secondoursimpleimplementationof F OL-B C-A SKspendsagooddealoftimegener-
atingsubstitutions. Insteadofexplicitlyconstructingsubstitutions Prologhaslogicvariables
that remember their current binding. At any point in time every variable in the program ei-
ther is unbound or is bound to some value. Together these variables and values implicitly
define the substitution for the current branch of the proof. Extending the path can only add
new variable bindings because an attempt to add a different binding for an already bound
variable results in a failure of unification. When a path in the search fails Prolog will back
uptoaprevious choice point andthen itmight have tounbind somevariables. Thisisdone
bykeepingtrackofallthevariables thathavebeenboundinastackcalledthetrail. Aseach
T RA IL
newvariableisboundby U NI FY-V ARthevariableispushedontothetrail. Whenagoalfails
and it is time to back up to a previous choice point each of the variables is unbound as it is
removedfromthetrail.
Even the most efficient Prolog interpreters require several thousand machine instruc-
tions per inference step because of the cost of index lookup unification and building the
recursive call stack. In effect the interpreter always behaves as if it has never seen the pro-
gram before; for example it has to find clauses that match the goal. A compiled Prolog
Section9.4. Backward Chaining 341
procedure A PP EN Daxyazcontinuation
trail G LO BA L-T RA IL-P OI NT ER
ifax and U NI FYyazthen C AL Lcontinuation
R ES ET-T RA ILtrail
axz N EW-V AR IA BL EN EW-V AR IA BL EN EW-V AR IA BL E
if U NI FYaxa xand U NI FYaza zthen A PP EN Dxyzcontinuation
Figure9.8 Pseudocoderepresentingtheresultofcompilingthe Appendpredicate. The
function N EW-V AR IA BL Ereturnsanewvariabledistinctfromallothervariablesusedsofar.
Theprocedure C AL Lcontinuationcontinuesexecutionwiththespecifiedcontinuation.
programontheotherhandisaninferenceprocedureforaspecificsetofclausessoitknows
whatclauses matchthegoal. Prologbasically generates aminiature theorem proverforeach
differentpredicate therebyeliminatingmuchoftheoverheadofinterpretation. Itisalsopos-
sible to open-code the unification routine for each different call thereby avoiding explicit
O PE N-C OD E
analysis oftermstructure. Fordetailsofopen-coded unification see Warren etal.1977.
The instruction sets of todays computers give a poor match with Prologs semantics
somost Prologcompilerscompileintoanintermediatelanguageratherthandirectlyintoma-
chine language. The most popular intermediate language is the Warren Abstract Machine
or W AMnamed after David H.D.Warren oneoftheimplementers ofthefirst Prologcom-
piler. The W AM is an abstract instruction set that is suitable for Prolog and can be either
interpretedortranslatedintomachinelanguage. Othercompilerstranslate Prologintoahigh-
levellanguagesuchas Lispor Candthenusethatlanguagescompilertotranslatetomachine
language. Forexamplethedefinitionofthe Appendpredicatecanbecompiledintothecode
shownin Figure9.8. Severalpointsareworthmentioning:
Rather than having to search the knowledge base for Appendclauses the clauses be-
comeaprocedure andtheinferences arecarriedoutsimplyby callingtheprocedure.
Asdescribedearlierthecurrentvariablebindingsarekeptonatrail. Thefirststepofthe
proceduresavesthecurrentstateofthetrailsothatitcan berestoredby R ES ET-T RA IL
ifthefirstclausefails. Thiswillundoanybindingsgeneratedbythefirstcallto U NI FY.
Thetrickiestpartistheuseofcontinuationstoimplementchoicepoints. Youcanthink
C ON TI NU AT IO N
of a continuation as packaging up a procedure and a list of arguments that together
define what should be done next whenever the current goal succeeds. It would not
do just to return from a procedure like A PP EN D when the goal succeeds because it
could succeed in several ways and each of them has to be explored. Thecontinuation
argument solves this problem because it can be called each time the goal succeeds. In
the A PP EN D code ifthe firstargument is empty and the second argument unifies with
the third then the A PP EN D predicate has succeeded. We then C AL L the continuation
with the appropriate bindings on the trail to do whatever should be done next. For
example if the call to A PP EN D were at the top level the continuation would print the
bindingsofthevariables.
Before Warrens work on the compilation of inference in Prolog logic programming was
too slow for general use. Compilers by Warren and others allowed Prolog code to achieve
speeds that are competitive with C on a variety of standard benchmarks Van Roy 1990.
Of course the fact that one can write a planner or natural language parser in a few dozen
linesof Prologmakesitsomewhatmoredesirablethan Cforprototypingmostsmall-scale A I
research projects.
Parallelizationcanalsoprovidesubstantialspeedup. Therearetwoprincipalsourcesof
parallelism. The first called O R-parallelism comes from the possibility of agoal unifying
O R-P AR AL LE LI SM
withmanydifferentclausesintheknowledgebase. Eachgivesrisetoanindependent branch
in the search space that can lead to a potential solution and all such branches can be solved
in parallel. The second called A ND-parallelism comes from the possibility of solving
A ND-P AR AL LE LI SM
each conjunct inthebody ofanimplication inparallel. A ND-parallelism ismoredifficult to
achieve because solutions for the whole conjunction require consistent bindings for all the
variables. Each conjunctive branch must communicate with the other branches to ensure a
globalsolution.
We now turn to the Achilles heel of Prolog: the mismatch between depth-first search and
search trees thatinclude repeated states andinfinite paths. Considerthefollowing logicpro-
gramthatdecidesifapathexistsbetweentwopointsonadirectedgraph:
path X Z :- link X Z.
path X Z :- path X Y link Y Z.
Asimple three-node graph described bythefacts linkaband linkbcisshown
in Figure 9.9a. With this program the query pathacgenerates the proof tree shown
in Figure9.10a. Ontheotherhandifweputthetwoclauses intheorder
path X Z :- path X Y link Y Z.
path X Z :- link X Z.
then Prologfollowstheinfinitepathshownin Figure9.10b. Prologistherefore incomplete
asatheoremproverfordefiniteclausesevenfor Datalogprogramsasthisexampleshows
because for some knowledge bases it fails to prove sentences that are entailed. Notice that
forward chaining does not suffer from this problem: once pathabpathbcand
pathacareinferred forwardchaining halts.
Depth-first backward chaining also has problems with redundant computations. For
examplewhenfindingapathfrom A to J in Figure9.9b Prologperforms877inferences
mostofwhichinvolvefindingallpossiblepathstonodesfromwhichthegoalisunreachable.
This is similar to the repeated-state problem discussed in Chapter 3. The total amount of
inference can be exponential in the number of ground facts that are generated. If we apply
forward chaining instead at most n2 path X Yfacts can be generated linking n nodes.
Fortheproblem in Figure9.9bonly62inferences areneeded.
D YN AM IC Forwardchainingongraphsearchproblemsisanexampleof dynamicprogramming
P RO GR AM MI NG
in which the solutions to subproblems are constructed incrementally from those of smaller
Section9.4. Backward Chaining 343
A
1
A B C
J
4
a b
graphinwhicheachnodeisconnectedtotworandomsuccessorsinthenextlayer.Findinga
pathfrom A to J requires877inferences.
pathac
pathac
patha Y link Yc
linkac patha Y linkbc
fail
patha Y link Y Y
linka Y
Y b
a b
whentheclausesareinthewrongorder.
subproblems and are cached to avoid recomputation. We can obtain a similar effect in a
backward chaining system using memoizationthat is caching solutions to subgoals as
they are found and then reusing those solutions when the subgoal recurs rather than repeat-
T AB LE DL OG IC ingthepreviouscomputation. Thisistheapproachtakenbytabledlogicprogrammingsys-
P RO GR AM MI NG
tems which use efficient storage and retrieval mechanisms to perform memoization. Tabled
logicprogramming combines thegoal-directedness ofbackward chaining withthedynamic-
programming efficiency of forward chaining. It is also complete for Datalog knowledge
bases whichmeansthattheprogrammerneedworrylessabout infiniteloops. Itisstillpos-
sible to get an infinite loop with predicates like father X Y that refer to a potentially
unbounded numberofobjects.
Prologusesdatabasesemanticsasdiscussedin Section8.2.8. Theuniquenamesassumption
says that every Prolog constant and every ground term refers to a distinct object and the
closed worldassumption says that the only sentences thatare true arethose that areentailed
bytheknowledgebase. Thereisnowaytoassertthatasentenceisfalsein Prolog. Thismakes
Prologlessexpressivethanfirst-orderlogicbutitispart ofwhatmakes Prologmoreefficient
andmoreconcise. Considerthefollowing Prologassertions aboutsomecourseofferings:
Course C S101 Course C S102 Course C S106 Course E E101. 9.11
Under the unique names assumption C S and E E are different as are 101 102 and 106
so this means that there are four distinct courses. Under the closed-world assumption there
are no other courses so there are exactly four courses. But if these were assertions in F OL
rather than in Prolog then all we could say is that there are somewhere between one and
infinitycourses. Thatsbecause theassertions in F OLdonotdenythepossibility thatother
unmentionedcoursesarealsoofferednordotheysaythatthecoursesmentionedaredifferent
fromeachother. Ifwewantedtotranslate Equation9.11into F OLwewouldgetthis:
Coursedn d C S n 101d C S n 102
d C S n 106d E E n 101. 9.12
Thisis called the completion of Equation 9.11. Itexpresses in F OLthe idea that there are
C OM PL ET IO N
atmostfourcourses. Toexpress in F OLtheideathatthereare atleastfourcourses weneed
towritethecompletion oftheequality predicate:
x y x C S y C Sx E E y E Ex 101y 101
x 102y 102x 106y 106.
Thecompletion isusefulforunderstanding database semantics butforpractical purposes if
your problem can be described with database semantics it is more efficient to reason with
Prolog or some other database semantics system rather than translating into F OL and rea-
soningwithafull F OLtheorem prover.
In our discussion of forward chaining Section 9.3 we showed how constraint satisfaction
problems C SPs can be encoded as definite clauses. Standard Prolog solves such problems
inexactlythesamewayasthebacktracking algorithm givenin Figure6.5.
Becausebacktracking enumeratesthedomainsofthevariablesitworksonlyforfinite-
domain C SPs. In Prolog terms there must be a finite number of solutions for any goal
withunbound variables. Forexample thegoal diff Q SAwhichsaysthat Queensland
and South Australia must be different colors has six solutions if three colors are allowed.
Infinite-domain C SPsforexamplewithintegerorreal-valuedvariablesrequire quitedif-
ferentalgorithms suchasboundspropagation orlinearprogramming.
Consider the following example. We define triangle X YZas a predicate that
holdsifthethreearguments arenumbersthatsatisfythetriangleinequality:
triangle X YZ :-
X0 Y0 Z0 X YZ Y ZX X ZY.
If we ask Prolog the query triangle345 it succeeds. On the other hand if we
ask triangle34 Zno solution will be found because the subgoal Z0 cannot be
handled by Prolog;wecantcompareanunbound valueto0.
Section9.5. Resolution 345
C ON ST RA IN TL OG IC Constraint logic programming C LP allows variables to be constrained rather than
P RO GR AM MI NG
bound. A CL Psolution isthemostspecific setofconstraints onthe query variables that can
bederivedfromtheknowledge base. Forexamplethesolution tothetriangle34 Z
query is the constraint 7 Z 1. Standard logic programs are just a special case of
C LPinwhichthesolutionconstraints mustbeequality constraintsthat isbindings.
C LP systems incorporate various constraint-solving algorithms for the constraints al-
lowed in the language. For example a system that allows linear inequalities on real-valued
variables might include a linear programming algorithm for solving those constraints. C LP
systems also adopt a much more flexible approach to solving standard logic programming
queries. Forexample instead ofdepth-first left-to-right backtracking theymightuseanyof
the more efficient algorithms discussed in Chapter 6 including heuristic conjunct ordering
backjumping cutset conditioning and so on. C LP systems therefore combine elements of
constraint satisfaction algorithms logicprogramming anddeductive databases.
Several systems that allow the programmer more control over the search order for in-
ferencehavebeendefined. The M RS language Genesereth and Smith1981;Russell1985
allows the programmer to write metarules to determine which conjuncts are tried first. The
M ET AR UL E
user could write a rule saying that the goal with the fewest variables should be tried first or
couldwritedomain-specific rulesforparticularpredicates.
Thelastofourthreefamiliesoflogicalsystemsisbasedonresolution. Wesawonpage250
that propositional resolution using refutation is a complete inference procedure for proposi-
tionallogic. Inthissection wedescribehowtoextendresolution tofirst-orderlogic.
As in the propositional case first-order resolution requires that sentences be in conjunctive
normalform C NFthatisaconjunction ofclauses whereeachclauseisadisjunction of
literals.6 Literals can contain variables which are assumed tobe universally quantified. For
examplethesentence
x Americanx Weapony Sellsxyz Hostilez Criminalx
becomes in C NF
Americanx Weapony Sellsxyz Hostilez Criminalx.
Every sentence of first-order logic can be converted into an inferentially equivalent C NF
sentence. Inparticularthe C NFsentencewillbeunsatisfiablejustwhentheoriginalsentence
isunsatisfiable sowehaveabasisfordoingproofsbycontradiction onthe C NFsentences.
ofatomsintheconclusion Exercise7.13.Thisiscalledimplicativenormalformor Kowalskiformespecially
whenwrittenwitharight-to-leftimplicationsymbol Kowalski1979andisoftenmucheasiertoread.
Theprocedureforconversionto C NFissimilartothepropositionalcasewhichwesaw
onpage253. Theprincipaldifferencearisesfromtheneedtoeliminateexistentialquantifiers.
We illustrate the procedure by translating the sentence Everyone who loves all animals is
lovedbysomeoneor
x y Animaly Lovesxy y Lovesyx.
Thestepsareasfollows:
Eliminateimplications:
x y Animaly Lovesxyy Lovesyx.
Moveinwards: Inaddition totheusual rulesfornegatedconnectives weneedrules
fornegatedquantifiers. Thuswehave
x p becomes x p
x p becomes x p.
Oursentencegoesthroughthefollowingtransformations:
x y Animaly Lovesxyy Lovesyx.
x y Animaly Lovesxyy Lovesyx.
x y Animaly Lovesxyy Lovesyx.
Notice how a universal quantifier y in the premise of the implication has become
an existential quantifier. The sentence now reads Either there is some animal that x
doesnt love or if this is not the case someone loves x. Clearly the meaning of the
originalsentence hasbeenpreserved.
Standardizevariables: Forsentenceslikex Pxx Qxwhichusethesame
variable name twice change the name of one of the variables. This avoids confusion
laterwhenwedropthequantifiers. Thuswehave
x y Animaly Lovesxyz Loveszx.
Skolemize: Skolemization is the process of removing existential quantifiers by elimi-
S KO LE MI ZA TI ON
nation. Inthesimplecaseitisjustlikethe Existential Instantiation ruleof Section9.1:
translatex Pxinto P Awhere Aisanewconstant. Howeverwecantapply Ex-
istential Instantiation tooursentence abovebecause itdoesnt matchthepattern v α;
only parts of the sentence match the pattern. If we blindly apply the rule to the two
matchingpartsweget
x Animal A Lovesx A Loves Bx
which has the wrong meaning entirely: it says that everyone either fails to love a par-
ticular animal Aoris loved by some particular entity B. In fact our original sentence
allowseachpersontofailtoloveadifferentanimalortobelovedbyadifferentperson.
Thuswewantthe Skolementitiestodepend onxandz:
x Animal Fx Lovesx Fx Loves Gzx.
Here F and G are Skolem functions. The general rule is that the arguments of the
S KO LE MF UN CT IO N
Skolem function are all the universally quantified variables in whose scope the exis-
tentialquantifierappears. Aswith Existential Instantiation the Skolemized sentence is
satisfiableexactlywhentheoriginal sentence issatisfiable.
Section9.5. Resolution 347
Dropuniversal quantifiers: Atthis point all remaining variables mustbe universally
quantified. Moreover thesentence isequivalent tooneinwhichalltheuniversal quan-
tifiershavebeenmovedtotheleft. Wecantherefore droptheuniversal quantifiers:
Animal Fx Lovesx Fx Loves Gzx.
Distributeover:
Animal Fx Loves Gzx Lovesx Fx Loves Gzx .
Thisstepmayalsorequireflatteningoutnestedconjunctions anddisjunctions.
The sentence is now in C NF and consists of two clauses. It is quite unreadable. It may
help to explain that the Skolem function Fxrefers tothe animal potentially unloved by x
whereas Gz refers to someone who might love x. Fortunately humans seldom need look
at C NFsentencesthe translation processiseasilyautomated.
Theresolution rule forfirst-order clauses issimply aliftedversion ofthepropositional reso-
lution rule given on page 253. Two clauses which are assumed to be standardized apart so
that they share no variables can be resolved if they contain complementary literals. Propo-
sitional literals are complementary if one is the negation of the other; first-order literals are
complementary ifoneunifieswiththenegationoftheother. Thuswehave
cid:3 cid:3 m m
S UB STθcid:3 1cid:3 i1cid:3 i1cid:3
k
m 1m j1m j1m n
where U NI FYcid:3 im jθ. Forexample wecanresolvethetwoclauses
Animal Fx Loves Gxx and Lovesuv Killsuv
by eliminating the complementary literals Loves Gxx and Lovesuv with unifier
θu Gxvx toproducetheresolvent clause
Animal Fx Kills Gxx.
This rule is called the binary resolution rule because it resolves exactly two literals. The
B IN AR YR ES OL UT IO N
binary resolution rulebyitself does notyield acomplete inference procedure. Thefullreso-
lutionruleresolvessubsetsofliteralsineachclausethatareunifiable. Analternativeapproach
is to extend factoringthe removal of redundant literalsto the first-order case. Proposi-
tional factoring reduces two literals to one if they are identical; first-order factoring reduces
twoliterals toone iftheyare unifiable. Theunifiermustbeapplied tothe entire clause. The
combination ofbinaryresolution andfactoring iscomplete.
Resolution proves that K B α by proving K B α unsatisfiable that is by deriving the
empty clause. The algorithmic approach is identical to the propositional case described in
Americanx Weapony Sellsxyz Hostilez Criminalx Criminal West
American West American West Weapony Sells Westyz Hostilez
Missilex Weaponx Weapony Sells Westyz Hostilez
Missile M1 Missiley Sells Westyz Hostilez
Missilex Owns Nonox Sells Westx Nono Sells West M1z Hostilez
Missile M1 Missile M1 Owns Nono M1 Hostile Nono
Owns Nono M1 Owns Nono M1 Hostile Nono
Enemyx America Hostilex Hostile Nono
Enemy Nono America Enemy Nono America
Figure9.11 Aresolutionproofthat Westisacriminal.Ateachsteptheliteralsthatunify
areinbold.
thecrimeexamplefrom Section9.3. Thesentences in C NFare
Americanx Weapony Sellsxyz Hostilez Criminalx
Missilex Owns Nonox Sells Westx Nono
Enemyx America Hostilex
Missilex Weaponx
Owns Nono M Missile M
American West Enemy Nono America.
Wealso include the negated goal Criminal West. Theresolution proof is shown in Fig-
ure9.11. Noticethestructure: singlespinebeginningwiththegoalclauseresolvingagainst
clauses from the knowledge base until the empty clause is generated. This is characteristic
of resolution on Horn clause knowledge bases. In fact the clauses along the main spine
correspond exactly to the consecutive values of the goals variable in the backward-chaining
algorithm of Figure 9.6. This is because we always choose to resolve with a clause whose
positive literal unified with the leftmost literal of the current clause on the spine; this is
exactly what happens in backward chaining. Thus backward chaining is just a special case
ofresolution withaparticularcontrol strategytodecidewhichresolution toperform next.
Oursecond examplemakesuseof Skolemization andinvolves clauses thatarenotdef-
inite clauses. This results in a somewhat more complex proof structure. In English the
problem isasfollows:
Everyonewholovesallanimalsislovedbysomeone.
Anyonewhokillsananimalislovedbynoone.
Jacklovesallanimals.
Either Jackor Curiositykilledthecatwhoisnamed Tuna.
Did Curiositykillthecat?
Section9.5. Resolution 349
First we express the original sentences some background knowledge and the negated goal
Ginfirst-orderlogic:
A. x y Animaly Lovesxy y Lovesyx
B. x z Animalz Killsxz y Lovesyx
C. x Animalx Loves Jackx
D. Kills Jack Tuna Kills Curiosity Tuna
E. Cat Tuna
F. x Catx Animalx
G. Kills Curiosity Tuna
Nowweapplytheconversion procedure toconverteachsentence to C NF:
A1. Animal Fx Loves Gxx
A2. Lovesx Fx Loves Gxx
B. Lovesyx Animalz Killsxz
C. Animalx Loves Jackx
D. Kills Jack Tuna Kills Curiosity Tuna
E. Cat Tuna
F. Catx Animalx
G. Kills Curiosity Tuna
Theresolutionproofthat Curiositykilledthecatisgivenin Figure9.12. In Englishtheproof
couldbeparaphrased asfollows:
Suppose Curiosity did not kill Tuna. We know that either Jack or Curiosity did; thus
Jackmusthave. Now Tunaisacatandcatsareanimalsso Tunaisananimal. Because
anyonewhokillsananimalislovedbynooneweknowthatnooneloves Jack. Onthe
other hand Jack loves all animals so someone loves him; so we have a contradiction.
Therefore Curiositykilledthecat.
Cat Tuna Catx Animalx Kills Jack Tuna Kills Curiosity Tuna Kills Curiosity Tuna
Animal Tuna Lovesy x Animalz Killsx z Kills Jack Tuna Lovesx Fx Loves Gx x Animalx Loves Jack x
""
Lovesy x Killsx Tuna Animal F Jack Loves G Jack Jack Animal Fx Loves Gx x
Lovesy Jack Loves G Jack Jack
Figure9.12 A resolutionproofthat Curiositykilled the cat. Notice the use of factoring
in the derivation of the clause Loves G Jack Jack. Notice also in the upper right the
unificationof Lovesx Fxand Loves Jackxcanonlysucceedafterthevariableshave
beenstandardizedapart.
Theproofanswersthequestion Did Curiosity killthecat? butoftenwewanttoposemore
general questions such as Who killed the cat? Resolution can do this but it takes a little
more work to obtain the answer. The goal is w Killsw Tuna which when negated
becomes Killsw Tunain C NF.Repeatingtheproofin Figure9.12withthenewnegated
goal we obtain a similar proof tree but with the substitution w Curiosity in one of the
steps. So in this case finding out who killed the cat is just a matter of keeping track of the
bindings forthequeryvariables intheproof.
N ON CO NS TR UC TI VE Unfortunately resolution can produce nonconstructive proofs for existential goals.
P RO OF
Forexample Killsw Tunaresolves with Kills Jack Tuna Kills Curiosity Tuna
to give Kills Jack Tuna which resolves again with Killsw Tuna to yield the empty
clause. Notice that w has two different bindings in this proof; resolution is telling us that
yes someone killed Tunaeither Jack or Curiosity. This is no great surprise! One so-
lution is to restrict the allowed resolution steps so that the query variables can be bound
only once in a given proof; then we need to be able to backtrack over the possible bind-
ings. Another solution is to add a special answer literal to the negated goal which be-
A NS WE RL IT ER AL
comes Killsw Tuna Answerw. Now the resolution process generates an answer
whenever a clause is generated containing just a single answer literal. Forthe proof in Fig-
ure 9.12 this is Answer Curiosity. The nonconstructive proof would generate the clause
Answer Curiosity Answer Jackwhichdoesnotconstitute ananswer.
Thissection givesacompleteness proof ofresolution. Itcanbesafely skipped bythosewho
arewillingtotakeitonfaith.
R EF UT AT IO N Weshowthatresolution isrefutation-complete whichmeansthatifasetofsentences
C OM PL ET EN ES S
is unsatisfiable then resolution will always be able to derive a contradiction. Resolution
cannot be used to generate all logical consequences of a set of sentences but it can be used
toestablish that agivensentence isentailed bythesetofsentences. Hence itcanbeusedto
findallanswerstoagivenquestion Qxbyprovingthat K B Qxisunsatisfiable.
Wetakeitasgiventhatanysentenceinfirst-orderlogicwithoutequalitycanberewrit-
ten as a set of clauses in C NF.This can be proved by induction on the form of the sentence
using atomic sentences as the base case Davis and Putnam 1960. Our goal therefore is to
prove the following: if S is an unsatisfiable set of clauses then the application of a finite
numberofresolution stepsto S willyieldacontradiction.
Our proof sketch follows Robinsons original proof with some simplifications from
Genesereth and Nilsson1987. Thebasicstructure oftheproof Figure9.13isasfollows:
1. First we observe that if S is unsatisfiable then there exists a particular set of ground
instancesoftheclausesof Ssuchthatthissetisalsounsatisfiable Herbrandstheorem.
2. Wethenappealtothegroundresolutiontheoremgivenin Chapter7whichstatesthat
propositional resolution iscompleteforgroundsentences.
3. Wethen usealiftinglemmatoshowthat foranypropositional resolution proof using
the set of ground sentences there is a corresponding first-order resolution proof using
thefirst-ordersentences fromwhichthegroundsentences wereobtained.
Section9.5. Resolution 351
Any set of sentences S is representable in clausal form
Assume S is unsatisfiable and in clausal form
Herbrands theorem
Some set S' of ground instances is unsatisfiable
Ground resolution
theorem
Resolution can find a contradiction in S'
Lifting lemma
There is a resolution proof for the contradiction in S'
Figure9.13 Structureofacompletenessproofforresolution.
Tocarryoutthefirststepweneedthreenewconcepts:
H ER BR AN D Herbrand universe: If S is a set of clauses then H the Herbrand universe of S is
U NI VE RS E S
thesetofallgroundtermsconstructable fromthefollowing:
a. Thefunction symbolsin Sifany.
b. Theconstant symbolsin Sifany;ifnonethentheconstant symbol A.
Forexampleif S containsjusttheclause Px Fx A Qx A Rx Bthen
H isthefollowinginfinitesetofground terms:
S
A BF AA FA BF BA FB BF AF AA....
Saturation: If S is a set of clauses and P is a set of ground terms then P S the
S AT UR AT IO N
saturation of S with respect to P isthe set of allground clauses obtained by applying
allpossible consistent substitutions ofgroundtermsin P withvariablesin S.
Herbrand base: The saturation of a set S of clauses with respect to its Herbrand uni-
H ER BR AN DB AS E
verse is called the Herbrand base of S written as H S. For example if S contains
S
solelytheclausejustgiventhen H Sistheinfinitesetofclauses
S
P AF AA QA AR AB
P BF BA QB AR BB
P FA AF FA AA Q FA AA RF AA B
P FA BF FA BA QF AB AR FA BB...
H ER BR AN DS Thesedefinitionsallowustostateaformof Herbrandstheorem Herbrand 1930:
T HE OR EM
Ifaset S ofclausesisunsatisfiable thenthereexistsafinitesubsetof H Sthat
S
isalsounsatisfiable.
cid:2
Let S bethisfinitesubsetofgroundsentences. Nowwecanappealtothegroundresolution
cid:2
theorem page 255 to show that the resolution closure R CS contains the empty clause.
cid:2
Thatisrunning propositional resolution tocompletion on S willderiveacontradiction.
Now that we have established that there is always a resolution proof involving some
finite subset of the Herbrand base of S the next step is to show that there is a resolution
G OD EL S I NC OM PL ET EN ES S T HE OR EM
Byslightly extending thelanguage offirst-orderlogic toallow forthe mathemat-
ical induction schema in arithmetic Kurt Godel was able to show in his incom-
pletenesstheoremthattherearetruearithmetic sentences thatcannotbeproved.
The proof of the incompleteness theorem is somewhat beyond the scope of
thisbook occupying asitdoes atleast30pages butwecangiveahinthere. We
beginwiththelogical theory ofnumbers. Inthis theory there isasingle constant
0 and a single function S the successor function. In the intended model S0
denotes 1 S S0 denotes 2 and soon; the language therefore has names forall
thenaturalnumbers. Thevocabulary alsoincludesthefunctionsymbolsand
Expt exponentiation andtheusualsetoflogicalconnectivesandquantifiers. The
firststepistonoticethatthesetofsentencesthatwecanwriteinthislanguagecan
be enumerated. Imagine defining an alphabetical order on the symbols and then
arranging in alphabetical order each of the sets of sentences of length 1 2 and
so on. We can then number each sentence α with a unique natural number α
the Godel number. This is crucial: number theory contains a name for each of
its own sentences. Similarly we can number each possible proof P with a Godel
number G Pbecauseaproofissimplyafinitesequence ofsentences.
Now suppose we have a recursively enumerable set A of sentences that are
true statements about the natural numbers. Recalling that A can be named by a
givensetofintegerswecanimaginewritinginourlanguage asentenceαj Aof
thefollowingsort:
i i is not the Godel number of a proof of the sentence whose Godel
numberisjwheretheproofusesonlypremisesin A.
Thenletσ bethesentenceασ Athatisasentencethatstatesitsownunprov-
abilityfrom A. Thatthissentence alwaysexistsistruebutnotentirely obvious.
Now we make the following ingenious argument: Suppose that σ is provable
from A; then σ is false because σ says it cannot be proved. But then we have a
falsesentencethatisprovablefrom Aso Acannotconsistofonlytruesentences
aviolationofourpremise. Therefore σ isnotprovablefrom A. Butthisisexactly
whatσ itselfclaims;henceσ isatruesentence.
So we have shown barring 291 pages that for any set of true sentences of
2
number theory and in particular any set of basic axioms there are other true sen-
tences that cannot be proved from those axioms. This establishes among other
things that we can never prove all the theorems of mathematics within any given
system of axioms. Clearly this was an important discovery for mathematics. Its
significancefor A Ihasbeenwidelydebatedbeginningwithspeculationsby Godel
himself. Wetakeupthedebatein Chapter26.
Section9.5. Resolution 353
proof using the clauses of S itself which are not necessarily ground clauses. We start by
considering asingleapplication oftheresolution rule. Robinsonstatedthislemma:
cid:2 cid:2
Let C and C be two clauses with no shared variables and let C and C be
cid:2 cid:2 cid:2
groundinstancesof C and C . If C isaresolventof C and C thenthereexists
cid:2
a clause C such that 1 C is a resolvent of C and C and 2 C is a ground
instanceof C.
Thisiscalledaliftinglemmabecauseitliftsaproofstepfromgroundclausesuptogeneral
L IF TI NG LE MM A
first-order clauses. In order to prove his basic lifting lemma Robinson had to invent unifi-
cation and derive all of the properties of most general unifiers. Rather than repeat the proof
herewesimplyillustrate thelemma:
C Px Fx A Qx A Rx B
1
C N Gyz P Hyz
2
Ccid:2 P HB FH BA QH BA RH BB
1
Ccid:2 N GB FH BA PH BF HB A
2
Ccid:2 N GB FH BA QH BA RH BB
C N Gy F Hy A QHy A RHy B.
cid:2 cid:2 cid:2
We see that indeed C is a ground instance of C. In general for C and C to have any
resolvents they must be constructed by first applying to C and C the most general unifier
ofapairofcomplementary literals in C and C . Fromtheliftinglemmaitiseasytoderive
asimilarstatement aboutanysequence ofapplications oftheresolution rule:
cid:2 cid:2
Foranyclause C intheresolution closureof S thereisaclause C intheresolu-
cid:2
tionclosureof S suchthat C isagroundinstanceof C andthederivationof C is
cid:2
thesamelengthasthederivation of C .
cid:2
From this fact it follows that if the empty clause appears in the resolution closure of S it
mustalsoappearintheresolution closureof S. Thisisbecausetheemptyclausecannotbea
ground instance ofanyotherclause. Torecap: wehaveshownthatif S isunsatisfiable then
thereisafinitederivationoftheemptyclauseusingtheresolution rule.
Theliftingoftheoremprovingfromgroundclausestofirst-orderclausesprovidesavast
increaseinpower. Thisincreasecomesfromthefactthatthefirst-orderproofneedinstantiate
variables only as far as necessary for the proof whereas the ground-clause methods were
required toexamineahugenumberofarbitrary instantiations.
Noneoftheinferencemethodsdescribedsofarinthischapterhandleanassertionoftheform
x y. Threedistinctapproachescanbetaken. Thefirstapproachistoaxiomatizeequality
towritedownsentencesabouttheequalityrelationintheknowledgebase. Weneedtosaythat
equalityisreflexivesymmetricandtransitiveandwealsohavetosaythatwecansubstitute
equals for equals in any predicate orfunction. So weneed three basic axioms and then one
foreachpredicate andfunction:
x xx
xy xy yx
xyz xyyz xz
xy xy P x P y
xy xy P x P y
.
.
.
wxyz wyxz F wx F yz
wxyz wyxz F wx F yz
.
.
.
Given these sentences a standard inference procedure such as resolution can perform tasks
requiringequalityreasoningsuchassolvingmathematicalequations. Howevertheseaxioms
will generate a lot of conclusions most of them not helpful to a proof. So there has been a
search formoreefficientwaysofhandling equality. Onealternative istoadd inference rules
rather than axioms. The simplest rule demodulation takes a unit clause xy and some
clause α that contains the term x and yields a new clause formed by substituting y for x
within α. It works if the term within α unifies with x; it need not be exactly equal to x.
Notethat demodulation isdirectional; given x ythexalways getsreplaced withynever
vice versa. That means that demodulation can be used for simplifying expressions using
demodulators suchasx0xorx1x. Asanotherexample given
Father Fatherx Paternal Grandfatherx
Birthdate Father Father Bella1926
wecanconclude bydemodulation
Birthdate Paternal Grandfather Bella1926.
Moreformallywehave
Demodulation: For any terms x y and z where z appears somewhere in literal m
D EM OD UL AT IO N i
andwhere U NI FYxz θ
xy m m
.
S UB SU BS Tθx S UB STθym 1m n
where S UB ST is the usual substitution of a binding list and S UBxym means to
replace xwithy everywherethat xoccurswithinm.
Therulecanalsobeextendedtohandlenon-unitclausesinwhichanequalityliteralappears:
Paramodulation: Foranyterms xyandzwherez appearssomewhereinliteral m
P AR AM OD UL AT IO N i
andwhere U NI FYxz θ
cid:3 cid:3 xy m m
.
S UB SU BS Tθx S UB STθy S UB STθcid:3 1cid:3
k
m 1m n
Forexamplefrom
P Fx Bx Qx and F Ayy Ry
Section9.5. Resolution 355
we have θ UN IF YF Ay Fx Bx Ay B and we can conclude by paramodu-
lationthesentence
P BA QA RB.
Paramodulation yieldsacompleteinference procedure forfirst-orderlogicwithequality.
A third approach handles equality reasoning entirely within an extended unification
algorithm. That is terms are unifiable if they are provably equal under some substitution
where provably allows for equality reasoning. For example the terms 1 2 and 2 1
normally are not unifiable but a unification algorithm that knows that xyy x could
E QU AT IO NA L unifythemwiththeemptysubstitution. Equationalunificationofthiskindcanbedonewith
U NI FI CA TI ON
efficientalgorithmsdesignedfortheparticularaxiomsusedcommutativityassociativityand
so on rather than through explicit inference with those axioms. Theorem provers using this
technique arecloselyrelatedtothe C LPsystemsdescribed in Section9.4.
We know that repeated applications of the resolution inference rule will eventually find a
proofifoneexists. Inthissubsection weexaminestrategies thathelpfindproofs efficiently.
Unitpreference: Thisstrategypreferstodoresolutionswhereoneofthesentencesisasingle
U NI TP RE FE RE NC E
literal also known as a unit clause. The idea behind the strategy is that we are trying to
produce anemptyclause soitmightbeagoodideatopreferinferences thatproduceshorter
clauses. Resolvingaunitsentencesuchas Pwithanyothersentencesuchas P QR
always yields a clause in this case Q R that is shorter than the other clause. When
the unit preference strategy was first tried for propositional inference in 1964 it led to a
dramaticspeedupmakingitfeasibletoprovetheoremsthat couldnotbehandledwithoutthe
preference. Unitresolution is arestricted form of resolution in which every resolution step
must involve a unit clause. Unit resolution is incomplete in general but complete for Horn
clauses. Unitresolution proofson Hornclauses resembleforwardchaining.
The O TT ER theoremprover Organized Techniquesfor Theorem-proving and Effective
Research Mc Cune 1992 uses a form of best-first search. Its heuristic function measures
theweightofeachclausewherelighterclausesarepreferred. Theexactchoiceofheuristic
is up to the user but generally the weight of a clause should be correlated with its size or
difficulty. Unitclauses aretreated aslight; thesearch can thus beseenasageneralization of
theunitpreference strategy.
Set of support: Preferences that try certain resolutions first are helpful but in general it is
S ET OF SU PP OR T
moreeffectivetotrytoeliminatesomepotential resolutions altogether. Forexamplewecan
insist that every resolution step involve at least one element of a special set of clausesthe
set of support. The resolvent is then added into the set of support. If the set of support is
smallrelativetothewholeknowledgebasethesearchspace willbereduced dramatically.
We have to be careful with this approach because a bad choice for the set of support
will make the algorithm incomplete. However if wechoose the set of support S so that the
remainder of the sentences are jointly satisfiable then set-of-support resolution is complete.
Forexample onecan usethenegated query asthe setofsupport ontheassumption that the
original knowledge base is consistent. After all if it is not consistent then the fact that the
queryfollowsfromitisvacuous. Theset-of-support strategyhastheadditionaladvantageof
generating goal-directed prooftreesthatareofteneasyforhumanstounderstand.
Inputresolution: Inthisstrategyeveryresolutioncombinesoneoftheinputsentencesfrom
I NP UT RE SO LU TI ON
the K B or the query with some other sentence. The proof in Figure 9.11 on page 348 uses
only input resolutions and has the characteristic shape of a single spine with single sen-
tences combining onto the spine. Clearly the space of proof trees of this shape is smaller
than the space of all proof graphs. In Horn knowledge bases Modus Ponens is a kind of
inputresolutionstrategybecauseitcombinesanimplicationfromtheoriginal K Bwithsome
othersentences. Thusitisnosurprise thatinputresolution iscomplete forknowledge bases
thatarein Hornformbutincomplete inthegeneral case. The linearresolution strategyisa
L IN EA RR ES OL UT IO N
slightgeneralization thatallows P and Qtoberesolvedtogethereitherif P isintheoriginal
K B orif P isanancestorof Qintheprooftree. Linearresolution iscomplete.
Subsumption: Thesubsumption methodeliminates allsentences thataresubsumed bythat
S UB SU MP TI ON
ismorespecificthananexistingsentenceinthe K B.Forexampleif Pxisinthe K Bthen
thereisnosenseinadding P Aandevenlesssenseinadding P AQ B. Subsumption
helpskeepthe K Bsmallandthushelpskeepthesearchspacesmall.
Practicalusesofresolution theoremprovers
Theorem provers can be applied to the problems involved in the synthesis and verification
S YN TH ES IS
ofbothhardwareandsoftware. Thustheorem-proving research iscarriedoutinthefieldsof
V ER IF IC AT IO N
hardwaredesign programminglanguages andsoftwareengineeringnot justin A I.
In the case of hardware the axioms describe the interactions between signals and cir-
cuit elements. See Section 8.4.2 on page 309 for an example. Logical reasoners designed
specially for verification have been able to verify entire C PUs including their timing prop-
erties Srivas and Bickford 1990. The A UR A theorem prover has been applied to design
circuitsthataremorecompactthananyprevious design Wojciechowski and Wojcik1983.
In the case of software reasoning about programs is quite similar to reasoning about
actions as in Chapter 7: axioms describe the preconditions and effects of each statement.
The formal synthesis of algorithms was one of the first uses of theorem provers as outlined
by Cordell Green 1969a who built on earlier ideas by Herbert Simon 1963. The idea
is to constructively prove a theorem to the effect that there exists a program p satisfying a
D ED UC TI VE certain specification. Although fully automated deductivesynthesis asitiscalled has not
S YN TH ES IS
yet become feasible forgeneral-purpose programming hand-guided deductive synthesis has
beensuccessfulindesigningseveralnovelandsophisticatedalgorithms. Synthesisofspecial-
purpose programssuchasscientificcomputing codeisalso anactiveareaofresearch.
Similartechniquesarenowbeingappliedtosoftwareverificationbysystemssuchasthe
S PI N model checker Holzmann 1997. Forexample the Remote Agent spacecraft control
program was verified before and after flight Havelund et al. 2000. The R SA public key
encryptionalgorithmandthe Boyer Moorestring-matching algorithmhavebeenverifiedthis
way Boyerand Moore1984.
10
C LA SS IC AL P LA NN IN G
Inwhichweseehowanagentcantakeadvantageofthestructureofaproblemto
construct complexplansofaction.
We have defined A I as the study of rational action which means that planningdevising a
plan of action to achieve ones goalsis a critical part of A I. We have seen two examples
of planning agents so far: the search-based problem-solving agent of Chapter 3 and the hy-
brid logical agent of Chapter 7. In this chapter we introduce a representation for planning
problemsthatscalesuptoproblemsthatcouldnotbehandled bythoseearlierapproaches.
Section10.1developsanexpressiveyetcarefullyconstrainedlanguageforrepresenting
planning problems. Section 10.2 shows how forward and backward search algorithms can
takeadvantageofthisrepresentationprimarilythroughaccurateheuristicsthatcanbederived
automaticallyfromthestructureoftherepresentation. Thisisanalogoustothewayinwhich
effectivedomain-independentheuristicswereconstructedforconstraintsatisfactionproblems
in Chapter6. Section10.3showshowadatastructurecalledtheplanninggraphcanmakethe
searchforaplanmoreefficient. Wethendescribe afewoftheotherapproaches toplanning
andconclude bycomparingthevariousapproaches.
This chapter covers fully observable deterministic static environments with single
agents. Chapters 11 and 17 cover partially observable stochastic dynamic environments
withmultipleagents.
The problem-solving agent of Chapter 3 can find sequences of actions that result in a goal
state. Butitdealswithatomicrepresentations ofstates andthus needsgood domain-specific
heuristicstoperformwell. Thehybridpropositional logicalagentof Chapter7canfindplans
without domain-specific heuristics because it uses domain-independent heuristics based on
the logical structure of the problem. But it relies on ground variable-free propositional
inference whichmeansthatitmaybeswampedwhentherearemanyactionsandstates. For
exampleinthewumpusworldthesimpleactionofmovingastepforwardhadtoberepeated
forallfouragentorientations T timestepsandn2 currentlocations.
366
Section10.1. Definitionof Classical Planning 367
In response to this planning researchers have settled on a factored representation
oneinwhichastateoftheworldisrepresentedbyacollectionofvariables. Weusealanguage
called P DD Lthe Planning Domain Definition Language thatallowsustoexpress all 4 Tn2
P DD L
actions with one action schema. There have been several versions of P DD L; we select a
simple version andalter itssyntax tobeconsistent with the rest ofthe book.1 Wenowshow
how P DD Ldescribesthefourthingsweneedtodefineasearchproblem: theinitialstatethe
actionsthatareavailable inastate theresultofapplying anaction andthegoaltest.
Eachstateisrepresentedasaconjunctionoffluentsthataregroundfunctionlessatoms.
For example Poor Unknown might represent the state of a hapless agent and a state
in a package delivery problem might be At Truck Melbourne At Truck Sydney.
Databasesemanticsisused: theclosed-worldassumptionmeansthatanyfluentsthatarenot
mentioned are false and the unique names assumption means that Truck and Truck are
distinct. Thefollowingfluentsarenotallowedinastate: Atxybecauseitisnon-ground
Poor becauseitisanegationand At Father Fred Sydneybecauseitusesafunction
symbol. The representation of states is carefully designed so that a state can be treated
either asaconjunction of fluents which can be manipulated by logical inference oras a set
of fluents which can be manipulated with set operations. The set semantics is sometimes
S ET SE MA NT IC S
easiertodealwith.
Actionsaredescribedbyasetofactionschemasthatimplicitlydefinethe A CT IO NSs
and R ES UL Tsafunctionsneededtodoaproblem-solvingsearch. Wesawin Chapter7that
anysystemforactiondescriptionneedstosolvetheframeproblemtosaywhatchangesand
what stays the same as the result ofthe action. Classical planning concentrates on problems
where mostactions leave mostthings unchanged. Think ofaworld consisting ofabunch of
objects onaflatsurface. Theaction ofnudging anobject causes that object tochange itslo-
cationbyavectorΔ. AconcisedescriptionoftheactionshouldmentiononlyΔ;itshouldnt
havetomentionalltheobjectsthatstayinplace. P DD L doesthatbyspecifying theresultof
anactionintermsofwhatchanges;everything thatstaysthe sameisleftunmentioned.
A set of ground variable-free actions can be represented by a single action schema.
A CT IO NS CH EM A
Theschemaisaliftedrepresentationit liftsthelevelofreasoning frompropositional logic
to a restricted subset of first-order logic. For example here is an action schema for flying a
planefromonelocation toanother:
Action Flypfromto
P RE CO ND:Atpfrom Planep Airportfrom Airportto
E FF EC T:Atpfrom Atpto
The schema consists of the action name a list of all the variables used in the schema a
precondition and an effect. Although we havent said yet how the action schema converts
P RE CO ND IT IO N
into logical sentences think of the variables as being universally quantified. We are free to
E FF EC T
choosewhatevervalueswewanttoinstantiatethevariables. Forexamplehereisoneground
morerestrictedthan P DD L:S TR IP Spreconditionsandgoalscannotcontainnegativeliterals.
actionthatresultsfromsubstituting valuesforallthevariables:
Action Fly P S FO JF K
1
P RE CO ND:At P 1 SF OPlane P 1 Airport S FO Airport J FK
E FF EC T:At P 1 SF OAt P 1 JF K
Theprecondition andeffectofanactionareeachconjunctions ofliteralspositiveornegated
atomic sentences. The precondition defines the states in which the action can be executed
andtheeffect definestheresult ofexecuting theaction. Anaction acan beexecuted instate
sifsentails theprecondition of a. Entailment can also beexpressed withthe setsemantics:
s q iff every positive literal in q is in s and every negated literal in q is not. In formal
notation wesay
a A CT IO NSs s P RE CO NDa
whereanyvariables in aareuniversally quantified. Forexample
pfromto Flypfromto A CT IO NSs
s Atpfrom Planep Airportfrom Airportto
We say that action a is applicable in state s if the preconditions are satisfied by s. When
A PP LI CA BL E
an action schema a contains variables it may have multiple applicable instantiations. For
example with the initial state defined in Figure 10.1 the Fly action can be instantiated as
Fly P S FO JF K or as Fly P J FK SF O both of which are applicable in the initial
state. Ifanactionahasvvariablestheninadomainwithkuniquenamesofobjectsittakes
Ovktimeintheworstcasetofindtheapplicable groundactions.
Sometimeswewanttopropositionalizea P DD Lproblemreplaceeachactionschema
P RO PO SI TI ON AL IZ E
with a set of ground actions and then use apropositional solver such as S AT PL AN to find a
solution. Howeverthisisimpractical whenv andk arelarge.
cid:2
The result of executing action a in state s is defined as a state s which is represented
by the set of fluents formed by starting with s removing the fluents that appear as negative
D EL ET EL IS T
literalsintheactions effectswhatwecallthedeletelistor D ELaandaddingthefluents
A DD LI ST
thatarepositiveliteralsintheactions effectswhatwecalltheaddlistor A DDa:
R ES UL Tsa s D ELa A DDa. 10.1
Forexample withtheaction Fly P S FO JF Kwewouldremove At P S FOandadd
At P J FK. Itisarequirement ofaction schemas that anyvariable intheeffect mustalso
1
appear in the precondition. That way when the precondition is matched against the state s
allthe variables willbebound and R ES UL Tsawilltherefore have only ground atoms. In
otherwordsground statesareclosedunderthe R ES UL T operation.
Alsonotethatthefluentsdonotexplicitlyrefertotimeastheydidin Chapter7. There
weneededsuperscripts fortimeandsuccessor-state axiomsoftheform
Ft1 Action Causes Ft Ft Action Causes Not Ft.
In P DD L the times and states are implicit in the action schemas: the precondition always
referstotimetandtheeffecttotimet1.
Asetofactionschemasservesasadefinitionofaplanningdomain. Aspecificproblem
within the domain is defined with the addition of an initial state and a goal. The initial
Section10.1. Definitionof Classical Planning 369
Init At C S FO At C J FK At P S FO At P J FK
Cargo C Cargo C Plane P Plane P
Airport J FK Airport S FO
Goal At C J FK At C S FO
Action Loadc p a
P RE CO ND:Atc a Atp a Cargoc Planep Airporta
E FF EC T:Atc a Inc p
Action Unloadc p a
P RE CO ND:Inc p Atp a Cargoc Planep Airporta
E FF EC T:Atc a Inc p
Action Flyp from to
P RE CO ND:Atp from Planep Airportfrom Airportto
E FF EC T:Atp from Atp to
Figure10.1 A PD DLdescriptionofanaircargotransportationplanningproblem.
state is a conjunction of ground atoms. As with all states the closed-world assumption is
I NI TI AL ST AT E
used which means that any atoms that are not mentioned are false. The goal is just like a
G OA L
precondition: aconjunction ofliterals positive ornegative thatmaycontain variables such
as Atp S FO Planep. Anyvariables aretreatedasexistentially quantified sothisgoal
is to have any plane at S FO. The problem is solved when wecan find a sequence of actions
thatendinastatesthatentailsthegoal. Forexamplethestate Rich Famous Miserable
entails the goal Rich Famous and the state Plane Plane At Plane S FO entails
thegoal Atp S FO Planep.
Nowwehavedefinedplanningasasearchproblem: wehaveaninitialstatean A CT IO NS
function a R ES UL T function and a goal test. Well look at some example problems before
investigating efficientsearchalgorithms.
Figure10.1showsanaircargotransport problem involving loading andunloading cargoand
flyingitfrom placetoplace. Theproblem canbedefined withthree actions: Load Unload
and Fly. Theactions affecttwopredicates: Incpmeansthatcargo cisinsideplanepand
Atxameansthatobjectxeitherplaneorcargoisatairport a. Notethatsomecaremust
be taken to make sure the At predicates are maintained properly. When a plane flies from
oneairporttoanother allthecargo insidetheplane goeswithit. Infirst-orderlogicitwould
beeasytoquantify overallobjects thatareinside theplane. Butbasic P DD L doesnothave
a universal quantifier so we need a different solution. The approach we use is to say that a
pieceofcargoceasestobe Atanywherewhenitis Inaplane;thecargoonlybecomes Atthe
new airport when it is unloaded. So At really means available for use at a given location.
Thefollowingplanisasolution totheproblem:
Load C P S FO Fly P S FO JF KUnload C P J FK
Load C P J FK Fly P J FK SF OUnload C P S FO.
Finally there is the problem ofspurious actions such as Fly P J FK JF K which should
1
beano-op butwhich hascontradictory effects according tothedefinition theeffect would
include At P J FK At P J FK. It is common to ignore such problems because
they seldom cause incorrect plans to beproduced. Thecorrect approach isto add inequality
preconditions saying thatthe from andto airports mustbedifferent; seeanother exampleof
thisin Figure10.3.
Consider the problem of changing a flat tire Figure 10.2. The goal is to have a good spare
tireproperlymountedontothecarsaxlewheretheinitial statehasaflattireontheaxleand
a good spare tire in the trunk. To keep it simple our version of the problem is an abstract
onewithnostickylugnutsorothercomplications. Therearejustfouractions: removingthe
spare from the trunk removing the flat tire from the axle putting the spare on the axle and
leaving the car unattended overnight. We assume that the car is parked in a particularly bad
neighborhood sothat the effect ofleaving itovernight isthat the tires disappear. Asolution
totheproblem is Remove Flat Axle Remove Spare Trunk Put On Spare Axle.
Init Tire Flat Tire Spare At Flat Axle At Spare Trunk
Goal At Spare Axle
Action Removeobjloc
P RE CO ND:Atobjloc
E FF EC T:Atobjloc Atobj Ground
Action Put Ont Axle
P RE CO ND:Tiret Att Ground At Flat Axle
E FF EC T:Att Ground Att Axle
Action Leave Overnight
P RE CO ND:
E FF EC T:At Spare Ground At Spare Axle At Spare Trunk
At Flat Ground At Flat Axle At Flat Trunk
Figure10.2 Thesimplesparetireproblem.
One of the most famous planning domains is known as the blocks world. This domain
B LO CK SW OR LD
consists of a set of cube-shaped blocks sitting on a table.2 The blocks can be stacked but
only one block can fitdirectly ontop ofanother. Arobot arm canpick up ablock and move
it to another position either on the table or on top of another block. The arm can pick up
onlyoneblockatatimesoitcannotpickupablockthathasanotheroneonit. Thegoalwill
alwaysbetobuildoneormorestacksofblocks specified intermsofwhatblocksareontop
Section10.1. Definitionof Classical Planning 371
Init On A Table On B Table On C A
Block A Block B Block C Clear B Clear C
Goal On A B On B C
Action Movebxy
P RE CO ND:Onbx Clearb Cleary Blockb Blocky
bcid:7x bcid:7y xcid:7y
E FF EC T:Onby Clearx Onbx Cleary
Action Move To Tablebx
P RE CO ND:Onbx Clearb Blockb bcid:7x
E FF EC T:Onb Table Clearx Onbx
Figure10.3 Aplanningproblemintheblocksworld: buildingathree-blocktower. One
solutionisthesequence Move To Table C AMove B Table C Move A Table B.
A
C B
B A C
Start State Goal State
Figure10.4 Diagramoftheblocks-worldproblemin Figure10.3.
of what other blocks. Forexample a goal might be to get block A on B and block B on C
see Figure10.4.
Weuse Onbxtoindicate thatblock bisonxwhere xiseitheranotherblock orthe
table. Theaction formoving block bfrom thetopof xtothe topofy willbe Movebxy.
Nowoneofthepreconditions onmovingbisthatnootherblockbeonit. Infirst-orderlogic
this would be x Onxb or alternatively x Onxb. Basic P DD L does not allow
quantifiers so instead we introduce a predicate Clearx that is true when nothing is on x.
Thecompleteproblem description isin Figure10.3.
Theaction Move movesablockbfrom xtoy ifbothbandy areclear. Afterthemove
ismadebisstillclearbuty isnot. Afirstattemptatthe Move schemais
Action Movebxy
P RE CO ND:Onbx Clearb Cleary
E FF EC T:Onby Clearx Onbx Cleary.
Unfortunately thisdoesnotmaintain Clear properly when xory isthetable. Whenxisthe
Table this action has the effect Clear Table but the table should not become clear; and
wheny Tableithastheprecondition Clear Tablebutthetabledoesnothavetobeclear
for us to move a block onto it. To fix this we do two things. First we introduce another
actiontomoveablockbfromxtothetable:
Action Move To Tablebx
P RE CO ND:Onbx Clearb
E FF EC T:Onb Table Clearx Onbx.
Second we take the interpretation of Clearx to be there is a clear space on x to hold a
block. Underthisinterpretation Clear Tablewillalwaysbetrue. Theonlyproblemisthat
nothing prevents the planner from using Movebx Table instead of Move To Tablebx.
Wecouldlivewiththisproblemitwillleadtoalarger-than-necessary searchspacebutwill
notleadtoincorrectanswersorwecouldintroducethepredicate Block andadd Blockb
Blockytotheprecondition of Move.
In this subsection we consider the theoretical complexity of planning and distinguish two
decision problems. Plan S AT is the question of whether there exists any plan that solves a
P LA NS AT
planning problem. Bounded Plan S AT asks whether there is a solution of length k or less;
B OU ND ED PL AN SA T
thiscanbeusedtofindanoptimalplan.
Thefirstresult isthat bothdecision problems aredecidable forclassical planning. The
prooffollowsfromthefactthatthenumberofstatesisfinite. Butifweaddfunctionsymbols
to the language then the number of states becomes infinite and Plan S AT becomes only
semidecidable: analgorithmexiststhatwillterminatewiththecorrectanswerforanysolvable
problem but may not terminate on unsolvable problems. The Bounded Plan S AT problem
remains decidable even in the presence of function symbols. For proofs of the assertions in
thissection see Ghallabetal.2004.
Both Plan S ATand Bounded Plan S ATareinthecomplexity class P SP AC Eaclassthat
is larger and hence more difficult than N P and refers to problems that can be solved by a
deterministic Turing machine with a polynomial amount of space. Even if we make some
rather severe restrictions the problems remain quite difficult. For example if we disallow
negative effects both problems are still N P-hard. However if we also disallow negative
preconditions Plan S ATreduces totheclass P.
These worst-case results may seem discouraging. We can take solace in the fact that
agents are usually not asked to find plans for arbitrary worst-case problem instances but
ratherareaskedforplansinspecificdomainssuchasblocks-worldproblemswithnblocks
which can be much easier than the theoretical worst case. For many domains including the
blocks world and the air cargo world Bounded Plan S AT is N P-complete while Plan S AT is
in P;inotherwordsoptimalplanningisusuallyhardbutsub-optimalplanningissometimes
easy. To do well on easier-than-worst-case problems we will need good search heuristics.
Thats the true advantage of the classical planning formalism: it has facilitated the develop-
ment of very accurate domain-independent heuristics whereas systems based on successor-
stateaxiomsinfirst-orderlogichavehadlesssuccess incomingupwithgoodheuristics.
Section10.2. Algorithmsfor Planningas State-Space Search 373
Nowweturnourattentiontoplanningalgorithms. Wesawhowthedescription ofaplanning
problem defines a search problem: we can search from the initial state through the space
of states looking for a goal. One of the nice advantages of the declarative representation of
actionschemasisthatwecanalsosearchbackwardfromthegoallookingfortheinitialstate.
Figure10.5comparesforwardandbackwardsearches.
Nowthatwehaveshownhowaplanning problem mapsintoasearch problem wecansolve
planning problems with any of the heuristic search algorithms from Chapter 3 or a local
search algorithm from Chapter 4 provided we keep track of the actions used to reach the
goal. From the earliest days of planning research around 1961 until around 1998 it was
assumed that forward state-space search was too inefficient to be practical. It is not hard to
comeupwithreasonswhy.
First forward search is prone to exploring irrelevant actions. Consider the noble task
of buying acopy of A I:A Modern Approach from an online bookseller. Suppose there is an
At P B
1
At P A
Fly P 1 A B 2
At P A
1
a
At P A
2
Fly P A B At P A
At P B
2
At P A
1
At P B Fly P A B
At P B
b 1
At P B
2
At P B Fly P A B
At P A
2
through the space of states starting in the initial state and using the problems actions to
search forward for a member of the set of goal states. b Backward regression search
throughsetsofrelevantstatesstartingatthesetofstatesrepresentingthegoalandusingthe
inverseoftheactionstosearchbackwardfortheinitialstate.
actionschema Buyisbnwitheffect Ownisbn. I SB Nsare10digitssothisactionschema
represents 10 billion ground actions. An uninformed forward-search algorithm would have
tostartenumerating these10billionactions tofindonethat leadstothegoal.
Secondplanningproblemsoftenhavelargestatespaces. Consideranaircargoproblem
with10airports whereeachairporthas5planesand20piecesofcargo. Thegoalistomove
allthecargo atairport Atoairport B. Thereisasimplesolution totheproblem: loadthe20
piecesofcargo intooneoftheplanes at Aflytheplaneto Bandunload thecargo. Finding
the solution can be difficult because the average branching factor is huge: each of the 50
planes can fly to 9 other airports and each of the 200 packages can be either unloaded if
it is loaded or loaded into any plane at its airport if it is unloaded. So in any state there
is a minimum of 450 actions when all the packages are at airports with no planes and a
maximumof10450whenallpackagesandplanesareatthesameairport. Onaveragelets
saythereareabout2000possible actionsperstatesothesearchgraphuptothedepthofthe
obvioussolution hasabout200041 nodes.
Clearly even this relatively small problem instance is hopeless without an accurate
heuristic. Althoughmanyreal-worldapplications ofplanninghavereliedondomain-specific
heuristicsitturnsoutasweseein Section10.2.3thatstrongdomain-independent heuristics
canbederivedautomatically; thatiswhatmakesforwardsearchfeasible.
In regression search we start at the goal and apply the actions backward until we find a
sequence ofstepsthatreaches theinitialstate. Itiscalled relevant-states search because we
R EL EV AN T-S TA TE S
only consider actions that arerelevant tothegoal orcurrent state. Asinbelief-state search
Section4.4thereisasetofrelevantstatestoconsiderateachstepnotjustasingle state.
Westartwiththegoalwhichisaconjunctionofliteralsformingadescriptionofasetof
statesforexamplethegoal Poor Famous describesthosestatesinwhich Poor isfalse
Famous is true and any other fluent can have any value. If there are n ground fluents in a
domainthenthereare 2n groundstateseachfluentcanbetrueorfalse but 3n descriptions
ofsetsofgoalstateseachfluentcanbepositive negativeornotmentioned.
In general backward search works only when we know how to regress from a state
description to the predecessor state description. Forexample it is hard to search backwards
forasolutiontothen-queensproblembecausethereisnoeasywaytodescribethestatesthat
areonemoveawayfromthe goal. Happily the P DD L representation wasdesigned tomake
iteasytoregressactionsifadomaincanbeexpressedin P DD Lthenwecandoregression
search onit. Givenaground goal description g and aground action atheregression from g
cid:2
overagivesusastatedescription g definedby
gcid:2 g A DDa Preconda.
That is the effects that were added by the action need not have been true before and also
the preconditions must have held before or else the action could not have been executed.
Note that D ELa does not appear in the formula; thats because while we know the fluents
in D ELa are no longer true after the action we dont know whether or not they were true
before sotheresnothing tobesaidaboutthem.
Section10.2. Algorithmsfor Planningas State-Space Search 375
Togetthefulladvantage ofbackwardsearchweneedtodealwithpartiallyuninstanti-
atedactionsandstatesnotjustgroundones. Forexamplesupposethegoalistodeliveraspe-
cid:2
cificpieceofcargoto S FO:At C S FO. Thatsuggeststheaction Unload C p S FO:
cid:2
Action Unload C p S FO
2
P RE CO ND:In C 2pcid:2 Atpcid:2 S FO Cargo C 2 Planepcid:2 Airport S FO
E FF EC T:At C 2 SF OIn C 2pcid:2 .
cid:2
Notethatwehave standardized variable nameschanging ptop inthiscase sothatthere
will be no confusion between variable names if we happen to use the same action schema
twice in a plan. The same approach was used in Chapter 9 for first-order logical inference.
This represents unloading the package from an unspecified plane at S FO;any plane will do
but we need not say which one now. We can take advantage of the power of first-order
representations: asingledescription summarizesthepossibility ofusinganyoftheplanesby
cid:2
implicitlyquantifying overp. Theregressedstatedescription is
gcid:2 In C pcid:2 Atpcid:2 S FO Cargo C Planepcid:2 Airport S FO.
Thefinalissueisdecidingwhichactionsarecandidates toregressover. Intheforwarddirec-
tion wechose actions that were applicablethose actions that could be the next step in the
plan. Inbackward search wewantactions that are relevantthose actions thatcould bethe
R EL EV AN CE
laststepinaplanleadinguptothecurrent goalstate.
Foran action to be relevant to a goal it obviously must contribute to the goal: at least
oneoftheactionseffectseitherpositiveornegativemustunifywithanelementofthegoal.
What is less obvious is that the action must not have any effect positive or negative that
negates an element of the goal. Now if the goal is A B C and an action has the effect
A BCthenthereisacolloquialsenseinwhichthatactionisveryrelevanttothegoalit
gets ustwo-thirds ofthe waythere. Butitisnot relevant inthe technical sense defined here
because this action could not be the final step of a solutionwe would always need at least
onemoresteptoachieve C.
Given the goal At C S FO several instantiations of Unload are relevant: we could
2
chose any specific plane to unload from or we could leave the plane unspecified by using
cid:2
theaction Unload C p S FO. Wecan reduce thebranching factorwithout ruling outany
2
solutions byalways using theaction formed bysubstituting the mostgeneral unifierinto the
standardized actionschema.
As another example consider the goal Own0136042597 given an initial state with
10billion I SB Nsandthesingleactionschema
A Action Buyi P RE CO ND:I SB Ni E FF EC T:Owni.
As we mentioned before forward search without a heuristic would have to start enumer-
ating the 10 billion ground Buy actions. But with backward search we would unify the
cid:2
goal Own0136042597 with the standardized effect Owni yielding the substitution
θ icid:2 0136042597. Then we would regress over the action Substθ Acid:2 to yield the
predecessor state description I SB N0136042597. This is part of and thus entailed by the
initialstatesowearedone.
We can make this more formal. Assume a goal description g which contains a goal
cid:2 cid:2
literalg andanactionschema Athatisstandardized toproduce A. If A hasaneffectliteral
i
cid:2 cid:2 cid:2 cid:2
e
j
where Unifyg ie jθ and wherewedefine a S UB STθ Aandif there isnoeffect
cid:2 cid:2
ina thatisthenegationofaliteralingthena isarelevantactiontowards g.
Backwardsearch keepsthebranching factorlowerthan forwardsearch formostprob-
lem domains. However the fact that backward search uses state sets rather than individual
states makes it harder to come up with good heuristics. That is the main reason why the
majorityofcurrentsystemsfavorforwardsearch.
Neither forward nor backward search is efficient without a good heuristic function. Recall
from Chapter 3 that a heuristic function hs estimates the distance from a state s to the
goal and that if we can derive an admissible heuristic for this distanceone that does not
""
overestimatethen we can use A search to find optimal solutions. An admissible heuristic
can be derived by defining a relaxed problem that is easier to solve. The exact cost of a
solution tothiseasierproblem thenbecomestheheuristicfortheoriginal problem.
By definition there is no way to analyze an atomic state and thus it it requires some
ingenuity by a human analyst to define good domain-specific heuristics for search problems
with atomic states. Planning uses a factored representation for states and action schemas.
That makes it possible to define good domain-independent heuristics and for programs to
automatically applyagooddomain-independent heuristic foragivenproblem.
Think of a search problem as a graph where the nodes are states and the edges are
actions. The problem is to find a path connecting the initial state to a goal state. There are
two ways we can relax this problem to make it easier: by adding more edges to the graph
making it strictly easier to find a path or by grouping multiple nodes together forming an
abstraction ofthestatespacethathasfewerstates andthusiseasiertosearch.
We look first at heuristics that add edges to the graph. For example the ignore pre-
I GN OR E
conditions heuristicdrops allpreconditions from actions. Everyaction becomes applicable
P RE CO ND IT IO NS
H EU RI ST IC
in every state and any single goal fluent can be achieved in one step if there is an applica-
ble actionif not the problem is impossible. This almost implies that the number of steps
required to solve the relaxed problem is the number of unsatisfied goalsalmost but not
quite because 1 some action may achieve multiple goals and 2 some actions may undo
theeffects ofothers. Formanyproblems anaccurate heuristic isobtained byconsidering 1
and ignoring 2. First we relax the actions by removing all preconditions and all effects
except those that are literals in the goal. Then we count the minimum number of actions
required such that the union of those actions effects satisfies the goal. This is an instance
S ET-C OV ER of the set-cover problem. There is one minor irritation: the set-cover problem is N P-hard.
P RO BL EM
Fortunately a simple greedy algorithm is guaranteed to return a set covering whose size is
within a factor of logn of the true minimum covering where n is the number of literals in
thegoal. Unfortunately thegreedyalgorithm losestheguarantee ofadmissibility.
Itisalsopossibletoignoreonlyselectedpreconditionsofactions. Considerthesliding-
block puzzle 8-puzzle or 15-puzzle from Section 3.2. We could encode this as a planning
Section10.2. Algorithmsfor Planningas State-Space Search 377
problem involving tileswithasingleschema Slide:
Action Slidets s
P RE CO ND:Onts 1 Tilet Blanks 2 Adjacents 1s 2
E FF EC T:Onts 2 Blanks 1 Onts 1 Blanks 2
As we saw in Section 3.6 if we remove the preconditions Blanks Adjacents s
then any tile can move in one action to any space and we get the number-of-misplaced-tiles
heuristic. Ifweremove Blanks thenwegetthe Manhattan-distance heuristic. Itiseasyto
2
see how these heuristics could be derived automatically from the action schema description.
Theeaseofmanipulatingtheschemasisthegreatadvantageofthefactoredrepresentation of
planning problems ascomparedwiththeatomicrepresentation ofsearchproblems.
I GN OR ED EL ET E Another possibility is the ignore delete lists heuristic. Assume for a moment that all
L IS TS
goalsandpreconditions contain onlypositiveliterals3 Wewanttocreatearelaxedversionof
theoriginalproblemthatwillbeeasiertosolveandwherethelengthofthesolutionwillserve
asagoodheuristic. Wecandothatbyremovingthedeletelistsfromallactionsi.e.removing
allnegativeliteralsfromeffects. Thatmakesitpossibletomakemonotonicprogresstowards
thegoalnoactionwilleverundoprogressmadebyanotheraction. Itturnsoutitisstill N P-
hardtofindtheoptimalsolution tothisrelaxed problem but anapproximate solution canbe
found in polynomial time by hill-climbing. Figure 10.6 diagrams part of the state space for
two planning problems using the ignore-delete-lists heuristic. The dots represent states and
the edges actions and theheight ofeach dot abovethe bottom plane represents the heuristic
value. States onthe bottom plane are solutions. Inboth these problems there isawidepath
tothegoal. Therearenodeadendssononeedforbacktracking; asimplehillclimbingsearch
willeasilyfindasolution totheseproblemsalthough itmaynotbeanoptimalsolution.
The relaxed problems leave us with a simplifiedbut still expensiveplanning prob-
states ormore and relaxing the actions does nothing to reduce the number of states. There-
fore we now look at relaxations that decrease the number of states by forming a state ab-
stractiona many-to-one mapping from states in the ground representation of the problem
S TA TE AB ST RA CT IO N
totheabstract representation.
The easiest form of state abstraction is to ignore some fluents. For example consider
an air cargo problem with 10 airports 50 planes and 200 pieces of cargo. Each plane can
be at one of 10 airports and each package can be either in one of the planes or unloaded at
one of theairports. Sothere are 5010 2005010 10155 states. Now consider a particular
problem inthat domaininwhichithappens thatallthepackages areatjust 5oftheairports
andallpackagesatagivenairporthavethesamedestination. Thenausefulabstractionofthe
problemistodropallthe Atfluentsexceptfortheonesinvolving oneplaneandonepackage
at each of the 5 airports. Now there are only 510 5510 1017 states. A solution in this
abstract state space will be shorter than a solution in the original space and thus will be an
admissible heuristic and the abstract solution is easy to extend to a solution to the original
problem byaddingadditional Load and Unload actions.
inagoalorpreconditionwithanewpositiveliteral Pcid:3.
Figure10.6 Twostatespacesfromplanningproblemswiththeignore-delete-listsheuris-
tic. Theheightabovethebottomplaneistheheuristicscoreofastate;statesonthebottom
planearegoals. Therearenolocalminimasosearchforthegoalisstraightforward. From
Hoffmann2005.
Akeyideaindefiningheuristicsisdecomposition: dividingaproblemintopartssolv-
D EC OM PO SI TI ON
S UB GO AL ing each part independently and then combining the parts. The subgoal independence as-
I ND EP EN DE NC E
sumption is that the cost of solving a conjunction of subgoals is approximated by the sum
of the costs of solving each subgoal independently. The subgoal independence assumption
canbeoptimisticorpessimistic. Itisoptimisticwhentherearenegativeinteractions between
the subplans for each subgoalfor example when an action in one subplan deletes a goal
achieved by another subplan. It is pessimistic and therefore inadmissible when subplans
containredundantactionsforinstancetwoactionsthatcouldbereplacedbyasingleaction
inthemergedplan.
Supposethegoalisasetoffluents Gwhichwedivideintodisjointsubsets G ...G .
Wethen findplans P ...P that solve therespective subgoals. Whatisanestimate ofthe
costoftheplanforachievingallof G? Wecanthinkofeach Cost P asaheuristicestimate
i
andweknowthatifwecombineestimatesbytakingtheirmaximumvaluewealwaysgetan
admissible heuristic. So max i C OS TP i is admissible and sometimes it is exactly correct:
it could be that P serendipitously achieves all the G . But in most cases in practice the
estimateistoolow. Couldwesumthecostsinstead? Formanyproblemsthatisareasonable
estimatebutitisnotadmissible. Thebestcaseiswhenwecandeterminethat G and G are
i j
independent. Iftheeffectsof P leaveallthepreconditions andgoalsof P unchanged then
i j
theestimate C OS TP i C OS TP jisadmissible andmoreaccuratethanthemaxestimate.
Weshowin Section10.3.1thatplanning graphscanhelpprovidebetterheuristic estimates.
Itisclearthat thereisgreat potential forcutting downthe search space byforming ab-
stractions. The trick is choosing the right abstractions and using them in a way that makes
thetotalcostdefininganabstraction doinganabstractsearchandmappingtheabstraction
back to the original problemless than the cost of solving the original problem. The tech-
Section10.3. Planning Graphs 379
niques of pattern databases from Section 3.6.3 can be useful because the cost of creating
thepatterndatabase canbeamortizedovermultipleproblem instances.
Anexampleofasystemthatmakesuseofeffectiveheuristicsis F For F AS TF OR WA RD
Hoffmann 2005 a forward state-space searcher that uses the ignore-delete-lists heuristic
estimating the heuristic with the help of a planning graph see Section 10.3. F F then uses
hill-climbing search modifiedtokeeptrack oftheplanwiththeheuristic tofindasolution.
Whenithitsaplateauorlocalmaximumwhennoactionleadstoastatewithbetterheuristic
scorethen F F uses iterative deepening search until it finds a state that is better or it gives
upandrestartshill-climbing.
All of the heuristics we have suggested can suffer from inaccuracies. This section shows
how a special data structure called a planning graph can be used to give better heuristic
P LA NN IN GG RA PH
estimates. These heuristics can be applied to any of the search techniques we have seen so
far. Alternatively wecansearch forasolution overthespace formedbytheplanning graph
usinganalgorithm called G RA PH PL AN.
Aplanningproblemasksifwecanreachagoalstatefromtheinitialstate. Supposewe
aregivenatreeofallpossible actions fromtheinitial statetosuccessorstates andtheirsuc-
cessors andsoon. Ifweindexedthistreeappropriately we couldanswertheplanning ques-
tion can wereach state G from state S immediately just by looking it up. Ofcourse the
0
tree is of exponential size so this approach is impractical. A planning graph is polynomial-
size approximation to this tree that can be constructed quickly. The planning graph cant
answer definitively whether G is reachable from S but it can estimate how many steps it
0
takestoreach G. Theestimateisalwayscorrectwhenitreportsthegoalisnotreachable and
itneveroverestimates thenumberofstepssoitisanadmissible heuristic.
Aplanninggraphisadirectedgraphorganizedinto levels: firstalevel S fortheinitial
L EV EL 0
stateconsisting ofnodesrepresenting eachfluentthatholdsin S ;thenalevel A consisting
of nodes for each ground action that might be applicable in S ; then alternating levels S
followedby A ;untilwereachatermination condition tobediscussed later.
i
Roughly speaking S contains all the literals that could hold at time i depending on
i
theactions executed atpreceding timesteps. Ifitispossible thateither P or P could hold
then both will be represented in S . Also roughly speaking A contains all the actions that
i i
could have their preconditions satisfied at time i. We say roughly speaking because the
planning graph records only a restricted subset of the possible negative interactions among
actions; therefore aliteral mightshow upatlevel S whenactually itcould notbetrueuntil
j
a later level if at all. A literal will never show up too late. Despite the possible error the
level j at which a literal first appears is a good estimate of how difficult it is to achieve the
literalfromtheinitialstate.
Planning graphs work only for propositional planning problemsones with no vari-
ables. As we mentioned on page 368 it is straightforward to propositionalize a set of ac-
Init Have Cake
Goal Have Cake Eaten Cake
Action Eat Cake
P RE CO ND:Have Cake
E FF EC T:Have Cake Eaten Cake
Action Bake Cake
P RE CO ND:Have Cake
E FF EC T:Have Cake
Figure10.7 Thehavecakeandeatcaketooproblem.
S A S A S
Bake Cake
Have Cake Have Cake Have Cake
Have Cake Have Cake
Eat Cake Eat Cake
Eaten Cake Eaten Cake
Eaten Cake Eaten Cake Eaten Cake
Figure10.8 Theplanninggraphforthehavecakeandeatcaketooproblemuptolevel
S . Rectangles indicate actions small squares indicate persistence actions and straight
2
linesindicatepreconditionsandeffects. Mutexlinksareshownascurvedgraylines. Notall
mutexlinksareshownbecausethegraphwouldbetoocluttered. Ingeneraliftwoliterals
are mutex at Si then the persistence actions for those literals will be mutex at Ai and we
neednotdrawthatmutexlink.
tion schemas. Despite the resulting increase inthe size ofthe problem description planning
graphshaveprovedtobeeffectivetoolsforsolvinghardplanningproblems.
graph. Each action at level A is connected to its preconditions at S and its effects at S .
i i i1
So a literal appears because an action caused it but we also want to say that a literal can
P ER SI ST EN CE persist ifnoaction negates it. Thisisrepresented bya persistence action sometimes called
A CT IO N
ano-op. Foreveryliteral Cweaddtotheproblemapersistence actionwithprecondition C
and effect C. Level A in Figure 10.8 shows one real action Eat Cake along with two
0
persistence actionsdrawnassmallsquareboxes.
Level A contains all the actions that could occur in state S but just as important it
recordsconflictsbetweenactionsthatwouldpreventthemfromoccurringtogether. Thegray
linesin Figure10.8indicate mutualexclusionormutexlinks. Forexample Eat Cakeis
M UT UA LE XC LU SI ON
mutually exclusive with the persistence of either Have Cake or Eaten Cake. We shall
M UT EX
seeshortlyhowmutexlinksarecomputed.
Level S containsalltheliteralsthatcouldresultfrompickinganysubsetoftheactions
1
in A as well as mutex links gray lines indicating literals that could not appear together
0
regardless ofthechoiceofactions. Forexample Have Cakeand Eaten Cakearemutex:
Section10.3. Planning Graphs 381
depending on the choice of actions in A either but not both could be the result. In other
0
words S represents a belief state: a set of possible states. The members of this set are all
1
subsetsoftheliteralssuchthatthereisnomutexlinkbetweenanymembersofthesubset.
Wecontinue inthiswayalternating betweenstatelevel S andaction level A untilwe
i i
reach apoint wheretwoconsecutive levels areidentical. At thispoint wesaythatthegraph
hasleveledoff. Thegraphin Figure10.8levelsoffat S .
L EV EL ED OF F 2
Whatweendupwithisastructurewhereevery A levelcontainsalltheactionsthatare
i
applicablein S alongwithconstraintssayingthattwoactionscannotbothbeexecutedatthe
i
samelevel. Every S level contains allthe literals that could result from anypossible choice
i
of actions in A i1 along with constraints saying which pairs of literals are not possible.
It is important to note that the process of constructing the planning graph does not require
choosingamongactionswhichwouldentailcombinatorialsearch. Insteaditjustrecordsthe
impossibility ofcertainchoices usingmutexlinks.
Wenowdefinemutexlinksforbothactionsandliterals. Amutexrelationholdsbetween
twoactions atagivenlevelifanyofthefollowingthreeconditions holds:
Inconsistent effects: oneactionnegatesaneffectoftheother. Forexample Eat Cake
and the persistence of Have Cake have inconsistent effects because they disagree on
theeffect Have Cake.
Interference: one of the effects of one action is the negation of a precondition of the
other. Forexample Eat Cakeinterfereswiththepersistenceof Have Cakebynegat-
ingitsprecondition.
Competing needs: one of the preconditions of one action is mutually exclusive with a
preconditionoftheother. Forexample Bake Cakeand Eat Cakearemutexbecause
theycompeteonthevalueofthe Have Cakeprecondition.
Amutexrelationholdsbetweentwoliteralsatthesamelevelifoneisthenegationoftheother
or if each possible pair of actions that could achieve the two literals is mutually exclusive.
This condition is called inconsistent support. Forexample Have Cake and Eaten Cake
are mutex in S because the only way of achieving Have Cake the persistence action is
1
mutex with the only way of achieving Eaten Cake namely Eat Cake. In S the two
2
literals are not mutex because there are new ways of achieving them such as Bake Cake
andthepersistence of Eaten Cakethatarenotmutex.
A planning graph is polynomial in the size of the planning problem. For a planning
problem with l literals and a actions each S has no more than l nodes and l2 mutex links
i
and each A has nomore than al nodes including theno-ops al2 mutex links and
i
2al l precondition and effect links. Thus an entire graph with n levels has a size of
Onal2. Thetimetobuildthegraphhasthesamecomplexity.
Aplanning graphonceconstructed isarichsource ofinformation abouttheproblem. First
ifanygoalliteralfailstoappearinthefinallevelofthegraphthentheproblemisunsolvable.
Second we can estimate the cost of achieving any goal literal g from state s as the level at
i
which g first appears in the planning graph constructed from initial state s. Wecall this the
i
levelcostofg . In Figure10.8 Have Cakehaslevelcost0and Eaten Cakehaslevelcost
L EV EL CO ST i
1. It is easy to show Exercise 10.10 that these estimates are admissible for the individual
goals. The estimate might not always be accurate however because planning graphs allow
several actions at each level whereas the heuristic counts just the level and not the number
S ER IA LP LA NN IN G of actions. For this reason it is common to use a serial planning graph for computing
G RA PH
heuristics. A serial graph insists that only one action can actually occur at any given time
step; thisisdonebyadding mutexlinks between everypairof nonpersistence actions. Level
costsextractedfromserialgraphsareoftenquitereasonable estimatesofactualcosts.
Toestimate the cost of a conjunction of goals there are three simple approaches. The
max-levelheuristic simplytakesthemaximumlevelcostofanyofthegoals;thisisadmissi-
M AX-L EV EL
blebutnotnecessarily accurate.
The level sum heuristic following the subgoal independence assumption returns the
L EV EL SU M
sum of the level costs of the goals; this can be inadmissible but works well in practice
for problems that are largely decomposable. It is much more accurate than the number-
of-unsatisfied-goals heuristic from Section 10.2. For our problem the level-sum heuristic
estimate for the conjunctive goal Have Cake Eaten Cake will be 01 1 whereas
the correct answeris 2 achieved by the plan Eat Cake Bake Cake. That doesnt seem
so bad. A more serious error is that if Bake Cake were not in the set of actions then the
estimatewouldstillbe1wheninfacttheconjunctive goalwouldbeimpossible.
Finally the set-level heuristic findsthe level at which all the literals inthe conjunctive
S ET-L EV EL
goal appear in the planning graph without any pair of them being mutually exclusive. This
heuristic gives the correct values of 2 for our original problem and infinity for the problem
without Bake Cake. It is admissible it dominates the max-level heuristic and it works
extremelywellontasksinwhichthereisagooddealofinteraction amongsubplans. Itisnot
perfect ofcourse; forexampleitignoresinteractions amongthreeormoreliterals.
Asatoolforgeneratingaccurateheuristicswecanviewtheplanninggraphasarelaxed
problem that is efficiently solvable. To understand the nature of the relaxed problem we
need to understand exactly what it means for a literal g to appear at level S in the planning
i
graph. Ideally wewould like itto beaguarantee that there exists aplan with iaction levels
that achieves g and also that if g does not appear there is no such plan. Unfortunately
makingthatguaranteeisasdifficultassolvingtheoriginalplanningproblem. Sotheplanning
graph makes the second half of the guarantee if g does not appear there is no plan but
if g does appear then all the planning graph promises is that there is a plan that possibly
achieves g and has no obvious flaws. An obvious flaw is defined as a flaw that can be
detected by considering two actions or two literals at a timein other words by looking at
themutexrelations. Therecould bemoresubtle flawsinvolving three four ormoreactions
but experience has shown that it is not worth the computational effort to keep track of these
possibleflaws. Thisissimilartoalessonlearnedfromconstraintsatisfactionproblemsthat
itisoftenworthwhiletocompute2-consistencybeforesearchingforasolutionbutlessoften
worthwhiletocompute3-consistency orhigher. Seepage211.
Oneexampleofanunsolvableproblemthatcannotberecognizedassuchbyaplanning
graphistheblocks-world problem wherethegoalistogetblock Aon B B on Cand C on
A. Thisisanimpossiblegoal;atowerwiththebottomontopofthetop. Butaplanninggraph
Section10.3. Planning Graphs 383
cannot detect the impossibility because any twoof the three subgoals are achievable. There
arenomutexesbetweenanypairofliteralsonlybetweenthe threeasawhole. Todetectthat
thisproblem isimpossible wewouldhavetosearchovertheplanning graph.
Thissubsectionshowshowtoextractaplandirectlyfromtheplanninggraphratherthanjust
using the graph to provide aheuristic. The G RA PH PL AN algorithm Figure 10.9 repeatedly
adds alevel to aplanning graph with E XP AN D-G RA PH. Once all the goals show up as non-
mutexinthegraph G RA PH PL AN calls E XT RA CT-S OL UT IO N tosearchforaplanthatsolves
the problem. If that fails it expands another level and tries again terminating with failure
whenthereisnoreasontogoon.
function G RA PH PL ANproblemreturnssolutionorfailure
graph I NI TI AL-P LA NN IN G-G RA PHproblem
goals C ON JU NC TSproblem.G OA L
nogoodsanemptyhashtable
fortl 0todo
ifgoals allnon-mutexin Stofgraph then
solution E XT RA CT-S OL UT IO Ngraphgoals N UM LE VE LSgraphnogoods
ifsolution cid:7failure thenreturnsolution
ifgraph andnogoods havebothleveledoffthenreturnfailure
graph E XP AN D-G RA PHgraphproblem
Figure10.9 The G RA PH PL AN algorithm. G RA PH PL AN calls E XP AN D-G RA PH toadda
leveluntileitherasolutionisfoundby E XT RA CT-S OL UT IO Nornosolutionispossible.
Letusnowtracetheoperationof G RA PH PL ANonthesparetireproblemfrompage370.
The graph is shown in Figure 10.10. The first line of G RA PH PL AN initializes the planning
graph to a one-level S graph representing the initial state. The positive fluents from the
0
problem descriptions initial state areshown asaretherelevant negative fluents. Notshown
aretheunchanging positiveliteralssuchas Tire Spareandtheirrelevantnegativeliterals.
Thegoal At Spare Axleisnotpresent in S 0soweneed notcall E XT RA CT-S OL UT IO N
wearecertain that thereisnosolution yet. Instead E XP AN D-G RA PH adds into A
0
thethree
actionswhosepreconditionsexistatlevel S i.e.alltheactionsexcept Put On Spare Axle
0
alongwithpersistence actionsforalltheliteralsin S . Theeffectsoftheactionsareaddedat
0
level S 1. E XP AN D-G RA PH thenlooksformutexrelations andaddsthemtothegraph.
At Spare Axleisstillnotpresentin S 1soagainwedonotcall E XT RA CT-S OL UT IO N.
Wecall E XP AN D-G RA PH again adding A
1
and S
1
and giving us the planning graph shown
in Figure10.10. Nowthatwehavethefullcomplementofactions itisworthwhiletolookat
someoftheexamplesofmutexrelations andtheircauses:
Inconsistent effects: Remove Spare Trunk is mutex with Leave Overnight because
onehastheeffect At Spare Groundandtheotherhasitsnegation.
S A S A S
At Spare Trunk At Spare Trunk At Spare Trunk
Remove Spare Trunk
Remove Spare Trunk
At Spare Trunk At Spare Trunk
Remove Flat Axle Remove Flat Axle
At Flat Axle At Flat Axle At Flat Axle
Leave Overnight At Flat Axle At Flat Axle
Leave Overnight
At Spare Axle At Spare Axle At Spare Axle
Put On Spare Axle At Spare Axle
At Flat Ground At Flat Ground At Flat Ground
At Flat Ground At Flat Ground
At Spare Ground At Spare Ground At Spare Ground
At Spare Ground At Spare Ground
Figure10.10 The planninggraphforthe spare tire problemafterexpansionto level S .
2
Mutexlinksareshownasgraylines. Notalllinksareshownbecausethegraphwouldbetoo
clutteredifweshowedthemall. Thesolutionisindicatedbyboldlinesandoutlines.
Interference: Remove Flat Axleismutexwith Leave Overnight becauseonehasthe
precondition At Flat Axleandtheotherhasitsnegationasaneffect.
Competing needs: Put On Spare Axle is mutex with Remove Flat Axle because
onehas At Flat Axleasaprecondition andtheotherhasitsnegation.
Inconsistentsupport: At Spare Axleismutexwith At Flat Axlein S becausethe
2
only way of achieving At Spare Axle is by Put On Spare Axle and that is mutex
withthepersistence actionthatistheonlywayofachieving At Flat Axle. Thusthe
mutexrelations detect theimmediate conflict that arises from trying toput twoobjects
inthesameplaceatthesametime.
This time when wego back to the start of the loop all the literals from the goal are present
in S and none of them is mutex with any other. That means that a solution might exist
2
and E XT RA CT-S OL UT IO N will try to find it. We can formulate E XT RA CT-S OL UT IO N as a
Boolean constraint satisfaction problem C SP where the variables are the actions at each
levelthevaluesforeachvariableareinoroutoftheplanandtheconstraintsarethemutexes
andtheneedtosatisfyeachgoalandprecondition.
Alternativelywecandefine E XT RA CT-S OL UT IO Nasabackwardsearchproblemwhere
each stateinthesearch contains apointertoalevelintheplanning graph and asetofunsat-
isfiedgoals. Wedefinethissearchproblem asfollows:
The initial state is the last level of the planning graph S along with the set of goals
n
fromtheplanning problem.
The actions available in a state at level S are to select any conflict-free subset of the
i
actions in A i1 whose effects coverthe goals in thestate. Theresulting state has level
S i1 and has as its set of goals the preconditions for the selected set of actions. By
conflictfreewemeanasetofactionssuchthatnotwoofthemaremutexandnotwo
oftheirpreconditions aremutex.
Section10.3. Planning Graphs 385
Thegoalistoreachastateatlevel S suchthatallthegoalsaresatisfied.
0
Thecostofeachactionis1.
Forthisparticularproblemwestartat S withthegoal At Spare Axle. Theonlychoicewe
2
haveforachievingthegoalsetis Put On Spare Axle. Thatbringsustoasearchstateat S
1
with goals At Spare Ground and At Flat Axle. The former can be achieved only by
Remove Spare Trunk and the latter by either Remove Flat Axle or Leave Overnight.
But Leave Overnight ismutexwith Remove Spare Trunksotheonlysolutionistochoose
Remove Spare Trunkand Remove Flat Axle. Thatbringsustoasearchstateat S with
0
the goals At Spare Trunk and At Flat Axle. Both of these are present in the state so
we have a solution: the actions Remove Spare Trunk and Remove Flat Axle in level
A followedby Put On Spare Axlein A .
In the case where E XT RA CT-S OL UT IO N fails to find a solution for a set of goals at
a given level we record the levelgoals pair as a no-good just as we did in constraint
learningfor C SPspage220. Whenever E XT RA CT-S OL UT IO N iscalledagainwiththesame
levelandgoals wecanfindtherecorded no-good andimmediately returnfailure ratherthan
searching again. Weseeshortlythatno-goods arealsousedinthetermination test.
Weknow that planning is P SP AC E-complete and that constructing the planning graph
takespolynomialtimesoitmustbethecasethatsolutionextractionisintractableintheworst
case. Thereforewewillneedsomeheuristicguidanceforchoosingamongactionsduringthe
backward search. One approach that works well in practice is a greedy algorithm based on
thelevelcostoftheliterals. Foranysetofgoalsweproceedinthefollowingorder:
1. Pickfirsttheliteralwiththehighestlevelcost.
2. Toachievethatliteralpreferactionswitheasierpreconditions. Thatischooseanaction
suchthatthesumormaximumofthelevelcostsofitspreconditions issmallest.
Sofarwehaveskatedoverthequestionoftermination. Hereweshowthat G RA PH PL ANwill
infactterminateandreturnfailure whenthereisnosolution.
Thefirstthingtounderstand iswhywecantstopexpanding thegraphassoonasithas
leveled off. Consider an air cargo domain with one plane and n pieces of cargo at airport
A all of which have airport B as their destination. In this version of the problem only one
piece of cargo can fitinthe plane at atime. Thegraph willlevel offatlevel 4 reflecting the
factthatforanysinglepieceofcargo wecanloaditflyitandunloaditatthedestination in
threesteps. Butthatdoesnotmeanthatasolution canbeextractedfromthegraphatlevel4;
infact asolution willrequire 4n1steps: foreachpiece ofcargo weload flyand unload
andforallbutthelastpieceweneedtoflybacktoairport Atogetthenextpiece.
Howlongdowehavetokeepexpandingafterthegraphhasleveledoff? Ifthefunction
E XT RA CT-S OL UT IO N fails to find a solution then there must have been at least one set of
goals that were not achievable and were marked as a no-good. So if it is possible that there
might be fewer no-goods in the next level then we should continue. As soon as the graph
itself and the no-goods have both leveled off withnosolution found wecan terminate with
failurebecause thereisnopossibility ofasubsequent changethatcouldaddasolution.
Nowallwehavetodoisprovethatthegraphandtheno-goodswillalwaysleveloff. The
keytothisproofisthatcertainproperties ofplanninggraphsaremonotonically increasingor
decreasing. Xincreases monotonically meansthat theset of Xsatleveli1isasuperset
notnecessarily properofthesetatlevel i. Theproperties areasfollows:
Literals increase monotonically: Once a literal appears at a given level it will appear
atallsubsequent levels. Thisisbecause ofthepersistence actions; oncealiteral shows
uppersistence actionscauseittostayforever.
Actionsincrease monotonically: Onceanaction appears atagiven level itwillappear
atall subsequent levels. Thisis aconsequence of the monotonic increase ofliterals; if
thepreconditionsofanactionappearatoneleveltheywillappearatsubsequentlevels
andthussowilltheaction.
Mutexesdecreasemonotonically: Iftwoactionsaremutexatagivenlevel A thenthey
i
willalsobemutexforallpreviouslevelsatwhichtheybothappear. Thesameholdsfor
mutexes between literals. It might not always appear that way in the figures because
the figures have a simplification: they display neither literals that cannot hold at level
S nor actions that cannot be executed at level A . We can see that mutexes decrease
i i
monotonically istrueifyouconsiderthattheseinvisible literalsandactionsaremutex
witheverything.
The proof can be handled by cases: if actions A and B are mutex at level A it
i
must be because of one of the three types of mutex. The first two inconsistent effects
and interference are properties of the actions themselves so if the actions are mutex
at A they will be mutex at every level. The third case competing needs depends on
i
conditions at level S : that level must contain a precondition of A that is mutex with
i
a precondition of B. Now these two preconditions can be mutex if they are negations
of each other in which case they would be mutex in every level or if all actions for
achieving one aremutex withallactions forachieving the other. Butwealready know
that the available actions are increasing monotonically so by induction the mutexes
mustbedecreasing.
No-goods decrease monotonically: If a set of goals is not achievable at a given level
thentheyarenotachievableinanypreviouslevel. Theproofisbycontradiction: ifthey
were achievable at some previous level then we could just add persistence actions to
makethemachievable atasubsequent level.
Because the actions and literals increase monotonically and because there are only a finite
number of actions and literals there must come a level that has the same number of actions
andliteralsasthepreviouslevel. Becausemutexesandno-goodsdecreaseandbecausethere
can never be fewer than zero mutexes or no-goods there must come a level that has the
same number of mutexes and no-goods as the previous level. Once agraph has reached this
state then if one of the goals is missing oris mutex with another goal then wecan stop the
G RA PH PL AN algorithm and return failure. That concludes a sketch of the proof; for more
detailssee Ghallabetal.2004.
Section10.4. Other Classical Planning Approaches 387
Year Track Winning Systemsapproaches
Figure10.11 Someofthetop-performingsystemsinthe International Planning Compe-
tition. Eachyearthere are varioustracks: Optimalmeans the plannersmustproducethe
shortestpossibleplanwhile Satisficingmeansnonoptimalsolutionsareaccepted. Hand-
codedmeansdomain-specificheuristicsareallowed;Automatedmeanstheyarenot.
Currentlythemostpopularandeffectiveapproaches tofullyautomated planning are:
Translatingtoa Booleansatisfiability S ATproblem
Forwardstate-space searchwithcarefully craftedheuristics Section10.2
Searchusingaplanning graph Section10.3
These three approaches are not the only ones tried inthe 40-year history of automated plan-
ning. Figure10.11showssomeofthetopsystemsinthe International Planning Competitions
whichhavebeenheldeveryevenyearsince1998. Inthissectionwefirstdescribethetransla-
tiontoasatisfiability problem andthendescribe threeotherinfluential approaches: planning
asfirst-orderlogicaldeduction; asconstraint satisfaction; andasplanrefinement.
In Section7.7.4wesawhow S AT PL ANsolvesplanningproblemsthatareexpressedinpropo-
sitional logic. Here we show how to translate a P DD L description into a form that can be
processed by S AT PL AN. Thetranslation isaseriesofstraightforward steps:
Propositionalize the actions: replace each action schema with a set of ground actions
formedbysubstitutingconstantsforeachofthevariables. Thesegroundactionsarenot
partofthetranslation butwillbeusedinsubsequent steps.
Define the initial state: assert F0 for every fluent F in the problems initial state and
F foreveryfluentnotmentioned intheinitialstate.
Propositionalize thegoal: foreveryvariableinthegoalreplacetheliteralsthatcontain
thevariablewithadisjunction overconstants. Forexamplethegoalofhavingblock A
onanotherblock On Ax Blockxinaworldwithobjects A B and Cwouldbe
replacedbythegoal
On A ABlock A On A BBlock B On A CBlock C.
Addsuccessor-state axioms: Foreachfluent Faddanaxiomoftheform
Ft1 Action Causes Ft Ft Action Causes Not Ft
where Action Causes F is a disjunction of all the ground actions that have F in their
addlist and Action Causes Not F isadisjunction ofalltheground actions thathave F
intheirdeletelist.
Addprecondition axioms: Foreach ground action A add the axiom At P RE At
thatisifanactionistakenattimetthenthepreconditions musthavebeentrue.
Addactionexclusion axioms: saythateveryactionisdistinctfromeveryotheraction.
Theresulting translation isintheformthatwecanhandto S AT PL AN tofindasolution.
P DD Lisalanguagethatcarefullybalancestheexpressiveness ofthelanguagewiththecom-
plexity ofthealgorithms thatoperate onit. Butsomeproblemsremaindifficulttoexpress in
P DD L. Forexample we cant express the goal move all the cargo from A to B regardless
ofhowmanypiecesofcargotherearein P DD Lbutwecandoitinfirst-orderlogicusinga
universal quantifier. Likewise first-orderlogiccanconcisely express global constraints such
asnomorethanfourrobotscanbeinthesameplaceatthesame time. P DD Lcanonlysay
thiswithrepetitious preconditions oneverypossible actionthatinvolves amove.
The propositional logic representation of planning problems also has limitations such
as the fact that the notion of time is tied directly to fluents. For example South2 means
the agent is facing south at time 2. With that representation there is no way to say the
agentwouldbefacingsouth attime2ifitexecuted aright turnattime1;otherwiseitwould
be facing east. First-order logic lets us get around this limitation by replacing the notion
of linear time with a notion of branching situations using a representation called situation
S IT UA TI ON calculusthatworkslikethis:
C AL CU LU S
The initial state is called a situation. If s is a situation and a is an action then
S IT UA TI ON
R ES UL Tsa is also a situation. There are no other situations. Thus a situation cor-
responds to a sequence or history of actions. You can also think of a situation as the
resultofapplyingtheactionsbutnotethattwosituationsarethesameonlyiftheirstart
and actions are the same: R ES UL Tsa R ES UL Tscid:2 acid:2 s scid:2 a acid:2 .
Someexamplesofactionsandsituations areshownin Figure10.12.
Afunctionorrelationthatcanvaryfromonesituationtothenextisafluent. Byconven-
tionthesituationsisalwaysthelastargumenttothefluentforexample Atxlsisa
relationalfluentthatistruewhenobjectxisatlocationlinsituationsand Location isa
functionalfluentsuchthat Locationxs lholdsinthesamesituationsas Atxls.
Each actions preconditions are described with a possibility axiom that says when the
P OS SI BI LI TY AX IO M
action can be taken. Ithas the form Φs Possaswhere Φsissome formula
Section10.4. Other Classical Planning Approaches 389
Gold P IT
P IT
P IT
Gold P IT
P IT
P IT
Gold P IT
P IT
P IT
Result Result S Forward
0
Turn Right
Turn Right
Result S Forward
0
Forward
S
0
Figure10.12 Situationsastheresultsofactionsinthewumpusworld.
involving sthatdescribes thepreconditions. Anexamplefromthewumpusworldsays
thatitispossible toshootiftheagentisaliveandhasanarrow:
Alive Agents Have Agent Arrows Poss Shoots
Each fluent is described with a successor-state axiom that says what happens to the
fluent depending on what action is taken. This is similar to the approach we took for
propositional logic. Theaxiomhastheform
Actionispossible
Fluentistrueinresultstate Actionseffectmadeittrue
Itwastruebeforeandactionleftitalone.
Forexample theaxiom forthe relational fluent Holding says that the agent isholding
somegold g afterexecuting apossible action ifand only ifthe action wasa Grab ofg
oriftheagentwasalreadyholding g andtheactionwasnotreleasing it:
Possas
Holding Agentg Resultas
a Grabg Holding Agentgsacid:7 Releaseg.
U NI QU EA CT IO N We need unique action axioms so that the agent can deduce that for example a cid:7
A XI OM S
Releaseg. For each distinct pair of action names A and A we have an axiom that
i j
saystheactionsaredifferent:
A x... cid:7 A y...
i j
and for each action name A wehave an axiom that says two uses of that action name
i
areequalifandonlyifalltheirarguments areequal:
A x ...x A y ...y x y ...x y .
i 1 n i 1 n 1 1 n n
Asolution isasituation andhenceasequence ofactions thatsatisfiesthegoal.
Work in situation calculus has done a lot to define the formal semantics of planning and to
open up new areas of investigation. But so far there have not been any practical large-scale
planning programs based on logical deduction over the situation calculus. This is in part
because of the difficulty of doing efficient inference in F OL but is mainly because the field
hasnotyetdevelopedeffectiveheuristics forplanning withsituation calculus.
Wehaveseenthatconstraint satisfaction hasalotincommonwith Booleansatisfiability and
wehaveseenthat C SPtechniquesareeffectiveforschedulingproblemssoitisnotsurprising
thatitispossibletoencodeaboundedplanningproblemi.e.theproblemoffindingaplanof
lengthkasaconstraint satisfaction problem C SP.Theencoding issimilartotheencoding
to a S AT problem Section 10.4.1 with one important simplification: at each time step we
need only a single variable Actiont whose domain is the set of possible actions. We no
longerneed one variable forevery action and wedont need theaction exclusion axioms. It
isalsopossibletoencodeaplanninggraphintoa C SP.Thisistheapproachtakenby G P-C SP
Doand Kambhampati 2003.
Alltheapproaches wehaveseensofarconstruct totallyorderedplansconsisting ofastrictly
linear sequences of actions. This representation ignores the fact that many subproblems are
independent. A solution to an air cargo problem consists of a totally ordered sequence of
actionsyetif30packagesarebeingloadedontooneplaneinoneairportand50packagesare
beingloadedontoanotheratanotherairportitseemspointlesstocomeupwithastrictlinear
ordering of80loadactions; thetwosubsetsofactions shouldbethought ofindependently.
An alternative is to represent plans as partially ordered structures: a plan is a set of
actions and a set of constraints of the form Beforea a saying that one action occurs
i j
beforeanother. Inthebottomof Figure10.13weseeapartiallyorderedplanthatisasolution
to the spare tire problem. Actions are boxes and ordering constraints are arrows. Note that
Remove Spare Trunkand Remove Flat Axlecanbedoneineitherorderaslongasthey
arebothcompleted beforethe Put On Spare Axleaction.
Partially ordered plans are created by a search through the space of plans rather than
through the state space. We start with the empty plan consisting of just the initial state and
thegoalwithnoactionsinbetweenasinthetopof Figure10.13. Thesearchprocedurethen
looks for a flaw in the plan and makes an addition to the plan to correct the flaw or if no
F LA W
correction can be made the search backtracks and tries something else. A flaw is anything
that keeps thepartial plan from being asolution. Forexample oneflawintheemptyplan is
thatnoactionachieves At Spare Axle. Onewaytocorrecttheflawistoinsertintotheplan
Section10.4. Other Classical Planning Approaches 391
At Spare Trunk
Start At Spare Axle Finish
At Flat Axle
a
At Spare Trunk Remove Spare Trunk
At Spare Trunk At Spare Ground
Start Put On Spare Axle At Spare Axle Finish
At Flat Axle At Flat Axle
b
At Spare Trunk Remove Spare Trunk
At Spare Trunk At Spare Ground
Start Put On Spare Axle At Spare Axle Finish
At Flat Axle At Flat Axle
At Flat Axle Remove Flat Axle
c
Figure10.13 athetireproblemexpressedasanemptyplan. banincompletepartially
orderedplanforthetireproblem.Boxesrepresentactionsandarrowsindicatethatoneaction
mustoccurbeforeanother.cacompletepartially-orderedsolution.
theaction Put On Spare Axle. Ofcoursethatintroducessomenewflaws: thepreconditions
of the new action are not achieved. The search keeps adding to the plan backtracking if
necessary until all flaws are resolved as in the bottom of Figure 10.13. At every step we
make the least commitment possible to fix the flaw. For example in adding the action
L EA ST CO MM IT ME NT
Remove Spare Trunkweneedtocommittohaving itoccurbefore Put On Spare Axle
butwemakenoothercommitmentthatplacesitbeforeorafter otheractions. Iftherewerea
variableintheactionschemathatcouldbeleftunbound wewoulddoso.
In the 1980s and 90s partial-order planning was seen as the best way to handle plan-
ning problems with independent subproblemsafter all it was the only approach that ex-
plicitlyrepresentsindependent branchesofaplan. Ontheotherhandithasthedisadvantage
of not having an explicit representation of states in the state-transition model. That makes
somecomputationscumbersome. By2000forward-search plannershaddevelopedexcellent
heuristics thatallowedthemtoefficientlydiscovertheindependent subproblems thatpartial-
order planning was designed for. As a result partial-order planners are not competitive on
fullyautomated classical planning problems.
However partial-order planning remains an important part of the field. Forsome spe-
cifictaskssuchasoperationsscheduling partial-orderplanningwithdomainspecificheuris-
tics is the technology of choice. Many of these systems use libraries of high-level plans as
describedin Section11.2. Partial-orderplanningisalsooftenusedindomainswhereitisim-
portantforhumanstounderstand theplans. Operationalplansforspacecraftand Marsrovers
aregeneratedbypartial-orderplannersandarethencheckedbyhumanoperatorsbeforebeing
uploaded tothevehicles forexecution. Theplan refinement approach makesiteasierforthe
humanstounderstandwhattheplanningalgorithmsaredoingandverifythattheyarecorrect.
Planning combines the two major areas of A I we have covered so far: search and logic. A
planner can beseen either asaprogram that searches forasolution oras onethat construc-
tively proves the existence ofa solution. Thecross-fertilization of ideas from the twoareas
has led both to improvements in performance amounting to several orders of magnitude in
the last decade and toan increased use of planners in industrial applications. Unfortunately
we do not yet have a clear understanding of which techniques work best on which kinds of
problems. Quitepossibly newtechniques willemergethatdominateexistingmethods.
Planning is foremost an exercise in controlling combinatorial explosion. If there are n
propositions in a domain then there are 2n states. As we have seen planning is P SP AC E-
hard. Against such pessimism the identification of independent subproblems can be apow-
erfulweapon. Inthebestcasefulldecomposability oftheproblemwegetanexponential
speedup. Decomposability is destroyed however by negative interactions between actions.
G RA PH PL ANrecordsmutexestopointoutwherethedifficultinteractionsare. S AT PL ANrep-
resents asimilarrange ofmutexrelations but does sobyusing thegeneral C NFform rather
than a specific data structure. Forward search addresses the problem heuristically by trying
tofind patterns subsets ofpropositions that covertheindependent subproblems. Since this
approachisheuristicitcanworkevenwhenthesubproblems arenotcompletelyindependent.
Sometimes it is possible to solve a problem efficiently by recognizing that negative
S ER IA LI ZA BL E interactions canberuledout. Wesaythataproblem has serializable subgoalsifthereexists
S UB GO AL
an order of subgoals such that the planner can achieve them in that order without having to
undo any of the previously achieved subgoals. Forexample in the blocks world if the goal
is to build a towere.g. Aon B which in turn is on C which in turn is on the Table as in
Figure10.4onpage371thenthesubgoalsareserializable bottomtotop: ifwefirstachieve
C on Table we will never have to undo it while we are achieving the other subgoals. A
planner that uses the bottom-to-top trick can solve any problem in the blocks world without
backtracking although itmightnotalwaysfindtheshortest plan.
Asa more complex example for the Remote Agent planner that commanded N AS As
Deep Space One spacecraft it was determined that the propositions involved in command-
ing a spacecraft are serializable. This is perhaps not too surprising because a spacecraft is
designed by its engineers to be as easy as possible to control subject to other constraints.
Taking advantage of the serialized ordering of goals the Remote Agent planner was able to
eliminate most of the search. This meant that it was fast enough to control the spacecraft in
realtimesomething previously considered impossible.
Planners such as G RA PH PL AN S AT PL AN and F F have moved the field of planning
forward by raising the level of performance of planning systems by clarifying the repre-
sentational and combinatorial issues involved and by the development of useful heuristics.
Howeverthereisaquestionofhowfarthesetechniqueswillscale. Itseemslikelythatfurther
progress on larger problems cannot rely only on factored and propositional representations
and will require some kind of synthesis of first-order and hierarchical representations with
theefficientheuristics currently inuse.
11
P LA NN IN G A ND A CT IN G
I N T HE R EA L W OR LD
Inwhichweseehow moreexpressive representations and moreinteractive agent
architectures leadtoplanners thatareusefulintherealworld.
Thepreviouschapterintroduced themostbasicconcepts representations andalgorithmsfor
planning. Planners that are are used in the real world for planning and scheduling the oper-
ations of spacecraft factories and military campaigns are more complex; they extend both
the representation language and the way the planner interacts with the environment. This
actions with durations and resource constraints. Section 11.2 describes methods for con-
structing plans thatareorganized hierarchically. Thisallowshuman experts tocommunicate
totheplannerwhattheyknowabouthowtosolvetheproblem. Hierarchyalsolendsitselfto
efficientplanconstructionbecausetheplannercansolveaproblematanabstractlevelbefore
delvingintodetails. Section11.3presentsagentarchitectures thatcanhandleuncertain envi-
ronments and interleave deliberation withexecution andgivessomeexamples ofreal-world
systems. Section11.4showshowtoplanwhentheenvironment containsotheragents.
Theclassicalplanningrepresentationtalksaboutwhattodoandinwhatorderbuttherepre-
sentation cannot talkabout time: howlong anaction takes andwhenitoccurs. Forexample
theplanners of Chapter10couldproduceascheduleforanairlinethatsayswhichplanesare
assignedtowhichflightsbutwereallyneedtoknowdepartureandarrivaltimesaswell. This
isthesubjectmatterofscheduling. Therealworldalsoimposesmanyresourceconstraints;
forexample anairlinehasalimitednumberofstaffand staffwhoareononeflightcannot
be on another at the same time. This section covers methods for representing and solving
planning problemsthatincludetemporalandresource constraints.
The approach we take in this section is plan first schedule later: that is we divide
the overall problem into a planning phase in which actions are selected with some ordering
constraints to meet the goals of the problem and a later scheduling phase in which tempo-
ralinformation isaddedtotheplantoensure thatitmeetsresource anddeadline constraints.
401
Jobs Add Engine1 Add Wheels1 Inspect1
Add Engine2 Add Wheels2 Inspect2
Resources Engine Hoists1 Wheel Stations1 Inspectors2 Lug Nuts500
Action Add Engine1 DU RA TI ON:30
U SE:Engine Hoists1
Action Add Engine2 DU RA TI ON:60
U SE:Engine Hoists1
Action Add Wheels1 DU RA TI ON:30
C ON SU ME:Lug Nuts20 US E:Wheel Stations1
Action Add Wheels2 DU RA TI ON:15
C ON SU ME:Lug Nuts20 US E:Wheel Stations1
Action Inspect i D UR AT IO N:10
U SE:Inspectors1
Figure11.1 Ajob-shopschedulingproblemforassemblingtwocarswith resourcecon-
straints. Thenotation A Bmeansthataction Amustprecedeaction B.
Thisapproachiscommoninreal-worldmanufacturingandlogisticalsettingswheretheplan-
ningphase isoftenperformed byhumanexperts. Theautomated methods of Chapter10can
also be used for the planning phase provided that they produce plans with just the minimal
ordering constraints required forcorrectness. G RA PH PL AN Section 10.3 S AT PL AN Sec-
tion 10.4.1 and partial-order planners Section 10.4.4 can do this; search-based methods
Section 10.2 produce totally ordered plans but these can easily be converted to plans with
minimalordering constraints.
A typical job-shop scheduling problem as first introduced in Section 6.1.2 consists of a
set of jobs each of which consists a collection of actions with ordering constraints among
J OB
them. Each action has a duration and a set of resource constraints required by the action.
D UR AT IO N
Each constraint specifies a type of resource e.g. bolts wrenches or pilots the number
of that resource required and whether that resource is consumable e.g. the bolts are no
C ON SU MA BL E
longer available foruse or reusable e.g. apilot is occupied during a flight but is available
R EU SA BL E
againwhentheflightisover. Resources canalsobe produced byactionswithnegativecon-
sumption including manufacturing growing andresupply actions. Asolution toajob-shop
scheduling problem mustspecify thestarttimesforeachaction andmustsatisfy allthetem-
poral ordering constraints and resource constraints. As with search and planning problems
solutions can be evaluated according to a cost function; this can be quite complicated with
nonlinear resource costs time-dependent delay costs and so on. For simplicity we assume
thatthecostfunction isjustthetotalduration oftheplan whichiscalledthemakespan.
M AK ES PA N
Figure11.1showsasimpleexample: aprobleminvolvingtheassemblyoftwocars. The
problemconsistsoftwojobseachoftheform Add Engine Add Wheels Inspect. Thenthe
Section11.1. Time Schedules and Resources 403
Resources statement declares that there are four types of resources and gives the number
of each type available at the start: 1 engine hoist 1 wheel station 2 inspectors and 500 lug
nuts. The action schemas give the duration and resource needs of each action. The lug nuts
are consumed as wheels are added to the car whereas the other resources are borrowed at
thestartofanactionandreleased attheactionsend.
The representation of resources as numerical quantities such as Inspectors2 rather
than as named entities such as Inspector I and Inspector I is an example of a very
general technique called aggregation. Thecentral idea ofaggregation istogroupindividual
A GG RE GA TI ON
objects into quantities when the objects are all indistinguishable with respect to the purpose
athand. Inourassemblyproblemitdoesnotmatterwhichinspectorinspectsthecarsothere
is no need to make the distinction. The same idea works in the missionaries-and-cannibals
problem in Exercise 3.9. Aggregation is essential for reducing complexity. Consider what
happens when a proposed schedule has 10 concurrent Inspect actions but only 9 inspectors
areavailable. Withinspectorsrepresentedasquantities afailureisdetectedimmediatelyand
thealgorithm backtracks totryanotherschedule. Withinspectors represented asindividuals
thealgorithm backtracks totryall 10!waysofassigning inspectors toactions.
Webeginbyconsideringjustthetemporalschedulingproblemignoringresourceconstraints.
Tominimizemakespanplandurationwemustfindtheearlieststarttimesforalltheactions
consistent withtheorderingconstraints suppliedwiththe problem. Itishelpfultoviewthese
orderingconstraintsasadirectedgraphrelatingtheactionsasshownin Figure11.2. Wecan
C RI TI CA LP AT H apply the critical path method C PM to this graph to determine the possible start and end
M ET HO D
times of each action. A path through a graph representing a partial-order plan is a linearly
ordered sequence of actions beginning with Start and ending with Finish. For example
therearetwopathsinthepartial-order planin Figure11.2.
The critical path is that path whose total duration is longest; the path is critical
C RI TI CA LP AT H
because itdetermines theduration oftheentireplanshortening otherpathsdoesnt shorten
the plan as a whole but delaying the start of any action on the critical path slows down the
wholeplan. Actionsthatareoffthecriticalpathhaveawindowoftimeinwhichtheycanbe
executed. Thewindowisspecifiedintermsofanearliestpossiblestarttime E Sandalatest
possible start time L S. The quantity L S E S is known as the slack of an action. We can
S LA CK
see in Figure 11.2 that the whole plan will take 85 minutes that each action in the top job
has15minutesofslack andthateachactiononthecritical pathhasnoslackbydefinition.
Togetherthe E S and L S timesforalltheactions constitute aschedulefortheproblem.
S CH ED UL E
Thefollowingformulasserveasadefinitionfor E S and L S andalsoastheoutlineofa
dynamic-programming algorithm to compute them. A and B are actions and A B means
that Acomesbefore B:
E SStart 0
E SB max A BE SA Duration A
L SFinish E SFinish
L SA min Bcid:9 AL SB Duration A.
015 3045 6075
Add Engine1 Add Wheels1 Inspect1
00 8585
Start Finish
00 6060 7575
Add Engine2 Add Wheels2 Inspect2
Add Wheels1
Add Engine1 Inspect1
Add Engine2 Inspect2
Add Wheels2
Figure11.2 Top:arepresentationofthetemporalconstraintsforthejob-shopscheduling
problemof Figure11.1.Thedurationofeachactionisgivenatthebottomofeachrectangle.
In solvingthe problemwe computethe earliest andlatest start times asthe pair E SL S
displayed in the upper left. The difference between these two numbers is the slack of an
action;actionswithzeroslackareonthecriticalpathshownwithboldarrows. Bottom: the
samesolutionshownasatimeline. Greyrectanglesrepresenttimeintervalsduringwhichan
actionmaybeexecutedprovidedthattheorderingconstraintsarerespected.Theunoccupied
portionofagrayrectangleindicatestheslack.
The idea is that we start by assigning E SStart to be 0. Then as soon as we get an action
B such that all the actions that come immediately before B have E S values assigned we
set E SB to be the maximum of the earliest finish times of those immediately preceding
actions wheretheearliestfinishtimeofanactionisdefined astheearlieststarttimeplusthe
duration. This process repeats until every action has been assigned an E S value. The L S
valuesarecomputed inasimilarmannerworkingbackwardfromthe Finish action.
Thecomplexity ofthecritical pathalgorithm isjust O Nbwhere N isthenumberof
actionsandbisthemaximumbranchingfactorintooroutofanaction. Toseethisnotethat
the L S and E S computations are done once for each action and each computation iterates
overatmostbotheractions. Thereforefindingaminimum-durationschedulegivenapartial
ordering ontheactionsandnoresource constraints isquiteeasy.
Mathematically speaking critical-path problems areeasy tosolvebecause theyarede-
fined as a conjunction of linear inequalities on the start and end times. When we introduce
resource constraints the resulting constraints on start and end times become more compli-
cated. For example the Add Engine actions which begin at the same time in Figure 11.2
Section11.1. Time Schedules and Resources 405
Engine Hoists1 Add Engine1 Add Engine2
Wheel Stations1 Add Wheels1 Add Wheels2
Inspect1
Inspectors2
Inspect2
Figure11.3 Asolutiontothejob-shopschedulingproblemfrom Figure11.1takinginto
account resource constraints. The left-hand margin lists the three reusable resources and
actions are shown aligned horizontally with the resources they use. There are two possi-
ble schedules depending on which assembly uses the engine hoist first; weve shown the
shortest-durationsolutionwhichtakes115minutes.
require the same Engine Hoist and so cannot overlap. The cannot overlap constraint is a
disjunction of two linear inequalities one for each possible ordering. The introduction of
disjunctions turnsouttomakescheduling withresourceconstraints N P-hard.
Notice that there is no time at which both inspectors are required so we can immediately
moveoneofourtwoinspectors toamoreproductive position.
The complexity of scheduling with resource constraints is often seen in practice as
well as in theory. A challenge problem posed in 1963to find the optimal schedule for a
problem involving just 10 machines and 10 jobs of 100 actions eachwent unsolved for
bound simulated annealing tabu search constraint satisfaction and other techniques from
Chapters 3 and 4. One simple but popular heuristic is the minimum slack algorithm: on
M IN IM UM SL AC K
each iteration schedule for the earliest possible start whichever unscheduled action has all
itspredecessors scheduled andhastheleastslack;thenupdatethe E S and L S timesforeach
affected action and repeat. The heuristic resembles the minimum-remaining-values M RV
heuristic in constraint satisfaction. It often works well in practice but for our assembly
problem ityieldsa130minute solution notthe115minute solution of Figure11.3.
Up to this point we have assumed that the set of actions and ordering constraints is
fixed. Undertheseassumptionseveryschedulingproblemcanbesolvedbyanonoverlapping
sequence that avoids all resource conflicts provided that each action is feasible by itself. If
a scheduling problem is proving very difficult however it may not be a good idea to solve
itthis wayitmaybebetter toreconsider theactions and constraints in casethat leads to a
much easier scheduling problem. Thus it makes sense to integrate planning and scheduling
bytakingintoaccount durations andoverlapsduring theconstruction ofapartial-order plan.
Severaloftheplanningalgorithmsin Chapter10canbeaugmentedtohandlethisinformation.
For example partial-order planners can detect resource constraint violations in much the
same way they detect conflicts with causal links. Heuristics can be devised to estimate the
totalcompletion timeofaplan. Thisiscurrently anactiveareaofresearch.
Theproblem-solving andplanningmethodsoftheprecedingchaptersalloperatewithafixed
set of atomic actions. Actions can be strung together into sequences orbranching networks;
state-of-the-art algorithms cangenerate solutions containing thousands ofactions.
Forplans executed by the human brain atomic actions are muscle activations. In very
round numbers we have about 103 muscles to activate 639 by some counts but many of
themhavemultiple subunits; wecanmodulatetheiractivation perhaps 10timespersecond;
and we are alive and awake for about 109 seconds in all. Thus a human life contains about
planning overmuchshorter timehorizonsfor example atwo-weekvacation in Hawaiia
detailed motorplanwouldcontainaround 1010 actions. Thisisalotmorethan1000.
Tobridgethisgap A Isystemswillprobablyhavetodowhathumansappeartodo: plan
at higher levels of abstraction. A reasonable plan for the Hawaii vacation might be Go to
San Franciscoairport;take Hawaiian Airlinesflight11to Honolulu;dovacationstufffortwo
weeks;take Hawaiian Airlinesflight12backto San Francisco;gohome. Givensuchaplan
the action Go to San Francisco airport can be viewed as a planning task in itself with a
solution such as Drive to the long-term parking lot; park; take the shuttle to the terminal.
Eachofthese actions inturn canbedecomposed further until wereach thelevelofactions
thatcanbeexecutedwithoutdeliberation togeneratetherequired motorcontrolsequences.
In this example we see that planning can occur both before and during the execution
of the plan; for example one would probably defer the problem of planning a route from a
parking spot in long-term parking to the shuttle bus stop until a particular parking spot has
been found during execution. Thus that particular action will remain at an abstract level
prior to the execution phase. We defer discussion of this topic until Section 11.3. Here we
H IE RA RC HI CA L concentrate on the aspect of hierarchical decomposition an idea that pervades almost all
D EC OM PO SI TI ON
attempts to manage complexity. Forexample complex software is created from a hierarchy
ofsubroutines orobjectclasses;armiesoperateasahierarchyofunits;governmentsandcor-
porations have hierarchies of departments subsidiaries and branch offices. The key benefit
of hierarchical structure is that at each level of the hierarchy a computational task military
missionoradministrativefunctionisreducedtoasmallnumberofactivitiesatthenextlower
level so the computational cost of finding the correct way to arrange those activities for the
current problem is small. Nonhierarchical methods on the other hand reduce a task to a
largenumberofindividual actions; forlarge-scale problemsthisiscompletely impractical.
Thebasicformalismweadopttounderstandhierarchicaldecompositioncomesfromthearea
H IE RA RC HI CA LT AS K of hierarchical task networksor H TNplanning. Asin classical planning Chapter 10 we
N ET WO RK
assumefullobservability anddeterminism andtheavailability ofasetofactions nowcalled
primitiveactionswithstandardpreconditioneffect schemas. Thekeyadditional conceptis
P RI MI TI VE AC TI ON
the high-levelaction or H LAforexample theaction Goto San Francisco airport inthe
H IG H-L EV EL AC TI ON
Section11.2. Hierarchical Planning 407
Refinement Go Home S FO
S TE PS:Drive Home S FO Long Term Parking
Shuttle S FO Long Term Parking S FO
Refinement Go Home S FO
S TE PS:Taxi Home S FO
Refinement Navigateabxy
P RE CO ND:ax by
S TE PS:
Refinement Navigateabxy
P RE CO ND:Connectedaba1b
S TE PS:Left Navigatea1bxy
Refinement Navigateabxy
P RE CO ND:Connectedaba1b
S TE PS:Right Navigatea1bxy
...
Figure11.4 Definitionsofpossiblerefinementsfortwohigh-levelactions: goingto San
Franciscoairportandnavigatinginthe vacuumworld. Inthe lattercase notethe recursive
natureoftherefinementsandtheuseofpreconditions.
example given earlier. Each H LA has one or more possible refinements into a sequence1
R EF IN EM EN T
of actions each of which may be an H LA or a primitive action which has no refinements
by definition. For example the action Go to San Francisco airport represented formally
as Go Home S FO might have two possible refinements as shown in Figure 11.4. The
same figure shows a recursive refinement for navigation in the vacuum world: to get to a
destination takeastepandthengotothedestination.
Theseexamplesshow thathigh-level actions andtheirrefinements embody knowledge
abouthowtodothings. Forinstance therefinements for Go Home S FOsaythattogetto
the airport you can drive ortake ataxi; buying milk sitting down and moving the knight to
e4arenottobeconsidered.
An H LA refinement that contains only primitive actions is called an implementation
I MP LE ME NT AT IO N
of the H LA. For example in the vacuum world the sequences Right Right Down and
Down Right Rightboth implement the H LA Navigate1332. Animplementation
of a high-level plan a sequence of H LAs is the concatenation of implementations of each
H LAinthesequence. Giventhepreconditioneffect definitionsofeachprimitiveactionitis
straightforward todeterminewhetheranygivenimplementationofahigh-levelplanachieves
the goal. We can say then that a high-level plan achieves the goal from a given state if at
least one ofitsimplementations achieves the goal from that state. Theatleast one in this
definitioniscrucialnotallimplementationsneedtoachievethegoalbecausetheagentgets
different H LAsinaplantoshareactions.Weomittheseimportantcomplicationsintheinterestofunderstanding
thebasicconceptsofhierarchicalplanning.
todecidewhichimplementation itwillexecute. Thusthesetofpossible implementations in
H TN planningeach of which may have a different outcomeis not the same as the set of
possible outcomes innondeterministic planning. There we required that aplan work for all
outcomesbecause theagentdoesnt gettochoosetheoutcome;naturedoes.
The simplest case is an H LA that has exactly one implementation. In that case we
can compute the preconditions and effects of the H LA from those of the implementation
see Exercise 11.3 and then treat the H LA exactly as if it were a primitive action itself. It
can be shown that the right collection of H LAs can result in the time complexity of blind
search dropping from exponential in the solution depth to linear in the solution depth al-
though devising such a collection of H LAs may be a nontrivial task in itself. When H LAs
have multiple possible implementations there are two options: one is to search among the
implementations foronethatworksasin Section11.2.2;theotheristoreasondirectlyabout
the H LAsdespitethemultiplicityofimplementationsas explainedin Section11.2.3. The
latter method enables the derivation of provably correct abstract plans without the need to
considertheirimplementations.
H TNplanningisoftenformulatedwithasingletoplevelactioncalled Actwheretheaimis
tofindanimplementationof Actthatachievesthegoal. Thisapproachisentirelygeneral. For
exampleclassicalplanning problemscanbedefinedasfollows: foreachprimitiveaction a
i
provide one refinement of Act with steps a Act. That creates a recursive definition of Act
i
thatletsusaddactions. Butweneedsomewaytostoptherecursion;wedothatbyproviding
one more refinement for Act one with an empty list of steps and with a precondition equal
to the goal of the problem. This says that if the goal is already achieved then the right
implementation istodonothing.
The approach leads to a simple algorithm: repeatedly choose an H LA in the current
planandreplace itwithoneofitsrefinements untiltheplan achievesthegoal. Onepossible
implementation based onbreadth-first treesearch isshownin Figure11.5. Plansareconsid-
eredinorderofdepth ofnesting oftherefinements ratherthannumberofprimitivesteps. It
isstraightforward todesignagraph-search versionofthealgorithm aswellasdepth-firstand
iterativedeepening versions.
Inessencethisformofhierarchicalsearchexploresthespaceofsequencesthatconform
totheknowledgecontained inthe H LAlibraryabouthowthingsaretobedone. Agreatdeal
ofknowledgecanbeencodednotjustintheactionsequencesspecifiedineachrefinementbut
also in the preconditions for the refinements. For some domains H TN planners have been
able to generate huge plans with very little search. For example O-P LA N Bell and Tate
1985whichcombines H TNplanning withscheduling hasbeenusedtodevelopproduction
plans for Hitachi. A typical problem involves a product line of 350 different products 35
assembly machines and over 2000 different operations. The planner generates a 30-day
schedulewiththree8-hourshiftsadayinvolvingtensofmillionsofsteps. Anotherimportant
aspect of H TN plans is that they are by definition hierarchically structured; usually this
makesthemeasyforhumanstounderstand.
Section11.2. Hierarchical Planning 409
function H IE RA RC HI CA L-S EA RC Hproblemhierarchyreturnsasolutionorfailure
frontiera F IF Oqueuewith Actastheonlyelement
loopdo
if E MP TY?frontierthenreturnfailure
plan P OPfrontier choosestheshallowestplaninfrontier
hlathefirst H LAinplanornull ifnone
prefixsuffixtheactionsubsequencesbeforeandafterhla inplan
outcome R ES UL Tproblem.I NI TI AL-S TA TEprefix
ifhla isnullthen soplan isprimitiveandoutcome isitsresult
ifoutcome satisfiesproblem.G OA L thenreturnplan
elseforeachsequence in R EF IN EM EN TShlaoutcomehierarchydo
frontier I NS ER TA PP EN Dprefixsequencesuffixfrontier
Figure11.5 Abreadth-firstimplementationofhierarchicalforwardplanningsearch. The
initial plan suppliedto the algorithmis Act. The R EF IN EM EN TS functionreturnsa set of
actionsequencesoneforeachrefinementofthe H LAwhosepreconditionsaresatisfied by
thespecifiedstateoutcome.
The computational benefits of hierarchical search can be seen by examining an ide-
alized case. Suppose that a planning problem has a solution with d primitive actions. For
a nonhierarchical forward state-space planner with b allowable actions at each state the
cost is Obd as explained in Chapter 3. For an H TN planner let us suppose a very reg-
ular refinement structure: each nonprimitive action has r possible refinements each into
k actions at the next lower level. We want to know how many different refinement trees
there are with this structure. Now if there are d actions at the primitive level then the
number of levels below the root is log d so the number of internal refinement nodes is
k
1k k2 klogkd1 d1k 1. Each internal node has r possible refine-
mentssord1k1
possibleregulardecompositiontreescouldbeconstructed. Examining
this formula we see that keeping r small and k large can result in huge savings: essentially
wearetakingthekthrootofthenonhierarchical costif bandr arecomparable. Smallrand
large k means a library of H LAs with a small number of refinements each yielding a long
action sequence that nonetheless allows us to solve any problem. This is not always pos-
sible: long action sequences that are usable across a wide range of problems are extremely
precious.
Thekeyto H TNplanning then isthe construction ofaplan library containing known
methods for implementing complex high-level actions. One method of constructing the li-
brary is to learn the methods from problem-solving experience. After the excruciating ex-
perience of constructing a plan from scratch the agent can save the plan in the library as a
methodforimplementingthehigh-levelactiondefinedbythetask. Inthiswaytheagentcan
becomemoreandmorecompetentovertimeasnewmethodsarebuiltontopofoldmethods.
One important aspect of this learning process is the ability to generalize the methods that
are constructed eliminating detail that is specific to the problem instance e.g. the name of
the builder or the address of the plot of land and keeping just the key elements of the plan.
Methodsforachievingthiskindofgeneralization aredescribed in Chapter19. Itseemstous
inconceivable thathumanscouldbeascompetentastheyarewithoutsomesuchmechanism.
Thehierarchicalsearchalgorithmintheprecedingsection refines H LAsallthewaytoprimi-
tiveactionsequencestodetermineifaplanisworkable. Thiscontradictscommonsense: one
shouldbeabletodetermine thatthetwo-H LAhigh-level plan
Drive Home S FO Long Term Parking Shuttle S FO Long Term Parking S FO
gets one to the airport without having to determine a precise route choice of parking spot
andsoon. Thesolution seemsobvious: writepreconditioneffect descriptions ofthe H LAs
just as we write down what the primitive actions do. From the descriptions it ought to be
easytoprovethatthehigh-level planachieves thegoal. Thisistheholygrailsotospeak of
hierarchical planning because if wederive ahigh-level plan that provably achieves the goal
working in a small search space of high-level actions then we can commit to that plan and
workontheproblemofrefiningeachstepoftheplan. Thisgivesustheexponentialreduction
we seek. For this to work it has to be the case that every high-level plan that claims to
achieve the goal by virtue of the descriptions of its steps does in fact achieve the goal in
thesensedefinedearlier: itmusthaveatleastoneimplementation thatdoesachievethegoal.
D OW NW AR D
Thisproperty hasbeencalledthe downwardrefinementpropertyfor H LAdescriptions.
R EF IN EM EN T
P RO PE RT Y
Writing H LA descriptions that satisfy the downward refinement property is in princi-
pleeasy: aslongasthedescriptions are truethenanyhigh-level planthatclaimstoachieve
the goal must in fact do sootherwise the descriptions are making some false claim about
whatthe H LAsdo. Wehave already seen howto writetrue descriptions for H LAsthathave
exactly one implementation Exercise 11.3; a problem arises when the H LA has multiple
implementations. How can we describe the effects of an action that can be implemented in
manydifferent ways?
Onesafeansweratleastforproblemswhereallpreconditionsandgoalsarepositiveis
toincludeonlythepositiveeffectsthatareachievedbyeveryimplementationofthe H LAand
the negative effects of any implementation. Then the downward refinement property would
besatisfied. Unfortunatelythissemanticsfor H LAsismuchtooconservative. Consideragain
the H LA Go Home S FO which has two refinements and suppose for the sake of argu-
mentasimpleworldinwhichonecanalwaysdrivetotheairport andpark buttaking ataxi
requires Cash as a precondition. In that case Go Home S FO doesnt always get you to
theairport. Inparticular itfailsif Cash isfalseandsowecannotassert At Agent S FOas
aneffectofthe H LA.Thismakesnosense however;iftheagentdidnt have Cashitwould
driveitself. Requiringthataneffectholdfor everyimplementation isequivalent toassuming
that someone elsean adversarywill choose the implementation. Ittreats the H LAsmul-
tiple outcomes exactly asifthe H LAwereanondeterministicaction asin Section 4.3. For
ourcasetheagentitselfwillchoosetheimplementation.
The programming languages community has coined the term demonic nondetermin-
D EM ON IC ismforthecasewhereanadversary makesthechoices contrasting thiswithangelicnonde-
N ON DE TE RM IN IS M
Section11.2. Hierarchical Planning 411
a b
Figure11.6 Schematicexamplesofreachablesets. Thesetofgoalstatesisshaded.Black
andgrayarrowsindicatepossibleimplementationsofh andh respectively.a Thereach-
ablesetofan H LAh inastates. b Thereachablesetforthesequenceh h . Because
thisintersectsthegoalsetthesequenceachievesthegoal.
A NG EL IC terminism wherethe agent itself makes the choices. Weborrow this term to defineangelic
N ON DE TE RM IN IS M
semantics for H LA descriptions. The basic concept required for understanding angelic se-
A NG EL IC SE MA NT IC S
mantics is the reachable set of an H LA: given a state s the reachable set for an H LA h
R EA CH AB LE SE T
written as R EA CHsh is the set of states reachable by any of the H LAsimplementations.
The key idea is that the agent can choose which element of the reachable set it ends up in
whenitexecutes the H LA;thus an H LAwithmultiple refinements ismorepowerful than
thesame H LAwithfewerrefinements. Wecanalsodefinethereachablesetofasequencesof
H LAs. Forexamplethereachable setofasequence h h istheunionofallthereachable
setsobtained byapplyingh ineachstateinthereachable setofh :
cid:2
R EA CHsh 1h 2 R EA CHsh 2.
scid:3 RE AC Hsh1
Given these definitions a high-level plana sequence of H LAsachieves the goal if its
reachable set intersects the set of goal states. Compare this to the much stronger condition
for demonic semantics where every member of the reachable set has to be a goal state.
Conversely if the reachable set doesnt intersect the goal then the plan definitely doesnt
work. Figure11.6illustrates theseideas.
The notion of reachable sets yields a straightforward algorithm: search among high-
level plans looking for one whose reachable set intersects the goal; once that happens the
algorithm can commit to that abstract plan knowing that it works and focus on refining
the plan further. We will come back to the algorithmic issues later; first we consider the
question ofhowtheeffects ofan H LAthereachable setforeachpossible initialstateare
represented. As with the classical action schemas of Chapter 10 we represent the changes
madetoeachfluent. Thinkofafluentasastatevariable. Aprimitiveactioncanaddordelete
a variable or leave it unchanged. With conditional effects see Section 11.3.1 there is a
fourthpossibility: flippingavariabletoitsopposite.
An H LA under angelic semantics can do more: it can control the value of a variable
setting ittotrueorfalse depending onwhichimplementation ischosen. Infact an H LAcan
have nine different effects on a variable: if the variable starts out true it can always keep
it true always make it false or have a choice; if the variable starts out false it can always
keep it false always make it true or have a choice; and the three choices for each case can
becombined arbitrarily making nine. Notationally this isabitchallenging. Wellusethe cid:23
cid:23
symboltomeanpossibly iftheagentsochooses. Thusaneffect Ameanspossibly add
A that is either leave Aunchanged ormake it true. Similarly
cid:23
A means possibly delete
A and
cid:23
A means possibly add or delete A. For example the H LA Go Home S FO
withthetworefinementsshownin Figure11.4possiblydeletes Cash iftheagentdecidesto
take ataxi soitshould havethe effect
cid:23
Cash. Thus weseethat thedescriptions of H LAs
arederivableinprinciplefromthedescriptionsoftheirrefinementsinfactthisisrequired
if wewanttrue H LAdescriptions such that the downward refinement property holds. Now
suppose wehavethefollowingschemasforthe H LAsh andh :
Actionh
1 PR EC ON D:A EF FE CT:Acid:23
B
Actionh
2 PR EC ON D:B EF FE CT:cid:23 Acid:23
C.
Thatish adds Aandpossibledeletes Bwhileh possiblyadds Aandhasfullcontrolover
C. Nowifonly B istrueintheinitial stateandthegoalis A C thenthesequence h h
achieves the goal: we choose an implementation of h that makes B false then choose an
1
implementation ofh thatleaves Atrueandmakes C true.
2
The preceding discussion assumes that the effects of an H LAthe reachable set for
anygiveninitial statecan bedescribed exactlybydescribing theeffect oneachvariable. It
would be nice if this were always true but in many cases we can only approximate the ef-
fectsbecausean H LAmayhaveinfinitelymanyimplementationsandmayproducearbitrarily
wiggly reachable setsrather like the wiggly-belief-state problem illustrated in Figure 7.21
on page 271. For example we said that Go Home S FO possibly deletes Cash; it also
possibly adds At Car S FO Long Term Parking; but it cannot do bothin fact it must do
exactly one. As with belief states we may need to write approximate descriptions. Wewill
O PT IM IS TI C usetwokindsofapproximation: anoptimisticdescription R EA CHshofan H LAhmay
D ES CR IP TI ON
""
P ES SI MI ST IC overstate the reachable set while a pessimistic description R EA CH sh may understate
D ES CR IP TI ON
thereachable set. Thuswehave
R EA CH sh R EA CHsh R EA CHsh.
Forexampleanoptimisticdescriptionof Go Home S FOsaysthatitpossibledeletes Cash
and possibly adds At Car S FO Long Term Parking. Another good example arises in the
8-puzzle half of whose states are unreachable from any given state see Exercise 3.4 on
theexactreachable setisquitewiggly.
With approximate descriptions the test for whether a plan achieves the goal needs to
be modified slightly. If the optimistic reachable set for the plan doesnt intersect the goal
Section11.2. Hierarchical Planning 413
a b
setofgoalstatesisshaded.Foreachplanthepessimisticsolidlinesandoptimisticdashed
linesreachablesetsareshown.a Theplanindicatedbytheblackarrowdefinitelyachieves
thegoalwhiletheplanindicatedbythegrayarrowdefinitelydoesnt.b Aplanthatwould
needtoberefinedfurthertodetermineifitreallydoesachievethegoal.
then the plan doesnt work; if the pessimistic reachable set intersects the goal then the plan
does work Figure 11.7a. With exact descriptions a plan either works or it doesnt but
with approximate descriptions there is a middle ground: if the optimistic set intersects the
goal but the pessimistic set doesnt then we cannot tell if the plan works Figure 11.7b.
When this circumstance arises the uncertainty can be resolved by refining the plan. This is
a very common situation in human reasoning. Forexample in planning the aforementioned
two-week Hawaii vacation one might propose to spend two days on each of seven islands.
Prudence would indicate that this ambitious plan needs to be refined by adding details of
inter-island transportation.
Analgorithm forhierarchical planning withapproximate angelicdescriptions isshown
in Figure 11.8. For simplicity we have kept to the same overall scheme used previously in
algorithmcandetectplansthatwillandwontworkbycheckingtheintersections oftheopti-
misticandpessimisticreachablesetswiththegoal. Thedetailsofhowtocomputethereach-
ablesetsofaplangivenapproximatedescriptionsofeachsteparecoveredin Exercise11.5.
Whenaworkableabstractplanisfound thealgorithm decomposes theoriginal probleminto
subproblems one for each step of the plan. The initial state and goal for each subproblem
are obtained by regressing a guaranteed-reachable goal state through the action schemas for
each step of the plan. See Section 10.2.2 for a discussion of how regression works. Fig-
ure 11.6b illustrates the basic idea: the right-hand circled state is the guaranteed-reachable
goal state and the left-hand circled state is the intermediate goal obtained by regressing the
function A NG EL IC-S EA RC Hproblemhierarchyinitial Planreturnssolutionorfail
frontiera F IF Oqueuewithinitial Plan astheonlyelement
loopdo
if E MP TY?frontierthenreturnfail
plan P OPfrontier choosestheshallowestnodeinfrontier
if R EA CHproblem.I NI TI AL-S TA TEplanintersectsproblem.G OA Lthen
ifplan isprimitivethenreturnplan R EA CH isexactforprimitiveplans
guaranteed R EA CH problem.I NI TI AL-S TA TEplan problem.G OA L
ifguaranteedcid:7and M AK IN G-P RO GR ES Splaninitial Planthen
final Stateanyelementofguaranteed
return D EC OM PO SEhierarchyproblem.I NI TI AL-S TA TEplanfinal State
hlasome H LAinplan
prefixsuffixtheactionsubsequencesbeforeandafterhla inplan
foreachsequence in R EF IN EM EN TShlaoutcomehierarchydo
frontier I NS ER TA PP EN Dprefixsequencesuffixfrontier
function D EC OM PO SEhierarchys0plansfreturnsasolution
solutionanemptyplan
whileplan isnotemptydo
action R EM OV E-L AS Tplan
siastatein R EA CH s0plansuchthatsf R EA CH
siaction
problemaproblemwith I NI TI AL-S TA TEsi and G OA Lsf
solution A PP EN DA NG EL IC-S EA RC Hproblemhierarchyactionsolution
sf si
returnsolution
Figure11.8 Ahierarchicalplanningalgorithmthatusesangelicsemanticstoidentifyand
committohigh-levelplansthatworkwhileavoidinghigh-levelplansthatdont. Thepredi-
cate M AK IN G-P RO GR ES S checkstomakesurethatwearentstuckinaninfiniteregression
ofrefinements.Attoplevelcall A NG EL IC-S EA RC Hwith Actastheinitial Plan.
goalthrough thefinalaction.
The ability to commit to or reject high-level plans can give A NG EL IC-S EA RC H a sig-
nificant computational advantage over H IE RA RC HI CA L-S EA RC H which in turn may have
a large advantage over plain old B RE AD TH-F IR ST-S EA RC H. Consider for example clean-
ing up a large vacuum world consisting of rectangular rooms connected by narrow corri-
dors. It makes sense to have an H LA for Navigate as shown in Figure 11.4 and one for
Clean Whole Room. Cleaning the room could be implemented with the repeated application
of another H LA to clean each row. Since there are five actions in this domain the cost
for B RE AD TH-F IR ST-S EA RC H grows as 5d where d is the length of the shortest solution
roughly twice the total number of squares; the algorithm cannot manage even two 22
rooms. H IE RA RC HI CA L-S EA RC H ismoreefficientbutstillsuffersfromexponential growth
becauseittriesallwaysofcleaningthatareconsistentwiththehierarchy. A NG EL IC-S EA RC H
scales approximately linearly inthe numberofsquaresit commits toagood high-level se-
Section11.3. Planningand Actingin Nondeterministic Domains 415
quence and prunes away the other options. Notice that cleaning a set of rooms by cleaning
each room in turn is hardly rocket science: it is easy for humans precisely because of the
hierarchical structure of the task. When we consider how difficult humans find it to solve
small puzzles such asthe 8-puzzle itseems likely that the human capacity forsolving com-
plex problems derives to a great extent from their skill in abstracting and decomposing the
problem toeliminatecombinatorics.
The angelic approach can be extended to find least-cost solutions by generalizing the
notion of reachable set. Instead of a state being reachable or not it has a cost for the most
efficient way to get there. The cost is for unreachable states. The optimistic and pes-
simisticdescriptions boundthesecosts. Inthiswayangelicsearchcanfindprovablyoptimal
abstract planswithoutconsidering theirimplementations. Thesameapproach canbeusedto
""
H IE RA RC HI CA L obtain effective hierarchical lookahead algorithms for online search in the style of L RT A
L OO KA HE AD
page152. Insomewayssuchalgorithmsmirroraspectsofhumandeliberationintaskssuch
asplanningavacationto Hawaiiconsideration ofalternativesisdoneinitiallyatanabstract
leveloverlongtimescales;somepartsoftheplanareleftquiteabstractuntilexecution time
suchashowtospendtwolazydayson Molokaiwhileotherspartsareplannedindetailsuch
as the flights to be taken and lodging to be reservedwithout these refinements there is no
guarantee thattheplanwouldbefeasible.
In this section we extend planning to handle partially observable nondeterministic and un-
known environments. Chapter 4 extended search in similar ways and the methods here are
also similar: sensorless planning also known as conformant planning for environments
with no observations; contingency planning for partially observable and nondeterministic
environments; andonlineplanningandreplanningforunknownenvironments.
While the basic concepts are the same as in Chapter 4 there are also significant dif-
ferences. These arise because planners deal withfactored representations ratherthan atomic
representations. Thisaffectsthewaywerepresenttheagentscapabilityforactionandobser-
vation and the way we represent belief statesthe sets of possible physical states the agent
might be infor unobservable and partially observable environments. We can also take ad-
vantage of many of the domain-independent methods given in Chapter 10 for calculating
searchheuristics.
Considerthisproblem: givenachairandatablethegoalistohavethemmatchhave
the same color. In the initial state wehave two cans of paint but the colors of the paint and
thefurniture areunknown. Onlythetableisinitiallyinthe agents fieldofview:
Init Object Table Object Chair Can C Can C In View Table
Goal Color Chairc Color Tablec
There are two actions: removing the lid from a paint can and painting an object using the
paintfromanopencan. Theactionschemasarestraightforward withoneexception: wenow
allow preconditions and effects to contain variables that are not part of the actions variable
list. That is Paintxcan does not mention the variable c representing the color of the
paint in the can. In the fully observable case this is not allowedwe would have to name
the action Paintxcanc. But in the partially observable case we might or might not
knowwhatcolorisinthecan. Thevariable cisuniversally quantified justlikealltheother
variables inanactionschema.
Action Remove Lidcan
P RE CO ND:Cancan
E FF EC T:Opencan
Action Paintxcan
P RE CO ND:Objectx Cancan Colorcanc Opencan
E FF EC T:Colorxc
Tosolveapartiallyobservableproblemtheagentwillhavetoreasonabouttheperceptsitwill
obtainwhenitisexecutingtheplan. Theperceptwillbesuppliedbytheagentssensorswhen
itisactually acting butwhenitisplanning itwillneed amodel ofitssensors. In Chapter4
this model was given by a function P ER CE PTs. For planning we augment P DD L with a
newtypeofschematheperceptschema:
P ER CE PT SC HE MA
Percept Colorxc
P RE CO ND:Objectx In Viewx
Percept Colorcanc
P RE CO ND:Cancan In Viewcan Opencan
The first schema says that whenever an object is in view the agent will perceive the color
of the object that is for the object x the agent will learn the truth value of Colorxc for
all c. The second schema says that if an open can is in view then the agent perceives the
color of the paint in the can. Because there are no exogenous events in this world the color
of an object will remain the same even if it is not being perceived until the agent performs
an action to change the objects color. Of course the agent will need an action that causes
objectsoneatatimetocomeintoview:
Action Look Atx
P RE CO ND:In Viewyxcid:7 y
E FF EC T:In Viewx In Viewy
Fora fully observable environment we would have a Percept axiom with no preconditions
for each fluent. A sensorless agent on the other hand has no Percept axioms at all. Note
that evenasensorless agent can solvethe painting problem. Onesolution istoopen anycan
of paint and apply it to both chair and table thus coercing them to be the same color even
thoughtheagentdoesntknowwhatthecoloris.
A contingent planning agent with sensors can generate a better plan. First look at the
table and chair to obtain their colors; if they are already the same then the plan is done. If
not look at the paint cans; if the paint in a can is the same color as one piece of furniture
thenapplythatpainttotheotherpiece. Otherwisepaintbothpieceswithanycolor.
Finallyanonlineplanningagentmightgenerateacontingent planwithfewerbranches
at firstperhaps ignoring the possibility that no cans match any of the furnitureand deal
Section11.3. Planningand Actingin Nondeterministic Domains 417
with problems when they arise by replanning. It could also deal with incorrectness of its
action schemas. Whereas a contingent planner simply assumes that the effects of an action
always succeedthat painting the chair does the joba replanning agent would check the
resultandmakeanadditional plantofixanyunexpectedfailuresuchasanunpaintedareaor
theoriginalcolorshowingthrough.
Intherealworldagentsuseacombinationofapproaches. Carmanufacturerssellspare
tires and air bags which are physical embodiments of contingent plan branches designed
to handle punctures or crashes. On the other hand most car drivers never consider these
possibilities; when a problem arises they respond as replanning agents. In general agents
plan only for contingencies that have important consequences and a nonnegligible chance
of happening. Thus a car driver contemplating a trip across the Sahara desert should make
explicit contingency plans for breakdowns whereas a trip to the supermarket requires less
advanceplanning. Wenextlookateachofthethreeapproaches inmoredetail.
Section 4.4.1 page 138 introduced the basic idea of searching in belief-state space to find
asolution forsensorless problems. Conversion ofasensorless planning problem to abelief-
state planning problem works muchthe samewayasit didin Section 4.4.1; the maindiffer-
encesarethattheunderlyingphysicaltransitionmodelisrepresentedbyacollectionofaction
schemas and the belief state can be represented by a logical formula instead of an explicitly
enumerated set of states. Forsimplicity weassume that the underlying planning problem is
deterministic.
The initial belief state for the sensorless painting problem can ignore In View fluents
because the agent has no sensors. Furthermore we take as given the unchanging facts
Object Table Object Chair Can C Can C because these hold in every be-
lief state. The agent doesnt know the colors of the cans or the objects or whether the cans
areopenorclosed butitdoesknowthatobjects andcanshave colors: x c Colorxc.
After Skolemizing see Section9.5weobtaintheinitial beliefstate:
b Colorx Cx.
0
In classical planning where the closed-world assumption is made we would assume that
any fluentnot mentioned inastate is false but in sensorless and partially observable plan-
ning we have to switch to an open-world assumption in which states contain both positive
and negative fluents and if a fluent does not appear its value is unknown. Thus the belief
state corresponds exactly to the set of possible worlds that satisfy the formula. Given this
initialbeliefstatethefollowingactionsequence isasolution:
Remove Lid Can Paint Chair Can Paint Table Can .
We now show how to progress the belief state through the action sequence to show that the
finalbeliefstatesatisfiesthegoal.
First note that in a given belief state b the agent can consider any action whose pre-
conditions aresatisfied by b. Theotheractions cannot beused because thetransition model
doesnt define the effects of actions whose preconditions might be unsatisfied. According
to Equation 4.4 page 139 the general formula for updating the belief state b given an
applicable actionainadeterministic worldisasfollows:
bcid:2 R ES UL Tba scid:2 : scid:2 R ES UL TPsaands b
where R ES UL TP definesthephysicaltransitionmodel. Forthetimebeingweassumethatthe
initial belief state is always a conjunction of literals that is a 1-C NF formula. To construct
cid:2
thenewbeliefstatebwemustconsiderwhathappens toeachliteral cid:3ineachphysicalstate
sinbwhenactionaisapplied. Forliteralswhosetruthvalueisalreadyknowninbthetruth
cid:2
value in b is computed from the current value and the add list and delete list of the action.
For example if cid:3 is in the delete list of the action then cid:3 is added to bcid:2 . What about a
literalwhosetruthvalueisunknowninb? Therearethreecases:
cid:2
1. Iftheactionaddscid:3thencid:3willbetrueinb regardless ofitsinitialvalue.
cid:2
2. Iftheactiondeletescid:3thencid:3willbefalseinb regardlessofitsinitialvalue.
3. Iftheactiondoesnotaffectcid:3thencid:3willretainitsinitialvaluewhichisunknownand
cid:2
willnotappearinb.
cid:2
Henceweseethatthecalculation ofb isalmostidentical totheobservable case whichwas
specifiedby Equation10.1onpage368:
bcid:2 R ES UL Tba b D ELa A DDa.
cid:2
We cannot quite use the set semantics because 1 we must make sure that b does not con-
tain both cid:3 and cid:3 and 2 atoms may contain unbound variables. But it is still the case
that R ES UL Tba is computed by starting with b setting any atom that appears in D ELa
to false and setting any atom that appears in A DDa to true. For example if we apply
Remove Lid Can totheinitialbeliefstateb weget
b Colorx Cx Open Can .
Whenweapplytheaction Paint Chair Can theprecondition Color Can cissatisfied
bytheknownliteral Colorx Cxwithbindingx Can c C Can andthenewbelief
stateis
b Colorx Cx Open Can Color Chair C Can .
Finallyweapplytheaction Paint Table Can toobtain
1
b Colorx Cx Open Can Color Chair C Can
Color Table C Can .
1
Thefinalbeliefstatesatisfiesthegoal Color Tablec Color Chaircwiththevariable
cboundto C Can .
1
The preceding analysis of the update rule has shown a very important fact: the family
of belief states defined as conjunctions of literals is closed under updates defined by P DD L
actionschemas. Thatisifthebelief statestartsasaconjunction ofliterals thenanyupdate
will yield a conjunction of literals. That means that in a world with n fluents any belief
state can be represented by a conjunction of size On. This is a very comforting result
considering that there are 2n states in the world. It says we can compactly represent all the
subsetsofthose2n statesthatwewilleverneed. Moreovertheprocessofcheckingforbelief
Section11.3. Planningand Actingin Nondeterministic Domains 419
states that are subsets or supersets of previously visited belief states is also easy at least in
thepropositional case.
Theflyinthe ointment of this pleasant picture isthat itonly works foraction schemas
that have the same effects for all states in which their preconditions are satisfied. It is this
propertythatenablesthepreservationofthe1-C NFbelief-staterepresentation. Assoonasthe
effect can depend on the state dependencies are introduced between fluents and the 1-C NF
property is lost. Consider for example the simple vacuum world defined in Section 3.2.1.
Let the fluents be At L and At R for the location of the robot and Clean L and Clean R for
the state of the squares. According to the definition of the problem the Suck action has no
preconditionitcanalwaysbedone. Thedifficultyisthatitseffectdependsontherobotslo-
cation: whentherobotis At Ltheresultis Clean Lbutwhenitis At Rtheresultis Clean R.
C ON DI TI ON AL Forsuch actions ouraction schemas will need something new: a conditional effect. These
E FF EC T
have the syntax when condition: effect where condition is a logical formula to be com-
pared against the current state and effect is a formula describing the resulting state. Forthe
vacuumworldwehave
Action Suck
E FF EC T:when At L:Clean Lwhen At R:Clean R.
When applied to the initial belief state True the resulting belief state is At L Clean L
At R Clean R which is no longer in 1-C NF. This transition can be seen in Figure 4.14
on page 141. In general conditional effects can induce arbitrary dependencies among the
fluentsinabeliefstate leadingtobeliefstatesofexponential sizeintheworstcase.
It is important to understand the difference between preconditions and conditional ef-
fects. Allconditionaleffectswhoseconditionsaresatisfiedhavetheireffectsappliedtogener-
atetheresultingstate;ifnonearesatisfiedthentheresultingstateisunchanged. Ontheother
hand if a precondition is unsatisfied then the action is inapplicable and the resulting state
is undefined. From the point of view of sensorless planning it is better to have conditional
effects than an inapplicable action. Forexample we could split Suck into two actions with
unconditional effectsasfollows:
Action Suck L
P RE CO ND:At L; E FF EC T:Clean L
Action Suck R
P RE CO ND:At R; E FF EC T:Clean R.
Nowwehaveonly unconditional schemas sothebelief states allremainin1-C NF;unfortu-
nately wecannot determinetheapplicability of Suck Land Suck R intheinitialbeliefstate.
Itseemsinevitable thenthatnontrivial problemswillinvolve wigglybeliefstates just
like those encountered when we considered the problem of state estimation for the wumpus
worldsee Figure7.21onpage271. Thesolution suggested thenwastouseaconservative
approximation to the exact belief state; for example the belief state can remain in 1-C NF
if it contains all literals whose truth values can be determined and treats all other literals as
unknown. While this approach is sound in that it never generates an incorrect plan it is
incomplete because it may be unable to find solutions to problems that necessarily involve
interactions among literals. To give a trivial example if the goal is for the robot to be on
a clean square then Suck is a solution but a sensorless agent that insists on 1-C NF belief
stateswillnotfindit.
Perhaps a better solution is to look for action sequences that keep the belief state
as simple as possible. For example in the sensorless vacuum world the action sequence
Right Suck Left Suckgenerates thefollowingsequence ofbeliefstates:
b True
0
b At R
1
b At R Clean R
2
b At L Clean R
3
b At L Clean R Clean L
4
That is the agent can solve the problem while retaining a 1-C NF belief state even though
some sequences e.g. those beginning with Suck go outside 1-C NF. The general lesson is
not lost on humans: we are always performing little actions checking the time patting our
pocketstomakesurewehavethecarkeysreadingstreetsignsaswenavigatethroughacity
toeliminate uncertainty andkeepourbeliefstatemanageable.
There is another quite different approach to the problem of unmanageably wiggly be-
lief states: dont bother computing them at all. Suppose the initial belief state is b and we
0
would like to know the belief state resulting from the action sequence a ...a . Instead
of computing it explicitly just represent it as b then a ...a . This is a lazy but un-
ambiguous representation of the belief state and its quite concise On m where n is
the size of the initial belief state assumed to be in 1-C NF and m is the maximum length
of an action sequence. As a belief-state representation it suffers from one drawback how-
ever: determining whether the goal is satisfied or an action is applicable may require a lot
ofcomputation.
Thecomputationcanbeimplementedasanentailmenttest: if A representsthecollec-
m
tion of successor-state axioms required to define occurrences of the actions a ...a as
explainedfor S AT PL ANin Section10.4.1and G massertsthatthegoalistrueaftermsteps
thentheplanachievesthegoalifb A G thatisifb A G isunsatisfiable.
Givenamodern S ATsolveritmaybepossibletodothismuchmorequicklythancomputing
thefull belief state. Forexample ifnoneofthe actions inthe sequence hasaparticular goal
fluent in its add list the solver will detect this immediately. It also helps if partial results
aboutthebeliefstateforexamplefluentsknowntobetrueorfalsearecachedtosimplify
subsequent computations.
The final piece of the sensorless planning puzzle is a heuristic function to guide the
search. The meaning of the heuristic function is the same as for classical planning: an esti-
mateperhaps admissible ofthecostofachieving thegoal from thegivenbelief state. With
belief states we have one additional fact: solving any subset of a belief state is necessarily
easierthansolvingthebeliefstate:
ifb b thenh b h b .
Henceanyadmissibleheuristiccomputedforasubsetisadmissibleforthebeliefstateitself.
Themostobvious candidates arethesingleton subsets thatisindividual physicalstates. We
Section11.3. Planningand Actingin Nondeterministic Domains 421
can take any random collection of states s ...s that are in the belief state b apply any
admissible heuristic hfrom Chapter10andreturn
Hb maxhs ...hs
astheheuristicestimateforsolving b. Wecouldalsouseaplanninggraphdirectlyonbitself:
if it is a conjunction of literals 1-C NF simply set those literals to be the initial state layer
ofthegraph. Ifbisnotin1-C NFitmaybepossibletofindsetsofliteralsthattogetherentail
b. For example if b is in disjunctive normal form D NF each term of the D NF formula is
a conjunction of literals that entails b and can form the initial layer of a planning graph. As
beforewecantakethemaximumoftheheuristics obtainedfromeachsetofliterals. Wecan
also use inadmissible heuristics such as the ignore-delete-lists heuristic page 377 which
seemstoworkquitewellinpractice.
We saw in Chapter 4 that contingent planningthe generation of plans with conditional
branchingbasedonperceptsisappropriateforenvironmentswithpartialobservabilitynon-
determinism orboth. Forthepartially observable painting problem withthepercept axioms
givenearlier onepossible contingent solution isasfollows:
Look At Table Look At Chair
if Color Tablec Color Chaircthen No Op
else Remove Lid Can Look At Can Remove Lid Can Look At Can
if Color Tablec Colorcancthen Paint Chaircan
elseif Color Chairc Colorcancthen Paint Tablecan
else Paint Chair Can Paint Table Can
Variables in this plan should be considered existentially quantified; the second line says
that if there exists some color c that is the color of the table and the chair then the agent
need not do anything to achieve the goal. When executing this plan a contingent-planning
agent can maintain its belief state as a logical formula and evaluate each branch condition
by determining if the belief state entails the condition formula or its negation. It is up to
the contingent-planning algorithm to make sure that the agent will never end up in a be-
lief state where the condition formulas truth value is unknown. Note that with first-order
conditions the formula may be satisfied in more than one way; for example the condition
Color Tablec Colorcancmightbesatisfiedbycan Can andbycan Can if
both cans are the same color as the table. In that case the agent can choose any satisfying
substitution toapplytotherestoftheplan.
As shown in Section 4.4.2 calculating the new belief state after an action and subse-
quentperceptisdoneintwostages. Thefirststagecalculates thebeliefstateaftertheaction
justasforthesensorless agent:
ˆb b D ELa A DDa
whereasbeforewehaveassumedabeliefstaterepresented asaconjunction ofliterals. The
second stage is a little trickier. Suppose that percept literals p ...p are received. One
might think that we simply need to add these into the belief state; in fact we can also infer
that the preconditions for sensing are satisfied. Now if a percept p has exactly one percept
axiom Perceptp P RE CO ND:c where c is a conjunction of literals then those literals can
bethrownintothebeliefstatealongwithp. Ontheotherhandifphasmorethanonepercept
axiomwhosepreconditions
mightholdaccordingtothepredictedbeliefstateˆbthenwehave
to add in the disjunction of the preconditions. Obviously this takes the belief state outside
1-C NF and brings up the same complications as conditional effects with much the same
classesofsolutions.
Givenamechanism forcomputing exact orapproximate belief states wecan generate
contingent plans with an extension of the A ND OR forward search over belief states used
in Section 4.4. Actions with nondeterministic effectswhich are defined simply by using a
disjunction inthe E FF EC T oftheactionschemacan beaccommodated withminorchanges
tothebelief-stateupdatecalculationandnochangetothesearchalgorithm.2 Fortheheuristic
function many of the methods suggested for sensorless planning are also applicable in the
partially observable nondeterministic case.
Imagine watching aspot-welding robot inacarplant. Therobots fast accurate motions are
repeated over and over again as each car passes down the line. Although technically im-
pressive the robot probably does not seem at all intelligent because the motion is a fixed
preprogrammed sequence; the robot obviously doesnt know what its doing inany mean-
ingful sense. Now suppose that a poorly attached door falls off the car just as the robot is
about to apply a spot-weld. The robot quickly replaces its welding actuator with a gripper
picks up the door checks it forscratches reattaches it to the car sends an email to the floor
supervisor switches back to the welding actuator and resumes its work. All of a sudden
the robots behavior seems purposive rather than rote; we assume it results not from a vast
precomputed contingent plan but from an online replanning processwhich means that the
robot doesneedtoknowwhatitstryingtodo.
E XE CU TI ON Replanningpresupposessomeformofexecutionmonitoringtodeterminetheneedfor
M ON IT OR IN G
a new plan. One such need arises when a contingent planning agent gets tired of planning
forevery little contingency such as whether the sky might fall on its head.3 Some branches
ofapartiallyconstructed contingent plancansimplysay Replan;ifsuchabranchisreached
during execution the agent reverts to planning mode. Aswementioned earlier the decision
as to how much of the problem to solve in advance and how much to leave to replanning
is one that involves tradeoffs among possible events with different costs and probabilities of
occurring. Nobodywantstohavetheircarbreakdowninthemiddleofthe Saharadesertand
onlythenthinkabouthavingenough water.
versionsuchas L AO Hansenand Zilberstein2001.
the Mbalemeteoritehitasmallboyonthehead;fortunatelyitsdescentwasslowedbybananaleaves Jenniskens
etal.1994.Andin2009a Germanboyclaimedtohavebeenhitinthehandbyapea-sizedmeteorite.Noserious
injuriesresultedfromanyoftheseincidentssuggestingthattheneedforpreplanningagainstsuchcontingencies
issometimesoverstated.
Section11.3. Planningand Actingin Nondeterministic Domains 423
whole plan
plan
S P E G
continuation
repair
O
Figure11.9 Beforeexecutiontheplannercomesupwithaplanherecalledwhole plan
togetfrom S to G. Theagentexecutesstepsoftheplanuntilitexpectstobeinstate Ebut
observesitisactuallyin O. Theagentthenreplansfortheminimalrepairpluscontinuation
toreach G.
Replanningmayalsobeneedediftheagentsmodeloftheworldisincorrect. Themodel
M IS SI NG foran action may have a missing preconditionfor example the agent may not know that
P RE CO ND IT IO N
removing the lid of a paint can often requires a screwdriver; the model may have a missing
effectforexample painting anobjectmaygetpaintontheflooraswell;orthemodelmay
M IS SI NG EF FE CT
M IS SI NG ST AT E have a missing state variablefor example the model given earlier has no notion of the
V AR IA BL E
amountofpaint inacanofhowitsactions affectthisamount oroftheneedfortheamount
to be nonzero. The model may also lack provision for exogenous events such as someone
E XO GE NO US EV EN T
knocking over the paint can. Exogenous events can also include changes in the goal such
as the addition of the requirement that the table and chair not be painted black. Without the
ability to monitor and replan an agents behavior is likely to be extremely fragile if it relies
onabsolutecorrectness ofitsmodel.
Theonline agent hasachoice ofhow carefully tomonitortheenvironment. Wedistin-
guishthreelevels:
Actionmonitoring: beforeexecutinganactiontheagentverifiesthatalltheprecondi-
A CT IO NM ON IT OR IN G
tionsstillhold.
Planmonitoring: beforeexecutinganactiontheagentverifiesthattheremainingplan
P LA NM ON IT OR IN G
willstillsucceed.
Goalmonitoring: beforeexecutinganactiontheagentcheckstoseeifthereisabetter
G OA LM ON IT OR IN G
setofgoalsitcouldbetryingtoachieve.
In Figure 11.9 we see a schematic of action monitoring. The agent keeps track of both its
original plan wholeplan and the part of the plan that has not been executed yet which is
denoted by plan. After executing the first few steps of the plan the agent expects to be in
state E. But the agent observes it is actually in state O. It then needs to repair the plan by
findingsomepoint P ontheoriginalplanthatitcangetbackto. Itmaybethat P isthegoal
state G. Theagenttriestominimizethetotalcostoftheplan: therepairpartfrom Oto P
plusthecontinuation from P to G.
Now lets return to the example problem of achieving a chair and table of matching
color. Supposetheagentcomesupwiththisplan:
Look At Table Look At Chair
if Color Tablec Color Chaircthen No Op
else Remove Lid Can Look At Can
if Color Tablec Color Can cthen Paint Chair Can
else R EP LA N.
Now the agent is ready to execute the plan. Suppose the agent observes that the table and
can of paint are white and the chair is black. It then executes Paint Chair Can . At this
1
point a classical planner would declare victory; the plan has been executed. But an online
execution monitoring agent needs tocheck thepreconditions ofthe remaining emptyplan
that the table and chair are the same color. Suppose the agent perceives that they do not
have the same colorin fact the chair is now a mottled gray because the black paint is
showing through. The agent then needs to figure out a position in whole plan to aim for
and arepair action sequence toget there. Theagent notices thatthe current state isidentical
to the precondition before the Paint Chair Can action so the agent chooses the empty
1
sequence for repair and makes its plan be thesame Paintsequence that it just attempted.
With this new plan in place execution monitoring resumes and the Paint action is retried.
Thisbehavior willloop until thechairisperceived tobecompletely painted. Butnotice that
the loop is created by a process of planexecutereplan rather than by an explicit loop in a
plan. Notealso that the original plan need notcoverevery contingency. Ifthe agent reaches
thestepmarked R EP LA Nitcanthengenerate anewplanperhaps involving Can 2.
Action monitoring is a simple method of execution monitoring but it can sometimes
lead to less than intelligent behavior. Forexample suppose there is no black orwhite paint
and the agent constructs a plan to solve the painting problem by painting both the chair and
table red. Suppose that there isonly enough red paint forthe chair. With action monitoring
theagentwouldgoaheadandpaintthechairredthennoticethatitisoutofpaintandcannot
paintthetableatwhichpointitwouldreplanarepairperhaps paintingbothchairandtable
green. Aplan-monitoring agent candetect failure whenever the current state issuch that the
remaining plan no longer works. Thus it would not waste time painting the chair red. Plan
monitoring achieves this by checking the preconditions for success of the entire remaining
planthat is the preconditions of each step in the plan except those preconditions that are
achieved by another step in the remaining plan. Plan monitoring cuts off execution of a
doomed plan as soon as possible rather than continuing until the failure actually occurs.4
Plan monitoring also allows for serendipityaccidental success. If someone comes along
andpaintsthetableredatthesametimethattheagentispaintingthechairredthenthefinal
planpreconditionsaresatisfiedthegoalhasbeenachievedandtheagentcangohomeearly.
It is straightforward to modify a planning algorithm so that each action in the plan
is annotated with the actions preconditions thus enabling action monitoring. It is slightly
page39. Aplan-monitoringagentwouldnoticethatthedungballwasmissingfromitsgraspandwouldreplan
togetanotherballandplugitshole.
Section11.4. Multiagent Planning 425
more complex to enable plan monitoring. Partial-order and planning-graph planners have
the advantage that they have already built up structures that contain the relations necessary
forplanmonitoring. Augmentingstate-space planners with thenecessary annotations canbe
donebycarefulbookkeeping asthegoalfluentsareregressed throughtheplan.
Now that we have described a method for monitoring and replanning we need to ask
Does it work? This is a surprisingly tricky question. If we mean Can we guarantee that
the agent will always achieve the goal? then the answer is no because the agent could
inadvertently arrive at a dead end from which there is no repair. For example the vacuum
agent might have a faulty model of itself and not know that its batteries can run out. Once
theydoitcannot repairanyplans. Ifweruleoutdeadendsassume thatthereexists aplan
to reach the goal from any state in the environmentand assume that the environment is
really nondeterministic in the sense that such a plan always has some chance of success on
anygivenexecutionattemptthentheagentwilleventually reachthegoal.
Trouble occurs when an action is actually not nondeterministic but rather depends on
some precondition that the agent does not know about. For example sometimes a paint
canmaybeemptysopaintingfromthatcanhasnoeffect. Noamountofretrying isgoingto
changethis.5 Onesolutionistochooserandomlyfromamongthesetofpossiblerepairplans
ratherthantotrythesameoneeachtime. Inthiscasetherepairplanofopening anothercan
might work. A better approach is to learn a better model. Every prediction failure is an
opportunity forlearning; anagent should be able tomodify itsmodel ofthe worldto accord
withitspercepts. Fromthen onthereplanner willbeable to comeupwitharepairthatgets
attherootproblemratherthanrelyingonlucktochooseagoodrepair. Thiskindoflearning
isdescribed in Chapters18and19.
So far we have assumed that only one agent is doing the sensing planning and acting.
Whentherearemultiple agentsintheenvironment eachagent facesamultiagentplanning
M UL TI AG EN T probleminwhichittriestoachieveitsowngoalswiththehelporhindrance ofothers.
P LA NN IN GP RO BL EM
Betweenthepurelysingle-agent andtrulymultiagentcases isawidespectrumofprob-
lems that exhibit various degrees of decomposition of the monolithic agent. An agent with
multiple effectors that can operate concurrentlyfor example a human who can type and
M UL TI EF FE CT OR speak atthe same timeneeds to do multieffector planningto manage each effector while
P LA NN IN G
handling positive and negative interactions among the effectors. When the effectors are
physically decoupled into detached unitsas in a fleet of delivery robots in a factory
M UL TI BO DY multieffector planning becomes multibody planning. A multibody problem is still a stan-
P LA NN IN G
dardsingle-agent problemaslongastherelevantsensorinformationcollectedbyeachbody
can be pooledeither centrally or within each bodyto form a common estimate of the
worldstatethattheninformstheexecution oftheoverallplan;inthiscase themultiplebod-
ies act as a single body. When communication constraints make this impossible we have
D EC EN TR AL IZ ED whatissometimescalledadecentralizedplanningproblem;thisisperhapsamisnomerbe-
P LA NN IN G
causetheplanningphaseiscentralizedbuttheexecutionphaseisatleastpartiallydecoupled.
Inthiscasethesubplanconstructed foreachbodymayneedtoincludeexplicitcommunica-
tiveactions withotherbodies. Forexample multiple reconnaissance robots covering awide
areamayoftenbeoutofradiocontactwitheachotherandshouldsharetheirfindingsduring
timeswhencommunication isfeasible.
When a single entity is doing the planning there is really only one goal which all the
bodiesnecessarilyshare. Whenthebodiesaredistinctagentsthatdotheirownplanningthey
may still share identical goals; for example two human tennis players who form a doubles
team share the goal of winning the match. Even with shared goals however the multibody
and multiagent cases are quite different. In a multibody robotic doubles team a single plan
dictates which body willgo where on the court and which body willhit the ball. Ina multi-
agent doubles teamontheotherhand eachagent decides whattodo;without somemethod
for coordination both agents may decide to cover the same part of the court and each may
C OO RD IN AT IO N
leavetheballfortheothertohit.
Theclearest caseofamultiagent problem ofcourse iswhen the agents havedifferent
goals. In tennis the goals of two opposing teams are in direct conflict leading to the zero-
sum situation of Chapter 5. Spectators could be viewed as agents if their support ordisdain
is a significant factor and can be influenced by the players conduct; otherwise they can be
treated asanaspect ofnaturejust liketheweatherthat is assumedtobeindifferent tothe
players intentions.6
Finally some systems are a mixture of centralized and multiagent planning. For ex-
ample a delivery company may do centralized offline planning for the routes of its trucks
and planes each day but leave some aspects open for autonomous decisions by drivers and
pilots who can respond individually to traffic and weather situations. Also the goals of the
company and its employees are brought into alignment to some extent by the payment of
incentivessalariesandbonusesa suresignthatthisisatruemultiagentsystem.
I NC EN TI VE
The issues involved in multiagent planning can be divided roughly into two sets. The
first covered in Section 11.4.1 involves issues of representing and planning for multiple
simultaneous actions;theseissuesoccurinallsettingsfrommultieffectortomultiagentplan-
ning. The second covered in Section 11.4.2 involves issues of cooperation coordination
andcompetition arisingintruemultiagentsettings.
Forthe time being wewill treat the multieffector multibody and multiagent settings in the
same way labeling them generically as multiactor settings using the generic term actor to
M UL TI AC TO R
cover effectors bodies and agents. The goal of this section is to work out how to define
A CT OR
transition models correct plans and efficient planning algorithms forthe multiactor setting.
Acorrectplanisonethatifexecutedbytheactorsachievesthegoal. Inthetruemultiagent
setting of course the agents may not agree to execute any particular plan but at least they
guaranteesrain.
Section11.4. Multiagent Planning 427
Actors A B
Init At A Left Baseline At B Right Net
Approaching Ball Right Baseline Partner A B Partner B A
Goal Returned Ball Ata Right Net Ata Left Net
Action Hitactor Ball
P RE CO ND:Approaching Ballloc Atactorloc
E FF EC T:Returned Ball
Action Goactorto
P RE CO ND:Atactorloc to cid:7 loc
E FF EC T:Atactorto Atactorloc
Figure11.10 Thedoublestennisproblem. Twoactors Aand B areplayingtogetherand
canbeinoneoffourlocations: Left Baseline Right Baseline Left Netand Right Net. The
ballcanbereturnedonlyifaplayerisintherightplace. Notethateachactionmustinclude
theactorasanargument.
will know what plans would work if they did agree to execute them. For simplicity we
assume perfect synchronization: each action takes the same amount of time and actions at
S YN CH RO NI ZA TI ON
eachpointinthejointplanaresimultaneous.
We begin with the transition model; for the deterministic case this is the function
R ES UL Tsa. In the single-agent setting there might be b different choices for the action;
b can be quite large especially for first-order representations with many objects to act on
but action schemas provide a concise representation nonetheless. In the multiactor setting
with n actors the single action a is replaced by a joint action cid:16a ...a cid:17 where a is the
J OI NT AC TI ON 1 n i
action taken by the ith actor. Immediately we see two problems: first we have to describe
the transition model for bn different joint actions; second we have a joint planning problem
withabranching factorofbn.
Having put the actors together into a multiactor system with a huge branching factor
the principal focus of research on multiactor planning has been to decouple the actors to
the extent possible so that the complexity of the problem grows linearly with n rather than
exponentially. Iftheactorshavenointeractionwithoneanotherforexamplenactorseach
playing agameofsolitairethen wecansimplysolve nseparate problems. Iftheactors are
loosely coupledcanweattainsomething closetothisexponential improvement? Thisisof
L OO SE LY CO UP LE D
course a central question in many areas of A I. We have seen it explicitly in the context of
C SPswhere tree like constraint graphs yielded efficient solution methods see page 225
as well as in the context of disjoint pattern databases page 106 and additive heuristics for
planning page378.
Thestandardapproachtolooselycoupledproblemsistopretendtheproblemsarecom-
pletelydecoupledandthenfixuptheinteractions. Forthetransitionmodelthismeanswriting
actionschemasasiftheactorsactedindependently. Letsseehowthisworksforthedoubles
tennisproblem. Letssupposethatatonepointinthegametheteamhasthegoalofreturning
the ball that has been hit to them and ensuring that at least one of them is covering the net.
A firstpass at amultiactor definition might look like Figure 11.10. With this definition itis
easytoseethatthefollowingjointplanplanworks:
J OI NT PL AN
P LA N 1:
A: Go A Right Baseline Hit A Ball
B : No Op B No Op B.
Problemsarisehoweverwhenaplanhasbothagentshitting theballatthesametime. Inthe
real world this wont work but the action schema for Hit says that the ball will be returned
successfully. Technically the difficulty is that preconditions constrain the state in which an
action canbeexecuted successfully butdonotconstrain otheractions thatmightmessitup.
C ON CU RR EN T Wesolvethisbyaugmenting action schemaswithonenewfeature: a concurrentaction list
A CT IO NL IS T
statingwhichactionsmustormustnotbeexecutedconcurrently. Forexamplethe Hit action
couldbedescribed asfollows:
Action Hita Ball
C ON CU RR EN T:bcid:7 a Hitb Ball
P RE CO ND:Approaching Ballloc Ataloc
E FF EC T:Returned Ball.
In other words the Hit action has its stated effect only if no other Hit action by another
agent occurs at the same time. In the S AT PL AN approach this would be handled by a
partial action exclusion axiom. Forsome actions the desired effect is achieved only when
anotheractionoccursconcurrently. Forexampletwoagentsareneededtocarryacoolerfull
ofbeverages tothetenniscourt:
Action Carryacoolerherethere
C ON CU RR EN T:b cid:7 a Carrybcoolerherethere
P RE CO ND:Atahere Atcoolerhere Coolercooler
E FF EC T:Atathere Atcoolerthere Atahere Atcoolerhere.
With these kinds of action schemas any of the planning algorithms described in Chapter 10
canbeadaptedwithonlyminormodificationstogeneratemultiactorplans. Totheextentthat
thecouplingamongsubplansisloosemeaning thatconcurrency constraints comeintoplay
only rarely during plan searchone would expect the various heuristics derived for single-
agent planning to also be effective in the multiactor context. Wecould extend this approach
withtherefinementsofthelasttwochapters H TNspartialobservability conditionals exe-
cutionmonitoring andreplanningbut thatisbeyondthescopeofthisbook.
Now let us consider the true multiagent setting in which each agent makes its own plan. To
start with let us assume that the goals and knowledge base are shared. One might think
that this reduces to the multibody caseeach agent simply computes the joint solution and
executes its own part of that solution. Alas the the in the joint solution is misleading.
Forourdoublesteammorethanonejointsolutionexists:
P LA N 2:
A: Go A Left Net No Op A
B : Go B Right Baseline Hit B Ball.
Section11.4. Multiagent Planning 429
Ifbothagentscanagreeoneitherplan1orplan2thegoalwillbeachieved. Butif Achooses
plan2and Bchoosesplan1thennobodywillreturntheball. Converselyif Achooses1and
B chooses 2then theywillboth trytohittheball. Theagents mayrealize this but howcan
theycoordinate tomakesuretheyagreeontheplan?
Oneoption is to adopt aconvention before engaging in joint activity. A convention is
C ON VE NT IO N
anyconstraintontheselectionofjointplans. Forexample theconvention sticktoyourside
ofthecourt wouldrule outplan 1causing thedoubles partners toselect plan 2. Driverson
aroadfacetheproblemofnotcollidingwitheachother;this ispartiallysolvedbyadopting
the convention stay on the right side of the road in most countries; the alternative stay
on the left side works equally well as long as all agents in an environment agree. Similar
considerations applytothedevelopmentofhumanlanguage wheretheimportantthingisnot
which language each individual should speak but the fact that a community all speaks the
samelanguage. Whenconventions arewidespread theyarecalled sociallaws.
S OC IA LL AW S
In the absence of a convention agents can use communication to achieve common
knowledge of a feasible joint plan. For example a tennis player could shout Mine! or
Yours! toindicateapreferredjointplan. Wecovermechanismsforcommunicationinmore
depth in Chapter 22 where we observe that communication does not necessarily involve a
verbalexchange. Forexampleoneplayercancommunicateapreferredjointplantotheother
simply by executing the first part of it. If agent Aheads forthe net then agent B is obliged
togobacktothebaselinetohittheballbecauseplan2istheonlyjointplanthatbeginswith
As heading forthe net. This approach to coordination sometimes called planrecognition
P LA NR EC OG NI TI ON
workswhenasingleactionorshortsequence ofactionsisenoughtodetermineajointplan
unambiguously. Note that communication can workas wellwith competitive agents aswith
cooperative ones.
Conventions can also arise through evolutionary processes. For example seed-eating
harvester ants are social creatures that evolved from the less social wasps. Colonies of ants
execute very elaborate joint plans without any centralized controlthe queens job is to re-
produce not to do centralized planningand with very limited computation communica-
tionandmemorycapabilities ineachant Gordon 20002007. Thecolonyhasmanyroles
including interior workers patrollers and foragers. Each ant chooses to perform a role ac-
cording tothe local conditions it observes. Forexample foragers travel awayfrom the nest
searchforaseedandwhentheyfindone bringitbackimmediately. Thustherateatwhich
foragers return to the nest is an approximation of the availability of food today. When the
rateishigh otherantsabandon theircurrent roleandtakeontheroleofscavenger. Theants
appeartohaveaconventionontheimportanceofrolesforagingisthemostimportantand
antswilleasilyswitchintothemoreimportantroles butnotintothelessimportant. Thereis
somelearningmechanism: acolonylearnstomakemoresuccessfulandprudentactionsover
thecourseofitsdecades-long lifeeventhoughindividual antsliveonlyaboutayear.
Onefinalexampleofcooperative multiagent behavior appears intheflocking behavior
of birds. We can obtain a reasonable simulation of a flock if each bird agent sometimes
B OI D called a boid observes the positions of its nearest neighbors and then chooses the heading
andacceleration thatmaximizestheweightedsumofthesethreecomponents:
12
K NO WL ED GE
R EP RE SE NT AT IO N
In which we show how to use first-order logic to represent the most important
aspectsoftherealworldsuchasaction spacetimethoughts andshopping.
The previous chapters described the technology for knowledge-based agents: the syntax
semantics andproof theoryofpropositional andfirst-orderlogic andtheimplementation of
agents that use these logics. In this chapter we address the question of what content to put
intosuchanagentsknowledge basehowtorepresent factsabouttheworld.
Section 12.1 introduces the idea of a general ontology which organizes everything in
the world into a hierarchy of categories. Section 12.2 covers the basic categories of objects
substances andmeasures;Section12.3coverseventsand Section12.4discussesknowledge
about beliefs. We then return to consider the technology for reasoning with this content:
Section 12.5 discusses reasoning systems designed for efficient inference with categories
and Section 12.6 discusses reasoning with default information. Section 12.7 brings all the
knowledgetogetherinthecontextofan Internet shopping environment.
Intoydomainsthechoiceofrepresentation isnotthatimportant;manychoiceswillwork.
Complex domains such as shopping on the Internet or driving a car in traffic require more
general andflexiblerepresentations. Thischaptershowshowtocreatethese representations
concentrating on general conceptssuch as Events Time Physical Objects and Beliefs
that occur in many different domains. Representing these abstract concepts is sometimes
O NT OL OG IC AL calledontological engineering.
E NG IN EE RI NG
Theprospect of representing everything in the world is daunting. Ofcourse wewont
actually write a complete description of everythingthat would be far too much for even a
1000-page textbookbut we will leave placeholders where new knowledge for any domain
canfitin. Forexamplewewilldefinewhatitmeanstobeaphysicalobjectandthedetailsof
differenttypesofobjectsrobotstelevisionsbooksorwhatevercanbefilledinlater. This
isanalogoustothewaythatdesignersofanobject-oriented programmingframeworksuchas
the Java Swinggraphicalframeworkdefinegeneralconcepts like Windowexpectingusersto
437
Anything
Abstract Objects Generalized Events
Sets Numbers Representational Objects Interval Places Physical Objects Processes
Categories Sentences Measurements Moments Things Stuff
Times Weights Animals Agents Solid Liquid Gas
Humans
thechapter. Each linkindicatesthatthe lowerconceptisa specializationof the upperone.
Specializations are not necessarily disjoint; a human is both an animal and an agent for
example.Wewillseein Section12.3.3whyphysicalobjectscomeundergeneralizedevents.
use these to define more specific concepts like Spreadsheet Window. The general framework
of concepts is called an upper ontology because of the convention of drawing graphs with
U PP ER ON TO LO GY
thegeneralconceptsatthetopandthemorespecificconcepts belowthemasin Figure12.1.
Before considering the ontology further we should state one important caveat. We
have elected to use first-order logic to discuss the content and organization of knowledge
althoughcertainaspectsoftherealworldarehardtocapturein F OL.Theprincipaldifficulty
isthatmostgeneralizations haveexceptions orholdonly to adegree. Forexample although
tomatoes are red is a useful rule some tomatoes are green yellow or orange. Similar
exceptionscanbefoundtoalmostalltherulesinthischapter. Theabilitytohandleexceptions
and uncertainty is extremely important but is orthogonal to the task of understanding the
generalontology. Forthisreasonwedelaythediscussionofexceptionsuntil Section12.5of
thischapter andthemoregeneraltopicofreasoning withuncertainty until Chapter13.
Ofwhat use is an upper ontology? Consider the ontology for circuits in Section 8.4.2.
Itmakesmanysimplifyingassumptions: timeisomittedcompletely; signalsarefixedanddo
not propagate; the structure of the circuit remains constant. Amore general ontology would
consider signals at particular times and would include the wire lengths and propagation de-
lays. This would allow us to simulate the timing properties of the circuit and indeed such
simulations are often carried out by circuit designers. We could also introduce more inter-
esting classes of gates for example by describing the technology T TL CM OSand so on
aswellastheinputoutput specification. Ifwewantedtodiscuss reliability ordiagnosis we
would include the possibility that the structure of the circuit or the properties of the gates
might change spontaneously. Toaccount forstray capacitances wewould need to represent
wherethewiresareontheboard.
Section12.1. Ontological Engineering 439
Ifwelookatthewumpusworldsimilarconsiderationsapply. Althoughwedorepresent
time ithasasimplestructure: Nothing happens except when theagent acts andallchanges
areinstantaneous. Amoregeneral ontology bettersuitedfortherealworldwouldallowfor
simultaneouschangesextendedovertime. Wealsouseda Pit predicatetosaywhichsquares
have pits. We could have allowed for different kinds of pits by having several individuals
belonging to the class of pits each having different properties. Similarly we might want to
allow for other animals besides wumpuses. It might not be possible to pin down the exact
species from the available percepts so we would need to build up a biological taxonomy to
helptheagentpredictthebehaviorofcave-dwellers fromscantyclues.
For any special-purpose ontology it is possible to make changes like these to move
toward greater generality. Anobvious question then arises: doall these ontologies converge
on a general-purpose ontology? After centuries of philosophical and computational inves-
tigation the answer is Maybe. In this section we present one general-purpose ontology
that synthesizes ideas from those centuries. Two major characteristics of general-purpose
ontologies distinguish themfromcollections ofspecial-purpose ontologies:
A general-purpose ontology should be applicable in more or less any special-purpose
domain with the addition of domain-specific axioms. This means that no representa-
tionalissuecanbefinessedorbrushed underthecarpet.
In any sufficiently demanding domain different areas of knowledge must be unified
because reasoning and problem solving could involve several areas simultaneously. A
robotcircuit-repairsystemforinstanceneedstoreason aboutcircuitsintermsofelec-
trical connectivity and physical layout and about time both forcircuit timing analysis
and estimating labor costs. The sentences describing time therefore must be capable
ofbeing combinedwiththosedescribing spatial layoutandmustworkequally wellfor
nanoseconds andminutesandforangstroms andmeters.
We should say up front that the enterprise of general ontological engineering has so far had
only limited success. None of the top A I applications as listed in Chapter 1 make use
of a shared ontologythey all use special-purpose knowledge engineering. Socialpolitical
considerations can make it difficult for competing parties to agree on an ontology. As Tom
Gruber 2004 says Every ontology is a treatya social agreementamong people with
some common motive in sharing. When competing concerns outweigh the motivation for
sharing therecanbenocommonontology. Thoseontologies thatdoexisthavebeencreated
alongfourroutes:
1. Byateamoftrainedontologistlogicians whoarchitect theontologyandwriteaxioms.
The C YC systemwasmostlybuiltthisway Lenatand Guha1990.
2. Byimporting categories attributes and values from an existing database ordatabases.
D BP ED IA wasbuiltbyimportingstructured factsfrom Wikipedia Bizeretal.2007.
3. Byparsing text documents and extracting information from them. T EX TR UN NE R was
builtbyreadingalargecorpusof Webpages Bankoand Etzioni2008.
4. By enticing unskilled amateurs to enter commonsense knowledge. The O PE NM IN D
system was built by volunteers who proposed facts in English Singh et al. 2002;
Chklovskiand Gil2005.
The organization of objects into categories is a vital part of knowledge representation. Al-
C AT EG OR Y
thoughinteraction withtheworldtakesplaceatthelevelof individual objects muchreason-
ing takes place at the level of categories. For example a shopper would normally have the
goalofbuying abasketball ratherthanaparticular basketball suchas B B . Categoriesalso
9
serve to make predictions about objects once they are classified. One infers the presence of
certainobjectsfromperceptualinputinferscategorymembershipfromtheperceivedproper-
tiesoftheobjects andthenusescategory information tomakepredictions abouttheobjects.
For example from its green and yellow mottled skin one-foot diameter ovoid shape red
fleshblackseedsandpresenceinthefruitaisleonecaninferthatanobjectisawatermelon;
fromthisoneinfersthatitwouldbeusefulforfruitsalad.
There are two choices for representing categories in first-order logic: predicates and
objects. That is we can use the predicate Basketballb or we can reify1 the category as
R EI FI CA TI ON
an object Basketballs. We could then say Memberb Basketballs which we will abbre-
viate as b Basketballs to say that b is a member of the category of basketballs. We say
Subset Basketballs Balls abbreviated as Basketballs Balls tosay that Basketballs is
asubcategoryof Balls. Wewillusesubcategory subclass andsubsetinterchangeably.
S UB CA TE GO RY
Categories serve to organize and simplify the knowledge base through inheritance. If
I NH ER IT AN CE
we say that all instances of the category Food are edible and if we assert that Fruit is a
subclass of Food and Apples is a subclass of Fruit then we can infer that every apple is
edible. We say that the individual apples inherit the property of edibility in this case from
theirmembership inthe Food category.
Subclassrelationsorganizecategoriesintoataxonomyortaxonomichierarchy. Tax-
T AX ON OM Y
onomieshavebeenusedexplicitlyforcenturiesintechnicalfields. Thelargestsuchtaxonomy
organizesabout10millionlivingandextinctspeciesmanyofthembeetles2 intoasinglehi-
erarchy;librarysciencehasdeveloped ataxonomyofallfieldsofknowledge encoded asthe
Dewey Decimal system; and tax authorities and other government departments have devel-
opedextensivetaxonomiesofoccupationsandcommercialproducts. Taxonomiesarealsoan
importantaspectofgeneralcommonsense knowledge.
First-order logic makes it easy to state facts about categories either by relating ob-
jects to categories orby quantifying overtheir members. Here are some types of facts with
examplesofeach:
Anobjectisamemberofacategory.
B B Basketballs
9
Acategoryisasubclassofanothercategory.
Basketballs Balls
Allmembersofacategoryhavesomeproperties.
x Basketballs Sphericalx
proposedthetermthingificationbutitnevercaughton.
Section12.2. Categoriesand Objects 441
Membersofacategory canberecognized bysomeproperties.
Orangex Roundx Diameterx9.5cid:2cid:2x Balls x Basketballs
Acategoryasawholehassomeproperties.
Dogs Domesticated Species
Notice that because Dogs is acategory and is amemberof Domesticated Species the latter
must be a category of categories. Of course there are exceptions to many of the above rules
punctured basketballs arenotspherical; wedealwiththeseexceptions later.
Although subclass and member relations are the most important ones for categories
we also want to be able to state relations between categories that are not subclasses of each
other. Forexample if wejust say that Males and Females are subclasses of Animals then
we have not said that a male cannot be a female. We say that two or more categories are
disjoint if they have no members in common. Andeven if weknow that males and females
D IS JO IN T
are disjoint we will not know that an animal that is not a male must be a female unless
E XH AU ST IV E we say that males and females constitute an exhaustive decomposition of the animals. A
D EC OM PO SI TI ON
disjointexhaustivedecompositionisknownasapartition. Thefollowingexamplesillustrate
P AR TI TI ON
thesethreeconcepts:
Disjoint Animals Vegetables
Exhaustive Decomposition Americans Canadians Mexicans
North Americans
Partition Males Females Animals.
Note that the Exhaustive Decomposition of North Americans is not a Partition because
somepeoplehavedualcitizenship. Thethreepredicates aredefinedasfollows:
Disjoints c c c sc sc cid:7 c Intersectionc c
Exhaustive Decompositionsc i ic c c sic
Partitionsc Disjoints Exhaustive Decompositionsc.
Categories can also be defined by providing necessary and sufficient conditions for
membership. Forexampleabachelorisanunmarriedadultmale:
x Bachelors Unmarriedxx Adults x Males .
Aswediscuss in thesidebar onnatural kinds onpage 443 strict logical definitions forcate-
goriesareneitheralwayspossible noralwaysnecessary.
The idea that one object can be part of another is a familiar one. Ones nose is part of ones
head Romania is part of Europe and this chapter is part of this book. We use the general
Part Of relationtosaythatonethingispartofanother. Objectscan begroupedinto Part Of
hierarchies reminiscent ofthe Subset hierarchy:
Part Of Bucharest Romania
Part Of Romania Eastern Europe
Part Of Eastern Europe Europe
Part Of Europe Earth.
The Part Of relation istransitive andreflexive;thatis
Part Ofxy Part Ofyz Part Ofxz.
Part Ofxx.
Therefore wecanconclude Part Of Bucharest Earth.
Categories of composite objects are often characterized by structural relations among
C OM PO SI TE OB JE CT
parts. Forexampleabipedhastwolegsattached toabody:
Bipeda l l b Legl Legl Bodyb
Part Ofl a Part Ofl a Part Ofba
Attachedl b Attachedl b
l cid:7l l Legl Part Ofl a l l l l .
The notation for exactly two is a little awkward; we are forced to say that there are two
legs that they are not the same and that if anyone proposes a third leg it must be the same
as one of the other two. In Section 12.5.2 we describe a formalism called description logic
makesiteasiertorepresentconstraints likeexactly two.
We can define a Part Partition relation analogous to the Partition relation for cate-
gories. See Exercise12.8. Anobjectiscomposedofthepartsinits Part Partition andcan
beviewedasderiving someproperties fromthoseparts. Forexamplethemassofacompos-
iteobjectisthesumofthemassesoftheparts. Noticethatthisisnotthecasewithcategories
whichhavenomasseventhoughtheirelementsmight.
It is also useful to define composite objects with definite parts but no particular struc-
ture. For example we might want to say The apples in this bag weigh two pounds. The
temptation would be to ascribe this weight to the set of apples in the bag but this would be
amistake because thesetisan abstract mathematical concept thathas elements but does not
have weight. Instead we need a new concept which we will call a bunch. Forexample if
B UN CH
theapplesare Apple Apple and Apple then
Bunch Of Apple Apple Apple
denotesthecompositeobjectwiththethreeapplesaspartsnotelements. Wecanthenusethe
bunchasanormalalbeitunstructuredobject. Noticethat Bunch Ofxx. Furthermore
Bunch Of Applesisthecompositeobjectconsisting ofallapplesnot tobeconfusedwith
Applesthecategory orsetofallapples.
Wecan define Bunch Of in terms of the Part Of relation. Obviously each element of
sispartof Bunch Ofs:
x xs Part Ofx Bunch Ofs.
Furthermore Bunch Ofs is the smallest object satisfying this condition. In other words
Bunch Ofsmustbepartofanyobjectthathasalltheelementsofsasparts:
y x xs Part Ofxy Part Of Bunch Ofsy.
L OG IC AL These axioms are an example of a general technique called logical minimization which
M IN IM IZ AT IO N
meansdefininganobjectasthesmallestonesatisfying certain conditions.
Section12.2. Categoriesand Objects 443
N AT UR AL K IN DS
Some categories have strict definitions: an object is a triangle if and only if it is
a polygon with three sides. On the other hand most categories in the real world
havenoclear-cutdefinition;thesearecallednaturalkindcategories. Forexample
tomatoestendtobeadullscarlet; roughly spherical; withanindentation atthetop
where the stem was; about two to four inches in diameter; with a thin but tough
skin; and with flesh seeds and juice inside. There is however variation: some
tomatoes are yellow or orange unripe tomatoes are green some are smaller or
larger than average and cherry tomatoes are uniformly small. Rather than having
acomplete definition of tomatoes wehave aset offeatures that serves toidentify
objects that are clearly typical tomatoes but might not be able to decide forother
objects. Couldtherebeatomatothatisfuzzylikeapeach?
This poses a problem for a logical agent. The agent cannot be sure that an
object it has perceived is a tomato and even if it were sure it could not be cer-
tain which of the properties of typical tomatoes this one has. This problem is an
inevitable consequence ofoperating inpartially observable environments.
One useful approach is to separate what is true of all instances of a cate-
gory from what is true only of typical instances. So in addition to the category
Tomatoeswewillalsohavethecategory Typical Tomatoes. Herethe Typical
functionmapsacategory tothesubclass thatcontainsonlytypicalinstances:
Typicalc c.
Mostknowledge aboutnatural kindswillactuallybeabouttheirtypicalinstances:
x Typical Tomatoes Redx Roundx.
Thus we can write down useful facts about categories without exact defini-
tions. Thedifficulty ofproviding exact definitions formost natural categories was
explainedindepthby Wittgenstein 1953. Heusedtheexampleofgamestoshow
that members of a category shared family resemblances rather than necessary
and sufficient characteristics: what strict definition encompasses chess tag soli-
taireanddodgeball?
The utility of the notion of strict definition was also challenged by
Quine 1953. He pointed out that even the definition of bachelor as an un-
married adult male is suspect; one might for example question a statement such
as the Pope is a bachelor. While not strictly false this usage is certainly infe-
licitous because it induces unintended inferences on the part of the listener. The
tension could perhaps be resolved by distinguishing between logical definitions
suitable for internal knowledge representation and the more nuanced criteria for
felicitous linguistic usage. Thelattermaybeachieved byfiltering theassertions
derivedfromtheformer. Itisalsopossiblethatfailuresof linguistic usageserveas
feedbackformodifyinginternaldefinitionssothatfilteringbecomesunnecessary.
In both scientific and commonsense theories of the world objects have height mass cost
and so on. The values that we assign for these properties are called measures. Ordi-
M EA SU RE
nary quantitative measures are quite easy to represent. We imagine that the universe in-
cludes abstract measure objects such as the length that is the length of this line seg-
ment: . Wecancallthislength1.5inchesor3.81centimeters. Thus
the same length has different names in our language.We represent the length with a units
function that takes a number as argument. An alternative scheme is explored in Exer-
U NI TS FU NC TI ON
cise12.9. Ifthelinesegmentiscalled L wecanwrite
1
Length L Inches1.5 Centimeters3.81.
1
Conversion betweenunitsisdonebyequatingmultiples ofoneunittoanother:
Centimeters2.54d Inchesd.
Similaraxioms can be written for pounds and kilograms seconds and days and dollars and
cents. Measurescanbeusedtodescribe objectsasfollows:
Diameter Basketball Inches9.5.
12
List Price Basketball 19.
12
d Days Durationd Hours24.
Notethat1 isnotadollarbill! Onecanhavetwodollarbills butthere isonly one object
named 1. Note also that while Inches0 and Centimeters0 refer to the same zero
length theyarenotidentical tootherzeromeasures suchas Seconds0.
Simple quantitative measures areeasy torepresent. Other measures present moreof a
problembecausetheyhavenoagreedscaleofvalues. Exerciseshavedifficultydessertshave
deliciousnessandpoemshavebeautyyetnumberscannotbeassignedtothesequalities. One
mightinamomentofpureaccountancydismisssuchpropertiesasuselessforthepurposeof
logical reasoning; or stillworse attempttoimpose anumerical scaleonbeauty. Thiswould
be agrave mistake because it isunnecessary. Themost important aspect of measures is not
theparticularnumericalvalues butthefactthatmeasures canbeordered.
Although measures are not numbers we can still compare them using an ordering
symbol such as . For example we might well believe that Norvigs exercises are tougher
than Russells andthatonescoreslessontougherexercises:
e Exercises e Exercises Wrote Norvige Wrote Russelle
Difficultye Difficultye .
e Exercises e Exercises Difficultye Difficultye
Expected Scoree Expected Scoree .
Thisisenoughtoallowonetodecidewhichexercisestodoeventhoughnonumericalvalues
for difficulty were ever used. One does however have to discover who wrote which exer-
cises. Thesesortsofmonotonicrelationships amongmeasuresformthebasisforthefieldof
qualitative physics a subfield of A I that investigates how to reason about physical systems
without plunging into detailed equations and numerical simulations. Qualitative physics is
discussed inthehistorical notessection.
Section12.2. Categoriesand Objects 445
The real world can be seen as consisting of primitive objects e.g. atomic particles and
composite objects built from them. Byreasoning at the level of large objects such as apples
andcarswecanovercomethecomplexityinvolvedindealingwithvastnumbersofprimitive
objectsindividually. Thereishoweverasignificantportionofrealitythatseemstodefyany
obviousindividuationdivision intodistinctobjects. Wegivethisportionthegenericname
I ND IV ID UA TI ON
stuff. For example suppose I have some butter and an aardvark in front of me. I can say
S TU FF
thereisoneaardvark butthereisnoobviousnumberofbutter-objects becauseanypartof
abutter-object isalso abutter-object atleast until wegettovery smallparts indeed. Thisis
the major distinction between stuff and things. If we cut an aardvark in half we do not get
twoaardvarks unfortunately.
The English language distinguishes clearly between stuff andthings. Wesay anaard-
vark but except in pretentious California restaurants one cannot say a butter. Linguists
distinguish between countnounssuch asaardvarks holes and theorems and mass nouns
C OU NT NO UN S
such as butter water and energy. Several competing ontologies claim tohandle this distinc-
M AS SN OU N
tion. Herewedescribe justone;theothersarecoveredinthe historical notessection.
To represent stuff properly we begin with the obvious. We need to have as objects in
our ontology at least the gross lumps of stuff we interact with. For example we might
recognize a lump of butter as the one left on the table the night before; wemight pick it up
weigh it sell it or whatever. In these senses it is an object just like the aardvark. Let us
callit Butter . Wealsodefinethecategory Butter. Informallyitselementswillbeallthose
3
thingsofwhichonemightsay Itsbutterincluding Butter . Withsomecaveatsaboutvery
3
smallpartsthatwewomitfornowanypartofabutter-object isalsoabutter-object:
b Butter Part Ofpb p Butter .
Wecannowsaythatbuttermeltsataround30degreescentigrade:
b Butter Melting Pointb Centigrade30.
Wecould goontosaythatbutterisyellowislessdensethanwater issoftatroomtempera-
turehasahighfatcontentandsoon. Ontheotherhandbutterhasnoparticularsizeshape
or weight. We can define more specialized categories of butter such as Unsalted Butter
which is also a kind of stuff. Note that the category Pound Of Butter which includes as
members all butter-objects weighing one pound is not a kind of stuff. If we cut a pound of
butterinhalfwedonotalasgettwopoundsofbutter.
Whatisactuallygoingonisthis: somepropertiesare intrinsic: theybelongtothevery
I NT RI NS IC
substance of the object rather than to the object as a whole. When you cut an instance of
stuff in half the twopieces retain the intrinsic propertiesthings like density boiling point
flavor color ownership and so on. On the other hand their extrinsic propertiesweight
E XT RI NS IC
length shape and so onare not retained under subdivision. A category of objects that
includes in its definition only intrinsic properties is then a substance or mass noun; a class
that includes any extrinsic properties in its definition is acount noun. Thecategory Stuff is
the mostgeneral substance category specifying nointrinsic properties. Thecategory Thing
isthemostgeneraldiscreteobjectcategory specifying no extrinsicproperties.
In Section 10.4.2 we showed how situation calculus represents actions and their effects.
Situationcalculus islimitedinitsapplicability: itwasdesigned todescribe aworldinwhich
actions are discrete instantaneous and happen one at a time. Consider a continuous action
suchasfillingabathtub. Situationcalculuscansaythatthetubisemptybeforetheactionand
full when the action is done but it cant talk about what happens during the action. It also
cant describe two actions happening at the same timesuch as brushing ones teeth while
waitingforthetubtofill. Tohandlesuchcasesweintroduce analternativeformalismknown
aseventcalculuswhichisbasedonpointsoftimeratherthanonsituations.3
E VE NT CA LC UL US
Event calculus reifies fluents and events. The fluent At Shankar Berkeley is an ob-
ject that refers to the fact of Shankar being in Berkeley but does not by itself say anything
about whether it is true. To assert that a fluent is actually true at some point in time we use
thepredicate Tasin T At Shankar Berkeleyt.
Eventsaredescribed asinstances ofeventcategories.4 Theevent E of Shankarflying
1
from San Franciscoto Washington D.C.isdescribed as
E Flyings Flyer E Shankar Origin E S FDestination E D C.
If this is too verbose we can define an alternative three-argument version of the category of
flyingeventsandsay
E Flyings Shankar S FD C.
1
Wethenuse Happens E itosaythattheevent E tookplaceoverthetimeinterval iand
we say the same thing in functional form with Extent E i. We represent time intervals
1
byastartendpairoftimes;thatis i t t isthetimeintervalthatstartsatt andends
att . Thecompletesetofpredicates foroneversionoftheeventcalculus is
2
Tft Fluentf istrueattimet
Happensei Eventehappens overthetimeinterval i
Initiateseft Eventecausesfluentf tostarttoholdattimet
Terminateseft Eventecausesfluentf toceasetoholdattimet
Clippedfi Fluentf ceasestobetrueatsomepointduringtimeinterval i
Restoredfi Fluentf becomestruesometimeduringtimeinterval i
Weassumeadistinguishedevent Startthatdescribestheinitialstatebysayingwhichfluents
areinitiatedorterminatedatthestarttime. Wedefine T bysayingthatafluentholdsatapoint
intimeifthefluentwasinitiatedbyaneventatsometimeinthepastandwasnotmadefalse
clippedbyaninterveningevent. Afluentdoesnotholdifitwasterminatedbyaneventand
eventconnotesthepossibilityofagentlessactions.
Section12.3. Events 447
notmadetruerestored byanotherevent. Formallytheaxiomsare:
Happenset t Initiateseft Clippedft tt t
Tft
Happenset t Terminateseft Restoredft tt t
Tft
where Clipped and Restored aredefinedby
Clippedft t
ett Happensett t t t Terminateseft
Restoredft t
ett Happensett t t t Initiateseft
Itisconvenient toextend T toworkoverintervals aswellastimepoints; afluentholds over
anintervalifitholdsoneverypointwithintheinterval:
Tft t t t t t Tft
Fluents and actions are defined with domain-specific axioms that are similar to successor-
state axioms. For example we can say that the only way a wumpus-world agent gets an
arrowisatthestartandtheonlywaytouseupanarrowistoshootit:
Initiatese Have Arrowat e Start
Terminatese Have Arrowat e Shootingsa
By reifying events we make it possible to add any amount of arbitrary information about
them. For example we can say that Shankars flight was bumpy with Bumpy E . In an
1
ontology whereevents are n-arypredicates there would benowaytoadd extra information
likethis;movingtoann1-arypredicate isntascalablesolution.
Wecanextendeventcalculustomakeitpossibletorepresentsimultaneouseventssuch
astwopeoplebeingnecessarytorideaseesawexogenouseventssuchasthewindblowing
and changing the location of an object continuous events such as the level of water in the
bathtubcontinuously risingandothercomplications.
Theeventswehaveseensofararewhatwecall discrete eventstheyhave adefinitestruc-
D IS CR ET EE VE NT S
ture. Shankarstriphasabeginning middleandend. Ifinterruptedhalfwaytheeventwould
besomethingdifferentitwouldnotbeatripfrom San Franciscoto Washingtonbutinstead
a trip from San Francisco to somewhere over Kansas. On the other hand the category of
events denoted by Flyings has a different quality. If we take a small interval of Shankars
flight say the third 20-minute segment while he waitsanxiously forabag of peanuts that
eventisstillamemberof Flyings. Infactthisistrueforanysubinterval.
Categories of events with this property are called process categories or liquid event
P RO CE SS
categories. Anyprocess ethathappens overanintervalalsohappensoveranysubinterval:
L IQ UI DE VE NT
e Processes Happenset t t t t t Happenset t .
The distinction between liquid and nonliquid events is exactly analogous to the difference
between substances or stuff and individual objects or things. In fact some have called
T EM PO RA L liquideventstemporalsubstanceswhereassubstances likebutterare spatialsubstances.
S UB ST AN CE
S PA TI AL SU BS TA NC E
Event calculus opens us up to the possibility of talking about time and time intervals. We
willconsidertwokindsoftimeintervals: momentsandextendedintervals. Thedistinction is
thatonlymomentshavezeroduration:
Partition Moments Extended Intervals Intervals
i Moments Durationi Seconds0.
Next we invent a time scale and associate points on that scale with moments giving us ab-
solute times. The time scale is arbitrary; we measure it in seconds and say that the moment
at midnight G MT on January 1 1900 has time 0. The functions Begin and End pick out
theearliestandlatestmomentsinanintervalandthefunction Time deliversthepointonthe
time scale for a moment. The function Duration gives the difference between the end time
andthestarttime.
Intervali Durationi Time Endi Time Begini.
Time Begin A D1900 Seconds0.
Time Begin A D2001 Seconds3187324800.
Time End A D2001 Seconds3218860800.
Duration A D2001 Seconds31536000.
To make these numbers easier to read we also introduce a function Date which takes six
arguments hours minutesseconds daymonthandyearandreturnsatimepoint:
Time Begin A D2001 Date0001 Jan2001
Date020212411995 Seconds3000000000.
Twointervals Meet if the end timeof the firstequals the start timeof the second. Thecom-
pletesetofintervalrelationsasproposedby Allen1983isshowngraphicallyin Figure12.2
andlogically below:
Meetij Endi Beginj
Beforeij Endi Beginj
Afterji Beforeij
Duringij Beginj Begini Endi Endj
Overlapij Begini Beginj Endi Endj
Beginsij Begini Beginj
Finishesij Endi Endj
Equalsij Begini Beginj Endi Endj
These all have their intuitive meaning with the exception of Overlap: we tend to think of
overlap as symmetric if i overlaps j then j overlaps i but in this definition Overlapij
onlyholdsifibeginsbeforej. Tosaythatthereignof Elizabeth I Iimmediatelyfollowedthat
of George V Iandthereignof Elvisoverlapped withthe1950s wecanwritethefollowing:
Meets Reign Of George V IReign Of Elizabeth I I.
Overlap Fifties Reign Of Elvis.
Begin Fifties Begin A D1950.
End Fifties End A D1959.
Section12.3. Events 449
Figure12.2 Predicatesontimeintervals.
W a sh in g to n
A d a m s
Je ffe
rso n
Figure12.3 Aschematicviewoftheobject President U SAforthefirst15yearsofits
existence.
Physical objects can be viewed as generalized events in the sense that a physical object is
a chunk of spacetime. For example U SA can be thought of as an event that began in
say 1776 as a union of 13 states and is still in progress today as a union of 50. We can
describe the changing properties of U SA using state fluents such as Population U SA. A
property ofthe U SAthatchanges everyfouroreightyears barring mishaps isitspresident.
One might propose that President U SA is a logical term that denotes a different object
at different times. Unfortunately this is not possible because a term denotes exactly one
objectinagivenmodelstructure. Theterm President U SAtcandenotedifferentobjects
depending onthevalueoftbutourontology keeps timeindices separate from fluents. The
only possibility is that President U SA denotes a single object that consists of different
people atdifferent times. Itistheobject thatis George Washington from1789to1797 John
Adamsfrom1797to1801andsoonasin Figure12.3. Tosaythat George Washingtonwas
president throughout 1790wecanwrite
T Equals President U SA George Washington A D1790.
We use the function symbol Equals rather than the standard logical predicate because
we cannot have a predicate as an argument to T and because the interpretation is not that
George Washington and President U SAare logically identical in1790; logical identity is
notsomethingthatcanchangeovertime. Theidentityisbetweenthesubeventsofeachobject
thataredefinedbytheperiod1790.
The agents we have constructed so far have beliefs and can deduce new beliefs. Yet none
of them has any knowledge about beliefs or about deduction. Knowledge about ones own
knowledgeandreasoningprocessesisusefulforcontrollinginference. Forexamplesuppose
Aliceasks whatisthesquare root of1764 and Bobreplies I dont know. If Alice insists
think harder Bob should realize that with some more thought this question can in fact
be answered. On the other hand if the question were Is your mother sitting down right
now? then Bob should realize that thinking harder is unlikely to help. Knowledge about
the knowledge of other agents is also important; Bob should realize that his mother knows
whethersheissittingornotandthataskingherwouldbeawaytofindout.
What we need is a model of the mental objects that are in someones head or some-
things knowledge base and of the mental processes that manipulate those mental objects.
The model does not have to be detailed. We do not have to be able to predict how many
milliseconds itwilltake foraparticular agent tomakeadeduction. Wewillbehappy justto
beabletoconclude thatmotherknowswhetherornotsheissitting.
P RO PO SI TI ON AL We begin with the propositional attitudes that an agent can have toward mental ob-
A TT IT UD E
jects: attitudes such as Believes Knows Wants Intends and Informs. The difficulty is
that these attitudes do not behave like normal predicates. Forexample suppose we try to
assertthat Loisknowsthat Supermancanfly:
Knows Lois Can Fly Superman.
Oneminorissuewiththisisthatwenormallythinkof Can Fly Supermanasasentencebut
hereitappearsasaterm. Thatissuecanbepatchedupjustbereifying Can Fly Superman;
making it a fluent. A more serious problem isthat ifit is true that Superman is Clark Kent
thenwemustconcludethat Loisknowsthat Clarkcanfly:
Superman Clark Knows Lois Can Fly Superman
Knows Lois Can Fly Clark.
This is a consequence of the fact that equality reasoning is built into logic. Normally that is
agood thing; ifouragent knowsthat 22 4and4 5 then wewantouragent toknow
Section12.4. Mental Eventsand Mental Objects 451
R EF ER EN TI AL that 2 2 5. This property is called referential transparencyit doesnt matter what
T RA NS PA RE NC Y
termalogicusestorefertoanobjectwhatmattersistheobjectthatthetermnames. Butfor
propositionalattitudeslikebelievesandknowswewouldliketohavereferentialopacitythe
termsuseddomatterbecausenotallagentsknowwhichtermsareco-referential.
Modallogicisdesignedtoaddressthisproblem. Regularlogicisconcernedwithasin-
M OD AL LO GI C
gle modality the modality of truth allowing us to express P is true. Modal logic includes
special modal operators that take sentences rather than terms as arguments. For example
Aknows Pisrepresentedwiththenotation K Pwhere Kisthemodaloperatorforknowl-
A
edge. Ittakes twoarguments anagent written asthe subscript and asentence. The syntax
ofmodallogicisthesameasfirst-orderlogic except thatsentences canalsobeformedwith
modaloperators.
The semantics of modal logic is more complicated. In first-order logic a model con-
tains a set of objects and an interpretation that maps each name to the appropriate object
relation orfunction. In modal logic wewant to be able to consider both the possibility that
Supermans secret identity is Clark and that it isnt. Therefore we will need a more com-
plicated model one that consists of a collection of possible worlds rather than just one true
P OS SI BL EW OR LD
A CC ES SI BI LI TY world. Theworlds areconnected inagraph by accessibility relations one relation foreach
R EL AT IO NS
modaloperator. Wesaythatworld w isaccessible fromworld w withrespect tothemodal
operator K if everything in w is consistent with what A knows in w and we write this
A 1 0
as Acc K w w . Indiagrams such as Figure 12.4 weshow accessibility asan arrow be-
A 0 1
tweenpossibleworlds. Asanexampleintherealworld Bucharestisthecapitalof Romania
but for an agent that did not know that other possible worlds are accessible including ones
wherethecapital of Romaniais Sibiuor Sofia. Presumably aworldwhere 22 5would
notbeaccessible toanyagent.
Ingeneral a knowledge atom K P is true in world w if and only if P istrue in every
A
worldaccessible from w. Thetruthofmorecomplexsentences isderivedbyrecursive appli-
cation ofthis rule and the normal rules offirst-order logic. Thatmeans that modal logic can
be used to reason about nested knowledge sentences: what one agent knows about another
agents knowledge. For example we can say that even though Lois doesnt know whether
Supermanssecretidentityis Clark Kentshedoesknowthat Clarkknows:
K K Identity Superman Clark K Identity Superman Clark
Lois Clark Clark
Figure12.4showssomepossibleworldsforthisdomainwithaccessibility relationsfor Lois
and Superman.
Inthe T OP-L EF T diagramitiscommonknowledgethat Supermanknowshisowniden-
tity and neither henor Loishasseen the weatherreport. Soinw theworlds w and w are
accessible to Superman;mayberainispredicted maybenot. For Loisallfourworldsareac-
cessiblefromeachother;shedoesntknowanythingaboutthereportorif Clarkis Superman.
Butshedoesknowthat Superman knowswhetherheis Clark because ineveryworldthatis
accessible to Lois either Superman knows I orhe knows I. Loisdoes not know which is
thecasebuteitherwaysheknows Supermanknows.
In the T OP-R IG HT diagram it is common knowledge that Lois has seen the weather
report. So in w she knows rain is predicted and in w she knows rain is not predicted.
w: I R w: I R w: I R w: I R
w: I R w: I R w: I R w: I R
a b
w: I R w: I R
w: I R w: I R
w: I R w: I R
w: I R w: I R
c
K Lois dottedarrows. Theproposition Rmeanstheweatherreportfortomorrowisrain
and I means Supermanssecretidentityis Clark Kent. Allworldsareaccessibletothem-
selves;thearrowsfromaworldtoitselfarenotshown.
Superman does not know the report but he knows that Lois knows because in every world
thatisaccessible tohimeithersheknows Rorsheknows R.
Inthe B OT TO M diagram werepresent thescenario whereitiscommonknowledge that
Supermanknowshisidentity and Loismightormightnothave seentheweatherreport. We
representthisbycombiningthetwotopscenarios andaddingarrowstoshowthat Superman
does not know which scenario actually holds. Lois does know so we dont need to add any
arrowsforher. In w Superman still knows I but not Rand nowhedoes notknow whether
0
Lois knows R. From what Superman knows he might be in w or w in which case Lois
doesnotknowwhether Ristrueorhecouldbeinw inwhichcasesheknows Rorw in
whichcasesheknows R.
Thereareaninfinitenumberofpossibleworldssothetrickistointroducejusttheones
you need to represent what you are trying to model. A new possible world is needed to talk
about different possible facts e.g. rain is predicted or not or to talk about different states
ofknowledge e.g. does Loisknow that rain ispredicted. Thatmeanstwopossible worlds
suchasw andw in Figure12.4mighthavethesamebasefactsabout theworld butdiffer
intheiraccessibility relations andthereforeinfactsaboutknowledge.
Modallogic solvessometricky issues withtheinterplay ofquantifiers and knowledge.
The Englishsentence Bondknowsthatsomeoneisaspyisambiguous. Thefirstreadingis
Section12.5. Reasoning Systemsfor Categories 453
thatthereisaparticularsomeonewho Bondknowsisaspy;wecanwritethisas
x K Spyx
Bond
which in modal logic means that there is an x that in all accessible worlds Bond knows to
beaspy. Thesecondreading isthat Bondjustknowsthatthere isatleastonespy:
K x Spyx.
Bond
Themodallogic interpretation isthat ineach accessible worldthere isan xthat isaspy but
itneednotbethesamexineachworld.
Now that we have a modal operator for knowledge we can write axioms for it. First
we can say that agents are able to draw deductions; if an agent knows P and knows that P
implies Qthentheagentknows Q:
K P K P Q K Q.
a a a
Fromthisand afewotherrules about logical identities we canestablish that K P P
A
is a tautology; every agent knows every proposition P is either true or false. On the other
hand K P K Pisnotatautology; ingeneral therewillbelotsofpropositions that
A A
anagentdoesnotknowtobetrueanddoesnotknowtobefalse.
It is said going back to Plato that knowledge is justified true belief. That is if it is
true if you believe it and if you have an unassailably good reason then you know it. That
meansthatifyouknowsomething itmustbetrueandwehavetheaxiom:
K P P .
a
Furthermore logical agents should be able to introspect on their own knowledge. If they
knowsomething thentheyknowthattheyknowit:
K P K K P.
a a a
Wecandefinesimilaraxiomsforbeliefoftendenotedby Bandothermodalities. However
L OG IC AL one problem with the modal logic approach is that it assumes logical omniscience on the
O MN IS CI EN CE
part of agents. Thatis ifan agent knows aset of axioms then it knows allconsequences of
those axioms. Thisisonshaky ground evenforthesomewhatabstract notion ofknowledge
butitseemsevenworseforbelief becausebeliefhasmoreconnotation ofreferringtothings
that are physically represented in the agent not just potentially derivable. There have been
attempts to define a form of limited rationality for agents; to say that agents believe those
assertions that can be derived with the application of no more than k reasoning steps or no
morethansseconds ofcomputation. Theseattemptshavebeengenerally unsatisfactory.
Categoriesaretheprimarybuilding blocksoflarge-scale knowledgerepresentation schemes.
This section describes systems specially designed for organizing and reasoning with cate-
gories. Therearetwocloselyrelatedfamiliesofsystems: semanticnetworksprovidegraph-
ical aids for visualizing a knowledge base and efficient algorithms for inferring properties
of an object on the basis of its category membership; and description logics provide a for-
mal language for constructing and combining category definitions and efficient algorithms
fordeciding subsetandsupersetrelationships betweencategories.
In1909 Charles S.Peirceproposedagraphicalnotationofnodesandedgescalledexistential
E XI ST EN TI AL graphs that he called the logic of the future. Thus began a long-running debate between
G RA PH S
advocates of logic and advocates of semantic networks. Unfortunately the debate ob-
scured the fact that semantics networksat least those with well-defined semanticsare a
form of logic. The notation that semantic networks provide for certain kinds of sentences
is often more convenient but if we strip away the human interface issues the underlying
conceptsobjects relations quantification andsoonarethesame.
There are many variants of semantic networks but all are capable of representing in-
dividual objects categories of objects and relations among objects. A typical graphical no-
tation displays object or category names in ovals or boxes and connects them with labeled
links. Forexample Figure 12.5 hasa Member Of link between Mary and Female Persons
corresponding tothelogical assertion Mary Female Persons;similarly the Sister Of link
between Mary and John corresponds totheassertion Sister Of Mary John. Wecancon-
nect categories using Subset Of links and so on. It is such fun drawing bubbles and arrows
that one can get carried away. For example we know that persons have female persons as
mothers socanwedrawa Has Mother link from Persons to Female Persons? Theanswer
isnobecause Has Mother isarelationbetweenapersonandhisorhermotherandcategories
donothavemothers.5
Forthisreasonwehaveusedaspecialnotationthedouble-boxedlinkin Figure12.5.
Thislinkassertsthat
x x Persons y Has Motherxy y Female Persons.
Wemightalsowanttoassertthatpersonshavetwolegsthatis
x x Persons Legsx2.
Asbefore weneed tobe careful not to assert that acategory has legs; the single-boxed link
in Figure12.5isusedtoassertproperties ofeverymemberof acategory.
The semantic network notation makes it convenient to perform inheritance reasoning
ofthekindintroducedin Section12.2. Forexamplebyvirtueofbeingaperson Maryinherits
the property of having two legs. Thus to find out how many legs Mary has the inheritance
algorithm follows the Member Of link from Mary to the category she belongs to and then
follows Subset Of links up the hierarchy until it finds a category for which there is a boxed
Legs linkinthiscasethe Persons category. Thesimplicityandefficiencyofthisinference
categoryasawhole. Thiscanleaddirectlytoinconsistenciesaspointedoutby Drew Mc Dermott1976inhis
article Artificial Intelligence Meets Natural Stupidity. Anothercommonproblemwastheuseof Is Alinksfor
bothsubsetandmembershiprelationsincorrespondencewith Englishusage: acatisamammaland Fifiisa
cat.See Exercise12.22formoreontheseissues.
Section12.5. Reasoning Systemsfor Categories 455
Mammals
Subset Of
Has Mother Legs
Persons 2
Subset Of Subset Of
Female Male
Persons Persons
Member Of Member Of
Sister Of Legs
Mary John 1
Figure12.5 Asemanticnetworkwithfourobjects John Mary1 and2andfourcate-
gories.Relationsaredenotedbylabeledlinks.
Fly Events
Member Of
Fly
17
Agent During
Origin Destination
Shankar New York New Delhi Yesterday
Figure12.6 Afragmentofasemanticnetworkshowingtherepresentation ofthelogical
assertion Fly Shankar New York New Delhi Yesterday.
mechanism compared withlogical theorem proving hasbeen oneofthemainattractions of
semanticnetworks.
Inheritancebecomescomplicatedwhenanobjectcanbelongtomorethanonecategory
orwhenacategorycanbeasubsetofmorethanoneothercategory;thisiscalledmultiplein-
M UL TI PL E heritance. Insuchcasestheinheritance algorithmmightfindtwoormoreconflictingvalues
I NH ER IT AN CE
answeringthequery. Forthisreasonmultipleinheritance isbannedinsomeobject-oriented
programming O OPlanguages such as Java that useinheritance inaclass hierarchy. Itis
usuallyallowedinsemanticnetworks butwedeferdiscussion ofthatuntil Section12.6.
Thereadermighthavenoticedanobviousdrawbackofsemanticnetworknotationcom-
paredtofirst-orderlogic: thefactthatlinks betweenbubbles represent only binary relations.
For example the sentence Fly Shankar New York New Delhi Yesterday cannot be as-
serted directly in a semantic network. Nonetheless we can obtain the effect of n-ary asser-
tionsbyreifyingthepropositionitselfasaneventbelongingtoanappropriateeventcategory.
restriction tobinaryrelations forcesthecreation ofarichontology ofreifiedconcepts.
Reification of propositions makes it possible to represent every ground function-free
atomicsentenceoffirst-orderlogicinthesemanticnetworknotation. Certainkindsofuniver-
sallyquantifiedsentencescanbeassertedusinginverselinksandthesinglyboxedanddoubly
boxed arrowsapplied tocategories butthatstillleaves us alongwayshort offullfirst-order
logic. Negation disjunction nested function symbols and existential quantification are all
missing. Nowitispossibletoextendthenotationtomakeitequivalenttofirst-orderlogicas
in Peirces existential graphsbut doing so negates one ofthe main advantages of semantic
networkswhichisthesimplicityandtransparency oftheinference processes. Designerscan
build alarge networkandstillhaveagood ideaabout whatqueries willbeefficient because
aitiseasytovisualizethestepsthattheinferenceprocedurewillgothroughandbinsome
cases the query language is so simple that difficult queries cannot be posed. In cases where
the expressive powerproves to be too limiting many semantic network systems provide for
procedural attachment to fill in the gaps. Procedural attachment is a technique whereby
a query about or sometimes an assertion of a certain relation results in a call to a special
procedure designed forthatrelationratherthanageneral inference algorithm.
One of the most important aspects of semantic networks is their ability to represent
defaultvaluesforcategories. Examining Figure12.5carefullyonenoticesthat Johnhasone
D EF AU LT VA LU E
legdespitethefactthatheisapersonandallpersonshavetwolegs. Inastrictlylogical K B
this would be a contradiction but in a semantic network the assertion that all persons have
two legs has only default status; that is a person is assumed to have two legs unless this is
contradictedbymorespecificinformation. Thedefaultsemanticsisenforcednaturallybythe
inheritance algorithm because it follows links upwards from the object itself John in this
caseandstopsassoonasitfindsavalue. Wesaythatthedefaultisoverriddenbythemore
O VE RR ID IN G
specific value. Notice that we could also override the default number of legs by creating a
categoryof One Legged Personsasubsetof Persons ofwhich John isamember.
Wecanretainastrictly logical semantics forthenetworkif wesaythatthe Legs asser-
tionfor Persons includes anexception for John:
x x Persons x cid:7 John Legsx2.
For a fixed network this is semantically adequate but will be much less concise than the
networknotationitselfiftherearelotsofexceptions. Foranetworkthatwillbeupdatedwith
moreassertions however such anapproach failswereally wanttosay thatanypersons as
yetunknownwithonelegareexceptionstoo. Section12.6goesintomoredepthonthisissue
andondefaultreasoning ingeneral.
The syntax of first-order logic is designed to make it easy to say things about objects. De-
scription logics are notations that are designed to make it easier to describe definitions and
D ES CR IP TI ON LO GI C
properties of categories. Description logic systems evolved from semantic networks in re-
sponse to pressure to formalize what the networks mean while retaining the emphasis on
taxonomic structureasanorganizing principle.
The principal inference tasks for description logics are subsumption checking if one
S UB SU MP TI ON
category is a subset of another by comparing their definitions and classification checking
C LA SS IF IC AT IO N
whetheranobject belongs toacategory.. Somesystems also include consistency ofacate-
gorydefinitionwhether themembershipcriteria arelogically satisfiable.
Section12.5. Reasoning Systemsfor Categories 457
Concept Thing Concept Name
And Concept...
All Role Name Concept
At Least Integer Role Name
At Most Integer Role Name
Fills Role Name Individual Name...
Same As Path Path
One Of Individual Name...
Path Role Name...
Figure12.7 Thesyntaxofdescriptionsinasubsetofthe C LA SS IClanguage.
The C LA SS IC language Borgida etal.1989isatypicaldescription logic. Thesyntax
of C LA SS IC descriptions is shown in Figure 12.7.6 For example to say that bachelors are
unmarriedadultmaleswewouldwrite
Bachelor And Unmarried Adult Male.
Theequivalent infirst-orderlogicwouldbe
Bachelorx Unmarriedx Adultx Malex.
Notice that the description logic has an an algebra of operations on predicates which of
coursewecantdoinfirst-orderlogic. Anydescription in C LA SS IC canbetranslated intoan
equivalent first-order sentence but some descriptions are more straightforward in C LA SS IC.
For example to describe the set of men with at least three sons who are all unemployed
and married to doctors and at mosttwodaughters who areall professors in physics ormath
departments wewoulduse
And Man At Least3 Son At Most2 Daughter
All Son And Unemployed Married All Spouse Doctor
All Daughter And Professor Fills Department Physics Math.
Weleaveitasanexercisetotranslatethisintofirst-orderlogic.
Perhapsthemostimportantaspectofdescriptionlogicsistheiremphasisontractability
ofinference. Aproblem instance issolved bydescribing itandthen asking ifitissubsumed
byoneofseveralpossiblesolutioncategories. Instandardfirst-orderlogicsystemspredicting
thesolutiontimeisoftenimpossible. Itisfrequently lefttotheusertoengineertherepresen-
tation to detour around sets of sentences that seem to be causing the system to take several
weekstosolveaproblem. Thethrustindescriptionlogicsontheotherhandistoensurethat
subsumption-testing canbesolvedintimepolynomial inthesizeofthedescriptions.7
another. Thisisadeliberatepolicy:subsumptionbetweencategoriesmustbederivablefromsomeaspectsofthe
descriptionsofthecategories.Ifnotthensomethingismissingfromthedescriptions.
This sounds wonderful in principle until one realizes that it can only have one of two
consequences: either hard problems cannot be stated at all or they require exponentially
large descriptions! Howeverthetractability results doshed lightonwhatsorts ofconstructs
cause problems and thus help the user to understand how different representations behave.
For example description logics usually lack negation and disjunction. Each forces first-
orderlogicalsystemstogothrough apotentially exponential caseanalysis inordertoensure
completeness. C LA SS IC allows only a limited form of disjunction in the Fills and One Of
constructs which permit disjunction overexplicitly enumerated individuals but not overde-
scriptions. Withdisjunctive descriptions nested definitions canleadeasily toanexponential
numberofalternative routesbywhichonecategory cansubsumeanother.
Inthepreceding sectionwesawasimpleexampleofanassertionwithdefaultstatus: people
have two legs. This default can be overridden by more specific information such as that
Long John Silverhasone leg. Wesaw thatthe inheritance mechanism insemantic networks
implements the overriding of defaults in a simple and natural way. In this section we study
defaults more generally with a view toward understanding the semantics of defaults rather
thanjustproviding aprocedural mechanism.
Wehaveseentwoexamplesofreasoningprocessesthatviolatethemonotonicitypropertyof
logic that was proved in Chapter 7.8 In this chapter we saw that a property inherited by all
membersofacategory inasemanticnetworkcouldbeoverridden bymorespecificinforma-
tionforasubcategory. In Section 9.4.5wesawthat undertheclosed-world assumption ifa
proposition αisnotmentioned in K B then K B αbut K B α α.
Simple introspection suggests that these failures of monotonicity are widespread in
commonsense reasoning. It seems that humans often jump to conclusions. For example
when one sees a car parked on the street one is normally willing to believe that it has four
wheels even though only three are visible. Now probability theory can certainly provide a
conclusion thatthefourth wheelexists withhighprobability yetformostpeople thepossi-
bility of the cars not having four wheels does not arise unless some new evidence presents
itself. Thus it seems that the four-wheel conclusion is reached by default in the absence of
anyreason todoubt it. Ifnewevidence arrivesforexample ifonesees theownercarrying
awheelandnoticesthatthecarisjackedupthentheconclusioncanberetracted. Thiskind
of reasoning is said to exhibit nonmonotonicity because the set of beliefs does not grow
N ON MO NO TO NI CI TY
N ON MO NO TO NI C monotonically over time as new evidence arrives. Nonmonotonic logics have been devised
L OG IC
withmodifiednotionsoftruthandentailmentinordertocapturesuchbehavior. Wewilllook
attwosuchlogicsthathavebeenstudied extensively: circumscription anddefaultlogic.
K B.Thatisif K B αthen K B β α.
Section12.6. Reasoning with Default Information 459
Circumscription can be seen as a more powerful and precise version of the closed-
C IR CU MS CR IP TI ON
worldassumption. Theideaistospecifyparticularpredicatesthatareassumedtobeasfalse
aspossiblethat isfalseforeveryobjectexceptthoseforwhichtheyareknowntobetrue.
Forexample suppose wewanttoassert thedefault rule thatbirds fly. Wewouldintroduce a
predicate say Abnormal xandwrite
1
Birdx Abnormal x Fliesx.
1
If we say that Abnormal is to be circumscribed a circumscriptive reasoner is entitled to
1
assume Abnormal x unless Abnormal x is known to be true. This allows the con-
clusion Flies Tweety to be drawn from the premise Bird Tweety but the conclusion no
longerholdsif Abnormal Tweetyisasserted.
1
M OD EL Circumscription can be viewed as an example of a model preference logic. In such
P RE FE RE NC E
logicsasentenceisentailedwithdefaultstatusifitistrueinallpreferredmodelsofthe K B
as opposed to the requirement of truth in all models in classical logic. For circumscription
one modelispreferred toanother ifithas fewerabnormal objects.9 Letusseehow thisidea
worksinthecontextofmultipleinheritance insemanticnetworks. Thestandardexamplefor
which multiple inheritance is problematic is called the Nixon diamond. It arises from the
observation that Richard Nixon was both a Quaker and hence by default a pacifist and a
Republican andhencebydefault notapacifist. Wecanwritethisasfollows:
Republican Nixon Quaker Nixon.
Republicanx Abnormal x Pacifistx.
2
Quakerx Abnormal x Pacifistx.
3
If we circumscribe Abnormal and Abnormal there are two preferred models: one in
which Abnormal Nixonand Pacifist Nixonholdandoneinwhich Abnormal Nixon
and Pacifist Nixonhold. Thusthecircumscriptivereasonerremainsproperly agnosticas
to whether Nixon was a pacifist. If we wish in addition to assert that religious beliefs take
P RI OR IT IZ ED precedenceoverpoliticalbeliefswecanuseaformalismcalledprioritizedcircumscription
C IR CU MS CR IP TI ON
togivepreference tomodelswhere Abnormal isminimized.
3
Defaultlogic is aformalism in which default rules can be written togenerate contin-
D EF AU LT LO GI C
gentnonmonotonic conclusions. Adefaultrulelookslikethis:
D EF AU LT RU LE S
Birdx :Fliesx Fliesx.
Thisrulemeansthatif Birdxistrueandif Fliesxisconsistentwiththeknowledgebase
then Fliesxmaybeconcluded bydefault. Ingeneral adefault rulehastheform
P :J ...J C
where P is called the prerequisite C is the conclusion and J are the justificationsif any
i
one of them can be proven false then the conclusion cannot be drawn. Any variable that
modelsareminimalmodels. Thereisanaturalconnectionbetweentheclosed-worldassumptionanddefinite-
clause K Bsbecausethefixedpointreachedbyforwardchainingondefinite-clause K Bsistheuniqueminimal
model.Seepage258formoreonthispoint.
appears in J or C must also appear in P. The Nixon-diamond example can be represented
i
indefault logicwithonefactandtwodefaultrules:
Republican Nixon Quaker Nixon.
Republicanx :Pacifistx Pacifistx.
Quakerx :Pacifistx Pacifistx.
To interpret what the default rules mean we define the notion of an extension of a default
E XT EN SI ON
theory to be a maximal set of consequences of the theory. That is an extension S consists
of the original known facts and a set of conclusions from the default rules such that no
additionalconclusions canbedrawnfrom S andthejustificationsofeverydefaultconclusion
in S areconsistentwith S. Asinthecaseofthepreferredmodelsincircumscription wehave
twopossibleextensionsforthe Nixondiamond: onewhereinheisapacifistandonewherein
heisnot. Prioritizedschemesexistinwhichsomedefaultrulescanbegivenprecedenceover
others allowingsomeambiguities toberesolved.
Since 1980 when nonmonotonic logics were first proposed a great deal of progress
has been made in understanding their mathematical properties. There are still unresolved
questions however. For example if Cars have four wheels is false what does it mean
to have it in ones knowledge base? What is a good set of default rules to have? If we
cannot decide for each rule separately whether it belongs in our knowledge base then we
haveaseriousproblem ofnonmodularity. Finally howcanbeliefsthathavedefaultstatusbe
used to make decisions? This is probably the hardest issue for default reasoning. Decisions
often involve tradeoffs and one therefore needs to compare the strengths of belief in the
outcomes ofdifferent actions and the costs of making awrong decision. In cases where the
same kinds of decisions are being made repeatedly it is possible to interpret default rules
as threshold probability statements. For example the default rule My brakes are always
O K really means The probability that my brakes are O K given no other information is
sufficiently high that the optimal decision is for me to drive without checking them. When
thedecisioncontextchangesforexamplewhenoneisdrivingaheavilyladentruckdowna
steep mountain roadthe default rulesuddenly becomes inappropriate eventhough thereis
nonewevidenceoffaultybrakes. Theseconsiderationshaveledsomeresearcherstoconsider
howtoembeddefaultreasoning withinprobability theoryor utilitytheory.
Wehave seen that many of the inferences drawn by a knowledge representation system will
have only default status rather than being absolutely certain. Inevitably some of these in-
ferredfactswillturnouttobewrongandwillhavetoberetractedinthefaceofnewinforma-
tion. This process is called belief revision.10 Suppose that a knowledge base K B contains
B EL IE FR EV IS IO N
a sentence Pperhaps a default conclusion recorded by a forward-chaining algorithm or
perhaps just an incorrect assertionand we want to execute T EL LK B P. Toavoid cre-
ating a contradiction we must first execute R ET RA CT KB P. This sounds easy enough.
achangeintheworldratherthannewinformationaboutafixed world. Beliefupdatecombinesbeliefrevision
withreasoningabouttimeandchange;itisalsorelatedtotheprocessoffilteringdescribedin Chapter15.
Section12.6. Reasoning with Default Information 461
Problems arise however if any additional sentences were inferred from P and asserted in
the K B.Forexampletheimplication P Qmighthavebeenusedtoadd Q. Theobvious
solutionretracting allsentencesinferredfrom Pfailsbecausesuchsentencesmayhave
other justifications besides P. For example if R and R Q are also in the K B then Q
T RU TH
does not have to be removed after all. Truth maintenance systems or T MSs are designed
M AI NT EN AN CE
S YS TE M
tohandle exactlythesekindsofcomplications.
One simple approach to truth maintenance is to keep track of the order in which sen-
tences are told to the knowledge base by numbering them from P to P . When the call
R ET RA CT KB P iismadethesystem revertstothestatejustbefore P
i
wasaddedthereby
removingboth P andanyinferences thatwerederivedfrom P . Thesentences P through
i i i1
P can then be added again. This is simple and it guarantees that the knowledge base will
n
be consistent but retracting P requires retracting and reasserting nisentences as wellas
i
undoing and redoing all the inferences drawn from those sentences. For systems to which
manyfactsarebeingaddedsuch aslargecommercialdatabasesthis isimpractical.
Amoreefficientapproachisthejustification-basedtruthmaintenancesystemor J TM S.
J TM S
Ina J TM Seach sentence inthe knowledge base isannotated withajustification consisting
J US TI FI CA TI ON
of the set of sentences from which it was inferred. For example if the knowledge base
already contains P Q then T EL LP will cause Q to be added with the justification
P P Q. In general a sentence can have any number of justifications. Justifica-
tions make retraction efficient. Given the call R ET RA CT P the J TM S will delete exactly
those sentences for which P is a member of every justification. So if a sentence Q had
the single justification P P Q it would be removed; if it had the additional justi-
fication P P R Q it would still be removed; but if it also had the justification
R P R Qthenitwouldbespared. Inthiswaythetimerequiredforretractionof P
depends onlyonthenumberofsentences derived from P ratherthanonthenumberofother
sentences addedsince P enteredtheknowledge base.
The J TM Sassumesthatsentencesthatareconsideredoncewillprobablybeconsidered
again so rather than deleting a sentence from the knowledge base entirely when it loses
all justifications we merely mark the sentence as being out of the knowledge base. If a
subsequent assertion restores one of the justifications then we mark the sentence as being
back in. In this way the J TM S retains all the inference chains that it uses and need not
rederivesentences whenajustification becomesvalidagain.
In addition to handling the retraction of incorrect information T MSs can be used to
speed up the analysis of multiple hypothetical situations. Suppose for example that the
Romanian Olympic Committee is choosing sites for the swimming athletics and eques-
trian events at the 2048 Games to be held in Romania. For example let the first hypothe-
sisbe Site Swimming Pitesti Site Athletics Bucharestand Site Equestrian Arad.
A great deal of reasoning must then be done to work out the logistical consequences and
hence the desirability of this selection. If we want to consider Site Athletics Sibiu in-
stead the T MS avoids the need to start again from scratch. Instead we simply retract
Site Athletics Bucharestandassert Site Athletics Sibiuandthe T MStakescareofthe
necessary revisions. Inference chains generated from the choice of Bucharest can be reused
with Sibiuprovidedthattheconclusions arethesame.
Anassumption-based truthmaintenancesystemor A TM Smakesthistypeofcontext-
A TM S
switching between hypothetical worldsparticularly efficient. Ina J TM Sthemaintenance of
justifications allows you to move quickly from one state to another by making a few retrac-
tionsandassertionsbutatanytimeonlyonestateisrepresented. An A TM Srepresentsallthe
states that have everbeen considered at the same time. Whereas a J TM S simply labels each
sentence as being in or out an A TM S keeps track for each sentence of which assumptions
wouldcausethesentencetobetrue. Inotherwordseachsentencehasalabelthatconsistsof
aset ofassumption sets. Thesentence holds justinthose cases inwhichalltheassumptions
inoneoftheassumption setshold.
Truth maintenance systems also provide a mechanism for generating explanations.
E XP LA NA TI ON
Technically an explanation of a sentence P is a set of sentences E such that E entails P.
If the sentences in E are already known to be true then E simply provides a sufficient ba-
sis for proving that P must be the case. But explanations can also include assumptions
A SS UM PT IO N
sentences that are not known to be true but would suffice to prove P if they were true. For
example one might not have enough information to prove that ones car wont start but a
reasonableexplanationmightincludetheassumptionthatthebatteryisdead. Thiscombined
with knowledge of how cars operate explains the observed nonbehavior. In most cases we
willpreferanexplanation E thatisminimalmeaningthatthereisnopropersubsetof E that
isalsoanexplanation. An A TM Scangenerateexplanations forthecarwontstartproblem
bymakingassumptions such asgasincarorbattery dead inanyorderwelike evenif
some assumptions are contradictory. Then we look at the label for the sentence car wont
starttoreadoffthesetsofassumptions thatwouldjustify thesentence.
Theexact algorithms used toimplement truth maintenance systems arealittle compli-
catedandwedonotcoverthemhere. Thecomputationalcomplexityofthetruthmaintenance
problem is at least as great as that of propositional inferencethat is N P-hard. Therefore
you should not expect truth maintenance to be a panacea. When used carefully however a
T MS can provide a substantial increase in the ability of a logical system to handle complex
environments andhypotheses.
Inthisfinalsection weputtogether allwehave learned toencode knowledge forashopping
research agent that helps a buyer find product offers on the Internet. The shopping agent is
given a product description by the buyer and has the task of producing a list of Web pages
that offer such a product for sale and ranking which offers are best. In some cases the
buyers product description will be precise as in Canon Rebel X Ti digital camera and the
taskisthentofindthestores withthebestoffer. Inothercases thedescription willbeonly
partially specified as in digital camera for under 300 and the agent will have to compare
differentproducts.
Theshoppingagentsenvironmentistheentire World Wide Webinitsfullcomplexity
not atoy simulated environment. Theagents percepts are Webpages but whereas ahuman
Section12.7. The Internet Shopping World 463
Example Online Store
Selectfromourfinelineofproducts:
Computers
Cameras
Books
Videos
Music
h1 Example Online Storeh1
i Selecti from our fine line of products:
ul
li a hrefhttp:example.comcompu Computersa
li a hrefhttp:example.comcamer Camerasa
li a hrefhttp:example.combooks Booksa
li a hrefhttp:example.comvideo Videosa
li a hrefhttp:example.commusic Musica
ul
Figure12.8 A Webpagefromagenericonlinestoreintheformperceivedby thehuman
userofabrowsertopandthecorresponding H TM Lstringas perceivedbythebrowseror
theshoppingagentbottom. In H TM Lcharactersbetweenandaremarkupdirectives
that specify how the page is displayed. For example the string i Selecti means
toswitch to italic font displaytheword Select andthenendtheuse ofitalic font. A page
identifiersuchashttp:example.combooksiscalledauniformresourcelocator
U RL.Themarkupa hrefurl Booksameanstocreateahypertextlinkto url
withtheanchortext Books.
Web user would see pages displayed as an array of pixels on a screen the shopping agent
willperceive apage asacharacter string consisting ofordinary words interspersed withfor-
matting commands in the H TM L markup language. Figure 12.8 shows a Web page and a
corresponding H TM L character string. The perception problem for the shopping agent in-
volvesextracting usefulinformation frompercepts ofthis kind.
Clearly perception on Webpages iseasier than say perception while driving ataxi in
Cairo. Nonetheless therearecomplications tothe Internetperception task. The Webpagein
Figure12.8issimplecomparedtorealshoppingsiteswhichmayinclude C SScookies Java
Javascript Flashrobotexclusionprotocols malformed H TM Lsoundfilesmoviesandtext
that appears only as part of a J PE G image. An agent that can deal with all of the Internet is
almost as complex as a robot that can move in the real world. We concentrate on a simple
agentthatignoresmostofthesecomplications.
Theagentsfirsttaskistocollectproductoffersthatarerelevanttoaquery. Ifthequery
is laptops then a Web page with a review of the latest high-end laptop would be relevant
butifitdoesnt provideawaytobuy itisnt anoffer. Fornowwecansayapageisanoffer
ifitcontainsthewordsbuyorpriceoraddtocartwithinan H TM Llinkorformonthe
page. Forexampleifthepagecontainsastringoftheforma...add to cart...a
thenitisanoffer. Thiscouldberepresentedinfirst-orderlogicbutitismorestraightforward
toencodeitintoprogramcode. Weshowhowtodomoresophisticatedinformationextraction
in Section22.4.
Thestrategy istostartatthehomepage ofanonline store and considerallpagesthatcanbe
reached byfollowing relevantlinks.11 Theagent willhaveknowledge ofanumberofstores
forexample:
Amazon Online Stores Homepage Amazonamazon.com.
Ebay Online Stores Homepage Ebayebay.com.
Example Store Online Stores Homepage Example Storeexample.com.
These stores classify their goods into product categories and provide links tothe major cat-
egories from their home page. Minor categories can be reached through a chain of relevant
links andeventually wewillreachoffers. Inotherwordsa pageisrelevanttothequeryifit
can be reached by achain of zero ormorerelevant category links from astores home page
andthenfromonemorelinktotheproductoffer. Wecandefinerelevance:
Relevantpagequery
storehome store Online Stores Homepagestorehome
urlurl Relevant Chainhomeurl query Linkurl url
page Contentsurl.
Here the predicate Linkfromto means that there is a hyperlink from the from U RL to
the to U RL. To define what counts as a Relevant Chain we need to follow not just any old
hyperlinksbutonlythoselinkswhoseassociatedanchortextindicatesthatthelinkisrelevant
to the product query. Forthis we use Link Textfromtotext to mean that there is a link
between from and to with text as the anchor text. A chain of links between two U RLs start
and end is relevant to a description d if the anchor text of each link is a relevant category
nameford. Theexistenceofthechainitselfisdetermined byarecursive definition withthe
emptychainstartendasthebasecase:
Relevant Chainstartendquery start end
utext Link Textstartutext Relevant Category Namequerytext
Relevant Chainuendquery.
Now we must define what it means for text to be a Relevant Category Name for query.
First weneed to relate strings to the categories they name. This is done using the predicate
Namescwhichsaysthatstring sisanameforcategory cforexamplewemightassert
that Namelaptops Laptop Computers. Some more examples of the Name predicate
appear in Figure 12.9b. Next we define relevance. Suppose that query is laptops. Then
Relevant Category Namequerytextistruewhenoneofthefollowingholds:
Thetextandquerynamethesamecategorye.g. notebooks andlaptops.
searchinformationretrievalwillbecoveredin Section22.3.
Section12.7. The Internet Shopping World 465
Namebooks Books
Books Products
Namemusic Music Recordings
Music Recordings Products
Name C Ds Music C Ds
Music C Ds Music Recordings
Nameelectronics Electronics
Electronics Products
Namedigitalcameras Digital Cameras
Digital Cameras Electronics
Namestereos Stereo Equipment
Stereo Equipment Electronics
Namecomputers Computers
Computers Electronics
Namedesktops Desktop Computers
Desktop Computers Computers
Namelaptops Laptop Computers
Laptop Computers Computers
Namenotebooks Laptop Computers
...
...
a b
Figure12.9 a Taxonomyofproductcategories.b Namesforthosecategories.
Thetextnamesasupercategory suchascomputers.
Thetextnamesasubcategory suchasultralight notebooks.
Thelogicaldefinition of Relevant Category Name isasfollows:
Relevant Category Namequerytext
c c Namequeryc Nametextc c c c c . 12.1
Otherwise theanchortextisirrelevant because itnamesacategory outside thisline suchas
clothes orlawngarden.
To follow relevant links then it is essential to have a rich hierarchy of product cate-
gories. Thetoppartofthishierarchymightlooklike Figure 12.9a. Itwillnotbefeasibleto
list all possible shopping categories because a buyer could always come up with some new
desire and manufacturers will always come out with new products to satisfy them electric
kneecap warmers?. Nonetheless anontology ofabout athousand categories willserve asa
veryusefultoolformostbuyers.
In addition to the product hierarchy itself we also need to have a rich vocabulary of
names for categories. Life would be much easier if there were a one-to-one correspon-
dence between categories and the character strings that name them. We have already seen
the problem of synonymytwo names for the same category such as laptop computers
andlaptops. Thereisalso theproblem of ambiguityone namefortwoormoredifferent
categories. Forexampleifweaddthesentence
Name C Ds Certificates Of Deposit
totheknowledge basein Figure12.9b then C Dswillnametwodifferent categories.
Synonymy and ambiguity can cause a significant increase in the number of paths that
the agent has to follow and can sometimes make it difficult to determine whether a given
pageisindeedrelevant. Amuchmoreseriousproblemistheverybroadrangeofdescriptions
thatausercantypeandcategorynamesthatastorecanuse. Forexample thelinkmightsay
laptop whenthe knowledge basehas only laptops orthe usermight askforacomputer
I can fit on the tray table of an economy-class airline seat. It is impossible to enumerate in
advance all the ways a category can be named so the agent will have to be able to do addi-
tionalreasoninginsomecasestodetermineifthe Namerelationholds. Intheworstcasethis
requiresfullnaturallanguageunderstanding atopicthatwewilldeferto Chapter22. Inprac-
ticeafewsimplerulessuchasallowinglaptoptomatchacategorynamedlaptopsgo
alongway. Exercise12.10asksyoutodevelopasetofsuchrules afterdoing someresearch
intoonlinestores.
Given the logical definitions from the preceding paragraphs and suitable knowledge
bases of product categories and naming conventions are we ready to apply an inference
algorithm to obtain a set of relevant offers for our query? Not quite! The missing element
is the Contentsurl function which refers to the H TM L page at a given U RL. The agent
doesnt havethepagecontents ofevery U RLinitsknowledge base;nordoesithaveexplicit
rulesfordeducing whatthosecontents mightbe. Instead wecanarrange fortheright H TT P
procedure to be executed whenever a subgoal involves the Contents function. In this way it
appears tothe inference engine as ifthe entire Webis inside the knowledge base. This isan
P RO CE DU RA L exampleofageneraltechniquecalledproceduralattachmentwherebyparticularpredicates
A TT AC HM EN T
andfunctions canbehandledbyspecial-purpose methods.
Let us assume that the reasoning processes of the preceding section have produced a set of
offerpagesforourlaptops query. Tocomparethoseoffers theagentmustextracttherele-
vantinformationprice speeddisksizeweightandsoonfromtheofferpages. Thiscan
beadifficult taskwithreal Webpages forallthereasons mentioned previously. Acommon
wayof dealing withthis problem is touse programs called wrappersto extract information
W RA PP ER
from a page. The technology of information extraction is discussed in Section 22.4. For
now weassume that wrappers exist and when given apage and a knowledge base they add
assertions to the knowledge base. Typically a hierarchy of wrappers would be applied to a
page: a very general one to extract dates and prices a morespecific one to extract attributes
forcomputer-related products andifnecessary asite-specific onethatknowstheformatofa
particularstore. Givenapageontheexample.com sitewiththetext
I BM Think Book 970. Our price: 399.00
followed byvarious technical specifications wewouldlike awrapper toextract information
suchasthefollowing:
coffer c Laptop Computers offer Product Offers
Manufacturerc I BM Modelc Think Book970
Screen Sizec Inches14 Screen Typec Color L CD
Memory Sizec Gigabytes2 CP USpeedc G Hz1.2
Offered Productofferc Storeoffer Gen Store
U RLofferexample.comcomputers34356.html
Priceoffer399 Dateoffer Today.
Thisexampleillustratesseveralissuesthatarisewhenwetakeseriouslythetaskofknowledge
engineering forcommercialtransactions. Forexample notice thatthepriceisanattribute of
Section12.8. Summary 467
the offer not the product itself. This is important because the offer at a given store may
change from day to day even for the same individual laptop; for some categoriessuch as
houses and paintingsthe same individual object may even be offered simultaneously by
different intermediaries at different prices. There are still more complications that we have
not handled such asthe possibility that the price depends onthe method ofpayment and on
the buyers qualifications for certain discounts. The final task is to compare the offers that
havebeenextracted. Forexampleconsiderthesethreeoffers:
A :1.4 GHz C PU2 GB RA M250 GBdisk299.
B : 1.2 GHz C PU4 GB RA M350 GBdisk500.
C : 1.2 GHz C PU2 GB RA M250 GBdisk399.
C is dominated by A; that is A is cheaper and faster and they are otherwise the same. In
general X dominates Y if X hasabettervalueonatleast oneattribute andisnotworseon
any attribute. But neither A nor B dominates the other. To decide which is better we need
to know how the buyer weighs C PU speed and price against memory and disk space. The
general topic ofpreferences among multiple attributes isaddressed in Section16.4; fornow
our shopping agent will simply return a list of all undominated offers that meet the buyers
description. Inthisexample both Aand B areundominated. Noticethatthisoutcomerelies
on the assumption that everyone prefers cheaper prices fasterprocessors and more storage.
Someattributessuchasscreensizeonanotebookdependontheusersparticularpreference
portability versusvisibility; forthesetheshopping agentwilljusthavetoasktheuser.
The shopping agent we have described here is a simple one; many refinements are
possible. Still it has enough capability that with the right domain-specific knowledge it can
actually be of use to a shopper. Because of its declarative construction it extends easily to
more complex applications. The main point of this section is to show that some knowledge
representationin particular theproducthierarchyisnecessaryforsuchanagentandthat
oncewehavesomeknowledgeinthisformtherestfollowsnaturally.
By delving into the details of how one represents a variety of knowledge we hope we have
given the reader a sense of how real knowledge bases are constructed and a feeling for the
interesting philosophical issuesthatarise. Themajorpointsareasfollows:
Large-scale knowledge representation requires ageneral-purpose ontology to organize
andtietogetherthevariousspecificdomainsofknowledge.
Ageneral-purpose ontology needs tocoverawidevariety ofknowledge andshould be
capable inprinciple ofhandling anydomain.
Building a large general-purpose ontology is a significant challenge that has yet to be
fullyrealized although currentframeworksseemtobequiterobust.
We presented an upper ontology based on categories and the event calculus. We
covered categories subcategories parts structured objects measurements substances
eventstimeandspacechange andbeliefs.
13
Q UA NT IF YI NG
U NC ER TA IN TY
Inwhichweseehowanagentcantameuncertainty withdegrees ofbelief.
Agents may need to handle uncertainty whether due to partial observability nondetermin-
U NC ER TA IN TY
ism ora combination of the two. An agent may never know for certain what state its in or
whereitwillendupafterasequence ofactions.
Wehaveseenproblem-solvingagents Chapter4andlogicalagents Chapters7and11
designed tohandleuncertainty bykeeping trackofabeliefstatearepresentation oftheset
of all possible world states that it might be inand generating a contingency plan that han-
dleseverypossibleeventualitythatitssensorsmayreport duringexecution. Despiteitsmany
virtues howeverthisapproach hassignificantdrawbacks whentakenliterally asarecipefor
creating agentprograms:
When interpreting partial sensor information a logical agent must consider every log-
ically possible explanation for the observations no matter how unlikely. This leads to
impossiblelargeandcomplexbelief-state representations.
Acorrect contingent plan that handles every eventuality can grow arbitrarily large and
mustconsiderarbitrarily unlikely contingencies.
Sometimes there is no plan that is guaranteed to achieve the goalyet the agent must
act. Itmusthavesomewaytocomparethemeritsofplansthatarenotguaranteed.
Suppose for example that an automated taxi!automated has the goal of delivering a pas-
senger to the airport on time. The agent forms a plan A that involves leaving home 90
90
minutes before the flight departs and driving at a reasonable speed. Even though the airport
is only about 5 miles away a logical taxi agent will not be able to conclude with certainty
that Plan A will get us to the airport in time. Instead it reaches the weaker conclusion
90
Plan A willget usto theairport in time aslong asthe cardoesnt break down orrun out
90
ofgasand Idontgetintoanaccidentandtherearenoaccidentsonthebridgeandtheplane
doesnt leave early and nometeorite hits the car and .... None ofthese conditions can be
480
Section13.1. Actingunder Uncertainty 481
deducedforsuresotheplanssuccesscannotbeinferred. Thisisthequalificationproblem
page268forwhichwesofarhaveseennorealsolution.
Nonetheless in some sense A is in fact the right thing to do. What do we mean by
90
this? Aswediscussed in Chapter2wemeanthatoutofalltheplansthatcouldbeexecuted
A isexpected tomaximize theagents performance measure wheretheexpectation isrel-
90
ative to the agents knowledge about the environment. The performance measure includes
getting to the airport in time for the flight avoiding a long unproductive wait at the airport
andavoidingspeedingticketsalongtheway. Theagentsknowledgecannotguaranteeanyof
these outcomes for A but it can provide some degree of belief that they will be achieved.
90
Other plans such as A might increase the agents belief that it will get to the airport on
180
time but also increase the likelihood of a long wait. The right thing to dothe rational
decisiontherefore depends on both the relative importance of various goals and the likeli-
hood that and degree to which they will be achieved. The remainder of this section hones
theseideasinpreparation forthedevelopmentofthegeneraltheories ofuncertainreasoning
andrationaldecisions thatwepresent inthisandsubsequent chapters.
Lets consider an example of uncertain reasoning: diagnosing a dental patients toothache.
Diagnosiswhether for medicine automobile repair or whateveralmost always involves
uncertainty. Letustrytowriterulesfordentaldiagnosisusingpropositional logicsothatwe
canseehowthelogicalapproach breaksdown. Considerthefollowingsimplerule:
Toothache Cavity .
The problem is that this rule is wrong. Not all patients with toothaches have cavities; some
ofthemhavegumdisease anabscess oroneofseveralotherproblems:
Toothache Cavity Gum Problem Abscess...
Unfortunately in order to make the rule true we have to add an almost unlimited list of
possible problems. Wecouldtryturningtheruleintoacausalrule:
Cavity Toothache .
But this rule is not right either; not all cavities cause pain. The only way to fix the rule
is to make it logically exhaustive: to augment the left-hand side with all the qualifications
required for a cavity to cause a toothache. Trying to use logic to cope with a domain like
medicaldiagnosis thusfailsforthreemainreasons:
Laziness: It is too much work to list the complete set of antecedents or consequents
L AZ IN ES S
neededtoensureanexceptionless ruleandtoohardtousesuchrules.
T HE OR ET IC AL Theoreticalignorance: Medicalsciencehasnocompletetheoryforthedomain.
I GN OR AN CE
P RA CT IC AL Practical ignorance: Even if we know all the rules we might be uncertain about a
I GN OR AN CE
particularpatientbecausenotallthenecessary testshave beenorcanberun.
The connection between toothaches and cavities is just not a logical consequence in either
direction. This is typical of the medical domain as well as most other judgmental domains:
lawbusinessdesignautomobilerepairgardeningdatingandsoon. Theagentsknowledge
can at best provide only a degree of belief in the relevant sentences. Our main tool for
D EG RE EO FB EL IE F
P RO BA BI LI TY dealing with degrees of belief is probability theory. In the terminology of Section 8.1 the
T HE OR Y
ontological commitments of logic and probability theory are the samethat the world is
composed of facts that do or do not hold in any particular casebut the epistemological
commitments are different: a logical agent believes each sentence to be true or false or has
no opinion whereas a probabilistic agent may have a numerical degree of belief between 0
forsentences thatarecertainly falseand1certainly true.
Probability provides a way of summarizing the uncertainty that comes from our lazi-
ness and ignorance thereby solving the qualification problem. We might not know forsure
what afflicts a particular patient but we believe that there is say an 80 chancethat is
a probability of 0.8that the patient who has a toothache has a cavity. That is we expect
that outofallthesituations that areindistinguishable from the current situation asfarasour
knowledge goes the patient will have acavity in 80 of them. This belief could be derived
from statistical data80 of the toothache patients seen so far have had cavitiesor from
somegeneraldentalknowledge orfromacombination ofevidencesources.
One confusing point is that at the time of our diagnosis there is no uncertainty in the
actual world: the patient either has a cavity or doesnt. So what does it mean to say the
probability of a cavity is 0.8? Shouldnt it be either 0 or 1? The answer is that probability
statementsaremadewithrespecttoaknowledgestatenotwithrespecttotherealworld. We
say Theprobabilitythatthepatienthasacavitygiventhatshehasatoothacheis0.8. Ifwe
later learn that the patient has a history of gum disease we can make a different statement:
Theprobability that the patient has acavity given that she has atoothache and ahistory of
gum disease is 0.4. If we gather further conclusive evidence against a cavity we can say
Theprobability thatthepatient hasacavity givenallwenowknowisalmost0. Notethat
these statements do not contradict each other; each is a separate assertion about a different
knowledgestate.
Consider again the A plan for getting to the airport. Suppose it gives us a 97 chance
90
of catching our flight. Does this mean it is a rational choice? Not necessarily: there might
be other plans such as A with higher probabilities. If it is vital not to miss the flight
180
then itisworth risking the longer waitattheairport. What about A aplan that involves
leavinghome24hoursinadvance? Inmostcircumstances thisisnotagoodchoicebecause
although it almost guarantees getting there on time it involves an intolerable waitnot to
mentionapossibly unpleasant dietofairportfood.
Tomakesuchchoices anagent mustfirsthave preferences between thedifferent pos-
P RE FE RE NC E
sible outcomes of the various plans. An outcome is a completely specified state including
O UT CO ME
suchfactorsaswhethertheagentarrivesontimeandthelengthofthewaitattheairport. We
useutilitytheorytorepresent andreason withpreferences. Theterm utilityisusedherein
U TI LI TY TH EO RY
the sense of the quality of being useful not in the sense of the electric company or water
works. Utility theory says that every state has a degree of usefulness orutility to an agent
andthattheagentwillpreferstateswithhigherutility.
Section13.2. Basic Probability Notation 483
Theutility of astate isrelative toan agent. Forexample the utility ofastate in which
Whitehascheckmated Blackinagameofchessisobviouslyhighfortheagentplaying White
butlowfortheagentplaying Black. Butwecantgostrictlybythescoresof112and0that
aredictatedbytherulesoftournamentchesssomeplayersincludingtheauthorsmightbe
thrilledwithadrawagainsttheworldchampionwhereasotherplayersincludingtheformer
worldchampionmightnot. Thereisnoaccounting fortasteorpreferences: youmightthink
that anagent whoprefers jalapeno bubble-gum ice cream to chocolate chocolate chip is odd
orevenmisguidedbutyoucouldnotsaytheagentisirrational. Autilityfunctioncanaccount
foranysetofpreferencesquirkyortypicalnobleorperverse. Notethatutilitiescanaccount
foraltruismsimplybyincluding thewelfareofothersasoneofthefactors.
Preferences as expressed by utilities are combined with probabilities in the general
theoryofrational decisions called decisiontheory:
D EC IS IO NT HE OR Y
Decisiontheory probability theoryutilitytheory.
The fundamental idea of decision theory is that an agent is rational if and only if it chooses
the action that yields the highest expected utility averaged over all the possible outcomes
M AX IM UM EX PE CT ED of the action. This is called the principle of maximum expected utility M EU. Note that
U TI LI TY
expected might seem like avague hypothetical term but asitisused here ithas aprecise
meaning: it means the average or statistical mean of the outcomes weighted by the
probability of the outcome. We saw this principle in action in Chapter 5 when we touched
brieflyonoptimaldecisions inbackgammon;itisinfactacompletely generalprinciple.
Figure13.1sketchesthestructureofanagentthatusesdecisiontheorytoselectactions.
The agent is identical at an abstract level to the agents described in Chapters 4 and 7 that
maintain a belief state reflecting the history of percepts to date. The primary difference is
that the decision-theoretic agents belief state represents not just the possibilities for world
states but also their probabilities. Given the belief state the agent can make probabilistic
predictions ofactionoutcomesandhenceselecttheactionwithhighestexpectedutility. This
chapterandthenextconcentrateonthetaskofrepresenting andcomputingwithprobabilistic
information ingeneral. Chapter 15 deals with methods forthe specific tasks of representing
and updating the belief state over time and predicting the environment. Chapter 16 covers
utility theory in more depth and Chapter 17 develops algorithms for planning sequences of
actionsinuncertain environments.
For our agent to represent and use probabilistic information we need a formal language.
The language of probability theory has traditionally been informal written by human math-
ematicians to other human mathematicians. Appendix Aincludes astandard introduction to
elementary probability theory; here wetake anapproach moresuited totheneeds of A Iand
moreconsistent withtheconcepts offormallogic.
function D T-A GE NTperceptreturnsanaction
persistent: belief stateprobabilisticbeliefsaboutthecurrentstateoftheworld
actiontheagentsaction
updatebelief state basedonaction andpercept
calculateoutcomeprobabilitiesforactions
givenactiondescriptionsandcurrentbelief state
selectaction withhighestexpectedutility
givenprobabilitiesofoutcomesandutilityinformation
returnaction
Figure13.1 Adecision-theoreticagentthatselectsrationalactions.
Like logical assertions probabilistic assertions are about possible worlds. Whereas logical
assertions saywhichpossible worldsarestrictly ruled out allthoseinwhichtheassertion is
false probabilistic assertions talkabouthowprobable thevariousworldsare. Inprobability
theory the set of all possible worlds is called the sample space. The possible worlds are
S AM PL ES PA CE
mutually exclusive and exhaustivetwo possible worlds cannot both be the case and one
possible world must be the case. For example if we are about to roll two distinguishable
dice there are 36 possible worlds to consider: 11 12 ... 66. The Greek letter Ω
uppercase omega is used to refer to the sample space and ω lowercase omega refers to
elementsofthespace thatisparticularpossible worlds.
Afullyspecifiedprobabilitymodelassociatesanumericalprobability Pωwitheach
P RO BA BI LI TY MO DE L
possible world.1 The basic axioms of probability theory say that every possible world has a
probability between0and1andthatthetotalprobability ofthesetofpossible worldsis1:
cid:12
ωΩ
Forexample if we assume that each die is fair and the rolls dont interfere with each other
then each of the possible worlds 11 12 ... 66 has probability 136. On the other
handifthediceconspiretoproducethesamenumberthentheworlds112233etc.
mighthavehigherprobabilities leaving theotherswithlowerprobabilities.
Probabilisticassertionsandqueriesarenotusuallyabout particularpossibleworldsbut
about setsofthem. Forexample wemightbeinterested inthe cases wherethetwodice add
up to 11 the cases where doubles are rolled and so on. In probability theory these sets are
called eventsa term already used extensively in Chapter 12for adifferent concept. In A I
E VE NT
the sets are always described by propositions in a formal language. One such language is
described in Section 13.2.2. Foreach proposition the corresponding set contains just those
possibleworldsinwhichthepropositionholds. Theprobabilityassociatedwithaproposition
certaincomplicationsthatarelessrelevantformostpurposesin A I.
Section13.2. Basic Probability Notation 485
isdefinedtobethesumoftheprobabilities oftheworldsinwhichitholds:
cid:12
Foranyproposition φ Pφ Pω. 13.2
ωφ
For example when rolling fair dice we have P Total 11 P56 P65
136 136 118. Note that probability theory does not require complete knowledge
of the probabilities of each possible world. For example if we believe the dice conspire to
producethesamenumberwemightassertthat Pdoubles 14withoutknowingwhether
thedicepreferdouble 6todouble 2. Just aswithlogical assertions thisassertion constrains
theunderlying probability modelwithoutfullydetermining it.
U NC ON DI TI ON AL Probabilitiessuchas P Total 11and Pdoublesarecalledunconditionalorprior
P RO BA BI LI TY
probabilitiesandsometimesjustpriorsforshort;theyrefertodegreesofbeliefinpropo-
P RI OR PR OB AB IL IT Y
sitions in the absence of any other information. Most of the time however we have some
information usually called evidence that has already been revealed. For example the first
E VI DE NC E
die may already be showing a 5 and we are waiting with bated breath for the other one to
stop spinning. In that case we are interested not in the unconditional probability of rolling
C ON DI TI ON AL doubles buttheconditionalorposteriorprobability orjustposterior forshortofrolling
P RO BA BI LI TY
P OS TE RI OR doublesgiventhatthefirstdieisa5. Thisprobabilityiswritten Pdoubles Die 5where
P RO BA BI LI TY 1
the is pronounced given. Similarly if I am going to the dentist for a regular checkup
the probability Pcavity0.2might beofinterest; but if Igo tothe dentist because Ihave
atoothache its Pcavitytoothache0.6thatmatters. Notethattheprecedence ofis
suchthatanyexpression oftheform P......alwaysmeans P.......
It is important to understand that Pcavity0.2 is still valid after toothache is ob-
served; it just isnt especially useful. When making decisions an agent needs to condition
on all the evidence it has observed. It is also important to understand the difference be-
tween conditioning and logical implication. The assertion that Pcavitytoothache0.6
does not mean Whenever toothache is true conclude that cavity is true with probabil-
ity 0.6 rather it means Whenever toothache is true and we have no further information
conclude that cavity is true with probability 0.6. The extra condition is important; for ex-
ample if we had the further information that the dentist found no cavities we definitely
would not want to conclude that cavity is true with probability 0.6; instead we need to use
Pcavitytoothache cavity0.
Mathematically speaking conditional probabilities are defined in terms of uncondi-
tionalprobabilities asfollows: foranypropositions aandbwehave
Pab
Pab 13.3
Pb
whichholdswhenever Pb 0. Forexample
Pdoubles Die 5
Pdoubles Die 5 1 .
1
P Die 5
1
The definition makes sense if you remember that observing b rules out all those possible
worldswhere bisfalseleaving asetwhosetotalprobability isjust Pb. Withinthatsetthe
a-worldssatisfy abandconstitute afraction Pab Pb.
Thedefinition of conditional probability Equation 13.3 can be written in a different
formcalledtheproductrule:
P RO DU CT RU LE
Pab Pab Pb
Theproduct ruleisperhaps easiertoremember: itcomesfrom thefactthatforaandbtobe
trueweneedbtobetrueandwealsoneed atobetruegivenb.
In this chapter and the next propositions describing sets of possible worlds are written in a
notationthatcombineselementsofpropositionallogicandconstraintsatisfactionnotation. In
the terminology of Section 2.4.7 it is a factored representation in which apossible world
isrepresented byasetofvariablevalue pairs.
Variablesinprobabilitytheoryarecalledrandomvariablesandtheirnamesbeginwith
R AN DO MV AR IA BL E
anuppercase letter. Thus inthe dice example Total and Die are random variables. Every
1
random variable has a domainthe set of possible values it can take on. The domain of
D OM AI N
Total for two dice is the set 2...12 and the domain of Die is 1...6. A Boolean
1
random variable has the domain truefalse notice that values are always lowercase; for
example the proposition that doubles are rolled can be written as Doublestrue. By con-
vention propositions of the form Atrue are abbreviated simply as a while Afalse is
abbreviated asa. Theusesofdoublescavityandtoothache inthepreceding section are
abbreviations of this kind. As in C SPs domains can be sets of arbitrary tokens; we might
choose the domain of Age to be juvenileteenadult and the domain of Weather might
besunnyraincloudysnow. Whennoambiguityispossibleitiscommontouseavalue
byitselftostandfortheproposition thataparticularvariablehasthatvalue;thus sunny can
standfor Weather sunny.
The preceding examples all have finite domains. Variables can have infinite domains
tooeitherdiscreteliketheintegersorcontinuouslikethereals. Foranyvariablewithan
ordereddomaininequalitiesarealsoallowedsuchas Number Of Atoms In Universe 1070.
Finally we can combine these sorts of elementary propositions including the abbre-
viated forms for Boolean variables by using the connectives of propositional logic. For
example we can express The probability that the patient has a cavity given that she is a
teenagerwithnotoothache is0.1asfollows:
Pcavitytoothache teen 0.1.
Sometimeswewillwanttotalkabouttheprobabilities ofallthepossible valuesofarandom
variable. Wecouldwrite:
P Weather sunny 0.6
P Weather rain 0.1
P Weather cloudy 0.29
P Weather snow 0.01
butasanabbreviation wewillallow
P Weathercid:160.60.10.290.01cid:17
Section13.2. Basic Probability Notation 487
wherethebold Pindicatesthattheresultisavectorofnumbersandwherewe assumeapre-
defined ordering cid:16sunnyraincloudysnowcid:17 on the domain of Weather. We say that the
P RO BA BI LI TY Pstatementdefinesaprobabilitydistributionfortherandomvariable Weather. The Pnota-
D IS TR IB UT IO N
tionisalsousedforconditionaldistributions: P XYgivesthevaluesof P Xx Y y
i j
foreachpossible ij pair.
Forcontinuousvariablesitisnotpossibletowriteouttheentiredistributionasavector
becausethereareinfinitelymanyvalues. Insteadwecandefinetheprobabilitythatarandom
variabletakesonsomevalue xasaparameterized function of x. Forexamplethesentence
P Noon Tempx Uniform x
18 C26 C
expresses thebelief that the temperature at noon isdistributed uniformly between 18and 26
P RO BA BI LI TY degrees Celsius. Wecallthisaprobabilitydensityfunction.
D EN SI TY FU NC TI ON
Probability density functions sometimes called pdfs differ in meaning from discrete
distributions. Saying that the probability density is uniform from 18 C to 26 C means that
there is a 100 chance that the temperature will fall somewhere in that 8 C-wide region
anda50chancethatitwillfallinany4 C-wideregionandsoon. Wewritetheprobability
densityforacontinuousrandomvariable X atvaluexas P Xxorjust Px;theintuitive
definition of Pxistheprobability that X fallswithinanarbitrarily smallregion beginning
atxdividedbythewidthoftheregion:
Px lim Px X xdxdx .
dx0
For Noon Temp wehave
cid:24
P Noon Tempx Uniform x 8 C
18 C26 C 0otherwise
where C stands forcentigrade not for aconstant. In P Noon Temp20.18 C 1 note
8 C
that 1 is not a probability it is a probability density. The probability that Noon Temp is
8 C
exactly 20.18 C is zero because 20.18 C is a region of width 0. Some authors use different
symbolsfordiscretedistributions anddensityfunctions; weuse P inbothcasessinceconfu-
sionseldomarisesandtheequationsareusuallyidentical. Notethatprobabilities areunitless
numberswhereasdensityfunctionsaremeasuredwithaunitinthiscasereciprocaldegrees.
In addition to distributions on single variables we need notation for distributions on
multiple variables. Commas are used for this. For example P Weather Cavity denotes
the probabilities of all combinations of the values of Weather and Cavity. This is a 42
J OI NT PR OB AB IL IT Y table of probabilities called the joint probability distribution of Weather and Cavity. We
D IS TR IB UT IO N
can also mix variables with and without values; Psunny Cavity would be a two-element
vector giving the probabilities of a sunny day with a cavity and a sunny day with no cavity.
The Pnotation makes certain expressions much moreconcise than they might otherwise be.
Forexampletheproduct rulesforallpossible valuesof Weather and Cavity canbewritten
asasingleequation:
P Weather Cavity P Weather Cavity P Cavity
insteadofasthese428equations usingabbreviations W and C:
P W sunny Ctrue P W sunny Ctrue P Ctrue
P W rain Ctrue P W rain Ctrue P Ctrue
P W cloudy Ctrue P W cloudy Ctrue P Ctrue
P W snow Ctrue P W snow Ctrue P Ctrue
P W sunny Cfalse P W sunny Cfalse P Cfalse
P W rain Cfalse P Wrain Cfalse P Cfalse
P W cloudy Cfalse P W cloudy Cfalse P Cfalse
P W snow Cfalse P W snow Cfalse P Cfalse.
As a degenerate case Psunnycavity has no variables and thus is a one-element vec-
tor that is the probability of a sunny day with a cavity which could also be written as
Psunnycavityor Psunnycavity. Wewillsometimesuse Pnotationtoderiveresults
aboutindividual P valuesandwhenwesay Psunny0.6itisreallyanabbreviationfor
Psunnyistheone-element vectorcid:160.6cid:17whichmeansthat Psunny0.6.
Nowwehave defined asyntax forpropositions andprobability assertions andwehave
givenpartofthesemantics: Equation13.2definestheprobabilityofapropositionasthesum
of the probabilities of worlds in which it holds. To complete the semantics we need to say
whattheworldsareandhowtodeterminewhetherapropositionholdsinaworld. Weborrow
this part directly from the semantics of propositional logic as follows. A possible world is
definedtobeanassignmentofvaluestoalloftherandomvariablesunderconsideration. Itis
easytoseethatthisdefinitionsatisfiesthebasicrequirementthatpossibleworldsbemutually
exclusive and exhaustive Exercise 13.5. For example if the random variables are Cavity
Toothache and Weather then there are 22416 possible worlds. Furthermore the
truth of any given proposition no matter how complex can be determined easily in such
worldsusingthesamerecursive definitionoftruthasforformulasinpropositional logic.
Fromthepreceding definition ofpossible worlds itfollows that aprobability modelis
completelydeterminedbythejointdistributionforalloftherandomvariablestheso-called
F UL LJ OI NT full joint probability distribution. For example if the variables are Cavity Toothache
P RO BA BI LI TY
D IS TR IB UT IO N
and Weather then the full joint distribution is given by P Cavity Toothache Weather.
Thisjoint distribution canberepresented asa 224tablewith16entries. Because every
propositions probability is a sum over possible worlds a full joint distribution suffices in
principle forcalculating theprobability ofanyproposition.
The basic axioms of probability Equations 13.1 and 13.2 imply certain relationships
amongthedegreesofbeliefthatcanbeaccordedtologically relatedpropositions. Forexam-
ple wecan derive the familiar relationship between the probability of a proposition and the
probability ofitsnegation:
cid:2
Pa Pω by Equation13.2
cid:2ωa cid:2 cid:2
Pω Pω Pω
cid:2ωa cid:2 ωa ωa
Pω Pω grouping thefirsttwoterms
ωΩ ωa
1 Pa by13.1and13.2.
Section13.2. Basic Probability Notation 489
We can also derive the well-known formula for the probability of a disjunction sometimes
I NC LU SI ON
calledtheinclusionexclusion principle:
E XC LU SI ON
P RI NC IP LE
Pab Pa Pb Pab. 13.4
Thisruleiseasilyrememberedbynotingthatthecaseswhereaholdstogetherwiththecases
where b holds certainly cover all the cases where ab holds; but summing the two sets of
casescountstheirintersection twicesoweneedtosubtract Pab. Theproofisleftasan
exercise Exercise13.6.
K OL MO GO RO VS Equations13.1and13.4areoftencalled Kolmogorovsaxiomsinhonorofthe Rus-
A XI OM S
sianmathematician Andrei Kolmogorov whoshowedhowtobuilduptherestofprobability
theory from this simple foundation and how to handle the difficulties caused by continuous
variables.2 While Equation 13.2 has a definitional flavor Equation 13.4 reveals that the
axioms really do constrain the degrees of belief an agent can have concerning logically re-
lated propositions. This is analogous to the fact that a logical agent cannot simultaneously
believe A B and A B because there is no possible world in which all three are true.
Withprobabilities howeverstatementsrefernottotheworlddirectly buttotheagents own
stateofknowledge. Whythencananagentnotholdthefollowingsetofbeliefseventhough
theyviolate Kolmogorovs axioms?
Pa 0.4 Pab 0.0
13.5
Pb 0.3 Pab 0.8.
This kind of question has been the subject of decades of intense debate between those who
advocate the use of probabilities as the only legitimate form for degrees of belief and those
whoadvocate alternative approaches.
One argument for the axioms of probability first stated in 1931 by Bruno de Finetti
andtranslatedinto Englishinde Finetti1993isasfollows: Ifanagenthassomedegreeof
belief inaproposition athen theagentshould beable tostate oddsatwhichitisindifferent
to a bet for or against a.3 Think of it as a game between two agents: Agent 1 states my
degree of belief in event a is 0.4. Agent 2 is then free to choose whether to wager for or
against aatstakes thatareconsistent withthestated degreeofbelief. Thatis Agent2could
choose toaccept Agent 1sbet that awilloccur offering 6against Agent 1s4. Or Agent
observe the outcome of a and whoever is right collects the money. If an agents degrees of
belief do not accurately reflect the world then you would expect that it would tend to lose
moneyoverthelongruntoanopposing agentwhosebeliefsmoreaccurately reflectthestate
oftheworld.
But de Finetti proved something much stronger: If Agent 1 expresses a set of degrees
of belief that violate the axioms of probability theory then there is a combination of bets by
Agent 2that guarantees that Agent 1 will lose money every time. Forexample suppose that
Agent1hasthesetofdegreesofbelieffrom Equation13.5. Figure13.2showsthatif Agent
1isnotcounterbalancedbyanequalpossibilityofwinning1.Onepossibleresponseistomakethebetamounts
smallenoughtoavoidthisproblem.Savagesanalysis1954circumventstheissuealtogether.
regardless of the outcomes for a and b. De Finettis theorem implies that no rational agent
canhavebeliefsthatviolatetheaxiomsofprobability.
Agent1 Agent2 Outcomesandpayoffs to Agent1
Proposition Belief Bet Stakes ab ab ab ab
a 0.4 a 4to6 6 6 4 4
b 0.3 b 3to7 7 3 7 3
ab 0.8 ab 2to8 2 2 2 8
11 1 1 1
Figure13.2 Because Agent1 hasinconsistentbeliefs Agent2is able to devisea set of
betsthatguaranteesalossfor Agent1nomatterwhattheoutcomeofaandb.
One common objection to de Finettis theorem is that this betting game is rather con-
trived. Forexample what ifone refuses to bet? Does that end the argument? Theansweris
that the betting game is an abstract model for the decision-making situation in which every
agent is unavoidably involved at every moment. Every action including inaction is a kind
ofbet andeveryoutcome canbeseenasapayoff ofthebet. Refusing tobetislikerefusing
toallowtimetopass.
Otherstrongphilosophicalargumentshavebeenputforwardfortheuseofprobabilities
mostnotably those of Cox1946 Carnap 1950 and Jaynes 2003. Theyeach construct a
set of axioms for reasoning with degrees of beliefs: no contradictions correspondence with
ordinary logic forexample ifbelief in Agoes up thenbelief in Amustgodown andso
on. The only controversial axiom is that degrees of belief must be numbers or at least act
likenumbersinthattheymustbetransitiveifbeliefin Aisgreaterthanbeliefin Bwhichis
greaterthan beliefin Cthen belief in Amustbegreater than Candcomparable thebelief
in Amustbeoneofequaltogreaterthanorlessthanbeliefin B. Itcanthenbeprovedthat
probability istheonlyapproach thatsatisfiestheseaxioms.
The world being the way it is however practical demonstrations sometimes speak
louder than proofs. The success of reasoning systems based on probability theory has been
muchmoreeffectiveinmakingconverts. Wenowlookathowtheaxiomscanbedeployedto
makeinferences.
P RO BA BI LI ST IC Inthissectionwedescribeasimplemethodforprobabilisticinferencethat isthecompu-
I NF ER EN CE
tation of posterior probabilities for query propositions given observed evidence. We use the
fulljointdistributionastheknowledgebasefromwhichanswerstoallquestionsmaybede-
rived. Alongthewaywealsointroduce severalusefultechniques formanipulating equations
involving probabilities.
Section13.3. Inference Using Full Joint Distributions 491
W HE RE D O P RO BA BI LI TI ES C OM E F RO M?
There has been endless debate over the source and status of probability numbers.
The frequentist position is that the numbers can come only from experiments: if
we test 100 people and find that 10 of them have a cavity then we can say that
the probability of a cavity is approximately 0.1. In this view the assertion the
probability ofacavityis0.1meansthat0.1isthefraction thatwouldbeobserved
in the limit of infinitely many samples. From any finite sample we can estimate
thetruefractionandalsocalculate howaccurate ourestimateislikelytobe.
The objectivist view is that probabilities are real aspects of the universe
propensities of objects to behave in certain waysrather than being just descrip-
tionsofanobservers degreeofbelief. Forexamplethefactthatafaircoincomes
up heads with probability 0.5 is a propensity of the coin itself. In this view fre-
quentist measurements areattempts toobserve these propensities. Mostphysicists
agreethatquantum phenomena areobjectively probabilistic butuncertainty atthe
macroscopic scalee.g. in coin tossingusually arises from ignorance of initial
conditions anddoesnotseemconsistent withthepropensity view.
The subjectivist view describes probabilities as a way of characterizing an
agents beliefs rather than as having any external physical significance. The sub-
jective Bayesianviewallowsanyself-consistent ascriptionofpriorprobabilitiesto
propositions buttheninsistsonproper Bayesianupdating asevidence arrives.
In the end even a strict frequentist position involves subjective analysis be-
causeofthereferenceclassproblem: intryingtodeterminetheoutcomeprobabil-
ityofaparticular experiment thefrequentist hastoplaceitinareference classof
similar experiments with known outcome frequencies. I. J. Good 1983 p. 27
wrote every event in life is unique and every real-life probability that we esti-
mate in practice is that of an event that has never occurred before. For example
given a particular patient a frequentist who wants to estimate the probability of a
cavitywillconsiderareferenceclassofotherpatientswho aresimilarinimportant
waysage symptoms dietand seewhatproportion ofthem hadacavity. Ifthe
dentistconsiderseverythingthatisknownaboutthepatientweight tothenearest
gramhaircolormothersmaidennamethenthereferenceclassbecomesempty.
Thishasbeenavexingproblem inthephilosophy ofscience.
The principle of indifference attributed to Laplace 1816 states that propo-
sitions that are syntactically symmetric with respect to the evidence should be
accorded equal probability. Various refinements have been proposed culminating
in the attempt by Carnap and others to develop a rigorous inductive logic capa-
bleofcomputingthecorrectprobabilityforanyproposition fromanycollectionof
observations. Currently itisbelieved thatno unique inductive logic exists; rather
any such logic rests on a subjective prior probability distribution whose effect is
diminished asmoreobservations arecollected.
toothache toothache
catch catch catch catch
cavity 0.108 0.012 0.072 0.008
cavity 0.016 0.064 0.144 0.576
Figure13.3 Afulljointdistributionforthe Toothache Cavity Catch world.
Webeginwithasimpleexample: adomainconsistingofjustthethree Booleanvariables
Toothache Cavityand Catch thedentistsnastysteelprobecatchesinmytooth. Thefull
jointdistribution isa222tableasshownin Figure13.3.
Noticethattheprobabilities inthejointdistributionsumto1asrequiredbytheaxioms
ofprobability. Noticealsothat Equation13.2givesusadirectwaytocalculatetheprobabil-
ityofanyproposition simpleorcomplex: simplyidentifythosepossibleworldsinwhichthe
proposition istrue and add uptheir probabilities. Forexample there are six possible worlds
inwhichcavity toothache holds:
Pcavity toothache 0.1080.0120.0720.0080.0160.064 0.28.
One particularly common task is to extract the distribution over some subset of variables or
a single variable. Forexample adding the entries in the first row gives the unconditional or
M AR GI NA L marginalprobability4 ofcavity:
P RO BA BI LI TY
Pcavity 0.1080.0120.0720.008 0.2.
Thisprocess iscalled marginalization orsummingoutbecause wesumuptheprobabil-
M AR GI NA LI ZA TI ON
ities for each possible value of the other variables thereby taking them out of the equation.
Wecanwritethefollowinggeneralmarginalization rulefor anysetsofvariables Yand Z:
cid:12
P Y P Yz 13.6
cid:2
z Z
where z Zmeanstosumoveralltcid:2hepossiblecombinationsofvaluesofthesetofvariables
Z. Wesometimesabbreviate thisas leaving Zimplicit. Wejustusedtheruleas
cid:12 z
P Cavity P Cavityz. 13.7
z Catch Toothache
A variant of this rule involves conditional probabilities instead of joint probabilities using
theproductrule:
cid:12
P Y P Yz Pz. 13.8
z
Thisruleiscalledconditioning. Marginalization andconditioning turnouttobeusefulrules
C ON DI TI ON IN G
forallkindsofderivations involving probability expressions.
In most cases we are interested in computing conditional probabilities of some vari-
ables given evidence about others. Conditional probabilities can be found by first using
marginsofinsurancetables.
Section13.3. Inference Using Full Joint Distributions 493
Equation13.3toobtainanexpressionintermsofunconditional probabilities andtheneval-
uating the expression from the full joint distribution. For example we can compute the
probability ofacavity givenevidence ofatoothache asfollows:
Pcavity toothache
Pcavitytoothache
Ptoothache
0.1080.012
0.6.
0.1080.0120.0160.064
Justtocheckwecanalsocomputetheprobability thatthere isnocavity givenatoothache:
Pcavity toothache
Pcavitytoothache
Ptoothache
0.0160.064
0.4.
0.1080.0120.0160.064
The two values sum to 1.0 as they should. Notice that in these two calculations the term
1 Ptoothache remains constant no matter which value of Cavity we calculate. In fact
it can be viewed as a normalization constant for the distribution P Cavitytoothache
N OR MA LI ZA TI ON
ensuring that it adds up to 1. Throughout the chapters dealing with probability we use α to
denotesuchconstants. Withthisnotation wecanwritethetwopreceding equations inone:
P Cavitytoothache α PCavitytoothache
α PCavitytoothachecatch P Cavitytoothachecatch
αcid:160.1080.016cid:17cid:160.0120.064cid:17 αcid:160.120.08cid:17 cid:160.60.4cid:17.
In other words we can calculate P Cavitytoothache even if we dont know the value of
Ptoothache! Wetemporarilyforgetaboutthefactor1 Ptoothacheandaddupthevalues
forcavity andcavitygetting0.12and0.08. Thosearethecorrect relativeproportions but
they dont sum to 1 so we normalize them by dividing each one by 0.12 0.08 getting
thetrue probabilities of0.6and 0.4. Normalization turns outtobeauseful shortcut inmany
probabilitycalculationsbothtomakethecomputationeasierandtoallowustoproceedwhen
someprobability assessment suchas Ptoothacheisnotavailable.
From the example we can extract a general inference procedure. We begin with the
case in whichthe query involves asingle variable X Cavity in theexample. Let Ebe the
listofevidencevariablesjust Toothache intheexampleletebethelistofobservedvalues
forthem and let Ybe theremaining unobserved variables just Catch intheexample. The
queryis P Xeandcanbeevaluatedas
cid:12
P Xe α PXe α P Xey 13.9
y
where the summation is over all possible ys i.e. all possible combinations of values of the
unobserved variables Y. Notice that together the variables X E and Yconstitute the com-
pletesetofvariablesforthedomainso P Xeyissimplyasubsetofprobabilities fromthe
fulljointdistribution.
Giventhe fulljoint distribution toworkwith Equation 13.9 can answerprobabilistic
queries for discrete variables. It does not scale well however: for a domain described by n
Booleanvariablesitrequiresaninputtableofsize O2nandtakes O2ntimetoprocessthe
table. Ina realistic problem wecould easily have n 100 making O2n impractical. The
fulljointdistributionintabularformisjustnotapracticaltoolforbuildingreasoningsystems.
Insteaditshouldbeviewedasthetheoreticalfoundationonwhichmoreeffectiveapproaches
maybebuiltjustastruthtablesformedatheoreticalfoundationformorepracticalalgorithms
like D PL L. The remainder of this chapter introduces some of the basic ideas required in
preparation forthedevelopment ofrealistic systemsin Chapter14.
Letusexpandthefulljointdistribution in Figure13.3byaddingafourthvariable Weather.
Thefulljointdistribution thenbecomes P Toothache Catch Cavity Weatherwhichhas
2224 32 entries. It contains four editions of the table shown in Figure 13.3
one for each kind of weather. What relationship do these editions have to each other and to
theoriginalthree-variable table? Forexamplehoware Ptoothachecatchcavitycloudy
and Ptoothachecatchcavityrelated? Wecanusetheproductrule:
Ptoothachecatchcavitycloudy
Pcloudytoothachecatchcavity Ptoothachecatchcavity.
Now unless one is in the deity business one should not imagine that ones dental problems
influence theweather. Andforindoordentistry atleast it seemssafetosaythattheweather
doesnotinfluencethedentalvariables. Therefore thefollowingassertion seemsreasonable:
Pcloudytoothachecatchcavity Pcloudy. 13.10
Fromthiswecandeduce
Ptoothachecatchcavitycloudy Pcloudy Ptoothachecatchcavity.
Asimilarequationexistsforeveryentryin P Toothache Catch Cavity Weather. Infact
wecanwritethegeneralequation
P Toothache Catch Cavity Weather P Toothache Catch Cavity P Weather.
Thus the 32-element table for four variables can be constructed from one 8-element table
andone4-elementtable. Thisdecomposition isillustrated schematically in Figure13.4a.
The property we used in Equation 13.10 is called independence also marginal in-
I ND EP EN DE NC E
dependenceandabsoluteindependence. Inparticular theweatherisindependent ofones
dentalproblems. Independence betweenpropositions aandbcanbewrittenas
Pab Pa or Pba Pb or Pab Pa Pb. 13.11
All these forms are equivalent Exercise 13.12. Independence between variables X and Y
canbewrittenasfollowsagain theseareallequivalent:
P XY PX or P Y X PY or P XY PX PY.
Independence assertions are usually based on knowledge of the domain. As the toothache
weather example illustrates they can dramatically reduce the amount of information nec-
essary to specify the full joint distribution. If the complete set of variables can be divided
Section13.5. Bayes Ruleand Its Use 495
Cavity Coin Coin
Toothache Catch
Weather
decomposes
decomposes
into
into
Cavity
Toothache Catch Weather
Coin Coin
a b
Figure13.4 Twoexamplesoffactoringalargejointdistributionintosmallerdistributions
using absolute independence. a Weather and dental problems are independent. b Coin
flipsareindependent.
into independent subsets then the full joint distribution can be factored into separate joint
distributions on those subsets. For example the full joint distribution on the outcome of n
independent coin flips P C ...C has 2n entries but it can be represented as the prod-
uct of n single-variable distributions P C . In a more practical vein the independence of
i
dentistry andmeteorology isagood thing because otherwise the practice ofdentistry might
requireintimateknowledgeofmeteorology andviceversa.
Whentheyareavailable thenindependence assertions can helpinreducingthesizeof
thedomainrepresentation andthecomplexityoftheinference problem. Unfortunately clean
separation of entire sets of variables by independence is quite rare. Whenever a connection
however indirect exists between two variables independence will fail to hold. Moreover
evenindependent subsetscanbequitelargeforexampledentistrymightinvolve dozensof
diseases and hundreds ofsymptoms allof which areinterrelated. Tohandle such problems
weneedmoresubtlemethodsthanthestraightforward conceptofindependence.
Onpage486wedefinedtheproductrule. Itcanactuallybewrittenintwoforms:
Pab Pab Pb and Pab Pba Pa.
Equatingthetworight-hand sidesanddividing by Paweget
Pab Pb
Pba . 13.12
Pa
This equation is known as Bayes rule also Bayes law or Bayes theorem. This simple
B AY ES RU LE
equation underlies mostmodern A Isystemsforprobabilistic inference.
Themore general case of Bayes rule formultivalued variables can bewritten in the P
notation asfollows:
P XY PY
P Y X
P X
Asbeforethisistobetakenasrepresentingasetofequationseachdealingwithspecificval-
uesofthevariables. Wewillalsohaveoccasiontouseamoregeneralversionconditionalized
onsomebackground evidence e:
P XYe P Y e
P Y Xe . 13.13
P Xe
On the surface Bayes rule does not seem very useful. It allows us to compute the single
term Pba in terms of three terms: Pab Pb and Pa. That seems like two steps
backwards but Bayes rule is useful in practice because there are many cases where we do
have good probability estimates for these three numbers and need to compute the fourth.
Often we perceive as evidence the effect of some unknown cause and we would like to
determinethatcause. Inthatcase Bayesrulebecomes
Peffectcause Pcause
Pcauseeffect .
Peffect
The conditional probability Peffectcause quantifies the relationship in the causal direc-
C AU SA L
tion whereas Pcauseeffectdescribes the diagnostic direction. In atask such as medical
D IA GN OS TI C
diagnosis weoften have conditional probabilities on causal relationships that is the doctor
knows Psymptomsdiseaseandwanttoderiveadiagnosis Pdiseasesymptoms. For
example a doctor knows that the disease meningitis causes the patient to have a stiff neck
say 70 of the time. The doctor also knows some unconditional facts: the prior probabil-
ity that a patient has meningitis is 150000 and the prior probability that any patient has a
stiff neck is 1. Letting s be the proposition that the patient has a stiff neck and m be the
proposition thatthepatienthasmeningitis wehave
Psm 0.7
Ps 0.01
Pms 0.0014. 13.14
Ps 0.01
Thatisweexpectlessthan1in700patientswithastiffnecktohavemeningitis. Noticethat
even though a stiff neck is quite strongly indicated by meningitis with probability 0.7 the
probabilityofmeningitisinthepatientremainssmall. Thisisbecausethepriorprobabilityof
stiffnecksismuchhigherthanthatofmeningitis.
Section13.3illustratedaprocessbywhichonecanavoidassessingthepriorprobability
of the evidence here Ps by instead computing a posterior probability for each value of
Section13.5. Bayes Ruleand Its Use 497
thequeryvariablehere mandmandthennormalizingtheresults. Thesameprocesscan
beappliedwhenusing Bayesrule. Wehave
P Ms αcid:16 Psm Pm Psm Pmcid:17.
Thus to use this approach we need to estimate Psm instead of Ps. There is no free
lunchsometimesthisiseasiersometimesitisharder. Thegeneralformof Bayesrulewith
normalization is
P Y X α PX YP Y 13.15
whereαisthenormalization constant neededtomaketheentriesin P Y Xsumto1.
One obvious question to ask about Bayes rule is why one might have available the
conditional probability inonedirection butnottheother. Inthemeningitis domain perhaps
thedoctorknowsthatastiffneckimpliesmeningitisin1outof5000cases;thatisthedoctor
has quantitative information in the diagnostic direction from symptoms to causes. Such a
doctor has no need to use Bayes rule. Unfortunately diagnostic knowledge is often more
fragilethancausalknowledge. Ifthereisasuddenepidemicofmeningitis theunconditional
probability of meningitis Pm will go up. The doctor who derived the diagnostic proba-
bility Pmsdirectly from statistical observation ofpatients before theepidemic willhave
noidea howto update the value butthe doctorwhocomputes Pmsfrom theotherthree
values will see that Pms should go up proportionately with Pm. Most important the
causalinformation Psmisunaffected bytheepidemicbecauseitsimplyreflectstheway
meningitis works. The use of this kind of direct causal ormodel-based knowledge provides
thecrucialrobustness neededtomakeprobabilistic systemsfeasibleintherealworld.
Wehave seen that Bayes rule can be useful for answering probabilistic queries conditioned
on one piece of evidencefor example the stiff neck. In particular we have argued that
probabilisticinformationisoftenavailableintheform Peffectcause. Whathappenswhen
we have two or more pieces of evidence? For example what can a dentist conclude if her
nastysteelprobecatchesintheachingtoothofapatient? Ifweknowthefulljointdistribution
Figure13.3wecanreadofftheanswer:
P Cavitytoothache catch αcid:160.1080.016cid:17 cid:160.8710.129cid:17.
We know however that such an approach does not scale up to larger numbers of variables.
Wecantryusing Bayesruletoreformulate theproblem:
P Cavitytoothache catch
α Ptoothache catch Cavity P Cavity. 13.16
Forthisreformulation toworkweneedtoknowtheconditional probabilities oftheconjunc-
tiontoothachecatch foreachvalueof Cavity. Thatmightbefeasibleforjusttwoevidence
variables but again it does not scale up. If there are n possible evidence variables X rays
dietoralhygieneetc.thenthereare2n possiblecombinationsofobservedvaluesforwhich
we would need to know conditional probabilities. We might as well go back to using the
full joint distribution. Thisiswhatfirstledresearchers awayfrom probability theory toward
approximate methodsforevidence combination that whilegiving incorrect answers require
fewernumberstogiveanyansweratall.
Rather than taking this route we need to find some additional assertions about the
domain that will enable ustosimplify the expressions. The notion of independencein Sec-
tion13.4provides aclue butneeds refining. Itwouldbenice if Toothache and Catch were
independent buttheyarenot: iftheprobe catches inthetooth thenitislikely thatthetooth
has a cavity and that the cavity causes a toothache. These variables are independent how-
ever given thepresence ortheabsence ofacavity. Eachisdirectly caused bythecavity but
neither has a direct effect on the other: toothache depends on the state of the nerves in the
tooth whereas the probes accuracy depends on the dentists skill to which the toothache is
irrelevant.5 Mathematically thisproperty iswrittenas
Ptoothache catch Cavity Ptoothache Cavity Pcatch Cavity. 13.17
C ON DI TI ON AL Thisequationexpressestheconditionalindependenceoftoothache andcatchgiven Cavity.
I ND EP EN DE NC E
Wecanplugitinto Equation13.16toobtaintheprobability ofacavity:
P Cavitytoothache catch
α Ptoothache Cavity Pcatch Cavity P Cavity. 13.18
Now the information requirements are the same as for inference using each piece of evi-
dence separately: the prior probability P Cavity for the query variable and the conditional
probability ofeacheffect givenitscause.
Thegeneraldefinitionofconditionalindependenceoftwovariables X and Ygivena
thirdvariable Zis
P XY Z P XZ PY Z.
Inthedentistdomainforexampleitseemsreasonabletoassertconditional independence of
thevariables Toothache and Catchgiven Cavity:
P Toothache Catch Cavity P Toothache Cavity P Catch Cavity. 13.19
Noticethatthisassertionissomewhatstrongerthan Equation13.17whichassertsindepen-
dence only for specific values of Toothache and Catch. As with absolute independence in
Equation13.11 theequivalent forms
P XY ZP XZ and P Y X ZP Y Z
can also be used see Exercise 13.17. Section 13.4 showed that absolute independence as-
sertionsallowadecomposition ofthefulljointdistribution intomuchsmallerpieces. Itturns
out that the same is true for conditional independence assertions. For example given the
assertion in Equation13.19 wecanderiveadecomposition asfollows:
P Toothache Catch Cavity
P Toothache Catch Cavity P Cavity productrule
P Toothache Cavity P Catch Cavity P Cavity using13.19.
Thereadercaneasilycheck thatthisequation doesinfacthold in Figure13.3. Inthisway
theoriginal large table isdecomposed intothree smallertables. Theoriginal table has seven
Section13.6. The Wumpus World Revisited 499
independent numbers 238 entries in the table but they must sum to 1 so 7 are indepen-
dent. The smaller tables contain five independent numbers for a conditional probability
distributions such as P TC there are tworowsof twonumbers and each row sums to1 so
thatstwoindependent numbers; forapriordistribution like P Cthereisonlyoneindepen-
dent number. Going from seven to five might not seem like a major triumph but the point
is that for n symptoms that are all conditionally independent given Cavity the size of the
representation grows as On instead of O2n. That means that conditional independence
assertions can allow probabilistic systems to scale up; moreover they are much more com-
monly available than absolute independence assertions. Conceptually Cavity separates
S EP AR AT IO N
Toothache and Catch because it is a direct cause of both of them. The decomposition of
largeprobabilistic domainsintoweaklyconnectedsubsets throughconditional independence
isoneofthemostimportantdevelopments intherecenthistoryof A I.
Thedentistryexampleillustratesacommonlyoccurringpatterninwhichasinglecause
directly influences anumberofeffects allofwhichareconditionally independent giventhe
cause. Thefulljointdistribution canbewrittenas
cid:25
P Cause Effect ...Effect P Cause P Effect Cause.
i
Such a probability distribution is called a naive Bayes modelnaive because it is often
N AI VE BA YE S
used as a simplifying assumption in cases where the effect variables are not actually
conditionally independent given the cause variable. The naive Bayes model is sometimes
called a Bayesian classifier a somewhat careless usage that has prompted true Bayesians
to call it the idiot Bayes model. In practice naive Bayes systems can work surprisingly
well even when the conditional independence assumption is not true. Chapter 20 describes
methodsforlearningnaive Bayesdistributions fromobservations.
Wecan combine of the ideas in this chapter to solve probabilistic reasoning problems in the
wumpusworld. See Chapter7foracompletedescriptionofthewumpusworld. Uncertainty
arises in the wumpus world because the agents sensors give only partial information about
the world. For example Figure 13.5 shows a situation in which each of the three reachable
squares13 22 and 31might contain a pit. Pure logical inference can conclude
nothing aboutwhichsquare ismostlikelytobesafesoalogicalagentmighthavetochoose
randomly. Wewillseethataprobabilistic agentcandomuchbetterthanthelogicalagent.
Ouraimistocalculatetheprobabilitythateachofthethree squarescontainsapit. For
this example we ignore the wumpus and the gold. The relevant properties of the wumpus
world are that 1 a pit causes breezes in all neighboring squares and 2 each square other
than 11 contains a pit with probability 0.2. The first step is to identify the set of random
variables weneed:
As in the propositional logic case we want one Boolean variable P for each square
ij
whichistrueiffsquareijactuallycontains apit.
14 24 34 44 14 24 34 44
13 23 33 43 13 23 33 43
O TH ER
Q UE RY
12 22 32 42 12 22 32 42
B
O K
F RO NT IE R
11 21 31 41 11 21 31 41
K NO WN
B
O K O K
a b
Figure13.5 a Afterfindingabreezeinboth12and21theagentisstuckthereis
nosafeplacetoexplore. b Divisionofthesquaresinto Known Frontierand Otherfor
aqueryabout13.
Wealso have Boolean variables B that are true iffsquare ij is breezy; weinclude
ij
thesevariables onlyfortheobservedsquaresin thiscase 1112and21.
Thenextstep istospecify thefulljoint distribution P P ...P B B B . Ap-
11 44 11 12 21
plyingtheproductrulewehave
P P ...P B B B
11 44 11 12 21
P B B B P ...P P P ...P .
11 12 21 11 44 11 44
This decomposition makes it easy to see what the joint probability values should be. The
first term is the conditional probability distribution of a breeze configuration given a pit
configuration; its values are 1 if the breezes are adjacent to the pits and 0 otherwise. The
second term is the prior probability of a pit configuration. Each square contains a pit with
probability 0.2independently oftheothersquares; hence
cid:2544
P P ...P P P . 13.20
11 44 ij
ij11
Foraparticularconfiguration withexactly npits P P ...P
0.2n0.816n.
11 44
In the situation in Figure 13.5a the evidence consists of the observed breeze or its
absenceineachsquarethatisvisited combinedwiththefactthateachsuchsquarecontains
nopit. Weabbreviatethesefactsasbb b b andknownp p p .
11 12 21 11 12 21
Weareinterested inanswering queries such as P P knownb: how likely isit that 13
13
contains apitgiventheobservations sofar?
Toanswerthisquery wecanfollow thestandard approach of Equation 13.9namely
summing over entries from the full joint distribution. Let Unknown be the set of P vari-
ij
Section13.6. The Wumpus World Revisited 501
ables forsquares otherthan the Known squares and the query square 13. Then by Equa-
tion13.9wehave
cid:12
P P knownb α P P unknownknownb.
13 13
unknown
The full joint probabilities have already been specified so we are donethat is unless we
care about computation. There are 12 unknown squares; hence the summation contains
2124096terms. Ingeneralthesummationgrowsexponentiallywiththenumberofsquares.
Surely one might ask arent the other squares irrelevant? How could 44 affect
whether 13 has a pit? Indeed this intuition is correct. Let Frontier be the pit variables
other than the query variable that are adjacent to visited squares in this case just 22 and
31. Alsolet Other bethepitvariablesfortheotherunknownsquares;inthiscasethereare
10othersquaresasshownin Figure13.5b. Thekeyinsight isthattheobservedbreezesare
conditionally independent of the other variables given the known frontier and query vari-
ables. Touse the insight wemanipulate the query formula into aform inwhich the breezes
areconditioned onalltheothervariables andthenweapply conditional independence:
P P knownb
13 cid:12
α P P knownbunknown by Equation13.9
13
uncid:12known
α Pb P knownunknown P P knownunknown
13 13
unknown
bytheproductrule
cid:12 cid:12
α Pbknown P frontierother P P knownfrontierother
13 13
frontierother
cid:12 cid:12
α Pbknown P frontier P P knownfrontierother
13 13
frontierother
wherethe finalstepuses conditional independence: bisindependent ofother givenknown
P and frontier. Now the first term in this expression does not depend on the Other
13
variables sowecanmovethesummationinward:
P P knownb
13 cid:12 cid:12
α Pbknown P frontier P P knownfrontierother.
13 13
frontier other
By independence as in Equation 13.20 the prior term can be factored and then the terms
canbereordered:
P P knownb
13 cid:12 cid:12
α Pbknown P frontier P P Pknown Pfrontier Pother
13 13
frontier other
cid:12 cid:12
α Pknown P P Pbknown P frontier Pfrontier Pother
13 13
frontier other
cid:12
αcid:2 P P Pbknown P frontier Pfrontier
13 13
frontier
13 13 13 13 13
12 22 12 22 12 22 12 22 12 22
B B B B B
O K O K O K O K O K
11 21 31 11 21 31 11 21 31 11 21 31 11 21 31
B B B B B
O K O K O K O K O K O K O K O K O K O K
a b
Pfrontier for each model: a three models with P 13true showing two or three pits
andbtwomodelswith P 13false showingoneortwopits.
where the last step folds Pknown into the normalizing constant and uses the fact that
cid:2
Potherequals1.
other
Now there are just four terms in the summation over the frontier variables P and
22
P . The use of independence and conditional independence has completely eliminated the
31
othersquaresfromconsideration.
Noticethattheexpression Pbknown P frontieris1whenthefrontierisconsis-
13
tentwiththebreezeobservations and0otherwise. Thusforeachvalueof P wesumover
13
the logical models for the frontier variables that are consistent with the known facts. Com-
pare with the enumeration over models in Figure 7.5 on page 241. The models and their
associated priorprobabilities Pfrontierareshownin Figure13.6. Wehave
P P knownb αcid:2cid:160.20.040.160.16 0.80.040.16cid:17 cid:160.310.69cid:17.
13
Thatis13and31bysymmetrycontainsapitwithroughly31probability. Asimilar
calculation which the reader might wish to perform shows that 22 contains a pit with
roughly 86 probability. The wumpus agent should definitely avoid 22! Note that our
logicalagentfrom Chapter7didnotknowthat22wasworse thantheothersquares. Logic
cantellusthatitisunknownwhetherthereisapitin22butweneedprobability totellus
howlikelyitis.
What this section has shown is that even seemingly complicated problems can be for-
mulated precisely in probability theory and solved with simple algorithms. To get efficient
solutions independence and conditional independence relationships can be used to simplify
the summations required. These relationships often correspond to ournatural understanding
ofhowtheproblemshouldbedecomposed. Inthenextchapter wedevelopformalrepresen-
tations for such relationships as well as algorithms that operate on those representations to
perform probabilistic inference efficiently.
14
P RO BA BI LI ST IC
R EA SO NI NG
In which we explain how to build network models to reason under uncertainty
according tothelawsofprobability theory.
independence and conditional independence relationships in simplifying probabilistic repre-
sentations ofthe world. Thischapter introduces asystematic waytorepresent such relation-
ships explicitly in the form of Bayesian networks. We define the syntax and semantics of
these networks and show how they can be used to capture uncertain knowledge in a natu-
ral and efficient way. We then show how probabilistic inference although computationally
intractable in the worst case can be done efficiently in many practical situations. We also
describe a variety of approximate inference algorithms that are often applicable when exact
inferenceisinfeasible. Weexplorewaysinwhichprobabilitytheorycanbeappliedtoworlds
withobjectsandrelationsthatistofirst-orderasopposedtopropositionalrepresentations.
Finallywesurveyalternative approaches touncertain reasoning.
In Chapter13wesawthatthefulljointprobabilitydistributioncanansweranyquestionabout
thedomainbutcanbecomeintractablylargeasthenumberofvariablesgrows. Furthermore
specifying probabilities forpossible worldsonebyoneisunnatural andtedious.
Wealsosawthatindependenceandconditionalindependencerelationshipsamongvari-
ablescangreatlyreducethenumberofprobabilitiesthatneedtobespecifiedinordertodefine
thefulljointdistribution. Thissectionintroducesadatastructurecalleda Bayesiannetwork1
B AY ES IA NN ET WO RK
torepresent the dependencies among variables. Bayesian networks can represent essentially
anyfulljointprobability distribution andinmanycasescandosoveryconcisely.
work causal network and knowledge map. In statistics the term graphical model refers to a somewhat
broaderclassthatincludes Bayesiannetworks.Anextensionof Bayesiannetworkscalledadecisionnetworkor
influencediagramiscoveredin Chapter16.
510
Section14.1. Representing Knowledgeinan Uncertain Domain 511
A Bayesiannetworkisadirected graph inwhicheachnodeisannotated withquantita-
tiveprobability information. Thefullspecification isasfollows:
1. Eachnodecorresponds toarandomvariable whichmaybediscrete orcontinuous.
2. Asetofdirectedlinksorarrowsconnectspairsofnodes. Ifthereisanarrowfromnode
X tonode Y X issaidtobeaparentof Y. Thegraphhasnodirectedcyclesandhence
isadirected acyclicgraph or D AG.
3. Eachnode X hasaconditionalprobabilitydistribution P X Parents X thatquan-
i i i
tifiestheeffectoftheparentsonthenode.
Thetopology ofthenetworkthe setofnodesandlinksspecifies theconditional indepen-
dence relationships that hold in the domain in a way that will be made precise shortly. The
intuitive meaning ofanarrow istypically that X hasadirect influence on Ywhichsuggests
thatcausesshouldbeparentsofeffects. Itisusuallyeasyforadomainexperttodecidewhat
directinfluencesexistinthedomainmucheasierinfactthanactuallyspecifyingtheprob-
abilities themselves. Once the topology of the Bayesian network is laid out we need only
specify a conditional probability distribution for each variable given its parents. We will
see that the combination of the topology and the conditional distributions suffices tospecify
implicitly thefulljointdistribution forallthevariables.
Recallthesimpleworlddescribedin Chapter13consisting ofthevariables Toothache
Cavity Catch and Weather. We argued that Weather is independent of the other vari-
ables; furthermore we argued that Toothache and Catch are conditionally independent
given Cavity. These relationships are represented by the Bayesian network structure shown
in Figure 14.1. Formally the conditional independence of Toothache and Catch given
Cavityisindicated bytheabsence ofalinkbetween Toothache and Catch. Intuitively the
network represents the fact that Cavity is a direct cause of Toothache and Catch whereas
nodirectcausalrelationship existsbetween Toothache and Catch.
Now consider the following example which is just a little more complex. You have
a new burglar alarm installed at home. It is fairly reliable at detecting a burglary but also
responds on occasion to minor earthquakes. This example is due to Judea Pearl a resident
of Los Angeleshencetheacuteinterestinearthquakes. Youalsohavetwoneighbors John
and Mary who have promised to call you at work when they hear the alarm. John nearly
always calls when he hears the alarm but sometimes confuses the telephone ringing with
Cavity
Weather
Toothache Catch
threevariablesand Toothache and Catch areconditionallyindependentgiven Cavity.
P B P E
Burglary Earthquake
.001 .002
B E P A
t t .95
Alarm
t f .94
f t .29
f f .001
A P J A P M
John Calls Mary Calls
t .90 t .70
f .05 f .01
Figure14.2 Atypical Bayesiannetworkshowingboththetopologyandtheconditional
probabilitytables C PTs. Inthe C PTs theletters B E A J and M standfor Burglary
Earthquake Alarm John Callsand Mary Callsrespectively.
the alarm and calls then too. Mary on the other hand likes rather loud music and often
misses thealarm altogether. Giventhe evidence ofwhohasor hasnot called wewouldlike
toestimatetheprobability ofaburglary.
A Bayesian network for this domain appears in Figure 14.2. The network structure
shows that burglary and earthquakes directly affect the probability of the alarms going off
but whether John and Mary call depends only on the alarm. The network thus represents
ourassumptions thattheydonotperceive burglaries directly theydonotnotice minorearth-
quakes andtheydonotconferbeforecalling.
The conditional distributions in Figure 14.2 are shown as a conditional probability
C ON DI TI ON AL table or C PT. This form of table can be used for discrete variables; other representations
P RO BA BI LI TY TA BL E
including those suitable for continuous variables are described in Section 14.2. Each row
in a C PT contains the conditional probability of each node value for a conditioning case.
C ON DI TI ON IN GC AS E
A conditioning case is just a possible combination of values for the parent nodesa minia-
ture possible world if you like. Each row must sum to 1 because the entries represent an
exhaustive setofcasesforthevariable. For Booleanvariables onceyouknowthattheprob-
abilityofatruevalueisptheprobability offalsemustbe1psoweoftenomitthesecond
number asin Figure 14.2. Ingeneral atable fora Boolean variable with k Boolean parents
contains2k independentlyspecifiableprobabilities. Anodewithnoparentshasonlyonerow
representing thepriorprobabilities ofeachpossible valueofthevariable.
Noticethatthenetworkdoesnothavenodescorrespondingto Maryscurrentlylistening
toloud musicortothetelephone ringing andconfusing John. Thesefactors aresummarized
intheuncertainty associated withthelinks from Alarm to John Calls and Mary Calls. This
showsbothlazinessandignoranceinoperation: itwouldbealotofworktofindoutwhythose
factorswouldbemoreorlesslikelyinanyparticularcaseandwehavenoreasonablewayto
obtain the relevant information anyway. The probabilities actually summarize a potentially
Section14.2. The Semanticsof Bayesian Networks 513
infinite set of circumstances in which the alarm might fail to go off high humidity power
failure dead battery cut wires a dead mouse stuck inside the bell etc. or John or Mary
mightfailtocallandreportitouttolunchonvacationtemporarilydeafpassinghelicopter
etc.. Inthiswayasmallagentcancopewithaverylargeworldatleastapproximately. The
degreeofapproximation canbeimprovedifweintroduce additional relevantinformation.
The previous section described what a network is but not what it means. There are two
ways in which one can understand the semantics of Bayesian networks. The first is to see
the network as a representation of the joint probability distribution. The second is to view
itas an encoding ofacollection ofconditional independence statements. Thetwoviews are
equivalent but the first turns out to be helpful in understanding how to construct networks
whereasthesecond ishelpfulindesigning inference procedures.
Viewed as a piece of syntax a Bayesian network is a directed acyclic graph with some
numeric parameters attached to each node. One way to define what the network meansits
semanticsistodefinethewayinwhichitrepresentsaspecificjointdistribution overallthe
variables. Todothis wefirstneedtoretract temporarily whatwesaidearlieraboutthepa-
rametersassociated witheachnode. Wesaidthatthoseparameterscorrespond toconditional
probabilities P X Parents X ; this is atrue statement but until weassign semantics to
i i
thenetworkasawholeweshould thinkofthemjustasnumbers θ X Parents X .
i i
Ageneric entry inthejointdistribution istheprobability ofaconjunction ofparticular
assignments to each variable such as P X x ... X x . We use the notation
Px ...x asanabbreviation forthis. Thevalueofthisentryisgivenbytheformula
cid:25n
Px ...x θx parents X 14.1
i1
where parents X denotes the values of Parents X that appear in x ...x . Thus
i i 1 n
each entry in the joint distribution is represented by the product of the appropriate elements
oftheconditional probability tables C PTsinthe Bayesiannetwork.
From this definition it is easy to prove that the parameters θ X Parents X are
i i
exactly the conditional probabilities P X Parents X implied by the joint distribution
i i
see Exercise14.2. Hencewecanrewrite Equation14.1as
cid:25n
Px ...x Px parents X . 14.2
i1
Inotherwords thetables wehavebeen calling conditional probability tables really arecon-
ditional probability tablesaccording tothesemanticsdefinedin Equation14.1.
Toillustratethiswecancalculatetheprobabilitythatthealarmhassoundedbutneither
aburglary noranearthquake hasoccurred andboth Johnand Marycall. Wemultiplyentries
fromthejointdistribution usingsingle-letter namesfor thevariables:
Pjmabe Pja Pma Pabe Pb Pe
0.900.700.0010.9990.998 0.000628 .
Section 13.3 explained that the full joint distribution can be used to answer any query about
thedomain. Ifa Bayesiannetworkisarepresentation ofthejointdistribution thenittoocan
beusedtoansweranyquery bysummingalltherelevantjoint entries. Section14.4explains
howtodothisbutalsodescribes methodsthataremuchmoreefficient.
Amethodforconstructing Bayesiannetworks
Equation 14.2 defines what a given Bayesian network means. The next step is to explain
how to construct a Bayesian network in such a way that the resulting joint distribution is a
goodrepresentationofagivendomain. Wewillnowshowthat Equation14.2impliescertain
conditional independence relationships that can be used to guide the knowledge engineer in
constructing thetopologyofthenetwork. Firstwerewrite theentriesinthejointdistribution
intermsofconditional probability usingtheproduct rule seepage486:
Px 1...x n Px nx n1...x 1 Px n1...x 1.
Thenwerepeattheprocessreducingeachconjunctiveprobabilitytoaconditionalprobability
andasmallerconjunction. Weendupwithonebigproduct:
Px 1...x n Px nx n1...x 1 Px n1x n2...x 1 Px 2x 1 Px 1
cid:25n
Px ix i1...x 1.
i1
Thisidentity iscalled thechainrule. Itholds foranysetofrandom variables. Comparingit
C HA IN RU LE
with Equation14.2weseethatthespecificationofthejointdistribution isequivalenttothe
generalassertion thatforeveryvariable X inthenetwork
i
P X i X i1...X 1 P X i Parents X i 14.3
providedthat Parents X i X i1...X 1. Thislastconditionissatisfiedbynumbering
thenodesinawaythatisconsistent withthepartialorderimplicitinthegraphstructure.
What Equation 14.3 says is that the Bayesian network is a correct representation of
the domain only if each node is conditionally independent of its other predecessors in the
nodeordering givenitsparents. Wecansatisfythiscondition withthismethodology:
1. Nodes: Firstdeterminethesetofvariables thatarerequired tomodelthedomain. Now
orderthem X ...X . Anyorderwillworkbuttheresultingnetworkwillbemore
compactifthevariables areorderedsuchthatcausesprecedeeffects.
2. Links: Fori1tondo:
Choose from X 1...X i1 a minimal set of parents for X i such that Equa-
tion14.3issatisfied.
Foreachparentinsertalinkfromtheparentto X .
i
C PTs: Writedowntheconditional probability table P X Parents X .
i i
Section14.2. The Semanticsof Bayesian Networks 515
Intuitively the parents of node X i should contain all those nodes in X 1 ... X i1 that
directly influence X . For example suppose we have completed the network in Figure 14.2
i
exceptforthechoiceofparentsfor Mary Calls. Mary Calls iscertainlyinfluencedbywhether
thereisa Burglary oran Earthquakebutnotdirectlyinfluenced. Intuitively ourknowledge
of the domain tells us that these events influence Marys calling behavior only through their
effectonthealarm. Alsogiventhestateofthealarmwhether Johncallshasnoinfluenceon
Marys calling. Formally speaking we believe that the following conditional independence
statementholds:
P Mary Calls John Calls Alarm Earthquake Burglary P Mary Calls Alarm.
Thus Alarm willbetheonlyparentnodefor Mary Calls.
Becauseeachnodeisconnectedonlytoearliernodesthisconstruction methodguaran-
teesthatthenetworkisacyclic. Anotherimportantpropertyof Bayesiannetworksisthatthey
contain no redundant probability values. If there is no redundancy then there is no chance
for inconsistency: it is impossible for the knowledge engineer or domain expert to create a
Bayesiannetworkthatviolatestheaxiomsofprobability.
Compactnessandnodeordering
Aswellasbeingacompleteandnonredundant representation ofthedomaina Bayesiannet-
work can often be far more compact than the full joint distribution. This property is what
makesitfeasible tohandle domains withmanyvariables. Thecompactness of Bayesian net-
L OC AL LY worksisanexampleofageneralpropertyoflocallystructuredalsocalledsparsesystems.
S TR UC TU RE D
In a locally structured system each subcomponent interacts directly with only a bounded
S PA RS E
number ofother components regardless ofthe total number of components. Local structure
isusually associated withlinearratherthanexponential growthincomplexity. Inthecaseof
Bayesian networks it is reasonable to suppose that in most domains each random variable
is directly influenced by at most k others for some constant k. If we assume n Boolean
variables for simplicity then the amount of information needed to specify each conditional
probability table will be at most 2k numbers and the complete network can be specified by
n2k numbers. Incontrast thejointdistribution contains 2n numbers. Tomakethisconcrete
suppose we have n30 nodes each with five parents k5. Then the Bayesian network
requires 960numbersbutthefulljointdistribution requiresoverabillion.
There are domains in which each variable can be influenced directly by all the others
sothatthenetwork isfullyconnected. Thenspecifying theconditional probability tables re-
quiresthesameamountofinformationasspecifying thejointdistribution. Insomedomains
there will be slight dependencies that should strictly be included by adding a new link. But
if these dependencies are tenuous then it maynot be worth the additional complexity in the
network for the small gain in accuracy. For example one might object to our burglary net-
work on the grounds that if there is an earthquake then John and Mary would not call even
if they heard the alarm because they assume that the earthquake is the cause. Whether to
add the link from Earthquake to John Calls and Mary Calls and thus enlarge the tables
depends oncomparing theimportance ofgetting moreaccurate probabilities withthecostof
specifying theextrainformation.
Mary Calls Mary Calls
John Calls John Calls
Alarm Earthquake
Burglary Burglary
Earthquake Alarm
a b
haveintroducednodesintop-to-bottomorder.
Even in a locally structured domain we will get a compact Bayesian network only if
we choose the node ordering well. What happens if we happen to choose the wrong or-
der? Consider theburglary example again. Suppose wedecide toadd the nodes inthe order
Mary Calls John Calls Alarm Burglary Earthquake. We then get the somewhat more
complicated networkshownin Figure14.3a. Theprocess goesasfollows:
Adding Mary Calls: Noparents.
Adding John Calls: If Mary calls that probably means the alarm has gone off which
of course would make it more likely that John calls. Therefore John Calls needs
Mary Calls asaparent.
Adding Alarm: Clearlyifbothcallitismorelikelythatthealarmhasgoneoffthanif
justoneorneithercallssoweneedboth Mary Calls and John Calls asparents.
Adding Burglary: If we know the alarm state then the call from John or Mary might
giveusinformation aboutourphoneringing or Marysmusic butnotaboutburglary:
P Burglary Alarm John Calls Mary Calls P Burglary Alarm.
Henceweneedjust Alarm asparent.
Adding Earthquake: If the alarm is on it is more likely that there has been an earth-
quake. The alarm is an earthquake detector of sorts. But if we know that there has
beenaburglarythenthatexplainsthealarmandtheprobabilityofanearthquakewould
beonlyslightly abovenormal. Henceweneedboth Alarm and Burglary asparents.
The resulting network has two more links than the original network in Figure 14.2 and re-
quires three more probabilities to be specified. Whats worse some of the links represent
tenuous relationships that require difficult and unnatural probability judgments such as as-
Section14.2. The Semanticsof Bayesian Networks 517
sessing the probability of Earthquake given Burglary and Alarm. This phenomenon is
quite general and is related to the distinction between causal and diagnostic models intro-
duced in Section 13.5.1 see also Exercise 8.13. If we try to build a diagnostic model with
links from symptoms to causes as from Mary Calls to Alarm or Alarm to Burglary we
enduphavingtospecifyadditionaldependenciesbetweenotherwiseindependentcausesand
oftenbetweenseparately occurring symptomsaswell. Ifwesticktoacausalmodelweend
uphavingtospecifyfewernumbersandthenumberswilloftenbeeasiertocomeupwith. In
the domain of medicine for example it has been shown by Tversky and Kahneman 1982
that expert physicians prefer to give probability judgments for causal rules rather than for
diagnostic ones.
Figure14.3b shows averybad node ordering: Mary Calls John Calls Earthquake
Burglary Alarm. Thisnetworkrequires31distinctprobabilitiestobespecifiedexactlythe
samenumberasthefull joint distribution. Itisimportant torealize however that anyofthe
threenetworkscanrepresent exactlythesamejointdistribution. Thelasttwoversionssimply
failtorepresentalltheconditionalindependence relationships andhenceendupspecifying a
lotofunnecessary numbersinstead.
We have provided a numerical semantics for Bayesian networks in terms of the represen-
tation of the full joint distribution as in Equation 14.2. Using this semantics to derive a
method for constructing Bayesian networks we were led to the consequence that a node is
conditionally independent of its other predecessors given its parents. It turns out that we
canalso gointheotherdirection. Wecanstart from atopological semantics that specifies
theconditional independence relationships encoded bythe graphstructure andfromthiswe
can derive the numerical semantics. The topological semantics2 specifies that each vari-
able is conditionally independent of its non-descendants given its parents. Forexample in
D ES CE ND AN T
Figure14.2 John Calls isindependent of Burglary Earthquakeand Mary Calls giventhe
value of Alarm. Thedefinition isillustrated in Figure 14.4a. From these conditional inde-
pendence assertions and the interpretation of the network parameters θ X Parents X
i i
asspecifications ofconditional probabilities P X Parents X thefull jointdistribution
i i
given in Equation 14.2 can be reconstructed. In this sense the numerical semantics and
thetopological semantics areequivalent.
Another important independence property is implied by the topological semantics: a
nodeisconditionallyindependentofallothernodesinthenetworkgivenitsparentschildren
and childrens parentsthat is given its Markovblanket. Exercise 14.7asks you toprove
M AR KO VB LA NK ET
this. Forexample Burglary isindependentof John Calls and Mary Callsgiven Alarm and
Earthquake. Thisproperty isillustrated in Figure14.4b.
conditionally independent of another set Y givenathirdset Z. Thecriterionisrathercomplicated and isnot
neededforderivingthealgorithmsinthischaptersoweomitit.Detailsmaybefoundin Pearl1988or Darwiche
2009.Shachter1998givesamoreintuitivemethodofascertainingd-separation.
. . .
U U
X X
Z Z Z Z
1j nj 1j nj
Y Y
Y Y 1 . . . n
a b
Figure14.4 a A node X is conditionallyindependentofits non-descendantse.g.the
Zijs given its parents the Uis shown in the gray area. b A node X is conditionally
independentofallothernodesinthenetworkgivenits Markovblanketthegrayarea.
Evenifthe maximumnumberofparents k issmallish filling inthe C PTforanode requires
upto O2knumbersandperhapsagreatdealofexperiencewithallthepossibleconditioning
cases. Infactthisisaworst-case scenario inwhichtherelationship betweentheparents and
the child is completely arbitrary. Usually such relationships are describable by a canonical
C AN ON IC AL distributionthatfitssomestandardpattern. Insuchcasesthecompletetablecanbespecified
D IS TR IB UT IO N
bynamingthepatternandperhaps supplying afewparametersmuch easierthansupplying
anexponential numberofparameters.
D ET ER MI NI ST IC The simplest example is provided by deterministic nodes. A deterministic node has
N OD ES
its value specified exactly by the values of its parents with no uncertainty. The relationship
canbealogicalone: forexampletherelationship betweentheparentnodes Canadian U S
Mexican and the child node North American is simply that the child is the disjunction of
the parents. The relationship can also be numerical: for example if the parent nodes are
the prices of a particular model of car at several dealers and the child node is the price that
a bargain hunter ends up paying then the child node is the minimum of the parent values;
or if the parent nodes are a lakes inflows rivers runoff precipitation and outflows rivers
evaporation seepageandthechildisthechangeinthewaterlevelofthelakethenthevalue
ofthechildisthesumoftheinflowparents minusthesumoftheoutflowparents.
Uncertain relationships can often be characterized by so-called noisy logical relation-
ships. The standard example is the noisy-O R relation which is a generalization of the log-
N OI SY-O R
ical O R. In propositional logic we might say that Fever is true if and only if Cold Flu or
Malaria is true. The noisy-O R model allows for uncertainty about the ability of each par-
ent to cause the child to be truethe causal relationship between parent and child may be
Section14.3. Efficient Representation of Conditional Distributions 519
inhibited and so a patient could have a cold but not exhibit a fever. The model makes two
assumptions. First it assumes that all the possible causes are listed. If some are missing
we can always add a so-called leak node that covers miscellaneous causes. Second it
L EA KN OD E
assumes that inhibition of each parent is independent of inhibition of any other parents: for
examplewhateverinhibits Malaria fromcausingafeverisindependentofwhateverinhibits
Flu from causing a fever. Given these assumptions Fever is false if and only if all its true
parents are inhibited and the probability of this is the product of the inhibition probabilities
q foreachparent. Letussuppose theseindividual inhibition probabilities areasfollows:
q Pfevercoldflumalaria 0.6
cold
q Pfevercoldflumalaria 0.2
flu
q Pfevercoldflumalaria 0.1.
malaria
Thenfromthisinformation andthenoisy-O R assumptions theentire C PTcanbebuilt. The
generalruleisthat
cid:25
Px parents X 1 q
i i j
j:Xjtrue
where the product is taken over the parents that are set to true for that row of the C PT. The
followingtableillustrates thiscalculation:
Cold Flu Malaria P Fever P Fever
F F F 0.0 1.0
F F T 0.9 0.1
F T F 0.8 0.2
F T T 0.98 0.02 0.20.1
T F F 0.4 0.6
T F T 0.94 0.06 0.60.1
T T F 0.88 0.12 0.60.2
T T T 0.988 0.012 0.60.20.1
In general noisy logical relationships in which a variable depends on k parents can be de-
scribed using Ok parameters instead of O2k for the full conditional probability table.
This makes assessment and learning much easier. For example the C PC S network Prad-
han etal. 1994 uses noisy-O R and noisy-M AX distributions tomodel relationships among
diseases and symptoms ininternal medicine. With 448 nodes and 906 links it requires only
8254valuesinsteadof133931430 foranetworkwithfull C PTs.
Bayesiannetswithcontinuousvariables
Manyreal-world problems involve continuous quantities such asheight mass temperature
andmoney;infactmuchofstatisticsdealswithrandomvariableswhosedomainsarecontin-
uous. By definition continuous variables have an infinite number of possible values so itis
impossibletospecifyconditional probabilities explicitlyforeachvalue. Onepossiblewayto
handlecontinuousvariablesistoavoidthembyusingdiscretizationthatisdividingupthe
D IS CR ET IZ AT IO N
Subsidy Harvest
Cost
Buys
Figure14.5 Asimplenetworkwithdiscretevariables Subsidyand Buysandcontinuous
variables Harvest and Cost.
possible values intoafixedsetofintervals. Forexample temperatures could bedivided into
0o C 0o C100o C and 100o C. Discretization is sometimes an adequate solution
but often results in a considerable loss of accuracy and very large C PTs. The most com-
monsolutionistodefinestandardfamiliesofprobability densityfunctionssee Appendix A
that are specified by a finite number of parameters. For example a Gaussian or normal
P AR AM ET ER
distribution Nμσ2x has the mean μ and the variance σ2 as parameters. Yet another
solutionsometimes called a nonparametric representationis to define the conditional
N ON PA RA ME TR IC
distribution implicitly with a collection of instances each containing specific values of the
parentandchildvariables. Weexplorethisapproach furtherin Chapter18.
A network with both discrete and continuous variables is called a hybrid Bayesian
H YB RI DB AY ES IA N network. To specify a hybrid network we have to specify two new kinds of distributions:
N ET WO RK
the conditional distribution for a continuous variable given discrete or continuous parents;
andtheconditionaldistributionforadiscretevariablegivencontinuousparents. Considerthe
simple example in Figure 14.5 in which a customer buys some fruit depending on its cost
whichdependsinturnonthesizeoftheharvestandwhetherthegovernmentssubsidyscheme
is operating. The variable Cost is continuous and has continuous and discrete parents; the
variable Buys isdiscreteandhasacontinuous parent.
For the Cost variable we need to specify P Cost Harvest Subsidy. The discrete
parent is handled by enumerationthat is by specifying both P Cost Harvestsubsidy
and P Cost Harvestsubsidy. Tohandle Harvestwespecify howthedistribution over
the cost c depends on the continuous value h of Harvest. In other words we specify the
parametersofthecostdistribution asafunctionofh. Themostcommonchoiceisthelinear
Gaussian distribution in which the child has a Gaussian distribution whose mean μ varies
L IN EA RG AU SS IA N
linearly with the value of the parent and whose standard deviation σ is fixed. We need two
distributions oneforsubsidy andoneforsubsidywithdifferentparameters:
""
Pchsubsidy Na thb tσ t2c e 2 σt
σ 2π
t
""
2
Pchsubsidy Na hb σ2c e 2 σf .
f f f
σ 2π
f
Forthisexamplethentheconditional distribution for Cost isspecifiedbynamingthelinear
Gaussiandistribution andprovidingtheparameters a b σ a b andσ . Figures14.6a
t t t f f f
Section14.3. Efficient Representation of Conditional Distributions 521
Pc hsubsidy Pc hsubsidy Pc h
rvesth
rvesth
rvesth
a b c
function of Harvest size with Subsidy true and false respectively. Graph c shows the
distribution P Cost Harvestobtainedbysummingoverthetwosubsidycases.
and b show these tworelationships. Notice that in each case the slope is negative because
cost decreases as supply increases. Of course the assumption of linearity implies that the
costbecomesnegativeatsomepoint;thelinearmodelisreasonableonlyiftheharvestsizeis
limitedtoanarrowrange. Figure14.6cshowsthedistribution Pchaveragingoverthe
twopossible valuesof Subsidy andassumingthateachhaspriorprobability 0.5. Thisshows
thatevenwithverysimplemodelsquiteinteresting distributions canberepresented.
The linear Gaussian conditional distribution has some special properties. A network
containing only continuous variables with linear Gaussian distributions has a joint distribu-
tionthatisamultivariate Gaussiandistributionsee Appendix Aoverallthevariables Exer-
cise14.9. Furthermoretheposteriordistribution given anyevidencealsohasthisproperty.3
When discrete variables are added as parents not as children of continuous variables the
C ON DI TI ON AL network defines a conditional Gaussian or C G distribution: given any assignment to the
G AU SS IA N
discretevariables thedistribution overthecontinuous variables isamultivariate Gaussian.
Now we turn to the distributions for discrete variables with continuous parents. Con-
sider for example the Buys node in Figure 14.5. It seems reasonable to assume that the
customer will buy if the cost is low and will not buy if it is high and that the probability of
buyingvariessmoothlyinsomeintermediateregion. Inotherwordstheconditionaldistribu-
tionislikeasoftthreshold function. Onewaytomakesoftthresholds istousetheintegral
ofthestandard normaldistribution:
cid:26
x
Φx N01xdx .
""
Thentheprobability of Buys given Cost mightbe
Pbuys Costc Φcμσ
whichmeansthatthecostthresholdoccursaround μthewidthofthethresholdregionispro-
portional to σ and the probability ofbuying decreases as cost increases. Thisprobit distri-
networktopology.In Section14.4weseethatinferencefornetworksofdiscretevariablesis N P-hard.
1
0.8
0.6
0.4
0.2
0
c P
1
0.8
0.6
0.4
0.2
0
Costc
c
syub P
Logit
Probit
Costc
a b
μ6.0withstandarddeviationσ1.0. b Logitandprobitdistributionsfortheprobability
ofbuys givencostfortheparametersμ6.0andσ1.0.
P RO BI T butionpronouncedpro-bitandshortforprobabilityunitis illustratedin Figure14.7a.
D IS TR IB UT IO N
Theformcanbejustifiedbyproposingthattheunderlyingdecisionprocesshasahardthresh-
oldbutthatthepreciselocationofthethreshold issubjecttorandom Gaussiannoise.
An alternative to the probit model is the logit distribution pronounced low-jit. It
L OG IT DI ST RI BU TI ON
usesthelogistic
function11extoproduce
asoftthreshold:
L OG IS TI CF UN CT IO N
1
Pbuys Costc .
1exp2cμ
σ
Thisisillustrated in Figure14.7b. Thetwodistributions look similar butthelogitactually
hasmuchlongertails. Theprobitisoftenabetterfittorealsituationsbutthelogitissome-
times easier to deal with mathematically. It is used widely in neural networks Chapter 20.
Both probit and logit can be generalized to handle multiple continuous parents by taking a
linearcombination oftheparentvalues.
The basic task for any probabilistic inference system is to compute the posterior probability
distribution for a set of query variables given some observed eventthat is some assign-
E VE NT
mentofvaluestoasetofevidencevariables. Tosimplifythepresentation wewillconsider
onlyonequeryvariable atatime;thealgorithms caneasilybeextended toqueries withmul-
tiple variables. We will use the notation from Chapter 13: X denotes the query variable; E
denotesthesetofevidencevariables E ...E andeisaparticularobservedevent; Ywill
denotesthenonevidence nonqueryvariables Y ...Y calledthehiddenvariables. Thus
H ID DE NV AR IA BL E 1 l
the complete set of variables is X XE Y. A typical query asks for the posterior
probability distribution P Xe.
Section14.4. Exact Inference in Bayesian Networks 523
In the burglary network we might observe the event in which John Callstrue and
Mary Callstrue. Wecouldthenaskforsaytheprobability thataburglary hasoccurred:
P Burglary John Callstrue Mary Callstrue cid:160.2840.716cid:17.
In this section we discuss exact algorithms for computing posterior probabilities and will
consider the complexity of this task. It turns out that the general case is intractable so Sec-
tion14.5coversmethodsforapproximate inference.
from the full joint distribution. More specifically a query P Xe can be answered using
Equation13.9whichwerepeathereforconvenience:
cid:12
P Xe α PXe α P Xey.
y
Nowa Bayesian networkgivesacompleterepresentation ofthefulljointdistribution. More
specifically Equation 14.2 on page 513 shows that the terms Pxey in the joint distri-
butioncanbewrittenasproducts ofconditional probabilities fromthenetwork. Therefore a
query can be answered using a Bayesian network by computing sums of products of condi-
tionalprobabilities fromthenetwork.
Consider the query P Burglary John Callstrue Mary Callstrue. The hidden
variables for this query are Earthquake and Alarm. From Equation 13.9 using initial
lettersforthevariables toshortentheexpressions wehave4
cid:12cid:12
P Bjm α PBjm α P Bjmea.
e a
The semantics of Bayesian networks Equation 14.2 then gives us an expression in terms
of C PTentries. Forsimplicity wedothisjustfor Burglarytrue:
cid:12cid:12
Pbjm α Pb Pe Pabe Pja Pma.
e a
To compute this expression we have to add four terms each computed by multiplying five
numbers. Intheworstcasewherewehavetosumoutalmostallthevariablesthecomplexity
ofthealgorithm foranetworkwith n Booleanvariables is On2n.
An improvement can be obtained from the following simple observations: the Pb
termisaconstantandcanbemovedoutsidethesummationsoveraandeandthe Peterm
canbemovedoutsidethesummationovera. Hencewehave
cid:12 cid:12
Pbjm α Pb Pe Pabe Pja Pma. 14.4
e a
Thisexpression canbeevaluated bylooping through thevariables inorder multiplying C PT
entries as we go. For each summation we also need to loop over the variables possible
P
e
Booleanthereisanambiguityinthat Peisusedtomeanboth P E trueand P E ebutitshouldbe
clearfromcontextwhichisintended;inparticularinthecontextofasumthelatterisintended.
values. The structure of this computation is shown in Figure 14.8. Using the numbers from
byieldsα0.0014919;hence
P Bjm αcid:160.000592240.0014919cid:17 cid:160.2840.716cid:17.
Thatisthechanceofaburglary givencallsfrombothneighbors isabout28.
Theevaluation processfortheexpression in Equation14.4isshownasanexpression
tree in Figure 14.8. The E NU ME RA TI ON-A SK algorithm in Figure 14.9 evaluates such trees
using depth-first recursion. Thealgorithm is verysimilar instructure tothe backtracking al-
gorithmforsolving C SPs Figure6.5andthe D PL Lalgorithmforsatisfiability Figure7.17.
Thespacecomplexityof E NU ME RA TI ON-A SK isonlylinearinthenumberofvariables:
thealgorithm sumsoverthefulljointdistribution without everconstructing itexplicitly. Un-
fortunately its time complexity for a network with n Boolean variables is always O2n
betterthanthe On2nforthesimpleapproach described earlier butstillrather grim.
Note that the tree in Figure 14.8 makes explicit the repeated subexpressions evalu-
atedbythealgorithm. Theproducts Pja Pmaand Pja Pmaarecomputed
twiceonceforeachvalueofe. Thenextsectiondescribesageneralmethodthatavoidssuch
wastedcomputations.
The enumeration algorithm can be improved substantially by eliminating repeated calcula-
tions of the kind illustrated in Figure 14.8. The idea is simple: do the calculation once and
savetheresultsforlateruse. Thisisaformofdynamicprogramming. Thereareseveralver-
V AR IA BL E sionsofthisapproach;wepresentthevariableeliminationalgorithmwhichisthesimplest.
E LI MI NA TI ON
Variableelimination worksbyevaluating expressions such as Equation14.4inright-to-left
orderthatisbottomupin Figure14.8. Intermediateresultsarestoredandsummationsover
eachvariable aredoneonlyforthoseportions oftheexpression thatdependonthevariable.
Letusillustrate thisprocessfortheburglary network. Weevaluatetheexpression
cid:12 cid:12
P Bjm α P B Pe Pa Be Pja Pma .
cid:27cid:28cid:29cid:30 cid:27cid:28cid:29cid:30 cid:27 cid:28cid:29 cid:30cid:27 cid:28cid:29 cid:30cid:27 cid:28cid:29 cid:30
e a
f1 B f2 E f3 AB E f4 A f5 A
Noticethatwehaveannotatedeachpartoftheexpressionwiththenameofthecorresponding
factor; each factorisamatrixindexed bythevalues ofitsargument variables. Forexample
F AC TO R
thefactorsf Aandf Acorresponding to Pjaand Pmadependjuston Abecause
J and M arefixedbythequery. Theyaretherefore two-elementvectors:
cid:13 cid:14 cid:13 cid:14 cid:13 cid:14 cid:13 cid:14
Pja 0.90 Pma 0.70
f A f A .
f A BEwillbea222matrixwhichishardtoshowontheprintedpage. Thefirst
3
element isgiven by Pabe0.95 andthelast by Pabe0.999. Intermsof
factors thequeryexpression iswrittenas
cid:12 cid:12
P Bjm αf B f E f A BEf Af A
e a
Section14.4. Exact Inference in Bayesian Networks 525
Pb
.001
Pe Pe
.002 .998
Pabe Pabe Pabe Pabe
.95 .05 .94 .06
Pja P ja P ja P ja
.90 .05 .90 .05
Pma Pma Pma Pma
.70 .01 .70 .01
proceedstopdownmultiplyingvaluesalongeachpathandsummingatthenodes.Notice
therepetitionofthepathsforjandm.
function E NU ME RA TI ON-A SK Xebnreturnsadistributionover X
inputs:X thequeryvariable
eobservedvaluesforvariables E
bna Bayesnetwithvariables X E Y Yhiddenvariables
Q Xadistributionover Xinitiallyempty
foreachvaluexi of X do
Qxi E NU ME RA TE-A LLbn.V AR Sexi
whereexi iseextendedwith X xi
return N OR MA LI ZE QX
function E NU ME RA TE-A LLvarsereturnsarealnumber
if E MP TY?varsthenreturn1.0
Y F IR STvars
if Y hasvaluey ine
thenreturncid:2 Pyparents Y E NU ME RA TE-A LL RE STvarse
elsereturn
y
Pyparents Y E NU ME RA TE-A LL RE STvarsey
whereey iseextendedwith Y y
Figure14.9 Theenumerationalgorithmforansweringquerieson Bayesiannetworks.
wheretheoperatorisnotordinarymatrixmultiplication butinsteadthepointwiseprod-
P OI NT WI SE uctoperation tobedescribed shortly.
P RO DU CT
The process of evaluation is a process of summing out variables right to left from
pointwise products of factors to produce new factors eventually yielding a factor that is the
solution i.e.theposteriordistribution overthequeryvariable. Thestepsareasfollows:
Firstwesumout Afromtheproduct off f andf . Thisgivesusanew22factor
f B Ewhoseindicesrangeoverjust B and E:
6
cid:12
f B E f A BEf Af A
a
f a B Ef af af a B Ef af a.
Nowweareleftwiththeexpression
cid:12
P Bjm αf B f Ef B E.
e
Nextwesumout E fromtheproductof f andf :
cid:12
f B f Ef B E
e
f ef Bef ef Be.
Thisleavestheexpression
P Bjm αf Bf B
whichcanbeevaluatedbytakingthepointwiseproduct andnormalizing theresult.
Examiningthissequenceweseethattwobasiccomputationaloperationsarerequired: point-
wiseproduct ofapairoffactors andsumming out avariable from aproduct offactors. The
nextsectiondescribes eachoftheseoperations.
Operationsonfactors
The pointwise product of two factors f and f yields a new factor f whose variables are
the union of the variables in f and f and whose elements are given by the product of the
correspondingelementsinthetwofactors. Supposethetwofactorshavevariables Y ...Y
incommon. Thenwehave
f X ...X Y ...Y Z ...Z f X ...X Y ...Y f Y ...Y Z ...Z .
If all the variables are binary then f and f have 2jk and 2kl entries respectively and
the pointwise product has 2jkl entries. For example given two factors f A B and
1
f B C the pointwise product f f f A BC has 21118 entries as illustrated
in Figure 14.10. Notice that the factor resulting from a pointwise product can contain more
variablesthananyofthefactorsbeingmultipliedandthatthesizeofafactorisexponentialin
the number of variables. This is where both space and time complexity arise in the variable
elimination algorithm.
Section14.4. Exact Inference in Bayesian Networks 527
A B f A B B C f B C A B C f A BC
T T .3 T T .2 T T T .3.2.06
T F .7 T F .8 T T F .3.8.24
F T .9 F T .6 T F T .7.6.42
F F .1 F F .4 T F F .7.4.28
F T T .9.2.18
F T F .9.8.72
F F T .1.6.06
F F F .1.4.04
Figure14.10 Illustratingpointwisemultiplication:f A Bf B Cf A BC.
Summingoutavariablefromaproductoffactorsisdonebyaddingupthesubmatrices
formed by fixing the variable to each of its values in turn. For example to sum out A from
f A BCwewrite
f B C f A BC f a B Cf a B C
cid:13a cid:14 cid:13 cid:14 cid:13 cid:14
.06 .24 .18 .72 .24 .96
.
.42 .28 .06 .04 .48 .32
Theonlytrick istonotice thatanyfactorthatdoes notdepend onthevariable tobesummed
out can be moved outside the summation. Forexample if wewere to sum out E first in the
burglary network therelevantpartoftheexpression would be
cid:12 cid:12
f Ef A BEf Af A f Af A f Ef A BE.
e e
Now the pointwise product inside the summation is computed and the variable is summed
outoftheresulting matrix.
Notice that matrices are not multiplied until we need to sum out a variable from the
accumulated product. Atthatpoint wemultiply justthose matrices thatinclude thevariable
to be summed out. Given functions for pointwise product and summing out the variable
elimination algorithm itselfcanbewrittenquitesimplyasshownin Figure14.11.
Variableorderingandvariablerelevance
Thealgorithmin Figure14.11includesanunspecified O RD ERfunctiontochooseanordering
for the variables. Every choice of ordering yields a valid algorithm but different orderings
cause different intermediate factors to be generated during the calculation. For example in
the calculation shown previously we eliminated A before E; if we do it the other way the
calculation becomes
cid:12 cid:12
P Bjm αf B f Af A f Ef A BE
a e
duringwhichanewfactor f A Bwillbegenerated.
6
In general the time and space requirements of variable elimination are dominated by
the size of the largest factor constructed during the operation of the algorithm. This in turn
function E LI MI NA TI ON-A SK Xebnreturnsadistributionover X
inputs:Xthequeryvariable
eobservedvaluesforvariables E
bna Bayesiannetworkspecifyingjointdistribution P X 1...Xn
factors
foreachvar in O RD ERbn.V AR Sdo
factors M AK E-F AC TO Rvarefactors
ifvar isahiddenvariablethenfactors S UM-O UTvarfactors
return N OR MA LI ZE PO IN TW IS E-P RO DU CTfactors
Figure14.11 Thevariableeliminationalgorithmforinferencein Bayesiannetworks.
is determined by the order of elimination of variables and by the structure of the network.
It turns out to be intractable to determine the optimal ordering but several good heuristics
are available. One fairly effective method is a greedy one: eliminate whichever variable
minimizesthesizeofthenextfactortobeconstructed.
Let us consider one more query: P John Calls Burglarytrue. As usual the first
stepistowriteoutthenestedsummation:
cid:12 cid:12 cid:12
P Jb α Pb Pe Pabe P Ja Pma.
e a m cid:2
Evaluating thisexpression from right toleft wenotice something interesting: Pma
m
isequal to1by definition! Hence there wasnoneed toinclude itin thefirstplace; the vari-
able M is irrelevant to this query. Another way of saying this is that the result of the query
P John Calls Burglarytrue is unchanged if we remove Mary Calls from the network
altogether. Ingeneralwecanremoveanyleafnodethatisnotaqueryvariableoranevidence
variable. Afteritsremoval theremaybesomemoreleafnodes andthesetoomaybeirrele-
vant. Continuing this process we eventually find that every variable that is not an ancestor
of a query variable or evidence variable is irrelevant to the query. A variable elimination
algorithm cantherefore removeallthesevariablesbeforeevaluating thequery.
Thecomplexityofexactinferencein Bayesiannetworksdependsstronglyonthestructureof
thenetwork. Theburglarynetworkof Figure14.2belongstothefamilyofnetworksinwhich
thereisatmostoneundirected pathbetweenanytwonodes inthenetwork. Thesearecalled
singlyconnectednetworksorpolytreesandtheyhaveaparticularlyniceproperty: Thetime
S IN GL YC ON NE CT ED
andspacecomplexityofexactinferenceinpolytreesislinearinthesizeofthenetwork. Here
P OL YT RE E
the size is defined as the number of C PT entries; if the number of parents of each node is
bounded byaconstant thenthecomplexity willalsobelinearinthenumberofnodes.
M UL TI PL Y Formultiplyconnectednetworkssuchasthatof Figure14.12avariableelimination
C ON NE CT ED
can have exponential time and space complexity in the worst case even when the number
of parents per node is bounded. This is not surprising when one considers that because it
Section14.4. Exact Inference in Bayesian Networks 529
P C.5
Cloudy
P C.5
C P S C P R
t .10 Sprinkler Rain t .80 Cloudy
f .50 f .20 P SRx
C t t t f f t f f
Wet
Grass Spr Rain t .08 .02.72 .18
f .10 .40.10 .40
S R P W S R P W
t t .99 t t .99 Wet
t f .90 t f .90 Grass
f t .90 f t .90
f f .00 f f .00
a b
Figure14.12 a Amultiplyconnectednetworkwithconditionalprobabilitytables. b A
clusteredequivalentofthemultiplyconnectednetwork.
includes inferenceinpropositional logicasaspecialcaseinference in Bayesiannetworksis
N P-hard. Infactitcanbeshown Exercise14.16thattheproblemisashardasthatofcom-
puting the number of satisfying assignments for a propositional logic formula. This means
thatitis P-hardnumber-P hardthat isstrictlyharderthan N P-completeproblems.
Thereisacloseconnection betweenthecomplexityof Bayesiannetworkinferenceand
the complexity of constraint satisfaction problems C SPs. As we discussed in Chapter 6
the difficulty of solving a discrete C SP is related to how treelike its constraint graph is.
Measures such as tree width which bound the complexity of solving a C SP can also be
applied directly to Bayesian networks. Moreover the variable elimination algorithm can be
generalized tosolve C SPsaswellas Bayesiannetworks.
Thevariableeliminationalgorithmissimpleandefficientforansweringindividualqueries. If
wewanttocompute posterior probabilities forallthevariables inanetwork however itcan
be less efficient. Forexample in a polytree network one would need to issue On queries
costing On each for a total of On2 time. Using clustering algorithms also known as
C LU ST ER IN G
jointreealgorithms thetimecanbereduced to On. Forthisreason thesealgorithms are
J OI NT RE E
widelyusedincommercial Bayesiannetworktools.
The basic idea of clustering is to join individual nodes of the network to form clus-
ter nodes in such a way that the resulting network is a polytree. For example the multiply
connected network shown in Figure 14.12a can be converted into a polytree by combin-
ing the Sprinkler and Rain node into a cluster node called Sprinkler Rain as shown in
possible values: tttfftandff. Themeganodehasonlyoneparent the Booleanvariable
Cloudy so there are two conditioning cases. Although this example doesnt show it the
processofclustering oftenproduces meganodesthatsharesomevariables.
Oncethenetworkisinpolytreeformaspecial-purpose inferencealgorithmisrequired
becauseordinary inference methodscannothandlemeganodesthatsharevariables witheach
other. Essentiallythealgorithmisaformofconstraintpropagationsee Chapter6wherethe
constraintsensurethatneighboringmeganodesagreeonthe posteriorprobabilityofanyvari-
ablesthattheyhaveincommon. Withcarefulbookkeeping thisalgorithmisabletocompute
posterior probabilities forallthe nonevidence nodes inthenetwork intime linear inthesize
oftheclustered network. Howeverthe N P-hardnessoftheproblem hasnotdisappeared: ifa
network requires exponential timeand space withvariable elimination then the C PTsinthe
clustered networkwillnecessarily beexponentially large.
Giventhe intractability ofexact inference in large multiply connected networks itis essen-
tialtoconsiderapproximate inferencemethods. Thissectiondescribesrandomized sampling
algorithms also called Monte Carlo algorithms that provide approximate answers whose
M ON TE CA RL O
accuracy depends on the number of samples generated. Monte Carlo algorithms of which
simulated annealing page 126 is an example are used in many branches of science to es-
timate quantities that are difficult to calculate exactly. In this section we are interested in
sampling applied to the computation of posterior probabilities. We describe two families of
algorithms: directsamplingand Markovchainsampling. Twootherapproachesvariational
methodsandloopypropagationare mentioned inthenotesattheendofthechapter.
Theprimitive elementinanysampling algorithm isthegeneration ofsamples from aknown
probabilitydistribution. Forexampleanunbiasedcoincanbethoughtofasarandomvariable
Coin with values cid:16headstailscid:17 and a prior distribution P Coin cid:160.50.5cid:17. Sampling
fromthisdistributionisexactlylikeflippingthecoin: withprobability0.5itwillreturnheads
and with probability 0.5 it will return tails. Given a source of random numbers uniformly
distributed in the range 01 it is a simple matter to sample any distribution on a single
variable whetherdiscreteorcontinuous. See Exercise14.17.
Thesimplestkindofrandomsamplingprocess for Bayesiannetworksgenerates events
from a network that has no evidence associated with it. The idea is to sample each variable
inturn intopological order. Theprobability distribution from which thevalue issampled is
conditioned onthevaluesalreadyassignedtothevariablesparents. Thisalgorithm isshown
in Figure 14.13. Wecan illustrate its operation on the network in Figure 14.12a assuming
anordering Cloudy Sprinkler Rain Wet Grass:
1. Samplefrom P Cloudy cid:160.50.5cid:17valueistrue.
2. Samplefrom P Sprinkler Cloudytrue cid:160.10.9cid:17valueisfalse.
3. Samplefrom P Rain Cloudytrue cid:160.80.2cid:17valueistrue.
4. Samplefrom P Wet Grass Sprinkler false Raintrue cid:160.90.1cid:17valueistrue.
Inthiscase P RI OR-S AM PL E returnstheevent truefalsetruetrue.
Section14.5. Approximate Inferencein Bayesian Networks 531
function P RI OR-S AM PL Ebnreturnsaneventsampledfromthepriorspecifiedbybn
inputs:bna Bayesiannetworkspecifyingjointdistribution P X 1...Xn
xaneventwithnelements
foreachvariable Xiin X 1...Xndo
xiarandomsamplefrom P Xi parents Xi
returnx
Figure14.13 Asamplingalgorithmthatgenerateseventsfroma Bayesiannetwork.Each
variableissampledaccordingtotheconditionaldistributiongiventhevaluesalreadysampled
forthevariablesparents.
Itiseasytoseethat P RI OR-S AM PL E generatessamplesfromthepriorjointdistribution
specifiedbythenetwork. Firstlet S x ...x betheprobability thataspecificeventis
P S 1 n
generated bythe P RI OR-S AM PL E algorithm. Justlooking atthesamplingprocess wehave
cid:25n
S x ...x Px parents X
P S 1 n i i
i1
because each sampling step depends only on the parent values. This expression should look
familiar because itisalsothe probability ofthe eventaccording tothe Bayesian nets repre-
sentation ofthejointdistribution asstatedin Equation14.2. Thatiswehave
S x ...x Px ...x .
P S 1 n 1 n
Thissimplefactmakesiteasytoanswerquestions byusingsamples.
In any sampling algorithm the answers are computed by counting the actual samples
generated. Suppose there are N total samples and let N x ...x be the number of
P S 1 n
timesthespecificeventx ...x occurs inthesetofsamples. Weexpect thisnumber asa
fraction of the total to converge in the limit to its expected value according to the sampling
probability:
N x ...x
P S 1 n
lim S x ...x Px ...x . 14.5
P S 1 n 1 n
N N
For example consider the event produced earlier: truefalsetruetrue. The sampling
probability forthiseventis
S truefalsetruetrue 0.50.90.80.9 0.324.
P S
Henceinthelimitoflarge Nweexpect32.4ofthesamplestobeofthisevent.
Wheneverweuseanapproximateequalityinwhatfollowswemeanitinexactly
this sensethat the estimated probability becomes exact in the large-sample limit. Such an
estimate is called consistent. For example one can produce a consistent estimate of the
C ON SI ST EN T
probability ofanypartially specifiedevent x ...x wherem nasfollows:
Px ...x N x ...x N . 14.6
That is the probability of the event can be estimated as the fraction of all complete events
generated by the sampling process that match the partially specified event. For example if
we generate 1000 samples from the sprinkler network and 511 of them have Raintrue
thentheestimatedprobability ofrainwrittenas
Pˆ Raintrueis0.511.
Rejectionsamplingin Bayesian networks
R EJ EC TI ON Rejectionsamplingisageneralmethodforproducingsamplesfromahard-to-sampledistri-
S AM PL IN G
bution given an easy-to-sample distribution. In its simplest form it can be used to compute
conditional probabilitiesthat istodetermine P Xe. The R EJ EC TI ON-S AM PL IN G algo-
rithmisshownin Figure14.14. Firstitgeneratessamplesfromthepriordistributionspecified
bythenetwork. Thenitrejectsallthosethatdonotmatchtheevidence. Finallytheestimate
Pˆ Xxeisobtained bycountinghowoften Xxoccursintheremaining samples.
Let Pˆ Xebetheestimateddistributionthatthealgorithmreturns.
Fromthedefinition
ofthealgorithm wehave
N Xe
Pˆ Xe α N Xe P S .
P S
N e
P S
From Equation14.6thisbecomes
P Xe
Pˆ Xe P Xe.
Pe
Thatisrejection samplingproduces aconsistent estimate ofthetrueprobability.
Continuing withourexamplefrom Figure14.12a letusassumethat wewishtoesti-
mate P Rain Sprinkler true using 100 samples. Ofthe 100 that we generate suppose
that 73 have Sprinkler false and are rejected while 27 have Sprinkler true; of the 27
8have Raintrue and19have Rainfalse. Hence
P Rain Sprinkler true N OR MA LI ZEcid:16819cid:17 cid:160.2960.704cid:17.
The true answer is cid:160.30.7cid:17. As more samples are collected the estimate will converge to
the true answer. The standard deviation of the error in each probability will be proportional
""
to1 nwherenisthenumberofsamplesusedintheestimate.
The biggest problem with rejection sampling is that it rejects so many samples! The
fraction ofsamples consistent withtheevidence edrops exponentially asthe numberofevi-
dencevariablesgrowssotheprocedure issimplyunusable forcomplexproblems.
Noticethatrejectionsamplingisverysimilartotheestimationofconditionalprobabili-
tiesdirectlyfromtherealworld. Forexampletoestimate P Rain Red Sky At Nighttrue
one can simply count how often it rains after a red sky is observed the previous evening
ignoring those evenings when the sky is not red. Here the world itself plays the role of
the sample-generation algorithm. Obviously this could take a long time if the sky is very
seldomredandthatistheweaknessofrejection sampling.
Likelihoodweighting
L IK EL IH OO D Likelihoodweightingavoidstheinefficiencyofrejectionsamplingbygeneratingonlyevents
W EI GH TI NG
that are consistent with the evidence e. It is a particular instance of the general statistical
I MP OR TA NC E techniqueofimportancesamplingtailoredforinferencein Bayesiannetworks. Webeginby
S AM PL IN G
Section14.5. Approximate Inferencein Bayesian Networks 533
function R EJ EC TI ON-S AM PL IN GXebn Nreturnsanestimateof P Xe
inputs:Xthequeryvariable
eobservedvaluesforvariables E
bna Bayesiannetwork
Nthetotalnumberofsamplestobegenerated
localvariables: Navectorofcountsforeachvalueof Xinitiallyzero
forj 1to N do
x P RI OR-S AM PL Ebn
ifxisconsistentwithethen
Nx Nx1wherex isthevalueof X inx
return N OR MA LI ZE N
Figure14.14 Therejection-samplingalgorithmforansweringqueriesgivenevidenceina
Bayesiannetwork.
describing howthealgorithmworks;thenweshowthatitworkscorrectlythat isgenerates
consistent probability estimates.
L IK EL IH OO D-W EI GH TI NG see Figure 14.15 fixes the values for the evidence vari-
ables E and samples only the nonevidence variables. This guarantees that each event gener-
ated is consistent with the evidence. Not all events are equal however. Before tallying the
countsinthedistribution forthequeryvariable eacheventisweightedbythelikelihood that
theeventaccordstotheevidence asmeasuredbytheproduct oftheconditionalprobabilities
foreach evidence variable given itsparents. Intuitively events inwhich the actual evidence
appearsunlikely shouldbegivenlessweight.
Let us apply the algorithm to the network shown in Figure 14.12a with the query
P Rain Cloudytrue Wet Grasstrue and the ordering Cloudy Sprinkler Rain Wet-
Grass. Anytopological ordering willdo. Theprocess goes asfollows: First theweight w
issetto1.0. Thenaneventisgenerated:
1. Cloudy isanevidence variablewithvaluetrue. Therefore weset
w w P Cloudytrue 0.5.
2. Sprinkler isnotanevidencevariablesosamplefrom P Sprinkler Cloudytrue
cid:160.10.9cid:17;suppose thisreturns false.
3. Similarly sample from P Rain Cloudytrue cid:160.80.2cid:17; suppose this returns
true.
4. Wet Grass isanevidence variablewithvalue true. Therefore weset
w w P Wet Grasstrue Sprinkler false Raintrue 0.45.
Here W EI GH TE D-S AM PL E returns the event truefalsetruetrue with weight 0.45 and
thisistalliedunder Raintrue.
To understand why likelihood weighting works we start by examining the sampling
probability S
W S
for W EI GH TE D-S AM PL E. Rememberthattheevidencevariables Earefixed
function L IK EL IH OO D-W EI GH TI NG Xebn Nreturnsanestimateof P Xe
inputs:Xthequeryvariable
eobservedvaluesforvariables E
bna Bayesiannetworkspecifyingjointdistribution P X 1...Xn
Nthetotalnumberofsamplestobegenerated
localvariables: Wavectorofweightedcountsforeachvalueof Xinitiallyzero
forj 1to N do
xw W EI GH TE D-S AM PL Ebne
Wx Wxw wherex isthevalueof X inx
return N OR MA LI ZE W
function W EI GH TE D-S AM PL Ebnereturnsaneventandaweight
w1;xaneventwithnelementsinitializedfrome
foreachvariable Xiin X 1...Xndo
if Xiisanevidencevariablewithvaluexi ine
thenww P Xi xi parents Xi
elsexiarandomsamplefrom P Xiparents Xi
returnxw
Figure14.15 Thelikelihood-weightingalgorithmforinferencein Bayesiannetworks. In
W EI GH TE D-S AM PL E each nonevidence variable is sampled according to the conditional
distribution given the values already sampled for the variables parents while a weight is
accumulatedbasedonthelikelihoodforeachevidencevariable.
with values e. We call the nonevidence variables Z including the query variable X. The
algorithm sampleseachvariablein Zgivenitsparentvalues:
cid:25l
S ze Pz parents Z . 14.7
W S i i
i1
Noticethat Parents Z canincludebothnonevidencevariablesandevidencevariables. Un-
i
likethepriordistribution Pzthedistribution S payssomeattentiontotheevidence: the
W S
sampled values foreach Z willbeinfluenced byevidence among Z sancestors. Forexam-
i i
plewhensampling Sprinkler thealgorithmpaysattentiontotheevidence Cloudytrue in
itsparent variable. Ontheotherhand S pays lessattention tothe evidence than does the
W S
true posterior distribution Pze because the sampled values for each Z ignore evidence
i
among Z snon-ancestors.5 Forexamplewhensampling Sprinkler and Rain thealgorithm
i
ignorestheevidenceinthechildvariable Wet Grasstrue;thismeansitwillgeneratemany
samples with Sprinkler false and Rainfalse despite the fact that the evidence actually
rulesoutthiscase.
into account. This cannot be done efficiently however. If it could then we could approximate the desired
probabilitytoarbitraryaccuracywithapolynomialnumberofsamples.Itcanbeshownthatnosuchpolynomial-
timeapproximationschemecanexist.
Section14.5. Approximate Inferencein Bayesian Networks 535
The likelihood weight w makes up for the difference between the actual and desired
sampling distributions. The weight for a given sample x composed from z and e is the
product of the likelihoods for each evidence variable given its parents some orall of which
maybeamongthe Z s:
i
cid:25m
wze Pe parents E . 14.8
i i
i1
Multiplying Equations14.7and14.8weseethattheweightedprobabilityofasamplehas
theparticularly convenient form
cid:25l cid:25m
S zewze Pz parents Z Pe parents E
W S i i i i
i1 i1
Pze 14.9
because the two products cover all the variables in the network allowing us to use Equa-
tion14.2forthejointprobability.
Now it is easy to show that likelihood weighting estimates are consistent. For any
particularvalue xof Xtheestimatedposteriorprobability canbecalculated asfollows:
cid:12
Pˆxe α N W Sxyewxye from L IK EL IH OO D-W EI GH TI NG
y
cid:12
αcid:2 S xyewxye forlarge N
W S
y
cid:12
cid:2
α Pxye by Equation14.9
y
αcid:2 Pxe Pxe.
Hencelikelihood weighting returnsconsistent estimates.
Because likelihood weighting uses all the samples generated it can be much more ef-
ficient than rejection sampling. It will however suffer a degradation in performance as the
number of evidence variables increases. This is because most samples will have very low
weights and hence the weighted estimate will be dominated by the tiny fraction of samples
thataccordmorethananinfinitesimallikelihoodtotheevidence. Theproblemisexacerbated
if the evidence variables occur late in the variable ordering because then the nonevidence
variableswillhavenoevidenceintheirparentsandancestorstoguidethegeneration ofsam-
ples. This means the samples will be simulations that bear little resemblance to the reality
suggested bytheevidence.
M AR KO VC HA IN Markovchain Monte Carlo M CM Calgorithmsworkquitedifferentlyfromrejectionsam-
M ON TE CA RL O
pling and likelihood weighting. Instead of generating each sample from scratch M CM Cal-
gorithms generate each sample by making a random change to the preceding sample. It is
thereforehelpfultothinkofan M CM Calgorithm asbeinginaparticular currentstatespeci-
fyingavalueforeveryvariableandgeneratinga nextstatebymakingrandomchangestothe
currentstate. Ifthisremindsyouofsimulatedannealingfrom Chapter4or W AL KS AT from
Chapter7 thatisbecause both aremembersofthe M CM Cfamily. Herewedescribe apar-
ticularform of M CM Ccalled Gibbssampling whichisespecially wellsuited for Bayesian
G IB BS SA MP LI NG
networks. Otherformssomeofthemsignificantlymorepowerfularediscussedinthenotes
attheendofthechapter. Wewillfirstdescribewhatthealgorithmdoesthenwewillexplain
whyitworks.
Gibbssamplingin Bayesian networks
The Gibbs sampling algorithm for Bayesian networks starts with an arbitrary state with the
evidence variables fixed at their observed values and generates a next state by randomly
sampling a value for one of the nonevidence variables X . The sampling for X is done
i i
conditioned onthecurrent valuesofthevariables inthe Markovblanketof X . Recallfrom
i
page517thatthe Markovblanketofavariableconsistsofitsparentschildrenandchildrens
parents. The algorithm therefore wanders randomly around the state spacethe space of
possible complete assignmentsflipping one variable at a time but keeping the evidence
variables fixed.
Consider the query P Rain Sprinkler true Wet Grasstrue applied to the net-
work in Figure 14.12a. Theevidence variables Sprinkler and Wet Grass are fixedtotheir
observed valuesand thenonevidence variables Cloudy and Rain areinitialized randomly
let us say to true and false respectively. Thus the initial state is truetruefalsetrue.
Nowthenonevidence variables aresampledrepeatedly inanarbitrary order. Forexample:
1. Cloudy is sampled given the current values of its Markov blanket variables: in this
case we sample from P Cloudy Sprinkler true Rainfalse. Shortly we will
show how to calculate this distribution. Suppose the result is Cloudyfalse. Then
thenewcurrentstateisfalsetruefalsetrue.
2. Rain issampled giventhecurrent valuesofits Markovblanket variables: inthiscase
wesamplefrom P Rain Cloudyfalse Sprinkler true Wet Grasstrue. Sup-
posethisyields Raintrue. Thenewcurrentstateis falsetruetruetrue.
Eachstatevisitedduringthisprocessisasamplethatcontributestotheestimateforthequery
variable Rain. Iftheprocess visits20stateswhere Rain istrueand60states where Rain is
false then the answer to the query is N OR MA LI ZEcid:162060cid:17 cid:160.250.75cid:17. The complete
algorithm isshownin Figure14.16.
Why Gibbssamplingworks
We will now show that Gibbs sampling returns consistent estimates for posterior probabil-
ities. The material in this section is quite technical but the basic claim is straightforward:
the sampling process settles into adynamic equilibrium in which the long-run fraction of
time spent in each state is exactly proportional to its posterior probability. This remarkable
T RA NS IT IO N propertyfollowsfromthespecifictransitionprobabilitywithwhichtheprocessmovesfrom
P RO BA BI LI TY
one state to another as defined by the conditional distribution given the Markov blanket of
thevariablebeingsampled.
Section14.5. Approximate Inferencein Bayesian Networks 537
function G IB BS-A SK Xebn Nreturnsanestimateof P Xe
localvariables: Navectorofcountsforeachvalueof Xinitiallyzero
Zthenonevidencevariablesinbn
xthecurrentstateofthenetworkinitiallycopiedfrome
initializexwithrandomvaluesforthevariablesin Z
forj 1to N do
foreach Ziin Zdo
setthevalueof Ziinxbysamplingfrom P Zimb Zi
Nx Nx1wherex isthevalueof X inx
return N OR MA LI ZE N
Figure14.16 The Gibbssamplingalgorithmforapproximateinferencein Bayesian net-
works;thisversioncyclesthroughthevariablesbutchoosingvariablesatrandomalsoworks.
Let qx xcid:2 be the probability that the process makes a transition from state x to
cid:2
statex. Thistransitionprobability defineswhatiscalleda Markovchainonthestatespace.
M AR KO VC HA IN
Markov chains also figure prominently in Chapters 15 and 17. Now suppose that we run
the Markov chain for t steps and let π x be the probability that the system is in state x at
t
cid:2 cid:2
time t. Similarly let π x be the probability of being in state x at time t 1. Given
t1
cid:2
π x wecan calculate π xby summing forallstates the system could be inat time t
t t1
cid:2
theprobability ofbeinginthatstatetimestheprobability ofmakingthetransition tox:
cid:12
π xcid:2 π xqx xcid:2 .
t1 t
x
S TA TI ON AR Y We say that the chain has reached its stationary distribution if π π . Let us call this
D IS TR IB UT IO N t t1
stationary distribution π;itsdefiningequation istherefore
cid:12
πxcid:2 πxqx xcid:2 forallxcid:2 . 14.10
x
Provided the transition probability distribution q isergodicthat is every state isreachable
E RG OD IC
fromeveryotherandtherearenostrictlyperiodiccyclesthere isexactlyonedistribution π
satisfying thisequation foranygivenq.
Equation14.10canbereadassayingthattheexpectedoutflowfromeachstatei.e.
its current population is equal to the expected inflow from all the states. One obvious
way to satisfy this relationship is if the expected flowbetween any pair of states is the same
inbothdirections; thatis
πxqx xcid:2 πxcid:2 qxcid:2 x forallx xcid:2 . 14.11
Whentheseequations holdwesaythatqx xcid:2 isindetailedbalancewithπx.
D ET AI LE DB AL AN CE
We can show that detailed balance implies stationarity simply by summing over x in
Equation14.11. Wehave
cid:12 cid:12 cid:12
πxqx xcid:2 πxcid:2 qxcid:2 x πxcid:2 qxcid:2 x πxcid:2
x x x
cid:2
wherethelaststepfollowsbecauseatransition from x isguaranteed tooccur.
The transition probability qx xcid:2 defined by the sampling step in G IB BS-A SK is
actually aspecial case ofthemoregeneral definition of Gibbs sampling according towhich
each variable is sampled conditionally on the current values of all the other variables. We
start by showing that this general definition of Gibbs sampling satisfies the detailed balance
equation with a stationary distribution equal to Pxe the true posterior distribution on
the nonevidence variables. Then wesimply observe that for Bayesian networks sampling
conditionallyonallvariablesisequivalenttosamplingconditionallyonthevariables Markov
blanketseepage517.
Toanalyze thegeneral Gibbssampler whichsamples each X inturnwithatransition
i
probability q that conditions on all the other variables we define X to be these other vari-
i i
ables except the evidence variables; their values in the current state are x . If we sample a
i
cid:2
newvaluex for X conditionally onalltheothervariables including theevidence wehave
i i
q x xcid:2 q x x xcid:2 x Pxcid:2x e.
i i i i i i i i
Nowweshow thatthetransition probability foreachstepofthe Gibbssamplerisindetailed
balancewiththetrueposterior:
πxq x xcid:2 Pxe Pxcid:2x e Px x e Pxcid:2x e
i i i i i i i
Px x e Px e Pxcid:2x e usingthechainruleonthefirstterm
i i i i i
Px x e Pxcid:2 x e usingthechainrulebackward
i i i i
πxcid:2 q xcid:2 x.
i
Wecanthinkoftheloopforeach Z in Zdoin Figure14.16asdefiningonelargetransition
i
probability qthatisthesequentialcompositionq q q ofthetransitionprobabilities
for the individual variables. It is easy to show Exercise 14.19 that if each of q and q has
i j
π as its stationary distribution then the sequential composition q q does too; hence the
i j
transition probability q for the whole loop has Pxeas its stationary distribution. Finally
unless the C PTscontain probabilities of 0 or 1which can cause the state space to become
disconnectedit is easy to see that q is ergodic. Hence the samples generated by Gibbs
samplingwilleventually bedrawnfromthetrueposteriordistribution.
The final step is to show how to perform the general Gibbs sampling stepsampling
X from P X x ein a Bayesian network. Recall from page 517 that avariable isinde-
i i i
pendent ofallothervariables givenits Markovblanket; hence
Pxcid:2x
e
Pxcid:2mb X
""
i i i i
where mb X denotes the values of the variables in X s Markov blanket M BX . As
i i i
shownin Exercise14.7theprobabilityofavariablegivenits Markovblanketisproportional
totheprobability ofthevariablegivenitsparentstimestheprobabilityofeachchildgivenits
respective parents:
cid:25
Pxcid:2mb X α Pxcid:2parents X Py parents Y . 14.12
i i i i j j
Yj Children Xi
Hencetoflipeachvariable X conditioned onits Markovblanket thenumberofmultiplica-
i
tionsrequiredisequaltothenumberof X schildren.
i
Section14.6. Relational and First-Order Probability Models 539
Quality B Quality B
Honesty C Kindness C Honesty C Kindness C
Quality B 1 1 2 2
1
Honesty C Kindness C
Recommendation C B Recommendation C B
Recommendation C B Recommendation C B Recommendation C B
a b
Honest C is Booleanwhiletheothervariableshaveintegervaluesfrom1to5. b Bayes
1
netwithtwocustomersandtwobooks.
In Chapter 8 we explained the representational advantages possessed by first-order logic in
comparison to propositional logic. First-order logic commits to the existence of objects and
relationsamongthemandcanexpressfactsaboutsomeoralloftheobjectsinadomain. This
oftenresultsinrepresentations thatarevastlymoreconcise thantheequivalent propositional
descriptions. Now Bayesian networks are essentially propositional: the set of random vari-
ables is fixed and finite and each has a fixed domain of possible values. This fact limits the
applicability of Bayesian networks. If wecan find a wayto combine probability theory with
theexpressive poweroffirst-order representations weexpecttobeabletoincreasedramati-
callytherangeofproblemsthatcanbehandled.
Forexample suppose that an online book retailer would like to provide overall evalu-
ations of products based on recommendations received from its customers. The evaluation
will take the form of a posterior distribution over the quality of the book given the avail-
able evidence. Thesimplest solution tobase theevaluation ontheaverage recommendation
perhaps withavariance determinedbythenumberofrecommendations butthisfailstotake
intoaccountthefactthatsomecustomersarekinderthanothersandsomearelesshonestthan
others. Kind customers tend to give high recommendations even to fairly mediocre books
while dishonest customers give very high or very low recommendations for reasons other
thanqualityforexample theymightworkforapublisher.6
For a single customer C recommending a single book B the Bayes net might look
like the one shown in Figure 14.17a. Just as in Section 9.1 expressions with parentheses
suchas Honest C arejustfancysymbolsinthiscasefancynamesforrandomvariables.
1
bookfromacompetitor.See Chapter17.
Withtwocustomers andtwobooks the Bayesnetlooks like the onein Figure14.17b. For
larger numbers of books and customers it becomes completely impractical to specify the
networkbyhand.
Fortunately the network has a lot of repeated structure. Each Recommendationcb
variablehasasitsparentsthevariables Honestc Kindnesscand Qualityb. Moreover
the C PTs for all the Recommendationcb variables are identical as are those for all the
Honestc variables and so on. The situation seems tailor-made for a first-order language.
Wewouldliketosaysomething like
Recommendationcb Rec C PT Honestc Kindnessc Qualityb
with the intended meaning that a customers recommendation for a book depends on the
customers honesty and kindness and the books quality according to some fixed C PT. This
sectiondevelops alanguage thatletsussayexactlythisandalotmorebesides.
Recall from Chapter 13 that a probability model defines a set Ω of possible worlds with
a probability Pω for each world ω. For Bayesian networks the possible worlds are as-
signments of values to variables; for the Boolean case in particular the possible worlds are
identical to those of propositional logic. For a first-order probability model then it seems
we need the possible worlds to be those of first-order logicthat is a set of objects with
relations among them and aninterpretation that maps constant symbols toobjects predicate
symbols to relations and function symbols to functions on those objects. See Section 8.2.
Themodelalsoneedstodefineaprobability foreachsuchpossibleworldjustasa Bayesian
networkdefinesaprobability foreachassignment ofvaluestovariables.
Letus suppose fora moment that wehave figured out how to do this. Then as usual
see page 485 we can obtain the probability of any first-order logical sentence φ as a sum
overthepossible worldswhereitistrue:
cid:12
Pφ Pω. 14.13
ω:φistrueinω
Conditional probabilities Pφecanbe obtained similarly sowecan inprinciple ask any
question we want of our modele.g. Which books are most likely to be recommended
highlybydishonest customers?and getananswer. Sofarsogood.
There is however a problem: the set of first-order models is infinite. We saw this
explicitlyin Figure8.4onpage293whichweshowagainin Figure14.18top. Thismeans
that1thesummationin Equation14.13couldbeinfeasibleand2specifyingacomplete
consistent distribution overaninfinitesetofworldscould beverydifficult.
Section 14.6.2 explores one approach to dealing with this problem. The idea is to
borrow not from the standard semantics of first-order logic but from the database seman-
tics defined in Section 8.2.8 page 299. The database semantics makes the unique names
assumptionhere weadoptitfortheconstant symbols. Italsoassumes domainclosure
there are no more objects than those that are named. We can then guarantee a finite set of
possible worlds by making the set of objects in each world be exactly the set of constant
Section14.6. Relational and First-Order Probability Models 541
R J R J R J R J R J R J
. . . . . .
. . .
R J R J R J R J R J
R R R R . . . R
J J J J J
Figure14.18 Top:Somemembersofthesetofallpossibleworldsforalanguagewithtwo
constantsymbols Rand Jandonebinaryrelationsymbolunderthestandardsemanticsfor
first-orderlogic. Bottom: thepossibleworldsunderdatabasesemantics. Theinterpretation
oftheconstantsymbolsisfixedandthereisadistinctobjectforeachconstantsymbol.
symbols that are used; as shown in Figure 14.18 bottom there is no uncertainty about the
mappingfromsymbolstoobjectsorabouttheobjectsthatexist. Wewillcallmodelsdefined
R EL AT IO NA L in this way relational probability models or R PMs.7 The most significant difference be-
P RO BA BI LI TY MO DE L
tween the semantics of R PMsand the database semantics introduced in Section 8.2.8 is that
R PMsdo not make the closed-world assumptionobviously assuming that every unknown
factisfalsedoesnt makesenseinaprobabilistic reasoning system!
Whentheunderlyingassumptionsofdatabasesemanticsfailtohold R PMswontwork
well. Forexampleabookretailermightusean I SB NInternational Standard Book Number
as a constant symbol to name each book even though a given logical book e.g. Gone
With the Wind may have several I SB Ns. It would make sense to aggregate recommenda-
tions across multiple I SB Ns but the retailer may not know for sure which I SB Ns are really
thesamebook. Notethatwearenotreifying theindividual copiesofthebookwhichmight
be necessary for used-book sales car sales and so on. Worse still each customer is iden-
tified by a login I D but a dishonest customer may have thousands of I Ds! In the computer
securityfieldthesemultiple I Dsarecalledsibylsandtheirusetoconfound areputation sys-
S IB YL
tem is called a sibyl attack. Thus even a simple application in a relatively well-defined
S IB YL AT TA CK
E XI ST EN CE online domain involves both existence uncertainty what are the real books and customers
U NC ER TA IN TY
I DE NT IT Y underlying the observed data and identity uncertainty which symbol really refer to the
U NC ER TA IN TY
sameobject. Weneedtobitethebullet anddefineprobability modelsbasedonthestandard
semantics of first-order logic for which the possible worlds vary in the objects they contain
andinthemappings fromsymbolstoobjects. Section14.6.3showshowtodothis.
theunderlyingideasarethesame.
Like first-order logic R PMshave constant function and predicate symbols. It turns out to
be easier to view predicates as functions that return true or false. We will also assume a
typesignatureforeachfunctionthatisaspecificationofthetypeofeachargumentandthe
T YP ES IG NA TU RE
functionsvalue. Ifthetypeofeachobjectisknownmanyspuriouspossibleworldsareelim-
inated by this mechanism. For the book-recommendation domain the types are Customer
and Bookandthetypesignatures forthefunctions andpredicates areasfollows:
Honest : Customer truefalse Kindness : Customer 12345
Quality :Book 12345
Recommendation : Customer Book 12345
Theconstantsymbolswillbewhatevercustomerandbooknamesappearintheretailersdata
set. Intheexamplegivenearlier Figure14.17b thesewere C C and B B .
Given the constants and their types together with the functions and their type signa-
turestherandomvariablesofthe R PMareobtainedbyinstantiating eachfunction witheach
possible combination of objects: Honest C Quality B Recommendation C B
and so on. These are exactly the variables appearing in Figure 14.17b. Because each type
hasonlyfinitelymanyinstances thenumberofbasicrandomvariables isalsofinite.
To complete the R PM we have to write the dependencies that govern these random
variables. Thereisonedependency statement foreachfunction whereeachargument ofthe
function isalogicalvariable i.e.avariablethatranges overobjects asinfirst-orderlogic:
Honestc cid:160.990.01cid:17
Kindnessc cid:160.10.10.20.30.3cid:17
Qualityb cid:160.050.20.40.20.15cid:17
Recommendationcb Rec C PT Honestc Kindnessc Qualityb
where Rec C PT is a separately defined conditional distribution with 25550 rows
each with 5 entries. The semantics of the R PM can be obtained by instantiating these de-
pendencies for all known constants giving a Bayesian network as in Figure 14.17b that
definesajointdistribution overthe R PMsrandomvariables.8
C ON TE XT-S PE CI FI C Wecanrefinethemodelbyintroducing a context-specific independencetoreflectthe
I ND EP EN DE NC E
factthatdishonestcustomersignorequalitywhengivingarecommendation; moreoverkind-
nessplaysnoroleintheirdecisions. Acontext-specific independence allowsavariable tobe
independentofsomeofitsparentsgivencertainvaluesofothers;thus Recommendationcb
isindependent of Kindnesscand Qualitybwhen Honestcfalse:
Recommendationcb if Honestc then
Honest Rec C PT Kindnessc Qualityb
elsecid:160.40.10.00.10.4cid:17 .
thedependenciesmustbeacyclicotherwisetheresulting Bayesiannetworkwillhavecyclesandwillnotdefine
aproperdistribution. Secondthedependenciesmustbe well-foundedthatistherecanbenoinfiniteancestor
chainssuchasmightarisefromrecursivedependencies. Undersomecircumstancessee Exercise14.6afixed-
pointcalculationyieldsawell-definedprobabilitymodelforarecursive R PM.
Section14.6. Relational and First-Order Probability Models 543
Fan C A Fan C A Author B
Quality B Honesty C Kindness C Quality B
Recommendation C B Recommendation C B
Figure14.19 Fragmentoftheequivalent Bayesnetwhen Author B isunknown.
2
Thiskindofdependencymaylooklikeanordinaryifthenelsestatementonaprogramming
language but there is a key difference: the inference engine doesnt necessarily know the
valueoftheconditional test!
We can elaborate this model in endless ways to make it more realistic. For example
suppose that an honest customer who is a fan of a books author always gives the book a 5
regardlessofquality:
Recommendationcb if Honestcthen
if Fanc Authorbthen Exactly5
else Honest Rec C PT Kindnessc Qualityb
elsecid:160.40.10.00.10.4cid:17
Againtheconditional test Fanc Authorbisunknownbutifacustomergivesonly5sto
aparticularauthorsbooksandisnototherwiseespecially kindthentheposteriorprobability
that the customer is afan of that author willbe high. Furthermore the posterior distribution
willtendtodiscount thecustomers5sinevaluating thequalityofthatauthorsbooks.
Inthepreceding exampleweimplicitlyassumedthatthevalueof Authorbisknown
forevery bbut this maynotbethe case. Howcanthesystem reason about whether say C
1
is a fan of Author B when Author B is unknown? The answer is that the system may
havetoreason about allpossible authors. Suppose tokeepthings simplethattherearejust
two authors A and A . Then Author B is a random variable with two possible values
A and A anditisaparentof Recommendation C B . Thevariables Fan C A and
Fan C A are parents too. Theconditional distribution for Recommendation C B is
then essentially a multiplexer in which the Author B parent acts as a selector to choose
M UL TI PL EX ER 2
which of Fan C A and Fan C A actually gets to influence the recommendation. A
fragment of the equivalent Bayes net is shown in Figure 14.19. Uncertainty in the value
of Author B which affects the dependency structure of the network is an instance of
2
R EL AT IO NA L relational uncertainty.
U NC ER TA IN TY
In case you are wondering how the system can possibly work out who the author of
B is: consider the possibility that three other customers are fans of A and have no other
favorite authors in common and all three have given B a 5 even though most other cus-
2
tomers find it quite dismal. In that case it is extremely likely that A is the author of B .
The emergence of sophisticated reasoning like this from an R PM model of just a few lines
isanintriguing exampleofhow probabilistic influences spread through thewebofintercon-
nectionsamongobjectsinthemodel. Asmoredependencies andmoreobjectsareaddedthe
pictureconveyed bytheposteriordistribution oftenbecomesclearerandclearer.
The next question is how to do inference in R PMs. One approach is to collect the
evidence and query and the constant symbols therein construct the equivalent Bayes net
and apply any of the inference methods discussed in this chapter. This technique is called
unrolling. Theobvious drawbackisthattheresulting Bayesnetmaybeverylarge. Further-
U NR OL LI NG
moreiftherearemanycandidate objectsforanunknownrelationorfunctionfor example
theunknownauthorof B thensomevariables inthenetworkmayhavemanyparents.
2
Fortunately much can be done to improve on generic inference algorithms. First the
presence of repeated substructure in the unrolled Bayes net means that many of the factors
constructed during variable elimination and similar kinds of tables constructed by cluster-
ing algorithms will be identical; effective caching schemes have yielded speedups of three
ordersofmagnitudeforlargenetworks. Secondinferencemethodsdevelopedtotakeadvan-
tage ofcontext-specific independence in Bayesnets findmanyapplications in R PMs. Third
M CM C inference algorithms have some interesting properties when applied to R PMs with
relational uncertainty. M CM Cworksbysamplingcomplete possible worldssoineachstate
therelational structure iscompletely known. Intheexamplegivenearlier each M CM Cstate
wouldspecifythevalueof Author B andsotheotherpotential authorsarenolongerpar-
2
entsoftherecommendationnodesfor B . For M CM Cthenrelationaluncertainty causesno
2
increaseinnetworkcomplexity; insteadthe M CM Cprocessincludestransitions thatchange
therelational structure andhencethedependency structure oftheunrolled network.
Allofthemethodsjustdescribedassumethatthe R PMhastobepartiallyorcompletely
unrolled into a Bayesian network. This is exactly analogous to the method of proposition-
alization for first-order logical inference. See page 322. Resolution theorem-provers and
logic programming systems avoid propositionalizing by instantiating the logical variables
onlyasneededtomaketheinferencegothrough;thatistheylifttheinferenceprocessabove
the level of ground propositional sentences and make each lifted step do the work of many
ground steps. Thesame idea applied inprobabilistic inference. Forexample inthe variable
elimination algorithm alifted factorcanrepresent anentire setofground factors that assign
probabilitiestorandomvariablesinthe R PMwherethoserandomvariablesdifferonlyinthe
constant symbols usedtoconstruct them. Thedetailsofthis methodarebeyond thescopeof
thisbookbutreferences aregivenattheendofthechapter.
We argued earlier that database semantics was appropriate for situations in which we know
exactlythesetofrelevantobjectsthatexistandcanidentifythemunambiguously. Inpartic-
ular all observations about an object are correctly associated with the constant symbol that
namesit. Inmanyreal-worldsettingshowevertheseassumptionsaresimplyuntenable. We
gavetheexamplesofmultiple I SB Nsandsibyl attacks inthebook-recommendation domain
towhichwewillreturninamomentbutthephenomenon isfarmorepervasive:
Section14.6. Relational and First-Order Probability Models 545
Avisionsystemdoesntknowwhatexistsifanythingaroundthenextcornerandmay
notknowiftheobjectitseesnowisthesameoneitsawafewminutesago.
Atext-understanding systemdoesnotknowinadvancetheentitiesthatwillbefeatured
in a text and must reason about whether phrases such as Mary Dr. Smith she
hiscardiologist hismotherandsoonrefertothesame object.
Anintelligence analyst hunting forspies never knows how many spies there really are
andcanonlyguesswhethervariouspseudonyms phonenumbersandsightingsbelong
tothesameindividual.
In fact a major part of human cognition seems to require learning what objects exist and
beingabletoconnectobservationswhich almostnevercomewithunique I Dsattachedto
hypothesized objectsintheworld.
For these reasons we need to be able to write so-called open-universe probability
O PE NU NI VE RS E
models or O UP Ms based on the standard semantics of first-order logic as illustrated at the
top of Figure 14.18. A language for O UP Ms provides a way of writing such models easily
while guaranteeing a unique consistent probability distribution over the infinite space of
possible worlds.
The basic idea is to understand how ordinary Bayesian networks and R PMs manage
to define a unique probability model and to transfer that insight to the first-order setting. In
essence a Bayes net generates each possible world event by event in the topological order
defined bythenetwork structure whereeach eventisanassignment ofavaluetoavariable.
An R PM extends this to entire sets of events defined by the possible instantiations of the
logical variables inagiven predicate orfunction. O UP Msgo further byallowing generative
steps that add objects to the possible world under construction where the number and type
of objects may depend on the objects that are already in that world. That is the event being
generated isnottheassignment ofavaluetoavariable buttheveryexistence ofobjects.
Onewaytodothisin O UP Msistoaddstatements thatdefineconditional distributions
over the numbers of objects of various kinds. For example in the book-recommendation
domain we might want to distinguish between customers real people and their login I Ds.
Supposeweexpectsomewherebetween100and10000distinctcustomerswhomwecannot
observedirectly. Wecanexpressthisasapriorlog-normal distribution9 asfollows:
Customer Log Normal6.92.32.
We expect honest customers to have just one I D whereas dishonest customers might have
anywherebetween10and1000 IDs:
Login I DOwner c if Honestcthen Exactly1
else Log Normal6.92.32.
This statement defines the number of login I Ds for a given owner who is a customer. The
Owner function is called an origin function because it says where each generated object
O RI GI NF UN CT IO N
camefrom. Intheformal semantics of B LO G asdistinct from first-order logic thedomain
elementsineachpossibleworldareactuallygenerationhistoriese.g.thefourthlogin I Dof
theseventhcustomerratherthansimpletokens.
e
Subject to technical conditions of acyclicity and well-foundedness similar to those for
R PMs open-universe models of this kind define a unique distribution over possible worlds.
Furthermore there exist inference algorithms such that for every such well-defined model
and every first-order query the answer returned approaches the true posterior arbitrarily
closely in the limit. There are some tricky issues involved in designing these algorithms.
For example an M CM C algorithm cannot sample directly in the space of possible worlds
when the size of those worlds is unbounded; instead it samples finite partial worlds rely-
ing on the fact that only finitely many objects can be relevant to the query in distinct ways.
Moreover transitions must allow formerging two objects into one orsplitting one into two.
Details are given in the references at the end of the chapter. Despite these complications
thebasicprincipleestablished in Equation14.13stillholds: theprobability ofanysentence
iswelldefinedandcanbecalculated.
Researchinthisareaisstillatanearlystage butalready itisbecoming clearthatfirst-
orderprobabilistic reasoning yieldsatremendous increase intheeffectiveness of A Isystems
at handling uncertain information. Potential applications include those mentioned above
computer vision text understanding and intelligence analysisas well asmanyother kinds
ofsensorinterpretation.
Other sciences e.g. physics genetics and economics have long favored probability as a
model foruncertainty. In 1819 Pierre Laplace said Probability theory is nothing but com-
mon sense reduced to calculation. In 1850 James Maxwell said The true logic for this
world isthe calculus of Probabilities which takes account ofthe magnitude ofthe probabil-
itywhichisoroughttobeinareasonable mansmind.
Given this long tradition it is perhaps surprising that A I has considered many alterna-
tives to probability. The earliest expert systems of the 1970s ignored uncertainty and used
strictlogicalreasoningbutitsoonbecameclearthatthis wasimpracticalformostreal-world
domains. The next generation ofexpert systems especially in medical domains used prob-
abilistic techniques. Initial results were promising but they did not scale up because of the
exponentialnumberofprobabilities requiredinthefulljointdistribution. Efficient Bayesian
network algorithms were unknown then. As a result probabilistic approaches fell out of
favor from roughly 1975 to1988 and avariety ofalternatives toprobability weretried fora
varietyofreasons:
One common view is that probability theory is essentially numerical whereas human
judgmental reasoning is more qualitative. Certainly we are not consciously aware
of doing numerical calculations of degrees of belief. Neither are we aware of doing
unification yet we seem to be capable of some kind of logical reasoning. It might be
that we have some kind of numerical degrees of belief encoded directly in strengths
of connections and activations in our neurons. In that case the difficulty of conscious
accesstothosestrengthsisnotsurprising. Oneshouldalso notethatqualitativereason-
Section14.7. Other Approaches to Uncertain Reasoning 547
ingmechanisms canbebuiltdirectly ontopofprobability theory sothenonumbers
argument against probability has little force. Nonetheless some qualitative schemes
have a good deal of appeal in their own right. One of the best studied is default rea-
soningwhichtreats conclusions notasbelieved toacertaindegree butasbelieved
until a better reason is found to believe something else. Default reasoning is covered
in Chapter12.
Rule-based approaches to uncertainty have also been tried. Such approaches hope to
build on the success of logical rule-based systems but add a sort of fudge factor to
eachruletoaccommodateuncertainty. Thesemethodsweredevelopedinthemid-1970s
andformedthebasisforalargenumberofexpertsystemsinmedicineandotherareas.
One area that we have not addressed so far is the question of ignorance as opposed
to uncertainty. Consider the flipping of a coin. If we know that the coin is fair then
a probability of 0.5 for heads is reasonable. If we know that the coin is biased but
we do not know which way then 0.5 for heads is again reasonable. Obviously the
twocasesaredifferent yettheoutcomeprobability seemsnottodistinguish them. The
Dempster Shafertheoryusesinterval-valueddegreesofbelieftorepresentanagents
knowledgeoftheprobability ofaproposition.
Probabilitymakesthesameontologicalcommitmentaslogic: thatpropositions aretrue
orfalseintheworld eveniftheagentisuncertain astowhichisthecase. Researchers
infuzzylogichaveproposedanontologythatallowsvagueness: thataproposition can
besortoftrue. Vaguenessanduncertainty areinfactorthogonal issues.
Thenextthreesubsectionstreatsomeoftheseapproachesinslightlymoredepth. Wewillnot
providedetailed technical materialbutwecitereferences forfurtherstudy.
Rule-based systems emerged from early work on practical and intuitive systems for logical
inference. Logicalsystemsingeneralandlogicalrule-basedsystemsinparticularhavethree
desirable properties:
Locality: In logical systems whenever we have a rule of the form A B we can
L OC AL IT Y
conclude Bgivenevidence Awithoutworryingaboutanyotherrules. Inprobabilistic
systemsweneedtoconsider alltheevidence.
Detachment: Oncealogicalproofisfoundforaproposition Btheproposition canbe
D ET AC HM EN T
usedregardlessofhowitwasderived. Thatisitcanbe detachedfromitsjustification.
Indealing with probabilities on the other hand the source of the evidence fora belief
isimportantforsubsequent reasoning.
T RU TH- Truth-functionality: In logic the truth of complex sentences can be computed from
F UN CT IO NA LI TY
the truth of the components. Probability combination does not work this way except
understrongglobalindependence assumptions.
There have been several attempts to devise uncertain reasoning schemes that retain these
advantages. The idea is to attach degrees of belief to propositions and rules and to devise
purely local schemes for combining and propagating those degrees of belief. The schemes
arealsotruth-functional; forexamplethedegreeofbeliefin A B isafunctionofthebelief
in Aandthebeliefin B.
Thebadnewsforrule-based systems isthattheproperties of locality detachment and
truth-functionality are simply not appropriate for uncertain reasoning. Let us look at truth-
functionality first. Let H betheeventthatafaircoinflipcomesupheadslet T betheevent
that the coin comes up tails on that same flip and let H be the event that the coin comes
2
up heads on a second flip. Clearly all three events have the same probability 0.5 and so a
truth-functional system must assign the same belief to the disjunction of any two of them.
But we can see that the probability of the disjunction depends on the events themselves and
notjustontheirprobabilities:
P A P B P AB
P H 0.5 P H H 0.50
P H 0.5 P T 0.5 P H T 1.00
P H 0.5 P H H 0.75
It gets worse when we chain evidence together. Truth-functional systems have rules of the
form A B that allow us to compute the belief in B as a function of the belief in the rule
andthebeliefin A. Bothforward-andbackward-chaining systemscanbedevised. Thebelief
intheruleisassumedtobeconstantandisusuallyspecifiedbytheknowledgeengineerfor
exampleas A B.
0.9
Consider the wet-grass situation from Figure 14.12a page 529. If we wanted to be
abletodobothcausalanddiagnostic reasoning wewouldneedthetworules
Rain Wet Grass and Wet Grass Rain .
Thesetworulesform afeedback loop: evidence for Rain increases thebelief in Wet Grass
which in turn increases the belief in Rain even more. Clearly uncertain reasoning systems
needtokeeptrackofthepathsalongwhichevidence ispropagated.
Intercausal reasoning orexplaining awayisalsotricky. Considerwhathappens when
wehavethetworules
Sprinkler Wet Grass and Wet Grass Rain .
Supposeweseethatthesprinklerison. Chainingforwardthroughourrulesthisincreasesthe
belief that the grass willbe wet which in turn increases the belief that it is raining. But this
is ridiculous: the fact that the sprinkler is on explains awaythe wetgrass and should reduce
thebeliefinrain. Atruth-functional systemactsasifitalsobelieves Sprinkler Rain.
Given these difficulties how can truth-functional systems be made useful in practice?
The answer lies in restricting the task and in carefully engineering the rule base so that un-
desirable interactions do not occur. The most famous example of a truth-functional system
C ER TA IN TY FA CT OR
foruncertain reasoning isthe certaintyfactors modelwhichwasdeveloped forthe M YC IN
medical diagnosis program and was widely used in expert systems of the late 1970s and
1980s. Almostallusesofcertainty factorsinvolved rulesetsthatwereeitherpurelydiagnos-
tic as in M YC IN or purely causal. Furthermore evidence was entered only at the roots
oftheruleset andmostrulesetsweresingly connected. Heckerman1986 hasshownthat
Section14.7. Other Approaches to Uncertain Reasoning 549
underthesecircumstances aminorvariationoncertainty-factor inferencewasexactlyequiv-
alentto Bayesianinferenceonpolytrees. Inothercircumstances certaintyfactorscouldyield
disastrously incorrect degrees of belief through overcounting of evidence. As rule sets be-
camelarger undesirable interactions betweenrulesbecamemorecommonandpractitioners
foundthatthecertaintyfactorsofmanyotherruleshadtobe tweakedwhennewruleswere
added. Forthesereasons Bayesiannetworkshavelargelysupplanted rule-based methodsfor
uncertain reasoning.
D EM PS TE RS HA FE R The Dempster Shafer theory is designed to deal with the distinction between uncertainty
T HE OR Y
and ignorance. Rather than computing the probability of a proposition it computes the
probability that the evidence supports the proposition. This measure of belief is called a
belieffunctionwritten Bel X.
B EL IE FF UN CT IO N
We return to coin flipping for an example of belief functions. Suppose you pick a
coin from a magicians pocket. Given that the coin might or might not be fair what belief
should you ascribe to the event that it comes up heads? Dempster Shafer theory says that
because you have no evidence either way you have to say that the belief Bel Heads 0
and also that Bel Heads 0. This makes Dempster Shafer reasoning systems skeptical
in a way that has some intuitive appeal. Now suppose you have an expert at your disposal
who testifies with 90 certainty that the coin is fair i.e. he is 90 sure that P Heads
0.5. Then Dempster Shafer theory gives Bel Heads 0.9 0.5 0.45 and likewise
Bel Heads 0.45. Thereisstilla10percentage pointgapthatisnotaccounted forby
theevidence.
The mathematical underpinnings of Dempster Shafer theory have a similar flavor to
those of probability theory; the main difference is that instead of assigning probabilities
to possible worlds the theory assigns masses to sets of possible world that is to events.
M AS S
The masses still must add to 1 over all possible events. Bel A is defined to be the sum of
masses for all events that are subsets of i.e. that entail A including A itself. With this
definition Bel Aand Bel Asumtoatmost1andthegaptheintervalbetween Bel A
and1 Bel Aisofteninterpreted asbounding theprobability of A.
Aswithdefaultreasoningthereisaprobleminconnectingbeliefstoactions. Whenever
there is a gap in the beliefs then a decision problem can be defined such that a Dempster
Shafer system is unable to make a decision. In fact the notion of utility in the Dempster
Shafer model is not yet well understood because the meanings of masses and beliefs them-
selves have yet tobeunderstood. Pearl1988 hasargued that Bel Ashould beinterpreted
not as adegree of belief in Abut as the probability assigned to all the possible worlds now
interpreted as logical theories in which A is provable. While there are cases in which this
quantity mightbeofinterest itisnotthesameastheprobability that Aistrue.
A Bayesiananalysisofthecoin-flippingexamplewouldsuggestthatnonewformalism
isnecessary tohandlesuchcases. Themodelwouldhavetwovariables: the Bias ofthecoin
anumberbetween0and1where0isacointhatalwaysshowstailsand1acointhatalways
shows heads and the outcome of the next Flip. The prior probability distribution for Bias
wouldreflectourbeliefsbasedonthesourceofthecointhemagicianspocket: somesmall
probability that it is fair and some probability that it is heavily biased toward heads or tails.
Theconditional distribution P Flip Biassimplydefineshowthebiasoperates. If P Bias
issymmetricabout0.5thenourpriorprobability fortheflipis
cid:26
1
P Flipheads P Biasx P Flipheads Biasxdx 0.5.
0
This is the same prediction as if we believe strongly that the coin is fair but that does not
mean that probability theory treats the two situations identically. The difference arises after
theflipsincomputing theposteriordistribution for Bias. Ifthecoincamefrom abank then
seeing itcomeupheads threetimesrunning wouldhavealmost noeffectonourstrong prior
belief in its fairness; but if the coin comes from the magicians pocket the same evidence
willleadtoastrongerposterior beliefthatthecoinisbiased towardheads. Thusa Bayesian
approach expresses ourignorance in termsofhow ourbeliefs would change in theface of
futureinformation gathering.
Fuzzy set theory is a means of specifying how well an object satisfies a vague description.
F UZ ZY SE TT HE OR Y
cid:2 cid:2cid:2
For example consider the proposition Nate is tall. Is this true if Nate is 5 10 ? Most
people would hesitate to answer true or false preferring to say sort of. Note that this
isnot a question of uncertainty about theexternal worldwe aresure of Nates height. The
issueisthatthelinguistictermtalldoesnotrefertoasharpdemarcationofobjectsintotwo
classesthere are degrees of tallness. For this reason fuzzy set theory is not a method for
uncertain reasoning atall. Rather fuzzy settheory treats Tall asafuzzy predicate andsays
that the truth value of Tall Nate is a number between 0 and 1 rather than being just true
or false. The name fuzzy set derives from the interpretation of the predicate as implicitly
definingasetofitsmembersasetthatdoesnothavesharpboundaries.
Fuzzylogicisamethodforreasoning withlogicalexpressions describing membership
F UZ ZY LO GI C
in fuzzy sets. For example the complex sentence Tall Nate Heavy Nate has a fuzzy
truth value that is a function of the truth values of its components. The standard rules for
evaluating thefuzzytruth Tofacomplexsentenceare
T AB min T AT B
T AB max T AT B
T A 1 TA.
Fuzzy logic is therefore a truth-functional systema fact that causes serious difficulties.
Forexample suppose that T Tall Nate0.6and T Heavy Nate0.4. Thenwehave
T Tall Nate Heavy Nate0.4 which seems reasonable but we also get the result
T Tall Nate Tall Nate0.4 which does not. Clearly the problem arises from the
inabilityofatruth-functionalapproachtotakeintoaccountthecorrelationsoranticorrelations
amongthecomponent propositions.
Fuzzycontrolisamethodologyforconstructingcontrolsystemsinwhichthemapping
F UZ ZY CO NT RO L
between real-valued input and output parameters is represented by fuzzy rules. Fuzzy con-
trolhasbeenverysuccessful incommercial products suchas automatic transmissions video
Section14.8. Summary 551
cameras and electric shavers. Critics see e.g. Elkan 1993 argue that these applications
are successful because they have small rule bases no chaining of inferences and tunable
parameters that can be adjusted to improve the systems performance. Thefact that they are
implemented with fuzzy operators might be incidental to their success; the key is simply to
provideaconcise andintuitivewaytospecify asmoothlyinterpolated real-valued function.
Therehavebeenattemptstoprovideanexplanation offuzzylogicintermsofprobabil-
itytheory. Oneideaistoviewassertionssuchas Nateis Tallasdiscreteobservations made
concerningacontinuoushiddenvariable Natesactual Height. Theprobabilitymodelspeci-
fies P Observersays Nateistall Heightperhapsusingaprobitdistributionasdescribed
on page 522. A posterior distribution over Nates height can then be calculated in the usual
wayforexampleifthemodelispartofahybrid Bayesiannetwork. Suchanapproach isnot
truth-functional ofcourse. Forexampletheconditional distribution
P Observersays Nateistallandheavy Height Weight
allows for interactions between height and weight in the causing of the observation. Thus
someone who iseight feet tall and weighs 190 pounds is very unlikely tobe called tall and
heavyeventhougheightfeetcountsastalland190poundscounts asheavy.
Fuzzy predicates can also be given a probabilistic interpretation in terms of random
setsthat is random variables whosepossible values are sets of objects. Forexample Tall
R AN DO MS ET
is a random set whose possible values are sets of people. The probability P Tall S
1
where S is some particular set of people is the probability that exactly that set would be
1
identified as tall by an observer. Then the probability that Nate is tall is the sum of the
probabilities ofallthesetsofwhich Nateisamember.
Both the hybrid Bayesian network approach and the random sets approach appear to
capture aspects offuzziness without introducing degrees oftruth. Nonetheless there remain
manyopenissuesconcerning theproperrepresentation oflinguisticobservations andcontin-
uousquantitiesissues thathavebeenneglected bymostoutside thefuzzycommunity.
Thischapterhasdescribed Bayesiannetworksawell-developedrepresentationforuncertain
knowledge. Bayesian networks play a role roughly analogous to that of propositional logic
fordefiniteknowledge.
A Bayesian network is a directed acyclic graph whose nodes correspond to random
variables;eachnodehasaconditional distribution forthe nodegivenitsparents.
Bayesiannetworksprovideaconcisewaytorepresent conditionalindependencerela-
tionships inthedomain.
A Bayesian network specifies afull joint distribution; each joint entry isdefined asthe
product of the corresponding entries in the local conditional distributions. A Bayesian
networkisoftenexponentially smallerthananexplicitlyenumerated jointdistribution.
Many conditional distributions can be represented compactly by canonical families of
15
P RO BA BI LI ST IC
R EA SO NI NG O VE R T IM E
Inwhichwetrytointerpret thepresent understand thepastandperhaps predict
thefuture evenwhenverylittleiscrystalclear.
Agentsinpartiallyobservableenvironmentsmustbeabletokeeptrackofthecurrentstateto
theextentthattheirsensorsallow. In Section4.4weshowedamethodologyfordoingthat: an
agentmaintainsabeliefstatethatrepresentswhichstatesoftheworldarecurrentlypossible.
From the belief state and a transition model the agent can predict how the world might
evolve in the next timestep. From the percepts observed and a sensor model the agent can
updatethebeliefstate. Thisisapervasiveidea: in Chapter4beliefstateswererepresentedby
explicitly enumerated sets of states whereas in Chapters 7 and 11 they were represented by
logicalformulas. Thoseapproaches definedbeliefstatesintermsofwhichworldstateswere
possible butcouldsaynothing aboutwhichstateswerelikelyorunlikely. Inthischapter we
useprobability theorytoquantify thedegreeofbeliefinelementsofthebeliefstate.
As we show in Section 15.1 time itself is handled in the same way as in Chapter 7: a
changingworldismodeledusingavariableforeachaspectoftheworldstateateachpointin
time. The transition and sensor models maybe uncertain: the transition model describes the
probability distribution of the variables at time t given the state of the world at past times
while the sensor model describes the probability of each percept at time t given the current
state of the world. Section 15.2 defines the basic inference tasks and describes the gen-
eral structure of inference algorithms for temporal models. Then we describe three specific
kinds of models: hidden Markov models Kalman filters and dynamic Bayesian net-
works which include hidden Markov models and Kalman filters as special cases. Finally
Section15.6examinestheproblemsfacedwhenkeeping track ofmorethanonething.
Wehavedevelopedourtechniques forprobabilistic reasoning inthecontextofstaticworlds
in which each random variable has a single fixed value. Forexample when repairing a car
we assume that whatever is broken remains broken during the process of diagnosis; our job
istoinferthestateofthecarfromobservedevidence which alsoremainsfixed.
566
Section15.1. Timeand Uncertainty 567
Nowconsideraslightly different problem: treating adiabetic patient. Asinthecaseof
carrepair wehave evidence such asrecent insulin doses food intake blood sugarmeasure-
mentsandotherphysicalsigns. Thetaskistoassessthecurrentstateofthepatientincluding
the actual blood sugar level and insulin level. Given this information we can make a deci-
sion about the patients food intake and insulin dose. Unlike the case of car repair here the
dynamic aspects of the problem are essential. Blood sugar levels and measurements thereof
can change rapidly over time depending on recent food intake and insulin doses metabolic
activity the time of day and so on. To assess the current state from the history of evidence
andtopredict theoutcomes oftreatmentactions wemustmodelthesechanges.
Thesame considerations arise in many other contexts such as tracking the location of
a robot tracking the economic activity of a nation and making sense of a spoken orwritten
sequence ofwords. Howcandynamicsituations likethesebemodeled?
We view the world as a series of snapshots or time slices each of which contains a set of
T IM ES LI CE
random variables some observable and some not.1 For simplicity we will assume that the
samesubsetofvariablesisobservableineachtimeslicealthoughthisisnotstrictlynecessary
inanything thatfollows. Wewilluse X todenote thesetofstate variables attime twhich
t
are assumed to be unobservable and E to denote the set of observable evidence variables.
t
Theobservation attimetis E e forsomesetofvalues e .
t t t
Considerthefollowingexample: Youarethesecurityguardstationedatasecretunder-
ground installation. Youwanttoknowwhetheritsrainingtoday butyouronlyaccesstothe
outside worldoccurseachmorningwhenyouseethedirectorcominginwithorwithout an
umbrella. Foreachdayttheset E thuscontainsasingleevidencevariable Umbrella or U
t t t
forshortwhethertheumbrellaappearsandtheset X containsasinglestatevariable Rain
t t
or R forshortwhetheritisraining. Otherproblemscaninvolve largersetsofvariables. In
t
thediabetes examplewemighthaveevidencevariables suchas Measured Blood Sugar and
t
Pulse Rate andstate variables such as Blood Sugar and Stomach Contents . Notice that
t t t
Blood Sugar and Measured Blood Sugar arenotthesamevariable; thisishowwedealwith
t t
noisymeasurements ofactualquantities.
Theintervalbetweentimeslicesalsodependsontheproblem. Fordiabetesmonitoring
asuitable interval might beanhourrather than aday. Inthis chapter weassume theinterval
between slices is fixed so we can label times by integers. We will assume that the state
sequencestartsatt0;forvariousuninterestingreasonswewillassumethatevidencestarts
arrivingatt1ratherthant0. Henceourumbrellaworldisrepresentedbystatevariables
R R R ... and evidence variables U U .... We will use the notation a:b to denote
the sequence of integers from a to b inclusive and the notation X to denote the set of
a:b
variables from X to X . Forexample U corresponds tothevariables U U U .
a b 1:3 1 2 3
studiedinthischaptercanbeviewedasdiscrete-timeapproximationsto S DEs.
a X X X X X
t2 t1 t t1 t2
b X X X X X
t2 t1 t t1 t2
Figure15.1 a Bayesiannetworkstructurecorrespondingtoafirst-order Markovprocess
withstatedefinedbythevariables Xt. b Asecond-order Markovprocess.
With the set of state and evidence variables for a given problem decided on the next step is
to specify how the world evolves the transition model and how the evidence variables get
theirvaluesthesensormodel.
Thetransitionmodelspecifiestheprobabilitydistribution overthelateststatevariables
given the previous values that is P X t X 0:t1. Now we face a problem: the set X 0:t1 is
M AR KO V unbounded insizeastincreases. Wesolvetheproblembymakinga Markovassumption
A SS UM PT IO N
that thecurrent state depends ononly a finite fixed number ofprevious states. Processes sat-
isfyingthisassumption werefirststudiedindepthbythe Russianstatistician Andrei Markov
18561922andarecalled Markovprocessesor Markovchains. Theycomeinvariousfla-
M AR KO VP RO CE SS
F IR ST-O RD ER vors;thesimplestisthefirst-order Markovprocessinwhichthecurrentstatedependsonly
M AR KO VP RO CE SS
on the previous state and not on any earlier states. In other words a state provides enough
information tomakethefutureconditionally independent ofthepastandwehave
P X t X 0:t1 P X t X t1. 15.1
Hence in a first-order Markov process the transition model is the conditional distribution
P X t X t1. The transition model for a second-order Markov process is the conditional
distribution P X t X t2 X t1. Figure 15.1 shows the Bayesian network structures corre-
sponding tofirst-orderandsecond-order Markovprocesses.
Even with the Markov assumption there is still a problem: there are infinitely many
possible values of t. Do we need to specify a different distribution for each time step? We
avoid this problem by assuming that changes in the world state are caused by a stationary
S TA TI ON AR Y processthatisaprocessofchangethatisgovernedbylawsthatdonotthemselveschange
P RO CE SS
overtime. Dont confuse stationary with static: in a static process the state itself does not
change. Intheumbrellaworldthen theconditional probability ofrain P R t R t1isthe
sameforalltandweonlyhavetospecifyoneconditional probability table.
Now for the sensor model. The evidence variables E could depend on previous vari-
t
ablesaswellasthecurrent state variables butanystatethats worthitssaltshould sufficeto
S EN SO RM AR KO V generatethecurrentsensorvalues. Thuswemakea sensor Markovassumptionasfollows:
A SS UM PT IO N
P E t X 0:t E 0:t1 P E t X t. 15.2
Thus P E X isoursensormodelsometimescalledtheobservation model. Figure15.2
t t
shows both the transition model and the sensor model for the umbrella example. Notice the
Section15.1. Timeand Uncertainty 569
R P R
t-1 t
t 0.7
f 0.3
Rain Rain Rain
t1 t t1
R P U
t t
t 0.9
f 0.2
Umbrella Umbrella Umbrella
t1 t t1
umbrella world. The transition model is P Raint Raint1 and the sensor model is
P Umbrellat Raint.
direction of the dependence between state and sensors: the arrows go from the actual state
of the world to sensor values because the state of the world causes the sensors to take on
particular values: the rain causes the umbrella to appear. The inference process of course
goes in the other direction; the distinction between the direction of modeled dependencies
andthedirection ofinference isoneoftheprincipal advantages of Bayesiannetworks.
In addition to specifying the transition and sensor models we need to say how every-
thing gets startedthe prior probability distribution at time 0 P X . With that we have a
0
specification of the complete joint distribution over all the variables using Equation 14.2.
Foranyt
cid:25t
P X 0:t E 1:t P X 0 P X i X i1 PE i X i. 15.3
i1
Thethreetermsontheright-hand sidearetheinitial statemodel P X thetransition model
0
P X i X i1andthesensormodel P E i X i.
Thestructure in Figure 15.2 is a first-order Markov processthe probability of rain is
assumed todepend only onwhetheritrained theprevious day. Whethersuch anassumption
isreasonable depends on the domain itself. Thefirst-order Markov assumption says that the
state variables contain all the information needed tocharacterize the probability distribution
forthenext time slice. Sometimes the assumption is exactly trueforexample ifaparticle
is executing a random walk along the x-axis changing its position by 1 at each time step
then using the x-coordinate as the state gives a first-order Markov process. Sometimes the
assumptionisonlyapproximate asinthecaseofpredicting rainonlyonthebasisofwhether
itrainedthepreviousday. Therearetwowaystoimprovetheaccuracyoftheapproximation:
1. Increasing the order of the Markov process model. For example we could make a
second-ordermodelbyadding Rain t2 asaparentof Rain twhichmightgiveslightly
more accurate predictions. For example in Palo Alto California it very rarely rains
morethantwodaysinarow.
2. Increasing the set of state variables. For example we could add Season to allow
t
us to incorporate historical records of rainy seasons or we could add Temperature
t
Humidity and Pressure perhapsatarangeoflocationstoallowustouseaphysical
t t
modelofrainyconditions.
Exercise 15.1 asks you to show that the first solutionincreasing the ordercan always be
reformulated as an increase in the set of state variables keeping the order fixed. Notice that
adding state variables might improve the systems predictive power but also increases the
prediction requirements: we now have to predict the new variables as well. Thus we are
lookingforaself-sufficientsetofvariableswhichreallymeansthatwehavetounderstand
the physics of the process being modeled. The requirement for accurate modeling of the
process isobviously lessened ifwecan add newsensors e.g. measurements oftemperature
andpressure thatprovideinformation directly aboutthenewstatevariables.
Considerforexampletheproblemoftrackingarobotwanderingrandomlyonthe X Y
plane. Onemightpropose thattheposition andvelocityarea sufficientsetofstatevariables:
onecansimplyuse Newtonslawstocalculatethenewpositionandthevelocitymaychange
unpredictably. Iftherobotisbattery-poweredhoweverthenbatteryexhaustionwouldtendto
haveasystematiceffectonthechangeinvelocity. Becausethisinturndependsonhowmuch
powerwasused byallprevious maneuvers the Markov property isviolated. Wecan restore
the Markovproperty byincluding thechargelevel Battery asoneofthestatevariables that
t
make up X . This helps in predicting the motion of the robot but in turn requires a model
t
for predicting Battery t from Battery t1 and the velocity. In some cases that can be done
reliably but more often we findthat error accumulates overtime. In that case accuracy can
beimprovedbyaddinganewsensorforthebatterylevel.
Havingsetupthestructureofagenerictemporalmodelwecanformulatethebasicinference
tasksthatmustbesolved:
Filtering: This is the task of computing the belief statethe posterior distribution
F IL TE RI NG
over the most recent stategiven all evidence to date. Filtering2 is also called state
B EL IE FS TA TE
estimation. Inourexamplewewishtocompute P X e . Intheumbrellaexample
S TA TE ES TI MA TI ON t 1:t
this would mean computing the probability of rain today given all the observations of
the umbrella carrier made so far. Filtering is what a rational agent does to keep track
of the current state so that rational decisions can be made. It turns out that an almost
identicalcalculation providesthelikelihood oftheevidence sequence Pe .
1:t
Prediction: Thisisthetaskofcomputingtheposteriordistributionoverthefuturestate
P RE DI CT IO N
givenall evidence todate. Thatis wewishtocompute P X e forsome k 0.
tk 1:t
Inthe umbrella example thismight meancomputing theprobability ofrainthree days
fromnowgivenalltheobservationstodate. Predictionisusefulforevaluatingpossible
coursesofactionbasedontheirexpectedoutcomes.
istofilteroutthenoiseinasignalbyestimatingitsunderlyingproperties.
Section15.2. Inference in Temporal Models 571
Smoothing: This is the task of computing the posterior distribution over a past state
S MO OT HI NG
givenallevidenceuptothepresent. Thatiswewishtocompute P X e forsomek
k 1:t
suchthat0 k t. Intheumbrellaexampleitmightmeancomputingtheprobability
that it rained last Wednesday given all the observations of the umbrella carrier made
uptotoday. Smoothing provides abetterestimate ofthestate thanwasavailable atthe
timebecauseitincorporates moreevidence.3
Mostlikelyexplanation: Givenasequence ofobservations wemightwishtofindthe
sequence ofstates thatismostlikely tohavegenerated those observations. Thatis we
wishtocomputeargmax Px e . Forexampleiftheumbrellaappearsoneach
x1:t 1:t 1:t
ofthefirstthreedaysandisabsentonthefourththenthemostlikelyexplanationisthat
itrained on the first three days and did not rain on the fourth. Algorithms forthis task
areusefulinmanyapplications includingspeechrecognitionwhere theaimistofind
themostlikely sequence ofwords givenaseries ofsoundsand thereconstruction of
bitstringstransmitted overanoisychannel.
Inaddition totheseinference taskswealsohave
Learning: The transition and sensor models if not yet known can be learned from
observations. Justaswithstatic Bayesiannetworksdynamic Bayesnetlearningcanbe
done as a by-product of inference. Inference provides an estimate of what transitions
actually occurred andofwhatstatesgenerated thesensorreadings andthese estimates
canbeused toupdate themodels. Theupdated models provide newestimates and the
process iterates to convergence. The overall process is an instance of the expectation-
maximizationor E Malgorithm. See Section20.3.
Notethatlearning requiressmoothing ratherthanfiltering becausesmoothing provides bet-
terestimatesofthestatesoftheprocess. Learningwithfilteringcanfailtoconvergecorrectly;
consider for example the problem of learning to solve murders: unless you are an eyewit-
ness smoothing is always required to infer what happened at the murder scene from the
observable variables.
Theremainderofthissection describes generic algorithms forthefourinference tasks
independent oftheparticularkindofmodelemployed. Improvementsspecifictoeachmodel
aredescribed insubsequent sections.
As we pointed out in Section 7.7.3 a useful filtering algorithm needs to maintain a current
stateestimateandupdateitratherthangoingbackovertheentirehistoryofperceptsforeach
update. Otherwise thecostofeachupdateincreases astimegoesby. Inotherwordsgiven
the result of filtering up to time t the agent needs to compute the result for t1 from the
newevidencee
t1
P X e fe P X e
t1 1:t1 t1 t 1:t
R EC UR SI VE forsomefunctionf. Thisprocessiscalledrecursiveestimation. Wecanviewthecalculation
E ST IM AT IO N
estimatedtrajectorythanfilteringhencethename.
asbeingcomposedoftwoparts: firstthecurrent statedistribution isprojected forwardfrom
ttot1;thenitisupdatedusingthenewevidencee . Thistwo-partprocessemergesquite
t1
simplywhentheformulaisrearranged:
P X e P X e e dividing uptheevidence
t1 1:t1 t1 1:t t1
α Pe X e P X e using Bayesrule
t1 t1 1:t t1 1:t
α Pe X P X e bythesensor Markovassumption. 15.4
t1 t1 t1 1:t
Hereandthroughoutthischapter αisanormalizingconstantusedtomakeprobabilities sum
up to 1. The second term P X e represents a one-step prediction of the next state
t1 1:t
andthefirsttermupdatesthiswiththenewevidence;noticethat Pe X isobtainable
t1 t1
directly from the sensor model. Now weobtain the one-step prediction for the next state by
conditioning onthecurrentstate X :
t cid:12
P X e α Pe X P X x e Px e
t1 1:t1 t1 t1 t1 t 1:t t 1:t
cid:12 xt
α Pe X P X x Px e Markovassumption. 15.5
t1 t1 t1 t t 1:t
xt
Withinthesummationthefirstfactorcomesfromthetransitionmodelandthesecondcomes
fromthecurrentstatedistribution. Hencewehavethedesiredrecursiveformulation. Wecan
thinkofthefilteredestimate P X e asamessagef thatispropagated forwardalong
t 1:t 1:t
thesequence modifiedbyeachtransition andupdatedbyeachnewobservation. Theprocess
isgivenby
f
1:t1
α FO RW AR Df 1:te t1
where F OR WA RDimplementstheupdatedescribedin Equation15.5andtheprocessbegins
with f P X . When all the state variables are discrete the time for each update is
1:0 0
constant i.e. independent of t and the space required is also constant. The constants
depend of course on the size of the state space and the specific type of the temporal model
inquestion. Thetimeandspacerequirementsforupdatingmustbeconstantifanagentwith
limitedmemoryistokeeptrackofthecurrentstatedistribution overanunbounded sequence
ofobservations.
Let us illustrate the filtering process for two steps in the basic umbrella example Fig-
ure15.2. Thatiswewillcompute P R u asfollows:
Onday0wehavenoobservations onlythesecurityguardspriorbeliefs;letsassume
thatconsistsof P R cid:160.50.5cid:17.
0
Onday1theumbrellaappears so U true. Theprediction from t0tot1is
cid:12 1
P R P R r Pr
r0
cid:160.70.3cid:170.5cid:160.30.7cid:170.5 cid:160.50.5cid:17.
Thentheupdate stepsimply multiplies bytheprobability of theevidence fort1and
normalizes asshownin Equation15.4:
P R u α Pu R P R αcid:160.90.2cid:17cid:160.50.5cid:17
αcid:160.450.1cid:17 cid:160.8180.182cid:17.
Section15.2. Inference in Temporal Models 573
Onday2theumbrellaappears so U true. Theprediction from t1tot2is
cid:12 2
P R u P R r Pr u
r1
cid:160.70.3cid:170.818cid:160.30.7cid:170.182 cid:160.6270.373cid:17
andupdatingitwiththeevidencefort2gives
P R u u α Pu R P R u αcid:160.90.2cid:17cid:160.6270.373cid:17
αcid:160.5650.075cid:17 cid:160.8830.117cid:17.
Intuitively the probability of rain increases from day 1to day 2 because rain persists. Exer-
cise15.2aasksyoutoinvestigate thistendencyfurther.
The task of prediction can be seen simply as filtering without the addition of new
evidence. In fact the filtering process already incorporates a one-step prediction and it is
easy toderivethe following recursive computation forpredicting the stateattk1from
aprediction fortk:
cid:12
P X e P X x Px e . 15.6
tk1 1:t tk1 tk tk 1:t
xtk
Naturally thiscomputation involves onlythetransition modelandnotthesensormodel.
It is interesting to consider what happens as we try to predict further and further into
the future. As Exercise 15.2b shows the predicted distribution for rain converges to a
fixed point cid:160.50.5cid:17 after which it remains constant for all time. This is the stationary
distribution ofthe Markov process defined by thetransition model. Seealso page 537. A
great deal is known about the properties of such distributions and about the mixing time
M IX IN GT IM E
roughly thetime taken to reach the fixedpoint. Inpractical terms this dooms tofailure any
attempt topredict the actual state foranumber ofsteps that is morethan asmall fraction of
themixing time unless the stationary distribution itself isstrongly peaked inasmallarea of
the state space. Themore uncertainty there is inthe transition model the shorter willbe the
mixingtimeandthemorethefutureisobscured.
In addition to filtering and prediction we can use a forward recursion to compute the
likelihoodoftheevidencesequence Pe . Thisisausefulquantityifwewanttocompare
1:t
different temporal models that might have produced the same evidence sequence e.g. two
different modelsforthepersistence ofrain. Forthisrecursion weusealikelihood message
cid:3 X P X e . Itisasimpleexercisetoshowthatthemessagecalculation isidentical
1:t t t 1:t
tothatforfiltering:
cid:3
1:t1
F OR WA RDcid:3 1:te t1.
Havingcomputedcid:3 weobtaintheactuallikelihood bysummingout X :
1:t cid:12 t
L Pe cid:3 x . 15.7
1:t 1:t 1:t t
xt
Noticethatthelikelihood messagerepresents theprobabilities oflongerandlongerevidence
sequencesastimegoesbyandsobecomesnumericallysmallerandsmallerleadingtounder-
flow problems with floating-point arithmetic. This is an important problem in practice but
weshallnotgointosolutions here.
X X X X
E E E
somepasttimekgivenacompletesequenceofobservationsfrom1tot.
As we said earlier smoothing is the process of computing the distribution over past states
given evidence up to the present; that is P X e for 0 k t. See Figure 15.3.
k 1:t
Inanticipation ofanotherrecursive message-passing approach wecansplit thecomputation
intotwopartsthe evidenceuptok andtheevidence fromk1tot
P X e P X e e
k 1:t k 1:k k1:t
α PX e Pe X e using Bayesrule
k 1:k k1:t k 1:k
α PX e Pe X usingconditional independence
k 1:k k1:t k
αf b . 15.8
1:k k1:t
where represents pointwise multiplication of vectors. Here we have defined a back-
wardmessage b Pe X analogous tothe forward message f . Theforward
k1:t k1:t k 1:k
message f can becomputed byfiltering forward from 1to k as givenby Equation 15.5.
1:k
It turns out that the backward message b can be computed by a recursive process that
k1:t
runsbackwardfrom t:
cid:12
Pe X Pe X x Px X conditioning on X
k1:t k k1:t k k1 k1 k k1
x cid:12k1
Pe x Px X byconditional independence
k1:t k1 k1 k
x cid:12k1
Pe e x Px X
k1 k2:t k1 k1 k
x cid:12k1
Pe x Pe x Px X 15.9
k1 k1 k2:t k1 k1 k
xk1
wherethelast stepfollows bytheconditional independence ofe and e given X .
k1 k2:t k1
Ofthethreefactorsinthissummationthefirstandthirdareobtaineddirectlyfromthemodel
andthesecondistherecursive call. Usingthemessagenotation wehave
b
k1:t
B AC KW AR Db k2:te k1
where B AC KW AR Dimplementstheupdatedescribedin Equation15.9. Aswiththeforward
recursion thetimeandspaceneededforeachupdateareconstantandthusindependent oft.
Wecan now see that the twoterms in Equation 15.8 can both be computed by recur-
sions through time one running forward from 1 to k and using the filtering equation 15.5
Section15.2. Inference in Temporal Models 575
and the other running backward from t to k 1 and using Equation 15.9. Note that the
backward phase is initialized with b Pe X P X 1 where 1 is a vector of
t1:t t1:t t t
1s. Becausee isanemptysequence theprobability ofobserving itis1.
t1:t
Let us now apply this algorithm to the umbrella example computing the smoothed
estimate for the probability of rain at time k1 given the umbrella observations on days 1
and2. From Equation15.8thisisgivenby
P R u u α PR u Pu R . 15.10
The first term we already know to be cid:16.818.182cid:17 from the forward filtering process de-
scribed earlier. The second term can be computed by applying the backward recursion in
Equation15.9:
cid:12
Pu R Pu r P r Pr R
r2
0.91cid:160.70.3cid:170.21cid:160.30.7cid:17 cid:160.690.41cid:17.
Pluggingthisinto Equation15.10 wefindthatthesmoothedestimateforrainonday1is
P R u u αcid:160.8180.182cid:17cid:160.690.41cid:17 cid:160.8830.117cid:17.
Thus the smoothed estimate for rain on day 1 is higher than the filtered estimate 0.818 in
this case. This is because the umbrella on day 2 makes it more likely to have rained on day
2;inturnbecauseraintendstopersist thatmakesitmorelikelytohaverainedonday1.
Both the forward and backward recursions take a constant amount of time per step;
hence the time complexity of smoothing with respect to evidence e is Ot. This is the
1:t
complexity for smoothing at a particular time step k. If we want to smooth the whole se-
quence one obvious method is simply to run the whole smoothing process once for each
time step to be smoothed. This results in a time complexity of Ot2. A better approach
usesasimpleapplication ofdynamicprogramming toreducethecomplexityto Ot. Aclue
appears in the preceding analysis of the umbrella example where wewere able to reuse the
results of the forward-filtering phase. The key to the linear-time algorithm is to record the
results of forward filtering over the whole sequence. Then we run the backward recursion
from tdownto1computing thesmoothed estimate ateachstep k fromthecomputed back-
ward message b and the stored forward message f . The algorithm aptly called the
k1:t 1:k
F OR WA RD
forwardbackwardalgorithmisshownin Figure15.4.
B AC KW AR D
A LG OR IT HM
The alert reader will have spotted that the Bayesian network structure shown in Fig-
ure 15.3 is a polytree as defined on page 528. This means that a straightforward application
of the clustering algorithm also yields a linear-time algorithm that computes smoothed es-
timates for the entire sequence. It is now understood that the forwardbackward algorithm
is in fact a special case of the polytree propagation algorithm used with clustering methods
although thetwoweredevelopedindependently.
Theforwardbackwardalgorithmformsthecomputationalbackboneformanyapplica-
tionsthatdealwithsequences ofnoisyobservations. Asdescribed sofarithastwopractical
drawbacks. Thefirstisthatitsspacecomplexitycanbetoohighwhenthestatespaceislarge
andthesequences arelong. Ituses Oftspacewherefisthesizeoftherepresentation of
the forward message. Thespace requirement can be reduced to Oflogt witha concomi-
tant increase inthe time complexity by afactor of logt asshown in Exercise 15.3. In some
casessee Section15.3aconstant-space algorithm canbeused.
The second drawback of the basic algorithm is that it needs to be modified to work
in an online setting where smoothed estimates must be computed for earlier time slices as
new observations are continuously added to the end of the sequence. The most common
F IX ED-L AG requirement is for fixed-lag smoothing which requires computing the smoothed estimate
S MO OT HI NG
P X tde 1:t for fixed d. That is smoothing is done for the time slice d steps behind the
current time t; as t increases the smoothing has to keep up. Obviously we can run the
forwardbackward algorithm over the d-step window as each new observation is added
butthisseemsinefficient. In Section15.3 wewillseethatfixed-lagsmoothing can insome
casesbedoneinconstanttimeperupdateindependent ofthelagd.
Suppose that truetruefalsetruetrue is the umbrella sequence for the security guards
first five days on the job. What is the weather sequence most likely to explain this? Does
the absence of the umbrella on day 3 mean that it wasnt raining or did the director forget
to bring it? If it didnt rain on day 3 perhaps because weather tends to persist it didnt
rain on day 4 either but the director brought the umbrella just in case. In all there are 25
possibleweathersequenceswecouldpick. Isthereawaytofindthemostlikelyoneshortof
enumerating allofthem?
Wecouldtrythislinear-timeprocedure: usesmoothingtofindtheposteriordistribution
fortheweatherateachtimestep;thenconstruct thesequence usingateachsteptheweather
that is most likely according to the posterior. Such an approach should set off alarm bells
in the readers head because the posterior distributions computed by smoothing are distri-
function F OR WA RD-B AC KW AR Devpriorreturnsavectorofprobabilitydistributions
inputs:evavectorofevidencevaluesforsteps1...t
priorthepriordistributionontheinitialstate P X
0
localvariables: fvavectorofforwardmessagesforsteps0...t
barepresentationofthebackwardmessageinitiallyall1s
svavectorofsmoothedestimatesforsteps1...t
fv0prior
fori 1totdo
fvi F OR WA RDfvi1evi
fori tdownto1do
svi N OR MA LI ZEfvib
b B AC KW AR Dbevi
returnsv
Figure15.4 Theforwardbackwardalgorithmforsmoothing: computingposteriorprob-
abilities of a sequence of states given a sequence of observations. The F OR WA RD and
B AC KW AR Doperatorsaredefinedby Equations15.5and15.9respectively.
Section15.2. Inference in Temporal Models 577
Rain Rain Rain Rain Rain
true true true true true
a
false false false false false
Umbrellat true true false true true
b
m m m m m
1:1 1:2 1:3 1:4 1:5
Figure15.5 a Possiblestatesequencesfor Raintcanbeviewedaspathsthroughagraph
of the possible states ateach time step. States are shownas rectanglesto avoid confusion
with nodes in a Bayes net. b Operationof the Viterbi algorithm for the umbrella obser-
vationsequencetruetruefalsetruetrue. Foreacht wehaveshownthevaluesofthe
messagem 1:twhichgivestheprobabilityofthebestsequencereachingeachstateattimet.
Alsoforeachstatetheboldarrowleadingintoitindicatesitsbestpredecessorasmeasured
bytheproductoftheprecedingsequenceprobabilityandthetransitionprobability.Following
theboldarrowsbackfromthemostlikelystateinm givesthemostlikelysequence.
1:5
butions over single time steps whereas to find the most likely sequence we must consider
joint probabilities over all the time steps. The results can in fact be quite different. See
Exercise15.4.
There is a linear-time algorithm for finding the most likely sequence but it requires a
littlemorethought. Itreliesonthesame Markovpropertythatyieldedefficientalgorithmsfor
filteringandsmoothing. Theeasiestwaytothinkabouttheproblemistovieweachsequence
as a path through a graph whose nodes are the possible states at each time step. Such a
graph is shown for the umbrella world in Figure 15.5a. Now consider the task of finding
the most likely path through this graph where the likelihood of any path is the product of
the transition probabilities along the path and the probabilities of the given observations at
each state. Lets focus in particular on paths that reach the state Rain true. Because of
5
the Markovpropertyitfollowsthatthemostlikelypathtothestate Rain true consistsof
5
themostlikelypathtosomestateattime4followedbyatransition to Rain true;andthe
5
stateattime4thatwillbecomepartofthepathto Rain true iswhichevermaximizesthe
5
likelihood of that path. In other words there is a recursive relationship between most likely
pathstoeachstatex andmostlikelypathstoeachstatex . Wecanwritethisrelationship
t1 t
asanequation connecting theprobabilities ofthepaths:
max Px ...x X e
x1...xt cid:13 cid:14
α Pe t1 X t1max P X t1x t max Px 1...x t1x te 1:t . 15.11
xt x1...xt1
Equation15.11isidentical tothefilteringequation 15.5exceptthat
1. Theforwardmessage f P X e isreplacedbythemessage
1:t t 1:t
m
1:t
max Px 1...x t1 X te 1:t
x1...xt1
thatistheprobabilities ofthemostlikelypathtoeachstatex ;and
t
2. the summation over x in Equation 15.5 is replaced by the maximization over x in
t t
Equation15.11.
Thusthealgorithmforcomputingthemostlikelysequenceissimilartofiltering: itrunsfor-
wardalongthesequencecomputingthemmessageateachtimestepusing Equation15.11.
The progress of this computation is shown in Figure 15.5b. At the end it will have the
probability forthemostlikelysequencereaching eachofthefinalstates. Onecanthuseasily
select the most likely sequence overall the states outlined in bold. In order to identify the
actualsequence asopposed tojustcomputing itsprobability thealgorithm willalsoneedto
record foreach state the best state that leads toit; these areindicated by thebold arrowsin
Figure15.5b. Theoptimalsequenceisidentifiedbyfollowingtheseboldarrowsbackwards
fromthebestfinalstate.
Thealgorithmwehavejustdescribediscalledthe Viterbialgorithmafteritsinventor.
V IT ER BI AL GO RI TH M
Like the filtering algorithm its time complexity is linear in t the length of the sequence.
Unlike filtering which uses constant space its space requirement is also linear in t. This
is because the Viterbi algorithm needs to keep the pointers that identify the best sequence
leadingtoeachstate.
Theprecedingsectiondevelopedalgorithmsfortemporalprobabilisticreasoningusingagen-
eralframeworkthatwasindependent ofthespecificformofthetransitionandsensormodels.
In this and the next two sections we discuss more concrete models and applications that
illustrate thepowerofthebasicalgorithms andinsomecasesallowfurtherimprovements.
H ID DE NM AR KO V We begin with the hidden Markov model or H MM. An H MM is a temporal proba-
M OD EL
bilistic modelinwhichthestateoftheprocess isdescribed byasingle discrete random vari-
able. The possible values of the variable are the possible states of the world. The umbrella
example described in the preceding section is therefore an H MM since it has just one state
variable: Rain . Whathappensifyouhaveamodelwithtwoormorestatevariables? Youcan
t
still fititinto the H MMframework by combining the variables into asingle megavariable
whose values are all possible tuples of values of the individual state variables. We will see
thattherestricted structure of H MMsallowsforasimpleand elegantmatriximplementation
ofallthebasicalgorithms.4
proceedingwiththissection.
Section15.3. Hidden Markov Models 579
With a single discrete state variable X we can give concrete form to the representations
t
of the transition model the sensor model and the forward and backward messages. Let the
statevariable X havevaluesdenotedbyintegers1...Swhere S isthenumberofpossible
t
states. Thetransition model P X t X t1becomesan S S matrix Twhere
T
ij
P X tj X t1i.
Thatis T istheprobabilityofatransitionfromstate itostatej. Forexamplethetransition
ij
matrixfortheumbrellaworldis
cid:13 cid:14
T P X t X t1
.
Wealso putthesensor modelinmatrixform. Inthis case because thevalue oftheevidence
variable E is known at time t call it e weneed only specify for each state how likely it
t t
isthatthestatecausese toappear: weneed Pe X iforeachstatei. Formathematical
t t t
convenience we place these values into an S S diagonal matrix O whose ith diagonal
t
entry is Pe X iand whose other entries are 0. Forexample on day 1 in the umbrella
t t
worldof Figure15.5 U trueandonday3 U falsesofrom Figure15.2wehave
cid:13 cid:141 cid:13 3cid:14
O ; O .
Nowifweusecolumnvectorstorepresenttheforwardandbackwardmessagesallthecom-
putations becomesimplematrixvectoroperations. Theforwardequation 15.5becomes
cid:12
f α O T f 15.12
1:t1 t1 1:t
andthebackwardequation 15.9becomes
b T O b . 15.13
k1:t k1 k2:t
From these equations we can see that the time complexity of the forwardbackward algo-
rithm Figure 15.4 applied to a sequence of length t is O S2t because each step requires
multiplying an S-element vector by an S S matrix. The space requirement is O St be-
causetheforwardpassstores tvectorsofsize S.
Besides providing an elegant description of the filtering and smoothing algorithms for
H MMs the matrix formulation reveals opportunities for improved algorithms. The first is
a simple variation on the forwardbackward algorithm that allows smoothing to be carried
out in constant space independently ofthe length of the sequence. Theidea isthat smooth-
ingforanyparticulartimeslice krequiresthesimultaneouspresenceofboththeforwardand
backwardmessagesf andb accordingto Equation15.8. Theforwardbackwardal-
1:k k1:t
gorithmachievesthisbystoringthefscomputedontheforwardpasssothattheyareavailable
during the backward pass. Another way to achieve this is with a single pass that propagates
both fandbinthesamedirection. Forexample theforward message fcanbepropagated
backwardifwemanipulate Equation15.12toworkintheotherdirection:
f
αcid:2 Tcid:12 1 O1
f .
1:t t1 1:t1
Themodifiedsmoothing algorithm worksbyfirstrunning thestandard forward passtocom-
putef forgetting alltheintermediate results andthen running thebackward passforboth
t:t
function F IX ED-L AG-S MO OT HI NGethmmdreturnsadistributionover Xtd
inputs:etthecurrentevidencefortimestept
hmmahidden Markovmodelwith S S transitionmatrix T
dthelengthofthelagforsmoothing
persistent: tthecurrenttimeinitially1
ftheforwardmessage P Xte 1:tinitiallyhmm.P RI OR
Bthed-stepbackwardtransformationmatrixinitiallytheidentitymatrix
etd:tdouble-endedlistofevidencefromtdtotinitiallyempty
localvariables: Otd Otdiagonalmatricescontainingthesensormodelinformation
addettotheendofetd:t
Otdiagonalmatrixcontaining Pet Xt
iftdthen
f F OR WA RDfet
removeetd1fromthebeginningofetd:t
Otddiagonalmatrixcontaining Petd Xtd
B O t1 d T1 BT Ot
else B BT Ot
tt 1
iftdthenreturn N OR MA LI ZEf B1elsereturnnull
as an online algorithm that outputs the new smoothed estimate given the observation for a
new time step. Notice that the final output N OR MA LI ZEf B1 is just αfb by Equa-
tion15.14.
b and ftogether using them to compute the smoothed estimate ateach step. Since only one
copy of each message is needed the storage requirements are constant i.e. independent of
t the length of the sequence. There are two significant restrictions on this algorithm: it re-
quires thatthetransition matrixbeinvertible andthatthe sensormodelhavenozeroesthat
isthateveryobservation bepossible ineverystate.
A second area in which the matrix formulation reveals an improvement is in online
smoothing with a fixed lag. The fact that smoothing can be done in constant space suggests
that there should exist an efficient recursive algorithm for online smoothingthat is an al-
gorithm whose time complexity is independent of the length of the lag. Let us suppose that
the lag is d; that is we are smoothing at time slice td where the current time is t. By
Equation15.8weneedtocompute
αf 1:tdb td1:t
forslicetd. Thenwhenanewobservation arrivesweneedtocompute
αf 1:td1b td2:t1
forslicetd1. Howcanthisbedoneincrementally? Firstwecancomputef 1:td1 from
f 1:tdusingthestandard filteringprocess Equation15.5.
Section15.3. Hidden Markov Models 581
Computing the backward message incrementally istrickier because there isnosimple
relationship between the old backward message b td1:t and the new backward message
b td2:t1. Instead we will examine the relationship between the old backward message
b td1:t andthebackward messageatthefrontofthesequence b t1:t. Todothisweapply
Equation15.13dtimestoget
cid:31
cid:25t
b td1:t T O i b t1:t B td1:t1 15.14
itd1
where the matrix B td1:t is the product of the sequence of T and O matrices. B can be
thought of as a transformation operator that transforms a later backward message into an
earlierone. Asimilarequation holds forthenewbackward messages afterthenextobserva-
tionarrives:
cid:31
tcid:251
b td2:t1 T O i b t2:t1 B td2:t11. 15.15
itd2
Examiningtheproductexpressions in Equations15.14and 15.15weseethattheyhavea
simple relationship: to get the second product divide the first product by the firstelement
T O td1 and multiply by the new last element T O t1. In matrix language then there is a
simplerelationship betweentheoldandnew Bmatrices:
B td2:t1 O t1 d1 T1 B td1:t T O t1 . 15.16
Thisequation providesanincremental updateforthe Bmatrixwhichinturnthrough Equa-
tion 15.15 allows us to compute the new backward message b td2:t1. The complete
algorithm whichrequiresstoring andupdating fand Bisshownin Figure15.6.
Onpage145weintroducedasimpleformofthelocalizationproblemforthevacuumworld.
In that version the robot had a single nondeterministic Move action and its sensors reported
perfectly whether or not obstacles lay immediately to the north south east and west; the
robotsbeliefstatewasthesetofpossiblelocations itcouldbein.
Here we make the problem slightly more realistic by including a simple probability
model forthe robots motion andby allowing fornoise inthesensors. Thestate variable X
t
represents the location of the robot on the discrete grid; the domain of this variable is the
set of empty squares s 1...s n. Let N EI GH BO RSs be the set of empty squares that are
adjacent to sand let Nsbe the size ofthat set. Then the transition model for Move action
saysthattherobotisequally likelytoendupatanyneighboring square:
P X t1j X ti T
ij
1 Niifj N EI GH BO RSielse0.
Wedont know where the robot starts so wewill assume auniform distribution overall the
squares; thatis P X i1n. Fortheparticularenvironment weconsider Figure15.7
0
n42andthetransition matrix Thas42421764entries.
Thesensorvariable E has16possiblevalueseachafour-bitsequencegivingthepres-
t
ence or absence of an obstacle in a particular compass direction. We will use the notation
a Posteriordistribution overrobotlocation after E N SW
1
b Posteriordistribution overrobotlocation after E N SW E N S
Figure15.7 Posteriordistributionoverrobotlocation: aone observation E N SW;
1
bafterasecondobservation E N S.Thesizeofeachdiskcorrespondstotheprobability
2
thattherobotisatthatlocation.Thesensorerrorrateis cid:20.2.
N Sforexampletomeanthatthenorthandsouthsensorsreport anobstacleandtheeastand
westdonot. Supposethateachsensorserrorrateis cid:2andthaterrorsoccurindependently for
thefoursensordirections. Inthatcasetheprobability of gettingallfourbitsrightis1cid:24
andtheprobabilityofgettingthemallwrongiscid:24. Furthermoreifd isthediscrepancythe
it
numberofbitsthataredifferentbetween thetruevaluesforsquare iandtheactualreading
e thentheprobability thatarobotinsquare iwouldreceiveasensorreading e is
t t
P E e X i O 1cid:24ditcid:2dit .
t t t tii
Forexampletheprobabilitythatasquarewithobstaclestothenorthandsouthwouldproduce
asensorreading N SE is1cid:23cid:21.
Given the matrices T and O the robot can use Equation 15.12 to compute the pos-
t
terior distribution over locationsthat is to work out where it is. Figure 15.7 shows the
distributions P X E N SWand P X E N SW E N S. Thisisthesamemaze
wesawbefore in Figure 4.18page 146 but there weused logical filtering tofindthe loca-
tions that were possible assuming perfect sensing. Those same locations are still the most
likelywithnoisysensing butnoweverylocation hassomenonzero probability.
In addition to filtering to estimate its current location the robot can use smoothing
Equation 15.13 to work out where it was at any given past timefor example where it
began attime 0and it can use the Viterbi algorithm towork out the mostlikely path it has
Section15.3. Hidden Markov Models 583
6
5.5
5
4.5
4
3.5
3
2.5
2
1.5
1
0.5
rorre
noitazilaco L
1
ε 0.20 0.9
ε 0.10
ε 0.05 0.8
ε 0.02 0.7
ε 0.00
0.6
0.5
0.4
0.3
0.2
0.1
Number of observations
ycarucca
hta P
ε 0.00
ε 0.02
ε 0.05
ε 0.10
ε 0.20
Number of observations
a b
Figure15.8 Performanceof H MMlocalizationasafunctionofthelengthoftheobserva-
tionsequenceforvariousdifferentvaluesofthesensorerrorprobabilitycid:2;dataaveragedover
400runs.a Thelocalizationerrordefinedasthe Manhattandistancefromthetruelocation.
b The Viterbipathaccuracydefinedasthefractionofcorrectstatesonthe Viterbipath.
takentogetwhereitisnow. Figure15.8showsthelocalizationerrorand Viterbipathaccuracy
forvarious values of the per-bit sensor errorrate cid:2. Evenwhencid:2 is20which means that
theoverallsensorreadingiswrong59ofthetimetherobot isusuallyabletoworkoutits
location within two squares after 25 observations. This is because of the algorithms ability
tointegrate evidenceovertimeandtotakeintoaccount theprobabilistic constraints imposed
on the location sequence by the transition model. When cid:2 is 10 the performance after
a half-dozen observations is hard to distinguish from the performance with perfect sensing.
Exercise15.7asksyoutoexplore howrobust the H MMlocalization algorithm istoerrorsin
thepriordistribution P X andinthetransition modelitself. Broadly speaking highlevels
0
of localization and path accuracy are maintained even in the face of substantial errors in the
modelsused.
The state variable for the example we have considered in this section is a physical
location in the world. Other problems can of course include other aspects of the world.
Exercise15.8asksyoutoconsideraversionofthevacuumrobotthathasthepolicyofgoing
straight for as long as it can; only when it encounters an obstacle does it change to a new
randomly selected heading. To model this robot each state in the model consists of a
location heading pair. For the environment in Figure 15.7 which has 42 empty squares
thisleadsto168statesandatransitionmatrixwith168228224entriesstillamanageable
number. Ifweaddthepossibility ofdirtinthesquares thenumberofstates ismultiplied by
number; Section 15.5showshowtousedynamic Bayesian networks tomodeldomains with
many state variables. If we allow the robot to move continuously rather than in a discrete
gridthenumberofstatesbecomesinfinite;thenextsection showshowtohandlethiscase.
Imagine watching a small bird flying through dense jungle foliage at dusk: you glimpse
briefintermittent flashesofmotion;youtryhardtoguesswherethebirdisandwhereitwill
appear next so that you dont lose it. Orimagine that you are a World War I I radar operator
peeringatafaintwanderingblipthatappearsonceevery10secondsonthescreen. Orgoing
back further still imagine you are Kepler trying to reconstruct the motions of the planets
fromacollectionofhighlyinaccurateangularobservationstakenatirregularandimprecisely
measuredintervals. Inallthesecasesyouaredoingfiltering: estimatingstatevariableshere
position and velocity from noisy observations over time. If the variables were discrete we
could model the system with a hidden Markov model. This section examines methods for
handling continuous variables using an algorithm called Kalman filtering after one of its
K AL MA NF IL TE RI NG
inventors Rudolf E.Kalman.
Thebirdsflightmightbespecifiedbysixcontinuousvariablesateachtimepoint;three
forposition X Y Z andthreeforvelocity X Y Z . Wewillneedsuitableconditional
t t t t t t
densities to represent the transition and sensor models; as in Chapter 14 we will use linear
Gaussian distributions. Thismeans that the next state X mustbe alinear function of the
t1
currentstate X plussome Gaussiannoiseaconditionthatturnsouttobequitereasonablein
t
practice. Consider forexamplethe X-coordinate ofthebirdignoring theothercoordinates
for now. Let the time interval between observations be Δ and assume constant velocity
duringtheinterval;thenthepositionupdateisgivenby X X X Δ. Adding Gaussian
tΔ t
noisetoaccount forwindvariation etc.weobtainalinear Gaussiantransition model:
P X x X x X x Nx x Δσ2x .
tΔ tΔ t t t t t t tΔ
The Bayesiannetworkstructureforasystemwithpositionvector X andvelocity X isshown
t t
in Figure 15.9. Note that this is a very specific form of linear Gaussian model; the general
formwillbedescribed laterinthissectionandcoversavast arrayofapplications beyondthe
simplemotionexamplesofthefirstparagraph. Thereadermightwishtoconsult Appendix A
for some of the mathematical properties of Gaussian distributions; for our immediate pur-
M UL TI VA RI AT E poses the most important is that a multivariate Gaussian distribution for d variables is
G AU SS IA N
specifiedbyad-elementmeanμandaddcovariance matrix Σ.
In Chapter14onpage521wealludedtoakeypropertyofthelinear Gaussianfamilyofdis-
tributions: itremainsclosedunderthestandard Bayesiannetworkoperations. Herewemake
this claim precise in the context of filtering in a temporal probability model. The required
properties correspond tothetwo-stepfilteringcalculation in Equation15.5:
1. Ifthecurrent distribution P X e is Gaussian andthetransition model P X x
t 1:t t1 t
islinear Gaussianthentheone-step predicted distribution givenby
cid:26
P X e P X x Px e dx 15.17
t1 1:t t1 t t 1:t t
xt
isalsoa Gaussiandistribution.
Section15.4. Kalman Filters 585
X t X t1
X t X t1
Z t Z t1
Figure15.9 Bayesiannetworkstructureforalineardynamicalsystemwithposition Xt
velocity X tandpositionmeasurement Zt.
2. Iftheprediction P X e is Gaussianandthesensormodel Pe X islinear
t1 1:t t1 t1
Gaussianthenafterconditioning onthenewevidence theupdated distribution
P X e α Pe X P X e 15.18
t1 1:t1 t1 t1 t1 1:t
isalsoa Gaussiandistribution.
Thus the F OR WA RD operator for Kalman filtering takes a Gaussian forward message f 1:t
specifiedbyameanμ andcovariance matrix Σ andproducesanewmultivariate Gaussian
t t
forward message f specified by a mean μ and covariance matrix Σ . So if we
1:t1 t1 t1
startwitha Gaussianprior f P X Nμ Σ filteringwithalinear Gaussianmodel
1:0 0 0 0
produces a Gaussianstatedistribution foralltime.
This seems to be a nice elegant result but why is it so important? The reason is that
except for a few special cases such as this filtering with continuous or hybrid discrete and
continuousnetworksgeneratesstatedistributionswhoserepresentationgrowswithoutbound
over time. This statement is not easy to prove in general but Exercise 15.10 shows what
happens forasimpleexample.
We have said that the F OR WA RD operator for the Kalman filtermaps a Gaussian into a new
Gaussian. Thistranslates into computing anew meanand covariance matrixfrom theprevi-
ous mean and covariance matrix. Deriving the update rule in the general multivariate case
requiresratheralotoflinearalgebrasowewillsticktoaverysimpleunivariatecasefornow;
and later give the results for the general case. Even for the univariate case the calculations
are somewhat tedious but we feel that they are worth seeing because the usefulness of the
Kalmanfilteristiedsointimatelytothemathematicalproperties of Gaussiandistributions.
Thetemporalmodelweconsiderdescribesarandomwalkofasinglecontinuousstate
variable X withanoisyobservation Z . Anexamplemightbetheconsumerconfidencein-
t t
dexwhichcanbemodeledasundergoing arandom Gaussian-distributed changeeachmonth
andismeasuredbyarandomconsumersurveythatalsointroduces Gaussiansamplingnoise.
Thepriordistribution isassumedtobe Gaussianwithvariance σ2:
0
""
1
x0μ02
Px 0 αe 2 σ02 .
Forsimplicity weusethesamesymbolαforallnormalizing constants inthissection. The
transition modeladdsa Gaussianperturbation ofconstant variance σ2 tothecurrentstate:
x
""
1
xt1xt2
Px x αe 2 σx2 .
t1 t
Thesensormodelassumes Gaussiannoisewithvariance σ2:
z
""
1
ztxt2
Pz x αe 2 σz2 .
t t
Nowgiventheprior P X theone-steppredicteddistributioncomesfrom Equation15.17:
0
cid:26 cid:26
1 x1x02 1 x0μ02
Px 1 Px 1x 0 Px 0dx
0
α e 2 σx2 e 2 σ02 dx
0
""
cid:26
1 σ02x1x02σx2x0μ02
α e 2 σ02σx2 dx
0
.
""
Thisintegrallooksrathercomplicated. Thekeytoprogress istonoticethattheexponentisthe
sumoftwoexpressionsthatarequadraticinx andhenceisitselfaquadraticinx . Asimple
C OM PL ET IN GT HE trick known as completing the square allows the rewriting of any quadratic ax2 bx c
S QU AR E 0 0
as the sum of asquared term ax b2 and a residual term c b2 that is independent of
x . Theresidual termcanbetakenoutside theintegralgiving us
0
cid:26
Px 1
αe 21 c 4b2
a
e1 2ax0 2ab2
dx
0
.
""
Nowtheintegralisjusttheintegralofa Gaussianoveritsfullrangewhichissimply1. Thus
weare left with only the residual term from the quadratic. Then wenotice that the residual
termisaquadratic inx ;infactaftersimplification weobtain
1
""
1
x1μ02
Px 1 αe 2 σ02σx2 .
Thatistheone-steppredicteddistributionisa Gaussianwiththesamemeanμ andavariance
0
equaltothesumoftheoriginal variance σ2 andthetransition variance σ2.
To complete the update step we need to condition on the observation at the first time
stepnamelyz . From Equation15.18 thisisgivenby
1
Px z α Pz x Px
""
1
z1x12
1
x1μ02
αe 2 σz2 e 2 σ02σx2 .
Onceagainwecombinetheexponents andcompletethesquare Exercise15.11 obtaining
1 B
Bx1σ02 σ 02σ x2 σz x21 σσ z2z2μ02
C
C
2 σ02σx2σz2σ02σx2σz2 A
Px z αe . 15.19
Section15.4. Kalman Filters 587
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
-10 -5 0 5 10
x P
Px z 2.5
Px 0
Px
1
z
1
x position
given by μ 00.0 and σ 01.0 transition noise given by σx2.0 sensor noise given by
σz1.0andafirstobservationz 12.5markedonthex-axis. Noticehowtheprediction
Px isflattenedoutrelativeto Px bythetransitionnoise. Noticealso thatthemean
oftheposteriordistribution Px z isslightlytotheleftoftheobservationz becausethe
meanisaweightedaverageofthepredictionandtheobservation.
Thusafteroneupdatecyclewehaveanew Gaussiandistribution forthestatevariable.
Fromthe Gaussianformulain Equation15.19weseethatthenewmeanandstandard
deviation canbecalculated fromtheoldmeanandstandard deviation asfollows:
σ2σ2z σ2μ σ2σ2σ2
μ t x t1 z t and σ2 t x z . 15.20
t1 σ2σ2 σ2 t1 σ2σ2 σ2
t x z t x z
Figure15.10showsoneupdatecycleforparticularvaluesofthetransitionandsensormodels.
Equation 15.20 plays exactly the samerole asthe general filtering equation 15.5 or
the H MMfilteringequation 15.12. Becauseofthespecial natureof Gaussiandistributions
however the equations have some interesting additional properties. First we can interpret
the calculation for the new mean μ as simply a weighted mean of the new observation
t1
z and theold mean μ . Ifthe observation is unreliable then σ2 islarge and wepay more
t1 t z
attention to the old mean; if the old mean is unreliable σ2 is large or the process is highly
t
unpredictable σ2 is large then we pay more attention to the observation. Second notice
x
that the update for the variance σ2 is independent of the observation. We can therefore
t1
compute in advance what the sequence of variance values will be. Third the sequence of
variance values converges quickly to a fixed value that depends only on σ2 and σ2 thereby
x z
substantially simplifying thesubsequent calculations. See Exercise15.12.
The preceding derivation illustrates the key property of Gaussian distributions that allows
Kalmanfiltering towork: thefact that the exponent isaquadratic form. Thisistrue not just
fortheunivariatecase;thefullmultivariate Gaussiandistribution hastheform
""
NμΣx αe 21 xμ cid:4Σ1 xμ .
Multiplying outthetermsintheexponent makesitclearthat theexponent isalsoaquadratic
function of the values x in x. As in the univariate case the filtering update preserves the
i
Gaussiannatureofthestatedistribution.
Letusfirstdefinethegeneraltemporalmodelusedwith Kalmanfiltering. Boththetran-
sition model and the sensor model allow for a linear transformation with additive Gaussian
noise. Thuswehave
Px x N Fx Σ x
t1 t t x t1
15.21
Pz x N Hx Σ z
t t t z t
where F and Σ are matrices describing the linear transition model and transition noise co-
x
varianceand HandΣ arethecorresponding matricesforthesensormodel. Nowthe update
z
equations forthemeanandcovariance intheirfullhairyhorribleness are
μ Fμ K z H Fμ
t1 t t1 t1 t 15.22
Σ I K H FΣ Fcid:12 Σ
t1 t1 t x
where K FΣ Fcid:12 Σ Hcid:12 H FΣ Fcid:12 Σ Hcid:12 Σ 1 iscalledthe Kalmangain
t1 t x t x z
K AL MA NG AI N matrix. Believeitornot these equations make someintuitive sense. Forexample consider
M AT RI X
the update for the mean state estimate μ. The term Fμ is the predicted state at t 1 so
t
H Fμ is the predicted observation. Therefore the term z H Fμ represents the errorin
t t1 t
the predicted observation. This is multiplied by K to correct the predicted state; hence
t1
K isameasureofhowseriously totakethenewobservation relativetotheprediction. As
t1
in Equation 15.20 wealsohavetheproperty thatthevariance update isindependent ofthe
observations. The sequence of values for Σ and K can therefore be computed offline and
t t
theactualcalculations requiredduringonlinetracking arequitemodest.
Toillustrate these equations at work we have applied them to the problem of tracking
anobject moving on the X Y plane. Thestate variables are X X YX Ycid:12 so F Σ
x
H and Σ are 44 matrices. Figure 15.11a shows the true trajectory a series of noisy
z
observations and the trajectory estimated by Kalman filtering along with the covariances
indicated by the one-standard-deviation contours. The filtering process does a good job of
tracking theactualmotionandasexpected thevariancequicklyreaches afixedpoint.
We can also derive equations for smoothing as well as filtering with linear Gaussian
models. Thesmoothing resultsareshownin Figure15.11b. Noticehowthevariance inthe
position estimate issharply reduced except attheends ofthetrajectory why?and thatthe
estimatedtrajectory ismuchsmoother.
The Kalmanfilteranditselaborationsareusedinavastarrayofapplications. Theclassical
application isinradartracking ofaircraftandmissiles. Relatedapplications includeacoustic
tracking of submarines and ground vehicles and visual tracking of vehicles and people. In a
slightly more esoteric vein Kalman filters are used to reconstruct particle trajectories from
bubble-chamber photographs and ocean currents from satellite surface measurements. The
rangeofapplicationismuchlargerthanjustthetrackingofmotion: anysystemcharacterized
by continuous state variables and noisy measurements will do. Such systems include pulp
millschemicalplants nuclearreactors plantecosystems andnational economies.
Section15.4. Kalman Filters 589
2 D filtering 2 D smoothing
true true
observed observed
smoothed smoothed
Y 9 Y 9
a b
showing the true trajectory left to right a series of noisy observations and the trajectory
estimatedby Kalmanfiltering.Varianceinthepositionestimateisindicatedbytheovals.b
Theresultsof Kalmansmoothingforthesameobservationsequence.
The fact that Kalman filtering can be applied to a system does not mean that the re-
sultswillbevalidoruseful. Theassumptions madealinear Gaussiantransition andsensor
E XT EN DE DK AL MA N modelsareverystrong. The extended Kalmanfilter E KFattemptstoovercomenonlin-
F IL TE RE KF
earities in the system being modeled. A system is nonlinear if the transition model cannot
N ON LI NE AR
be described as a matrix multiplication of the state vector as in Equation 15.21. The E KF
worksbymodelingthesystemaslocallylinearinx intheregionofx μ themeanofthe
t t t
current state distribution. This works well forsmooth well-behaved systems and allows the
trackertomaintainandupdatea Gaussianstatedistributionthatisareasonableapproximation
tothetrueposterior. Adetailedexampleisgivenin Chapter 25.
What does it mean for a system to be unsmooth or poorly behaved? Technically
it means that there is significant nonlinearity in system response within the region that is
close according to the covariance Σ to the current mean μ . To understand this idea
t t
in nontechnical terms consider the example of trying to track a bird as it flies through the
jungle. The bird appears to be heading at high speed straight for a tree trunk. The Kalman
filterwhetherregularorextendedcanmakeonlya Gaussian predictionofthelocationofthe
birdandthemeanofthis Gaussianwillbecenteredonthetrunkasshownin Figure15.12a.
Areasonablemodelofthebirdontheotherhandwouldpredictevasiveactiontoonesideor
the other asshown in Figure 15.12b. Sucha model is highly nonlinear because the birds
decision variessharply depending onitsprecise locationrelativetothetrunk.
To handle examples like these we clearly need a more expressive language for repre-
senting thebehaviorofthesystem being modeled. Withinthe control theorycommunity for
which problems such asevasive maneuvering by aircraft raise the same kinds of difficulties
S WI TC HI NG KA LM AN the standard solution isthe switching Kalmanfilter. Inthis approach multiple Kalman fil-
F IL TE R
a b
Figure15.12 Abirdflyingtowardatreetopviews. a A Kalmanfilterwillpredictthe
location of the bird using a single Gaussian centered on the obstacle. b A more realistic
modelallowsforthebirdsevasiveactionpredictingthatitwillflytoonesideortheother.
tersruninparallel eachusingadifferentmodelofthesystemforexampleoneforstraight
flight one forsharp left turns and one for sharp right turns. A weighted sum of predictions
is used where the weight depends on how well each filter fits the current data. We will see
in the next section that this is simply a special case of the general dynamic Bayesian net-
work model obtained by adding a discrete maneuver state variable to the network shown
in Figure15.9. Switching Kalmanfiltersarediscussed furtherin Exercise15.10.
D YN AM IC BA YE SI AN A dynamic Bayesian network or D BN is a Bayesian network that represents a temporal
N ET WO RK
probability model of the kind described in Section 15.1. We have already seen examples of
D BNs: theumbrellanetworkin Figure15.2andthe Kalmanfilternetworkin Figure15.9. In
generaleachsliceofa D BNcanhaveanynumberofstatevariables X andevidencevariables
t
E . For simplicity we assume that the variables and their links are exactly replicated from
t
slice toslice andthat the D BNrepresents afirst-order Markov process sothat eachvariable
canhaveparentsonlyinitsownsliceortheimmediately preceding slice.
It should be clear that every hidden Markov model can be represented as a D BN with
a single state variable and a single evidence variable. It is also the case that every discrete-
variable D BNcanberepresented asan H MM;asexplained in Section15.3wecancombine
all the state variables in the D BN into a single state variable whose values are all possible
tuples of values of the individual state variables. Now if every H MM is a D BN and every
D BN can be translated into an H MM whats the difference? The difference is that by de-
Section15.5. Dynamic Bayesian Networks 591
composingthestateofacomplexsystemintoitsconstituentvariablesthecantakeadvantage
of sparseness in the temporal probability model. Suppose for example that a D BN has 20
Boolean state variables each of which has three parents in the preceding slice. Then the
D BNtransitionmodelhas2023160probabilities whereasthecorresponding H MMhas
is bad for at least three reasons: first the H MM itself requires much more space; second
thehugetransition matrixmakes H MMinference muchmoreexpensive;andthirdtheprob-
lem of learning such a huge number of parameters makes the pure H MM model unsuitable
forlarge problems. Therelationship between D BNsand H MMsis roughly analogous to the
relationship betweenordinary Bayesiannetworksandfulltabulated jointdistributions.
We have already explained that every Kalman filter model can be represented in a
D BN with continuous variables and linear Gaussian conditional distributions Figure 15.9.
Itshouldbeclearfromthediscussion attheendofthepreceding sectionthatnotevery D BN
canberepresented bya Kalmanfiltermodel. Ina Kalmanfilterthecurrentstatedistribution
isalwaysasinglemultivariate Gaussiandistributionthat isasinglebumpinaparticular
location. D BNs on the other hand can model arbitrary distributions. For many real-world
applications this flexibility is essential. Consider for example the current location of my
keys. They might be in my pocket on the bedside table on the kitchen counter dangling
from the front door or locked in the car. A single Gaussian bump that included all these
places would have toallocate significant probability tothe keysbeing inmid-airin thefront
hall. Aspects of the real world such as purposive agents obstacles and pockets introduce
nonlinearities thatrequirecombinationsofdiscreteandcontinuousvariablesinordertoget
reasonable models.
Toconstruct a D BNonemustspecify three kinds ofinformation: thepriordistribution over
thestatevariables P X ;thetransitionmodel P X X ;andthesensormodel P E X .
To specify the transition and sensor models one must also specify the topology of the con-
nections between successive slices and between the state and evidence variables. Because
thetransition andsensormodelsareassumedtobestationarythe sameforalltitismost
convenient simply tospecify them forthe firstslice. Forexample thecomplete D BNspeci-
ficationfortheumbrellaworldisgivenbythethree-node networkshownin Figure15.13a.
Fromthis specification the complete D BNwithanunbounded numberoftimeslices canbe
constructed asneededbycopying thefirstslice.
Let us now consider a more interesting example: monitoring a battery-powered robot
moving in the X Y plane as introduced at the end of Section 15.1. First we need state
variables which will include both X X Y for position and X X Y for velocity.
t t t t t t
We assume some method of measuring positionperhaps a fixed camera or onboard G PS
Global Positioning Systemyielding measurements Z . Theposition atthenext timestep
t
depends on the current position and velocity as in the standard Kalman filter model. The
velocity at the next step depends on the current velocity and the state of the battery. We
add Battery to represent the actual battery charge level which has as parents the previous
t
B Meter
1
R P R
P R 0 1
Rain Rain
R P U
t 0.9
X Xt
0
X
1
f 0.2
Umbrella 1 Z 1
a b
umbrella D BN.Allsubsequentslicesareassumedtobecopiesofslice1. b Asimple D BN
forrobotmotioninthe X Yplane.
batterylevelandthevelocityandweadd B Meter whichmeasuresthebatterychargelevel.
t
Thisgivesusthebasicmodelshownin Figure15.13b.
It is worth looking in more depth at the nature of the sensor model for B Meter . Let
t
us suppose for simplicity that both Battery and B Meter can take on discrete values 0
t t
through 5. Ifthemeterisalwaysaccurate thenthe C PT PB Meter Battery shouldhave
t t
probabilities of 1.0 along the diagonal and probabilities of 0.0elsewhere. In reality noise
always creeps into measurements. For continuous measurements a Gaussian distribution
with a small variance might be used.5 For our discrete variables we can approximate a
Gaussian using a distribution in which the probability of error drops off in the appropriate
way so that the probability of a large error is very small. We use the term Gaussian error
G AU SS IA NE RR OR modeltocoverboththecontinuous anddiscrete versions.
M OD EL
Anyone with hands-on experience of robotics computerized process control or other
formsofautomaticsensingwillreadilytestifytothefactthatsmallamountsofmeasurement
noise are often the least of ones problems. Real sensors fail. When a sensor fails it does
not necessarily send a signal saying Oh by the way the data Im about to send you is a
load of nonsense. Instead it simply sends the nonsense. The simplest kind of failure is
calledatransientfailurewherethesensoroccasionallydecidestosendsomenonsense. For
T RA NS IE NT FA IL UR E
example thebattery levelsensormighthaveahabitofsending azerowhensomeonebumps
therobotevenifthebatteryisfullycharged.
Letsseewhathappenswhenatransientfailureoccurswitha Gaussianerrormodelthat
doesntaccommodatesuchfailures. Supposeforexamplethattherobotissittingquietlyand
observes20consecutivebatteryreadingsof5. Thenthebatterymeterhasatemporaryseizure
tivechargelevels.Thebetadistributionissometimesabetterchoiceforavariablewhoserangeisrestricted.
Section15.5. Dynamic Bayesian Networks 593
andthenextreadingis B Meter 0. Whatwillthesimple Gaussianerrormodelleadusto
21
believe about Battery ? According to Bayes rule the answer depends on both the sensor
21
model P BMeter 0 Battery and the prediction P Battery B Meter . If the
probabilityofalargesensorerrorissignificantlylesslikelythantheprobabilityofatransition
to Battery 0evenifthelatterisveryunlikely thentheposteriordistribution willassign
21
a high probability to the batterys being empty. A second reading of 0 at t22 will make
thisconclusion almostcertain. Ifthetransientfailurethendisappears andthereadingreturns
to 5 from t23 onwards the estimate for the battery level will quickly return to 5 as if by
magic. Thiscourseofeventsisillustratedintheuppercurveof Figure15.14awhichshows
theexpectedvalueof Battery overtimeusingadiscrete Gaussianerrormodel.
t
Despitetherecoverythereisatimet22whentherobotisconvincedthatitsbattery
is empty; presumably then it should send out a mayday signal and shut down. Alas its
oversimplified sensor model has led it astray. How can this be fixed? Consider a familiar
examplefromeverydayhumandriving: onsharpcurvesorsteephillsonesfueltankempty
warninglightsometimesturnson. Ratherthanlookingfortheemergency phone onesimply
recallsthatthefuelgaugesometimesgivesaverylargeerrorwhenthefuelissloshingaround
in the tank. The moral of the story is the following: for the system to handle sensor failure
properly thesensormodelmustincludethepossibility offailure.
The simplest kind of failure model for a sensor allows a certain probability that the
sensor will return some completely incorrect value regardless of the true state of the world.
Forexampleifthebatterymeterfailsbyreturning 0wemightsaythat
P BMeter 0 Battery 50.03
t t
whichispresumably muchlarger thantheprobability assigned bythesimple Gaussian error
T RA NS IE NT FA IL UR E model. Lets call this the transient failure model. How does it help when we are faced
M OD EL
with a reading of 0? Provided that the predicted probability of an empty battery according
to the readings so far is much less than 0.03 then the best explanation of the observation
B Meter 0isthatthesensorhastemporarilyfailed. Intuitively wecanthinkofthebelief
21
aboutthebatterylevelashaving acertainamountofinertia thathelpstoovercometempo-
rary blips in the meter reading. The upper curve in Figure 15.14b shows that the transient
failuremodelcanhandletransient failureswithoutacatastrophic changeinbeliefs.
Somuchfortemporary blips. Whataboutapersistent sensorfailure? Sadlyfailures of
this kind are alltoo common. Ifthesensor returns 20 readings of5followed by20 readings
of 0 then the transient sensor failure model described in the preceding paragraph willresult
in the robot gradually coming to believe that its battery is empty when in fact it may be that
the meter has failed. The lower curve in Figure 15.14b shows the belief trajectory for
this case. By t25five readings of 0the robot is convinced that its battery is empty.
Obviously we would prefer the robot to believe that its battery meter is brokenif indeed
thisisthemorelikelyevent.
P ER SI ST EN T Unsurprisingly to handle persistent failure we need a persistent failure model that
F AI LU RE MO DE L
describes how the sensor behaves under normal conditions and after failure. To do this we
need to augment the state of the system with an additional variable say B MBroken that
describes the status of the battery meter. The persistence of failure must be modeled by an
5
4
3
2
1
0
-1
yretta B E t
E Battery ...5555005555...
t
5
4
3
2
1
0
E Battery ...5555000000...
t
-1
Time step t
yretta B E t
E Battery ...5555005555...
t
E Battery ...5555000000...
t
Time step
a b
Figure15.14 a Uppercurve:trajectoryoftheexpectedvalueof Batterytforanobserva-
tionsequenceconsistingofall5sexceptfor0satt21andt22usingasimple Gaussian
errormodel.Lowercurve:trajectorywhentheobservationremainsat0fromt21onwards.
b Thesameexperimentrunwiththetransientfailuremodel. Noticethatthetransientfail-
ureishandledwellbutthepersistentfailureresultsinexcessivepessimismaboutthebattery
charge.
B
0
P B
1
""
5
t 1.000
f 0.001 4
3
B MBroken B MBroken
2
1
B Meter
1
0
-1
Battery Battery 15 20 25 30
yretta B E t
E Battery ...5555005555...
t
E Battery ...5555000000...
t
P BM Broken ...5555000000...
t
P BM Broken ...5555005555...
t
Time step
a b
Figure15.15 a A DB N fragmentshowingthesensorstatusvariablerequiredformod-
eling persistentfailureof the battery sensor. b Uppercurves: trajectoriesof the expected
valueof Batterytforthetransientfailureandpermanentfailureobservationssequences.
Lowercurves:probabilitytrajectoriesfor B MBroken giventhetwoobservationsequences.
arc linking B MBroken to B MBroken . This persistence arc has a C PTthat gives asmall
P ER SI ST EN CE AR C 0 1
probability of failure in any given time step say 0.001 but specifies that the sensor stays
broken once it breaks. When the sensor is O K the sensor model for B Meter is identical to
thetransientfailuremodel;whenthesensorisbrokenitsays B Meter isalways0regardless
oftheactualbatterycharge.
Section15.5. Dynamic Bayesian Networks 595
P R 0 R t0 P 0 R .71 P R 0 R t0 P 0 R .71 R t1 P 0 R .72 R t2 P 0 R .73 R t3 P 0 R .74
Rain Rain Rain Rain Rain Rain Rain
Umbrella 1 Umbrella 1 Umbrella 2 Umbrella 3 Umbrella 4
R 1 P U 1 R 1 P U 1 R 2 P U 2 R 3 P U 3 R 4 P U 4
t 0.9 t 0.9 t 0.9 t 0.9 t 0.9
f 0.2 f 0.2 f 0.2 f 0.2 f 0.2
Figure15.16 Unrollingadynamic Bayesiannetwork: slicesarereplicatedtoaccommo-
datetheobservationsequence Umbrella .Furthersliceshavenoeffectoninferenceswithin
1:3
theobservationperiod.
The persistent failure model for the battery sensor is shown in Figure 15.15a. Its
performance on the two data sequences temporary blip and persistent failure is shown in
of the temporary blip the probability that the sensor is broken rises significantly after the
second 0 reading but immediately drops back to zero once a 5 is observed. Second in the
case of persistent failure the probability that the sensor is broken rises quickly to almost 1
and stays there. Finally once the sensor is known to be broken the robot can only assume
thatitsbatterydischargesatthenormalrateasshownby thegraduallydescending levelof
E Battery ....
t
So far we have merely scratched the surface of the problem of representing complex
processes. The variety of transition models is huge encompassing topics as disparate as
modeling thehumanendocrine system andmodelingmultiple vehicles driving onafreeway.
Sensor modeling is also a vast subfield in itself but even subtle phenomena such as sensor
drift sudden decalibration and the effects of exogenous conditions such as weather on
sensorreadingscanbehandledbyexplicitrepresentation withindynamic Bayesiannetworks.
Having sketched some ideas for representing complex processes as D BNs we now turn to
the question of inference. In a sense this question has already been answered: dynamic
Bayesian networks are Bayesian networks and we already have algorithms for inference in
Bayesian networks. Given a sequence of observations one can construct the full Bayesian
network representation of a D BN by replicating slices until the network is large enough to
accommodate theobservations asin Figure15.16. Thistechnique mentioned in Chapter14
inthecontext ofrelational probability models iscalled unrolling. Technically the D BNis
equivalent to the semi-infinite network obtained by unrolling forever. Slices added beyond
the last observation have no effect on inferences within the observation period and can be
omitted. Once the D BN is unrolled one can use any of the inference algorithmsvariable
elimination clustering methodsandsoondescribed in Chapter14.
Unfortunately a naive application of unrolling would not be particularly efficient. If
we want to perform filtering or smoothing with a long sequence of observations e the
1:t
unrolled network would require Ot space and would thus grow without bound as more
observations were added. Moreover if we simply run the inference algorithm anew each
timeanobservation isaddedtheinference timeperupdatewillalsoincreaseas Ot.
Lookingbackto Section15.2.1weseethatconstanttimeandspaceperfilteringupdate
can be achieved if the computation can be done recursively. Essentially the filtering update
in Equation 15.5 works by summing out the state variables ofthe previous time step to get
the distribution for the new time step. Summing out variables is exactly what the variable
elimination Figure14.11algorithm doesanditturnsoutthatrunning variable elimination
with the variables in temporal order exactly mimics the operation of the recursive filtering
update in Equation 15.5. The modified algorithm keeps at most two slices in memory at
anyonetime: startingwithslice0weaddslice1thensumoutslice0thenaddslice2then
sum out slice 1 and soon. In this way wecan achieve constant space and timeperfiltering
update. The same performance can be achieved by suitable modifications to the clustering
algorithm. Exercise15.17asksyoutoverifythisfactfortheumbrellanetwork.
Somuch for the good news; now forthe bad news: It turns out that the constant for
theper-updatetimeandspacecomplexityisinalmostallcasesexponentialinthenumberof
state variables. What happens is that as the variable elimination proceeds the factors grow
toincludeallthestatevariablesormoreprecisely allthosestatevariablesthathaveparents
intheprevioustimeslice. Themaximumfactorsizeis Odnkandthetotalupdatecostper
stepis Ondnkwheredisthedomainsizeofthevariablesandk isthemaximumnumber
ofparents ofanystatevariable.
Of course this is much less than the cost of H MM updating which is Od2n but it
is still infeasible for large numbers of variables. This grim fact is somewhat hard to accept.
What it means is that even though we can use D BNs to represent very complex temporal
processes with many sparsely connected variables we cannot reason efficiently and exactly
about those processes. The D BN model itself which represents the prior joint distribution
overall the variables is factorable into its constituent C PTs but the posterior joint distribu-
tionconditioned onanobservation sequencethat istheforwardmessageisgenerally not
factorable. So far no one has found a way around this problem despite the fact that many
importantareasofscienceandengineeringwouldbenefitenormouslyfromitssolution. Thus
wemustfallbackonapproximate methods.
Section 14.5 described two approximation algorithms: likelihood weighting Figure 14.15
and Markovchain Monte Carlo M CM CFigure14.16. Ofthetwotheformerismosteasily
adapted tothe D BNcontext. An M CM Cfiltering algorithm isdescribed brieflyinthenotes
attheendofthechapter. Wewillseehoweverthatseveral improvementsarerequired over
thestandard likelihood weightingalgorithm beforeapracticalmethodemerges.
Recall that likelihood weighting works by sampling the nonevidence nodes of the net-
workintopologicalorderweightingeachsamplebythelikelihooditaccordstotheobserved
evidence variables. As with the exact algorithms we could apply likelihood weighting di-
rectly to an unrolled D BNbut this would suffer from the same problems of increasing time
Section15.5. Dynamic Bayesian Networks 597
and space requirements per update as the observation sequence grows. The problem is that
the standard algorithm runs each sample in turn all the way through the network. Instead
we can simply run all N samples together through the D BN one slice at a time. The mod-
ified algorithm fits the general pattern of filtering algorithms with the set of N samples as
the forward message. The first key innovation then is to use the samples themselves as an
approximaterepresentation ofthecurrentstatedistribution. Thismeetstherequirementofa
constanttimeperupdatealthoughtheconstantdependsonthenumberofsamplesrequired
tomaintainanaccurateapproximation. Thereisalsononeed tounrollthe D BNbecause we
needtohaveinmemoryonlythecurrent sliceandthenextslice.
In our discussion of likelihood weighting in Chapter 14 we pointed out that the al-
gorithms accuracy suffers if the evidence variables are downstream from the variables
being sampled because in that case the samples are generated without any influence from
the evidence. Looking at the typical structure of a D BNsay the umbrella D BN in Fig-
ure15.16weseethatindeedtheearlystatevariableswillbesampledwithoutthebenefitof
thelaterevidence. Infact looking morecarefully weseethatnoneofthestatevariables has
anyevidence variables amongitsancestors! Hence although theweightofeachsamplewill
depend on the evidence the actual set of samples generated will be completely independent
of the evidence. For example even if the boss brings in the umbrella every day the sam-
pling process could stillhallucinate endless days ofsunshine. Whatthismeansinpractice is
that the fraction of samples that remain reasonably close to the actual series of events and
therefore have nonnegligible weights drops exponentially with t the length of the observa-
tionsequence. Inotherwords tomaintain agivenlevelofaccuracy weneed toincrease the
number of samples exponentially with t. Given that a filtering algorithm that works in real
timecanuseonlyafixednumberofsampleswhathappensinpracticeisthattheerrorblows
upafteraverysmallnumberofupdatesteps.
Clearly we need a better solution. The second key innovation is to focus the set of
samples on the high-probability regions of the state space. This can be done by throwing
away samples that have very low weight according to the observations while replicating
thosethathavehighweight. Inthatwaythepopulationofsampleswillstayreasonablyclose
toreality. Ifwethinkofsamplesasaresource formodelingtheposteriordistribution thenit
makessensetousemoresamplesinregionsofthestatespacewheretheposteriorishigher.
A family of algorithms called particle filtering is designed to do just that. Particle
P AR TI CL EF IL TE RI NG
filteringworksasfollows: Firstapopulationof N initial-statesamplesiscreatedbysampling
fromthepriordistribution P X . Thentheupdate cycleisrepeated foreachtimestep:
0
1. Each sample is propagated forward by sampling the next state value x given the
t1
currentvalue x forthesamplebasedonthetransition model P X x .
t t1 t
2. Eachsampleisweightedbythelikelihooditassignstothenewevidence Pe x .
t1 t1
3. The population is resampled to generate a new population of N samples. Each new
sample isselected from the current population; theprobability that aparticular sample
isselectedisproportional toitsweight. Thenewsamplesareunweighted.
The algorithm is shown in detail in Figure 15.17 and its operation for the umbrella D BNis
illustrated in Figure15.18.
function P AR TI CL E-F IL TE RI NGe Ndbnreturnsasetofsamplesforthenexttimestep
inputs:ethenewincomingevidence
Nthenumberofsamplestobemaintained
dbna D BNwithprior P X transitionmodel P X X sensormodel P E X
persistent: Savectorofsamplesofsize Ninitiallygeneratedfrom P X
0
localvariables: Wavectorofweightsofsize N
fori 1to N do
Sisamplefrom P X X Si step1
Wi Pe X Si step2
1
S WE IG HT ED-S AM PL E-W IT H-R EP LA CE ME NT NS W step3
return S
eration with state the set of samples. Each of the sampling operations involves sam-
pling the relevant slice variables in topological order much as in P RI OR-S AM PL E. The
W EI GH TE D-S AM PL E-W IT H-R EP LA CE ME NToperationcanbeimplementedtorunin O N
expectedtime. Thestepnumbersrefertothedescriptioninthetext.
Rain Rain Rain Rain
t t1 t1 t1
true
false
a Propagate b Weight c Resample
Figure15.18 Theparticlefilteringupdatecyclefortheumbrella D BNwith N10show-
ingthesamplepopulationsofeachstate. a Attimet8samplesindicaterain and2indicate
rain. Eachispropagatedforwardbysamplingthenextstatethroughthetransitionmodel.
Attimet1 6samplesindicaterain and4indicaterain. bumbrella isobservedat
t1. Eachsampleisweightedbyitslikelihoodfortheobservationasindicatedbythesize
ofthecircles. c Anewsetof10samplesisgeneratedbyweightedrandomselectionfrom
thecurrentsetresultingin2samplesthatindicaterain and8thatindicaterain.
Wecanshowthatthisalgorithmisconsistentgivesthecorrectprobabilitiesas N tends
toinfinitybyconsideringwhathappensduringoneupdatecycle. Weassumethatthesample
population starts with a correct representation of the forward message f P X e at
1:t t 1:t
time t. Writing Nx e for the number of samples occupying state x after observations
t 1:t t
e havebeenprocessed wetherefore have
1:t
Nx e N Px e 15.23
t 1:t t 1:t
forlarge N. Nowwepropagateeachsampleforwardbysamplingthestatevariablesatt1
given the values for the sample at t. The number of samples reaching state x from each
t1
Section15.6. Keeping Trackof Many Objects 599
x isthetransition probability timesthepopulation of x ;hence thetotalnumberofsamples
t t
reaching x is
t1 cid:12
Nx e Px x Nx e .
t1 1:t t1 t t 1:t
xt
Nowweweighteachsamplebyitslikelihoodfortheevidenceatt1. Asampleinstatex
t1
receives weight Pe x . The total weight of the samples in x after seeing e is
t1 t1 t1 t1
therefore
Wx e Pe x Nx e .
t1 1:t1 t1 t1 t1 1:t
Now for the resampling step. Since each sample is replicated with probability proportional
toitsweightthenumberofsamplesinstatex afterresampling isproportional tothetotal
t1
weightinx beforeresampling:
t1
Nx e N α Wx e
t1 1:t1 t1 1:t1
α Pe x Nx e
t1 t1 cid:12t1 1:t
α Pe x Px x Nx e
t1 t1 t1 t t 1:t
xtcid:12
α NPe x Px x Px e by15.23
t1 t1 t1 t t 1:t
cid:12xt
αcid:2 Pe x Px x Px e
t1 t1 t1 t t 1:t
xt
Px e by15.5.
t1 1:t1
Thereforethesamplepopulationafteroneupdatecyclecorrectlyrepresentstheforwardmes-
sageattimet1.
Particlefiltering is consistent therefore butisit efficient? Inpractice itseemsthatthe
answerisyes: particle filteringseemstomaintain agoodapproximation tothetrueposterior
usingaconstantnumberofsamples. Undercertainassumptionsinparticular thattheprob-
abilities in the transition and sensor models are strictly greater than 0 and less than 1it is
possible to prove that the approximation maintains bounded error with high probability. On
the practical side the range of applications has grown to include many fields of science and
engineering; somereferences aregivenattheendofthechapter.
The preceding sections have consideredwithout mentioning itstate estimation problems
involving a single object. In this section we see what happens when two or more objects
generate the observations. What makes this case different from plain old state estimation is
that there is now thepossibility of uncertainty about which object generated which observa-
tion. Thisistheidentityuncertaintyproblemof Section14.6.3page544nowviewedina
temporal context. Inthecontrol theory literature this is the dataassociation problemthat
D AT AA SS OC IA TI ON
istheproblem ofassociating observation datawiththeobjectsthatgenerated them.
a b
track termination
detection
failure
track
c d
Figure15.19 a Observationsmadeofobjectlocationsin2 Dspaceoverfivetimesteps.
Eachobservationislabeledwiththetimestepbutdoesnotidentifytheobjectthatproduced
it. bc Possible hypothesesabout the underlyingobject tracks. d A hypothesisfor the
caseinwhichfalsealarmsdetectionfailuresandtrackinitiationterminationarepossible.
The data association problem was studied originally in the context of radar tracking
wherereflectedpulsesaredetectedatfixedtimeintervalsbyarotatingradarantenna. Ateach
timestepmultipleblipsmayappearonthescreenbutthereisnodirectobservationofwhich
blips at timet belong to which blips at timet1. Figure 15.19a shows a simple example
with two blips pertime step forfive steps. Let the two blip locations at time t be e1 and e2.
t t
Thelabelingofblipswithinatimestepas1and2iscompletelyarbitraryandcarriesno
information. Letusassumeforthetimebeingthatexactlytwoaircraft Aand Bgenerated
theblips; theirtrue positions are X A and X B. Just tokeep things simple well alsoassume
t t
that the each aircraft moves independently according to a known transition modele.g. a
linear Gaussianmodelasusedinthe Kalmanfilter Section15.4.
Suppose we try to write down the overall probability model for this scenario just as
we did for general temporal processes in Equation 15.3 on page 569. As usual the joint
distribution factorsintocontributions foreachtimestep asfollows:
Px A x B e1 e2
0:t 0:t 1:t 1:t
cid:25t
Px A Px B Px Ax A Px Bx B Pe1e2x Ax B. 15.24
i1
Wewould like tofactor the observation term Pe1e2x Ax Binto aproduct oftwoterms
i i i i
one for each object but this would require knowing which observation was generated by
whichobject. Instead wehavetosumoverallpossible waysofassociating theobservations
Section15.6. Keeping Trackof Many Objects 601
with the objects. Some of those ways are shown in Figure 15.19bc; in general for n
objectsand T timestepsthereare n!T waysofdoingitanawfullylargenumber.
Mathematically speaking the way of associating the observations with the objects
is a collection of unobserved random variable that identify the source of each observation.
Wellwriteω todenote theone-to-one mappingfromobjects toobservations attimetwith
t
ω A and ω B denoting the specific observations 1 or 2 that ω assigns to A and B.
t t t
For n objects ω will have n! possible values; here n!2. Because the labels 1 ad
t
2 on the observations are assigned arbitrarily the prior on ω is uniform and ω is inde-
t t
pendent of the states of the objects x A and x B. So we can condition the observation term
t t
Pe1e2x Ax Bonω andthensimplify:
i i i i t cid:12
Pe1e2x Ax B Pe1e2x Ax Bω Pω x Ax B
i i i i i i i i i i i i
cid:12ωi
Peωi Ax A Peωi Bx B Pω x Ax B
i i i i i i i
ωi
cid:12
1
""
Peωi Ax A Peωi Bx B.
ωi
Plugging this into Equation 15.24 we get an expression that is only in terms of transition
andsensormodelsforindividual objects andobservations.
As for all probability models inference means summing out the variables other than
the query and the evidence. Forfiltering in H MMs and D BNs wewere able to sum out the
statevariablesfrom1tot1byasimpledynamicprogrammingtrick;for Kalmanfilterswe
tookadvantageofspecialpropertiesof Gaussians. Fordataassociation wearelessfortunate.
There is no known efficient exact algorithm for the same reason that there is none for the
switching Kalman filter page 589: the filtering distribution Px Ae1 e2 for object A
t 1:t 1:t
ends up as a mixture of exponentially many distributions one for each way of picking a
sequence ofobservations toassignto A.
As a result of the complexity of exact inference many different approximate methods
have been used. The simplest approach is to choose a single best assignment at each time
step given the predicted positions of the objects at the current time step. This assignment
associates observations with objects and enables the track of each object to be updated and
a prediction made for the next time step. Forchoosing the best assignment it is common
N EA RE ST-N EI GH BO R to use the so-called nearest-neighbor filter which repeatedly chooses the closest pairing
F IL TE R
of predicted position and observation and adds that pairing to the assignment. The nearest-
neighborfilterworkswellwhentheobjectsarewellseparatedinstatespaceandtheprediction
uncertainty and observation error are smallin other words when there is no possibility of
confusion. When there is more uncertainty as to the correct assignment a better approach
is to choose the assignment that maximizes the joint probability of the current observations
given the predicted positions. This can be done very efficiently using the Hungarian algo-
H UN GA RI AN rithm Kuhn1955 eventhoughtherearen!assignments tochoosefrom.
A LG OR IT HM
Anymethod that commits toasingle best assignment ateach timestep fails miserably
under more difficult conditions. In particular if the algorithm commits to an incorrect as-
signment the prediction at the next time step may be significantly wrong leading to more
a b
Figure15.20 Imagesfromaupstreamandbdownstreamsurveillancecamerasroughly
two miles apart on Highway 99 in Sacramento California. The boxed vehicle has been
identifiedatbothcameras.
incorrect assignments and so on. Two modern approaches turn out to be much more effec-
tive. Aparticlefilteringalgorithm seepage598fordataassociation worksbymaintaining
alarge collection ofpossible current assignments. An M CM Calgorithm explores thespace
ofassignment historiesfor example Figure15.19bc mightbestatesinthe M CM Cstate
spaceand can change its mind about previous assignment decisions. Current M CM Cdata
association methods can handle many hundreds of objects in real time while giving a good
approximation tothetrueposteriordistributions.
The scenario described so far involved n known objects generating n observations at
each time step. Real application of data association are typically much more complicated.
Oftenthereported observations include falsealarmsalsoknownasclutterwhicharenot
F AL SE AL AR M
causedbyrealobjects. Detectionfailurescanoccurmeaningthatnoobservationisreported
C LU TT ER
forarealobject. Finallynewobjectsarriveandoldonesdisappear. Thesephenomenawhich
D ET EC TI ON FA IL UR E
createevenmorepossible worldstoworryabout areillustrated in Figure15.19d.
Figure15.20showstwoimagesfromwidelyseparatedcamerasona Californiafreeway.
Inthis application weareinterested intwogoals: estimating thetimeittakes undercurrent
traffic conditions to go from one place to another in the freeway system; and measuring
demand i.e. how many vehicles travel between any two points in the system at particular
times of the day and on particular days of the week. Both goals require solving the data
association problem over a wide area with many cameras and tens of thousands of vehicles
per hour. With visual surveillance false alarms are caused by moving shadows articulated
vehiclesreflectionsinpuddlesetc.;detectionfailuresarecausedbyocclusionfogdarkness
and lack of visual contrast; and vehicles are constantly entering and leaving the freeway
system. Furthermore the appearance of any given vehicle can change dramatically between
cameras depending on lighting conditions and vehicle pose in the image and the transition
modelchangesastrafficjamscomeandgo. Despitetheseproblemsmoderndataassociation
algorithms havebeensuccessful inestimating trafficparameters inreal-world settings.
16
M AK IN G S IM PL E
D EC IS IO NS
Inwhichweseehowanagentshouldmakedecisionssothatitgetswhatitwants
onaverage atleast.
Inthischapter wefillinthedetailsofhowutilitytheorycombineswithprobability theoryto
yield adecision-theoretic agentan agent that can make rational decisions based on whatit
believesandwhatitwants. Suchanagentcanmakedecisionsincontextsinwhichuncertainty
and conflicting goals leave a logical agent with no way to decide: a goal-based agent has a
binary distinction between good goal and bad non-goal states while a decision-theoretic
agenthasacontinuous measureofoutcomequality.
Section 16.1 introduces the basic principle of decision theory: the maximization of
expected utility. Section 16.2 shows that the behavior of any rational agent can be captured
bysupposing autility function thatisbeingmaximized. Section 16.3discusses thenatureof
utilityfunctionsinmoredetailandinparticulartheirrelationtoindividualquantitiessuchas
money. Section16.4showshowtohandleutilityfunctions thatdependonseveralquantities.
In Section 16.5 we describe the implementation of decision-making systems. In particular
weintroduce aformalism called a decision network also known asaninfluencediagram
that extends Bayesian networks by incorporating actions and utilities. The remainder of the
chapterdiscusses issuesthatariseinapplications ofdecisiontheorytoexpertsystems.
Decision theory initssimplest form deals withchoosing among actions based on thedesir-
abilityoftheirimmediateoutcomes; thatistheenvironment isassumedtobeepisodic inthe
sense defined on page 43. Thisassumption isrelaxed in Chapter 17. In Chapter3weused
the notation R ES UL Ts 0a for the state that is the deterministic outcome of taking action a
in state s . In this chapter wedeal withnondeterministic partially observable environments.
0
Sincetheagentmaynotknowthecurrentstateweomititanddefine R ES UL Taasarandom
cid:2
variable whose values arethepossible outcome states. Theprobability of outcome s given
evidence observations eiswritten
P RE SU LTascid:2ae
610
Section16.2. The Basisof Utility Theory 611
wheretheaontheright-handsideoftheconditioning barstandsforthe eventthatactionais
executed.1
Theagentspreferencesarecapturedbyautilityfunction Uswhichassignsasingle
U TI LI TY FU NC TI ON
numbertoexpressthedesirability ofastate. The expected utilityofanactiongiventheevi-
E XP EC TE DU TI LI TY
dence E Uaeisjusttheaverageutilityvalueoftheoutcomesweightedbytheprobability
thattheoutcomeoccurs:
cid:12
E Uae P RE SU LTascid:2ae Uscid:2 . 16.1
scid:3
M AX IM UM EX PE CT ED Theprinciple ofmaximumexpectedutility M EUsaysthatarational agentshouldchoose
U TI LI TY
theactionthatmaximizestheagents expectedutility:
action argmax E Uae
a
Inasensethe M EUprinciplecouldbeseenasdefiningallof A I.Allanintelligent agenthas
to do is calculate the various quantities maximize utility over its actions and away it goes.
Butthisdoesnotmeanthatthe A Iproblemissolvedbythedefinition!
The M EU principle formalizes the general notion that the agent should do the right
thing but goes only a small distance toward a full operationalization of that advice. Es-
timating the state of the world requires perception learning knowledge representation and
inference. Computing P RE SU LTaae requires a complete causal model of the world
andaswesawin Chapter14 NP-hardinferenceinverylarge Bayesiannetworks. Comput-
cid:2
ing the outcome utilities Us often requires searching or planning because an agent may
not know how good astate is until itknows where it can get to from that state. So decision
theoryisnotapanacea thatsolvesthe A Iproblembut itdoes provideausefulframework.
The M EUprinciplehasaclearrelationtotheideaofperformancemeasuresintroduced
in Chapter 2. The basic idea is simple. Consider the environments that could lead to an
agent having a given percept history and consider the different agents that wecould design.
If an agent acts so as to maximize a utility function that correctly reflects the performance
measure then the agent will achieve the highest possible performance score averaged over
all the possible environments. This is the central justification for the M EU principle itself.
While the claim may seem tautological it does in fact embody a very important transition
from a global external criterion of rationalitythe performance measure over environment
historiestoalocalinternalcriterioninvolvingthemaximizationofautilityfunctionapplied
tothenextstate.
Intuitively the principle of Maximum Expected Utility M EU seems like a reasonable way
to make decisions but it is by no means obvious that it is the only rational way. After all
why should maximizing the average utility be so special? Whats wrong with an agent that
P RE SU LTascid:3ae s P RE SU LTsascid:3a P S0se.
maximizes the weighted sum of the cubes of the possible utilities or tries to minimize the
worst possible loss? Could an agent act rationally just by expressing preferences between
states without giving them numeric values? Finally why should a utility function with the
required properties existatall? Weshallsee.
Thesequestions canbeansweredbywritingdownsomeconstraints onthepreferences thata
rational agentshouldhaveandthenshowingthatthe M EUprinciple canbederivedfromthe
constraints. Weusethefollowingnotation todescribe anagentspreferences:
A B theagentprefers Aover B.
A B theagentisindifferent between Aand B.
""
A B theagentprefers Aover B orisindifferent betweenthem.
Now the obvious question is what sorts of things are A and B? They could be states of the
world but more often than not there is uncertainty about what is really being offered. For
example an airline passenger who is offered the pasta dish or the chicken does not know
whatlurksbeneath thetinfoilcover.2 Thepastacouldbedelicious orcongealed thechicken
juicyorovercooked beyondrecognition. Wecanthinkofthesetofoutcomes foreachaction
asalotterythink ofeachactionasaticket. Alottery Lwithpossibleoutcomes S ...S
L OT TE RY 1 n
thatoccurwithprobabilities p ...p iswritten
L p S ; p S ; ... p S .
Ingeneral each outcome S ofalottery canbeeither anatomicstate oranother lottery. The
i
primary issue for utility theory is to understand how preferences between complex lotteries
are related to preferences between the underlying states in those lotteries. To address this
issuewelistsixconstraints thatwerequire anyreasonable preference relationtoobey:
Orderability: Given any two lotteries a rational agent must either prefer one to the
O RD ER AB IL IT Y
otherorelseratethetwoasequallypreferable. Thatisthe agentcannotavoiddeciding.
Aswesaidonpage490refusing tobetislikerefusing toallowtimetopass.
Exactlyoneof A B B A or A Bholds.
Transitivity: Givenany three lotteries ifan agent prefers Ato B and prefers B to C
T RA NS IT IV IT Y
thentheagentmustprefer Ato C.
A B B C A C.
Continuity: If some lottery B is between A and C in preference then there is some
C ON TI NU IT Y
probability pforwhichtherationalagentwillbeindifferentbetweengetting B forsure
andthelotterythatyields Awithprobability pand C withprobability 1p.
A B C p p A; 1p C B .
Substitutability: If an agent is indifferent between two lotteries A and B then the
S UB ST IT UT AB IL IT Y
agentisindifferentbetweentwomorecomplexlotteriesthatarethesameexceptthat B
Section16.2. The Basisof Utility Theory 613
is substituted for A in one of them. This holds regardless of the probabilities and the
otheroutcomes inthelotteries.
A B p A; 1p C p B;1p C.
Thisalsoholdsifwesubstitute forinthisaxiom.
Monotonicity: Suppose two lotteries have the sametwo possible outcomes Aand B.
M ON OT ON IC IT Y
If an agent prefers A to B then the agent must prefer the lottery that has a higher
probability for Aandviceversa.
A B p q p A; 1p B q A; 1q B.
Decomposability: Compound lotteries can be reduced to simpler ones using the laws
D EC OM PO SA BI LI TY
of probability. This has been called the no fun in gambling rule because it says that
two consecutive lotteries can be compressed into a single equivalent lottery as shown
in Figure16.1b.3
p A; 1pq B; 1q C p A; 1pq B; 1p1q C.
These constraints are known as the axioms of utility theory. Each axiom can be motivated
by showing that an agent that violates it will exhibit patently irrational behavior in some
situations. Forexample we can motivate transitivity by making an agent with nontransitive
preferences give us all its money. Suppose that the agent has the nontransitive preferences
A B C A where A B and C are goods that can be freely exchanged. If the agent
currently has A then we could offer to trade C for A plus one cent. The agent prefers C
andsowould bewilling tomakethistrade. Wecould then offer totrade B for Cextracting
another cent and finally trade A for B. This brings us back where we started from except
that theagent hasgiven usthree cents Figure16.1a. Wecankeep going around thecycle
untiltheagenthasnomoneyatall. Clearly theagenthasactedirrationally inthiscase.
Noticethattheaxiomsofutilitytheoryarereallyaxiomsaboutpreferencestheysaynothing
about a utility function. But in fact from the axioms of utility we can derive the following
consequences fortheproof seevon Neumannand Morgenstern 1944:
Existenceof Utility Function: Ifanagentspreferencesobeytheaxiomsofutilitythen
there exists a function U such that U A U B if and only if A is preferred to B
and U A U Bifandonlyiftheagentisindifferent between Aand B.
U A U B A B
U A U B A B
Expected Utility of a Lottery: Theutility of a lottery is the sum of the probability of
eachoutcometimestheutilityofthatoutcome.
cid:12
Up S ;...;p S p U S .
i
example Have10andgambledcouldbepreferredto Have10anddidntgamble.
A
p
A B
q
1 1
1p
1q
C
is equivalent to
A
B C p
1pq
B
1
1p1q C
a b
B C Aresultinirrationalbehavior.b Thedecomposabilityaxiom.
Inotherwordsoncetheprobabilitiesandutilitiesofthepossibleoutcomestatesarespecified
theutilityofacompoundlotteryinvolvingthosestatesiscompletelydetermined. Becausethe
outcomeofanondeterministic actionisalotteryitfollowsthatanagentcanactrationally
thatisconsistentlywithitspreferencesonlybychoosinganactionthatmaximizesexpected
utilityaccording to Equation16.1.
Theprecedingtheoremsestablishthatautilityfunctionexistsforanyrationalagentbut
theydonotestablish thatitisunique. Itiseasytoseeinfactthatanagentsbehaviorwould
notchangeifitsutilityfunction U Sweretransformed according to
cid:2
U S a U Sb 16.2
where a and b are constants and a 0; an affine transformation.4 This fact was noted in
Chapter5fortwo-playergamesofchance;hereweseethatit iscompletelygeneral.
As in game-playing in a deterministic environment an agent just needs a preference
ranking on statesthe numbers dont matter. This is called a value function or ordinal
V AL UE FU NC TI ON
O RD IN AL UT IL IT Y utilityfunction.
F UN CT IO N
It is important to remember that the existence of a utility function that describes an
agentspreferencebehaviordoesnotnecessarilymeanthat theagentisexplicitlymaximizing
thatutilityfunctioninitsowndeliberations. Asweshowedin Chapter2rationalbehaviorcan
be generated in any number of ways. By observing a rational agents preferences however
anobservercanconstruct theutilityfunction thatrepresents whattheagentisactually trying
toachieveeveniftheagentdoesnt knowit.
plus32.Yougetthesameresultsineithermeasurementsystem.
Section16.3. Utility Functions 615
Utilityisafunctionthatmapsfromlotteriestorealnumbers. Weknowtherearesomeaxioms
on utilities that all rational agents must obey. Is that all we can say about utility functions?
Strictlyspeaking thatisit: anagentcanhaveanypreferences itlikes. Forexampleanagent
mightprefertohaveaprimenumberofdollarsinitsbankaccount;inwhichcaseifithad16
itwouldgiveaway3. Thismightbeunusual butwecant callitirrational. Anagentmight
prefer adented 1973 Ford Pinto toa shiny new Mercedes. Preferences can also interact: for
example the agent might prefer prime numbers of dollars only when it owns the Pinto but
whenitownsthe Mercedesitmightprefermoredollarstofewer. Fortunatelythepreferences
ofrealagentsareusually moresystematic andthuseasiertodealwith.
If we want to build a decision-theoretic system that helps the agent make decisions or acts
onhisorherbehalf wemustfirstworkoutwhattheagentsutilityfunction is. Thisprocess
P RE FE RE NC E often called preference elicitation involves presenting choices to the agent and using the
E LI CI TA TI ON
observed preferences topindowntheunderlying utilityfunction.
Equation16.2saysthatthereisnoabsolutescaleforutilitiesbutitishelpfulnonethe-
lesstoestablishsomescaleonwhichutilitiescanberecordedandcomparedforany particu-
larproblem. Ascalecanbeestablished byfixingtheutilitiesofanytwoparticularoutcomes
just as we fix a temperature scale by fixing the freezing point and boiling point of water.
Typically we fix the utility of a best possible prize at U S ucid:12 and a worst possible
N OR MA LI ZE D catastrophe at U S u. Normalizedutilitiesuseascalewithu 0anducid:12 1.
U TI LI TI ES
Given a utility scale between ucid:12 and u we can assess the utility of any particular
S TA ND AR DL OT TE RY
prize S byaskingtheagenttochoosebetween S andastandardlotterypucid:12; 1pu.
Theprobability pisadjusteduntiltheagentisindifferentbetween S andthestandardlottery.
Assumingnormalizedutilitiestheutilityof S isgivenbyp. Oncethisisdoneforeachprize
theutilitiesforalllotteriesinvolving thoseprizesaredetermined.
In medical transportation and environmental decision problems among others peo-
pleslivesareatstake. Insuchcasesu isthevalueassignedtoimmediatedeathorperhaps
many deaths. Although nobody feels comfortable withputting avalue on human life itisa
fact that tradeoffs are made all the time. Aircraft are given a complete overhaul at intervals
determined by trips and miles flown rather than after every trip. Cars are manufactured in
a way that trades off costs against accident survival rates. Paradoxically a refusal to put a
monetary value on life means that life is often undervalued. Ross Shachter relates an ex-
perience with a government agency that commissioned a study on removing asbestos from
schools. Thedecisionanalystsperformingthestudyassumedaparticulardollarvalueforthe
life of a school-age child and argued that the rational choice under that assumption was to
remove the asbestos. The agency morally outraged at the idea of setting the value of a life
rejectedthereportoutofhand. Itthendecidedagainstasbestosremovalimplicitlyasserting
alowervalueforthelifeofachildthanthatassigned bytheanalysts.
Some attempts have been made to find out the value that people place on their own
lives. One common currency used in medical and safety analysis is the micromort a
M IC RO MO RT
one in a million chance of death. If you ask people how much they would pay to avoid a
riskfor example to avoid playing Russian roulette with a million-barreled revolverthey
will respond with very large numbers perhaps tens of thousands of dollars but their actual
behaviorreflectsamuchlowermonetaryvalueforamicromort. Forexampledrivinginacar
for230 miles incurs arisk of one micromort; overthe life of yourcarsay 92000 miles
thats 400 micromorts. People appear to be willing to pay about 10000 at 2009 prices
more for a safer car that halves the risk of death or about 50 per micromort. A number
of studies have confirmed a figure in this range across many individuals and risk types. Of
course this argument holds only forsmallrisks. Mostpeople wont agree tokillthemselves
for50million.
Another measure is the Q AL Yor quality-adjusted life year. Patients with a disability
Q AL Y
are willing to accept a shorter life expectancy to be restored to full health. For example
kidneypatientsonaverageareindifferentbetweenlivingtwoyearsonadialysismachineand
oneyearatfullhealth.
Utility theory has its roots in economics and economics provides one obvious candidate
for a utility measure: money or more specifically an agents total net assets. The almost
universal exchangeability of money for all kinds of goods and services suggests that money
playsasignificantroleinhumanutilityfunctions.
Itwillusuallybethecasethatanagentprefersmoremoneytolessallotherthingsbeing
M ON OT ON IC equal. We say that the agent exhibits a monotonic preference for more money. This does
P RE FE RE NC E
notmeanthatmoneybehaves asautility function because itsaysnothing aboutpreferences
betweenlotteriesinvolving money.
Supposeyouhavetriumphedovertheothercompetitorsinatelevisiongameshow. The
host now offers you achoice: either you can take the 1000000 prize oryou can gamble it
on the flip of a coin. If the coin comes up heads you end up with nothing but if it comes
up tails you get 2500000. If youre like most people you would decline the gamble and
pocketthemillion. Areyoubeingirrational?
E XP EC TE D Assumingthecoinisfairtheexpectedmonetaryvalue E MVofthegambleis 10
M ON ET AR YV AL UE 2
12500000 1250000 which is more than the original 1000000. But that does
2
not necessarily mean that accepting the gamble is a better decision. Suppose we use S to
n
denote the state of possessing total wealth n and that your current wealth is k. Then the
expectedutilities ofthetwoactions ofaccepting anddeclining thegambleare
E UAccept 1 US 1 US
E UDecline U S .
k1000000
To determine what to do we need to assign utilities to the outcome states. Utility is not
directly proportional tomonetary value because theutilityforyourfirstmillionisveryhigh
orso they say whereas the utility foran additional million is smaller. Suppose you assign
autilityof5toyourcurrentfinancialstatus S a9tothestate S andan8tothe
k k2500000
Section16.3. Utility Functions 617
U U
o o o o o o
o o
o
o
o
o
-150000 o 800000
o
o
a b
Figure16.2 Theutilityofmoney. a Empiricaldatafor Mr.Beardoveralimitedrange.
b Atypicalcurveforthefullrange.
state S . Then the rational action would beto decline because the expected utility
k1000000
of accepting is only 7 less than the 8 for declining. On the other hand a billionaire would
mostlikely haveautility function that islocally linearoverthe range ofafewmillionmore
andthuswouldacceptthegamble.
Inapioneeringstudyofactualutilityfunctions Grayson1960foundthattheutilityof
money was almost exactly proportional to the logarithm of the amount. This idea was first
suggested by Bernoulli 1738; see Exercise 16.3. Oneparticular utility curve foracertain
Mr. Beard is shown in Figure 16.2a. The data obtained for Mr. Beards preferences are
consistent withautilityfunction
U S 263.3122.09logn150000
kn
fortherangebetween n 150000andn 800000.
Weshould notassume thatthisisthedefinitive utility function formonetary value but
itislikely thatmostpeople haveautility function thatisconcave forpositive wealth. Going
into debt is bad but preferences between different levels of debt can display a reversal of
theconcavity associatedwithpositivewealth. Forexamplesomeonealready10000000in
debt might well accept a gamble on a fair coin with a gain of 10000000 for heads and a
lossof20000000 fortails.5 Thisyieldsthe S-shapedcurveshownin Figure16.2b.
Ifwerestrictourattention tothepositivepartofthecurveswheretheslopeisdecreas-
ingthenforanylottery Ltheutilityofbeingfacedwiththatlotteryislessthantheutilityof
beinghandedtheexpectedmonetaryvalueofthelotteryasasurething:
U L U S .
E MV L
That is agents with curves of this shape are risk-averse: they prefer a sure thing with a
R IS K-A VE RS E
payoff that is less than the expected monetary value of a gamble. On the other hand in the
desperate region at large negative wealth in Figure 16.2b the behavior is risk-seeking.
R IS K-S EE KI NG
C ER TA IN TY The value an agent will accept in lieu of a lottery is called the certainty equivalent of the
E QU IV AL EN T
lottery. Studies have shownthat most people willaccept about 400 inlieu ofagamble that
gives1000halfthetimeand0theotherhalfthatisthecertaintyequivalentofthelottery
is400whilethe E MVis500. Thedifferencebetweenthe E MVofalotteryanditscertainty
I NS UR AN CE equivalent is called the insurance premium. Risk aversion is the basis for the insurance
P RE MI UM
industry because it means that insurance premiums are positive. People would rather pay a
small insurance premium than gamble the price of their house against the chance of a fire.
From the insurance companys point of view the price of the house is very small compared
with the firms total reserves. This means that the insurers utility curve is approximately
linearoversuchasmallregion andthegamblecoststhecompanyalmostnothing.
Noticethatforsmallchanges inwealthrelative tothecurrentwealth almostany curve
willbeapproximately linear. Anagent thathasalinearcurveissaidtoberisk-neutral. For
R IS K-N EU TR AL
gambles with small sums therefore we expect risk neutrality. In a sense this justifies the
simplified procedure that proposed small gambles to assess probabilities and to justify the
axiomsofprobability in Section13.2.3.
""
Therational waytochoosethebestaction a istomaximizeexpected utility:
a argmax E Uae.
a
Ifwehavecalculatedtheexpectedutilitycorrectlyaccordingtoourprobability modelandif
the probability model correctly reflects the underlying stochastic processes that generate the
outcomes thenonaveragewewillgettheutilityweexpectifthewholeprocess isrepeated
manytimes.
In reality however our model usually oversimplifies the real situation either because
we dont know enough e.g. when making a complex investment decision or because the
computation of the true expected utility is too difficult e.g. when estimating the utility of
successor states of the root node in backgammon. In that case we are really working with
estimates E! Uae of the true expected utility. We will assume kindly perhaps that the
estimates are unbiasedthatistheexpected valueoftheerror E E! Uae E Uaeis
U NB IA SE D
zero. In that case it still seems reasonable to choose the action with the highest estimated
utilityandtoexpecttoreceivethatutility onaverage whentheactionisexecuted.
Unfortunately the real outcome will usually be significantly worse than we estimated
even though the estimate was unbiased! To see why consider a decision problem in which
there are k choices each of which has true estimated utility of 0. Suppose that the error in
each utility estimate has zero mean and standard deviation of 1 shown as the bold curve in
negative pessimistic and some will be positive optimistic. Because we select the action
with the highest utility estimate we are obviously favoring the overly optimistic estimates
and that is the source of the bias. It is a straightforward matter to calculate the distribution
of the maximum of the k estimates see Exercise 16.11 and hence quantify the extent of
our disappointment. The curve in Figure 16.3 for k3 has a mean around 0.85 so the
average disappointment will be about 85 of the standard deviation in the utility estimates.
Section16.3. Utility Functions 619
0.9
k30
0.8
0.7
k10
0.6
k3
0.5
0.4
0.3
0.2
0.1
0
-5 -4 -3 -2 -1 0 1 2 3 4 5
Error in utility estimate
Figure16.3 Plot of the errorin each of k utility estimates andof the distributionof the
maximumofkestimatesfork310and30.
With more choices extremely optimistic estimates are more likely to arise: for k30 the
disappointment willbearoundtwicethestandarddeviation intheestimates.
This tendency for the estimated expected utility of the best choice to be too high is
called the optimizers curse Smith and Winkler 2006. It afflicts even the most seasoned
O PT IM IZ ER SC UR SE
decision analysts and statisticians. Serious manifestations include believing that an exciting
new drug that has cured 80 patients in a trial will cure 80 of patients its been chosen
from k thousands of candidate drugs or that a mutual fund advertised as having above-
average returns will continue to have them its been chosen to appear in the advertisement
out of k dozens of funds in the companys overall portfolio. It can even be the case that
what appears to be the best choice may not be if the variance in the utility estimate is high:
adrug selected from thousands tried that has cured 9of 10 patients is probably worse than
onethathascured800of1000.
Theoptimizerscursecropsupeverywherebecauseoftheubiquityofutility-maximizing
selectionprocessessotakingtheutilityestimatesatfacevalueisabadidea. Wecanavoidthe
cursebyusinganexplicitprobability model P E! U E Uoftheerrorintheutilityestimates.
Giventhis model andaprior P EUonwhat wemight reasonably expect the utilities tobe
wetreattheutilityestimateonceobtainedasevidenceandcomputetheposteriordistribution
forthetrueutilityusing Bayesrule.
Decision theory is a normative theory: it describes how a rational agent should act. A
N OR MA TI VE TH EO RY
D ES CR IP TI VE descriptivetheoryontheotherhanddescribeshowactualagentsforexamplehumans
T HE OR Y
really do act. The application of economic theory would be greatly enhanced if the two
coincided butthereappears tobesomeexperimental evidence tothecontrary. Theevidence
suggests thathumansarepredictably irrational Ariely2009.
Thebest-knownproblemisthe Allaisparadox Allais1953. Peoplearegivenachoice
betweenlotteries Aand B andthenbetween C and Dwhichhavethefollowingprizes:
Most people consistently prefer B over A taking the sure thing and C over D taking the
higher E MV. The normative analysis disagrees! We can see this most easily if we use the
freedom implied by Equation 16.2 to set U0 0. In that case then B A implies
that U3000 0.8 U4000 whereas C D implies exactly the reverse. In other
words there is no utility function that is consistent with these choices. One explanation for
the apparently irrational preferences is the certainty effect Kahneman and Tversky 1979:
C ER TA IN TY EF FE CT
peoplearestronglyattractedtogainsthatarecertain. Thereareseveralreasonswhythismay
be so. First people may prefer to reduce their computational burden; by choosing certain
outcomes they dont have to compute with probabilities. But the effect persists even when
thecomputations involved areveryeasyones. Second peoplemaydistrust thelegitimacyof
thestatedprobabilities. Itrustthatacoinflipisroughly 5050 if Ihavecontrol overthecoin
andthe flipbut I maydistrust theresult iftheflipisdone bysomeone withavested interest
intheoutcome.6 Inthepresenceofdistrustitmightbebettertogoforthesurething.7 Third
people may be accounting for their emotional state as well as their financial state. People
knowtheywouldexperienceregretiftheygaveupacertainreward Bforan80chanceat
R EG RE T
ahigherrewardandthenlost. Inotherwordsif Aischosenthereisa20chanceofgetting
no money and feeling like a complete idiot which is worse than just getting no money. So
perhaps people who choose B over A and C over D are not being irrational; they are just
sayingthattheyarewillingtogiveup200of E MVtoavoida20chanceoffeelinglikean
idiot.
Arelatedproblemisthe Ellsbergparadox. Heretheprizesarefixedbuttheprobabilities
areunderconstrained. Yourpayoffwilldependonthecolorofaballchosenfromanurn. You
aretoldthattheurncontains 13redballs and23eitherblackoryellowballsbutyoudont
knowhowmanyblackandhowmanyyellow. Againyouareaskedwhetheryoupreferlottery
Aor B;andthen C or D:
A: 100foraredball C : 100foraredoryellowball
B : 100forablackball D : 100forablackoryellowball .
Itshouldbeclearthatifyouthinktherearemoreredthanblackballsthenyoushould prefer
A over B and C over D; if you think there are fewer red than black you should prefer the
opposite. But it turns out that most people prefer A over B and also prefer D over C even
though there is no state of the world for which this is rational. It seems that people have
A MB IG UI TY ambiguity aversion: A gives you a 13 chance of winning while B could be anywhere
A VE RS IO N
between0and23. Similarly Dgivesyoua23chancewhile C couldbeanywherebetween
13and33. Mostpeopleelecttheknownprobability ratherthantheunknownunknowns.
everytime Landhuis2004.
fromthe Nigerianbankaccountofapreviouslyunknowndeceasedrelative.
Section16.3. Utility Functions 621
Yet another problem is that the exact wording of a decision problem can have a big
impactontheagentschoices;thisiscalledtheframingeffect. Experimentsshowthatpeople
F RA MI NG EF FE CT
like a medical procedure that it is described as having a 90 survival rate about twice as
muchasonedescribedashavinga10deathrateeventhoughthesetwostatementsmean
exactlythesamething. Thisdiscrepancyinjudgmenthasbeenfoundinmultipleexperiments
andisaboutthesamewhetherthesubjectswerepatientsinaclinicstatisticallysophisticated
business schoolstudents orexperienced doctors.
People feel more comfortable making relative utility judgments rather than absolute
ones. Imayhavelittleideahowmuch Imightenjoythevariouswinesofferedbyarestaurant.
Therestauranttakesadvantageofthisbyofferinga200bottlethatitknowsnobodywillbuy
butwhichserves toskewupward thecustomers estimate ofthevalueofallwinesandmake
the55bottleseemlikeabargain. Thisiscalledtheanchoringeffect.
A NC HO RI NG EF FE CT
Ifhumaninformants insistoncontradictory preference judgments thereisnothing that
automatedagentscandotobeconsistentwiththem. Fortunatelypreferencejudgmentsmade
by humans are often open to revision in the light of further consideration. Paradoxes like
the Allais paradox are greatly reduced but not eliminated if the choices are explained bet-
ter. In work at the Harvard Business School on assessing the utility of money Keeney and
Raiffa1976p.210foundthefollowing:
Subjectstendtobetoorisk-averseinthesmallandtherefore...thefittedutilityfunctions
exhibitunacceptablylargeriskpremiumsforlotterieswithalargespread. ...Mostofthe
subjects howevercanreconciletheirinconsistenciesandfeelthattheyhavelearnedan
importantlessonabouthowtheywanttobehave.Asaconsequencesomesubjectscancel
theirautomobilecollisioninsuranceandtakeoutmoreterminsuranceontheirlives.
The evidence for human irrationality is also questioned by researchers in the field of evo-
E VO LU TI ON AR Y lutionary psychology who point to the fact that our brains decision-making mechanisms
P SY CH OL OG Y
did not evolve to solve word problems with probabilities and prizes stated as decimal num-
bers. Let us grant for the sake of argument that the brain has built-in neural mechanism
forcomputingwithprobabilities andutilities orsomethingfunctionally equivalent; ifsothe
requiredinputswouldbeobtainedthroughaccumulatedexperienceofoutcomesandrewards
ratherthanthroughlinguisticpresentationsofnumerical values. Itisfarfromobviousthatwe
candirectlyaccessthebrainsbuilt-inneuralmechanisms bypresentingdecisionproblemsin
linguisticnumerical form. The very fact that different wordings of the same decision prob-
lem elicit different choices suggests that the decision problem itself is not getting through.
Spurred by this observation psychologists have tried presenting problems in uncertain rea-
soning and decision making in evolutionarily appropriate forms; for example instead of
saying 90 survival rate the experimenter might show 100 stick-figure animations of the
operation wherethepatient diesin10ofthemandsurvives in90. Boredom isacomplicat-
ing factor in these experiments! With decision problems posed in this way people seem to
bemuchclosertorationalbehaviorthanpreviously suspected.
Decision making in the field of public policy involves high stakes in both money and lives.
Forexample indeciding whatlevelsofharmfulemissions to allow fromapowerplant pol-
icymakersmustweighthepreventionofdeathanddisability againstthebenefitofthepower
and the economic burden of mitigating the emissions. Siting a new airport requires consid-
eration of the disruption caused by construction; the cost of land; the distance from centers
ofpopulation; the noise offlight operations; safety issues arising from local topography and
weatherconditions; and so on. Problems like these inwhich outcomes are characterized by
M UL TI AT TR IB UT E twoormoreattributes arehandledby multiattributeutilitytheory.
U TI LI TY TH EO RY
We will call the attributes X X ...X ; a complete vector of assignments will be
xcid:16x ...x cid:17whereeachx iseitheranumericvalueoradiscretevaluewithanassumed
ordering on values. We will assume that higher values of an attribute correspond to higher
utilities all other things being equal. For example if we choose Absence Of Noise as an
attributeintheairportproblemthenthegreateritsvaluethebetterthesolution.8 Webeginby
examining casesinwhichdecisions canbemadewithoutcombining theattribute valuesinto
a single utility value. Then we look at cases in which the utilities of attribute combinations
canbespecifiedveryconcisely.
Supposethatairportsite S costslessgenerateslessnoisepollutionandissaferthansite S .
One would not hesitate to reject S . We then say that there is strict dominance of S over
S TR IC TD OM IN AN CE 2 1
S . Ingeneral ifanoption isoflowervalueonallattributes thansomeotheroption itneed
2
not be considered further. Strict dominance is often very useful in narrowing down the field
of choices to the real contenders although it seldom yields a unique choice. Figure 16.4a
showsaschematicdiagram forthetwo-attribute case.
Thatisfineforthedeterministic case inwhichtheattribute valuesareknownforsure.
What about the general case where the outcomes are uncertain? A direct analog of strict
dominancecanbeconstructed wheredespitetheuncertainty allpossibleconcreteoutcomes
for S strictly dominate all possible outcomes for S . See Figure 16.4b. Of course this
willprobably occurevenlessoftenthaninthedeterministic case.
S TO CH AS TI C Fortunately there isa more useful generalization called stochastic dominance which
D OM IN AN CE
occurs very frequently in real problems. Stochastic dominance is easiest to understand in
thecontextofasingleattribute. Supposewebelievethatthecostofsitingtheairportat S is
1
uniformlydistributedbetween2.8billionand4.8billionandthatthecostat S isuniformly
2
distributedbetween3billionand5.2billion. Figure16.5ashowsthesedistributions with
cost plotted asanegative value. Then given only the information that utility decreases with
eachrange.Forexampleifthe Room Temperatureattributehasautilitypeakat70 Fwewouldsplititintotwo
attributesmeasuringthedifferencefromtheidealonecolderandonehotter.Utilitywouldthenbemonotonically
increasingineachattribute.
Section16.4. Multiattribute Utility Functions 623
X X
This region
dominates A
B
C B
C
A A
D
X X
a b
Figure16.4 Strictdominance.a Deterministic:Option Aisstrictlydominatedby Bbut
notby Cor D.b Uncertain:Aisstrictlydominatedby Bbutnotby C.
0.6
0.5
0.4
0.3
0.2
0.1
0
-6 -5.5 -5 -4.5 -4 -3.5 -3 -2.5 -2
ytilibabor P
1.2
1
0.8
S S 0.6
0.4
0.2
0
-6 -5.5 -5 -4.5 -4 -3.5 -3 -2.5 -2
Negative cost
ytilibabor P
S
2
S
1
Negative cost
a b
Figure16.5 Stochastic dominance. a S stochastically dominates S on cost. b Cu-
mulativedistributionsforthenegativecostof S and S .
costwecansaythat S stochasticallydominates S i.e.S canbediscarded. Itisimportant
tonotethatthisdoesnotfollowfromcomparingtheexpectedcosts. Forexampleifweknew
thecostof S tobeexactly3.8billionthenwewouldbeunabletomakeadecisionwithout
1
additional information on theutility of money. It might seem odd that moreinformation on
the cost of S could make the agent less able to decide. The paradox is resolved by noting
1
thatintheabsenceofexactcostinformation thedecisioniseasiertomakebutismorelikely
tobewrong.
Theexactrelationship betweentheattributedistributions neededtoestablishstochastic
dominanceisbestseenbyexaminingthecumulativedistributionsshownin Figure16.5b.
Seealso Appendix A. Thecumulative distribution measures theprobability thatthecostis
less than or equal to any given amountthat is it integrates the original distribution. If the
cumulative distribution for S is always to the right of the cumulative distribution for S
thenstochastically speaking S ischeaperthan S . Formallyiftwoactions A and A lead
toprobability distributions p xandp xonattribute Xthen A stochastically dominates
A on X if
2
cid:26x cid:26x
x p xcid:2 dxcid:2 p xcid:2 dxcid:2 .
""
Therelevanceofthisdefinitiontotheselectionofoptimaldecisionscomesfromthefollowing
property: if A stochasticallydominates A thenforanymonotonicallynondecreasingutility
function Ux the expected utility of A is at least as high as the expected utility of A .
Henceifanactionisstochastically dominated byanotheraction onallattributes thenitcan
bediscarded.
The stochastic dominance condition might seem rather technical and perhaps not so
easy to evaluate without extensive probability calculations. In fact it can be decided very
easilyinmanycases. Supposeforexamplethattheconstructiontransportation costdepends
on the distance to the supplier. The cost itself is uncertain but the greater the distance the
greater the cost. If S is closer than S then S will dominate S on cost. Although we
will not present them here there exist algorithms for propagating this kind of qualitative
Q UA LI TA TI VE
information among uncertain variables in qualitative probabilistic networks enabling a
P RO BA BI LI ST IC
N ET WO RK S
systemtomakerationaldecisionsbasedonstochasticdominancewithoutusinganynumeric
values.
Suppose we have n attributes each of which has d distinct possible values. To specify the
completeutilityfunction Ux ...x weneeddnvaluesintheworstcase. Nowtheworst
casecorrespondstoasituationinwhichtheagentspreferenceshavenoregularityatall. Mul-
tiattributeutilitytheoryisbasedonthesuppositionthatthepreferencesoftypicalagentshave
muchmorestructurethanthat. Thebasicapproachistoidentifyregularitiesinthepreference
R EP RE SE NT AT IO N behaviorwewouldexpecttoseeandtousewhatarecalledrepresentationtheoremstoshow
T HE OR EM
thatanagentwithacertainkindofpreference structure has autilityfunction
Ux ...x Ff x ...f x
where F is we hope a simple function such as addition. Notice the similarity to the use of
Bayesiannetworkstodecomposethejointprobability ofseveralrandom variables.
Preferences withoutuncertainty
Let us begin with the deterministic case. Rememberthat for deterministic environments the
agent has a value function Vx ...x ; the aim is to represent this function concisely.
The basic regularity that arises in deterministic preference structures is called preference
P RE FE RE NC E independence. Twoattributes X and X are preferentially independent of a third attribute
I ND EP EN DE NC E 1 2
X ifthe preference between outcomes cid:16x x x cid:17and cid:16xcid:2 xcid:2 x cid:17does not depend on the
particularvalue x forattribute X .
Going back to the airport example where we have among other attributes Noise
Costand Deaths toconsider onemayproposethat Noise and Cost arepreferentially inde-
Section16.4. Multiattribute Utility Functions 625
pendentof Deaths. Forexampleifwepreferastatewith20000peopleresidingintheflight
pathandaconstructioncostof4billionoverastatewith70000peopleresidingintheflight
pathandacostof3.7billionwhenthesafetylevelis0.06deathspermillionpassengermiles
inboth cases thenwewouldhavethesamepreference whenthe safetylevelis0.12or0.03;
and the sameindependence would hold forpreferences between any other pairof values for
Noise and Cost. It is also apparent that Cost and Deaths are preferentially independent of
Noise and that Noise and Deaths are preferentially independent of Cost. We say that the
M UT UA L set ofattributes Noise Cost Deathsexhibits mutualpreferential independence M PI.
P RE FE RE NT IA L
I ND EP EN DE NC E
M PI says that whereas each attribute may be important it does not affect the way in which
onetradesofftheotherattributes againsteachother.
Mutual preferential independence is something of a mouthful but thanks to a remark-
abletheoremduetotheeconomist Gerard Debreu1960wecanderivefromitaverysimple
form fortheagents value function: Ifattributes X ... X aremutually preferentially in-
dependent thentheagentspreferencebehaviorcanbedescribedasmaximizingthefunction
cid:12
Vx ...x V x
i
where each V is a value function referring only to the attribute X . For example it might
i i
wellbethecasethattheairportdecision canbemadeusingavaluefunction
Vnoisecostdeaths noise 104cost deaths 1012 .
A DD IT IV EV AL UE Avalue function ofthistypeiscalled anadditivevaluefunction. Additivefunctions arean
F UN CT IO N
extremely natural way to describe an agents preferences and are valid in many real-world
situations. Fornattributes assessinganadditivevaluefunctionrequires assessingnseparate
one-dimensionalvaluefunctionsratherthanonen-dimensionalfunction;typicallythisrepre-
sentsanexponentialreductioninthenumberofpreferenceexperimentsthatareneeded. Even
when M PI does not strictly hold as might be the case at extreme values of the attributes an
additive value function might still provide a good approximation to the agents preferences.
This is especially true when the violations of M PI occur in portions of the attribute ranges
thatareunlikely tooccurinpractice.
Tounderstand M PI better ithelps tolook at cases where it doesnt hold. Suppose you
are at a medieval market considering the purchase of some hunting dogs some chickens
and some wicker cages for the chickens. The hunting dogs are very valuable but if you
dont haveenough cages forthechickens thedogs willeatthechickens; hence thetradeoff
between dogs and chickens depends strongly on the number of cages and M PI is violated.
Theexistenceofthesekindsofinteractionsamongvariousattributesmakesitmuchharderto
assesstheoverallvaluefunction.
Preferences withuncertainty
When uncertainty is present in the domain we also need to consider the structure of prefer-
ences between lotteries and to understand the resulting properties of utility functions rather
than just value functions. The mathematics of this problem can become quite complicated
sowepresent justoneofthemainresults togiveaflavorofwhatcanbedone. Thereaderis
referredto Keeneyand Raiffa1976forathorough surveyof thefield.
U TI LI TY The basic notion of utility independence extends preference independence to cover
I ND EP EN DE NC E
lotteries: aset ofattributes Xisutility independent ofaset ofattributes Yifpreferences be-
tweenlotteriesontheattributes in Xareindependent oftheparticularvaluesoftheattributes
M UT UA LL YU TI LI TY in Y. A set of attributes is mutually utility independent M UI if each of its subsets is
I ND EP EN DE NT
utility-independent of the remaining attributes. Again it seems reasonable to propose that
theairportattributes are M UI.
M UI implies that the agents behavior can be described using a multiplicative utility
M UL TI PL IC AT IV E function Keeney1974. Thegeneralformofamultiplicativeutilityfunctionisbestseenby
U TI LI TY FU NC TI ON
looking atthecaseforthreeattributes. Forconciseness weuse U tomean U x :
i i i
U k U k U k U k k U U k k U U k k U U
k k k U U U .
Althoughthisdoesnotlookverysimpleitcontainsjustthreesingle-attributeutilityfunctions
andthreeconstants. Ingeneralann-attributeproblemexhibiting M UIcanbemodeledusing
n single-attribute utilities and n constants. Each of the single-attribute utility functions can
be developed independently of the other attributes and this combination will be guaranteed
to generate the correct overall preferences. Additional assumptions are required to obtain a
purelyadditiveutilityfunction.
In this section welook at a general mechanism formaking rational decisions. The notation
is often called an influence diagram Howard and Matheson 1984 but we will use the
I NF LU EN CE DI AG RA M
more descriptive term decision network. Decision networks combine Bayesian networks
D EC IS IO NN ET WO RK
withadditional nodetypesforactionsandutilities. Weuseairportsitingasanexample.
Initsmostgeneral formadecisionnetworkrepresents information abouttheagentscurrent
state its possible actions the state that will result from the agents action and the utility of
that state. It therefore provides a substrate forimplementing utility-based agents of the type
first introduced in Section 2.4.5. Figure 16.6 shows a decision network forthe airport siting
problem. Itillustrates thethreetypesofnodesused:
Chancenodesovalsrepresentrandomvariablesjustastheydoin Bayesiannetworks.
C HA NC EN OD ES
Theagentcould beuncertain about theconstruction costthelevelofairtrafficandthe
potential forlitigation andthe Deaths Noiseandtotal Cost variables eachofwhich
alsodepends onthesitechosen. Eachchance node hasassociated withitaconditional
distribution that is indexed by the state of the parent nodes. In decision networks the
parent nodes can include decision nodes as well as chance nodes. Note that each of
the current-state chance nodes could be part of alarge Bayesian network for assessing
construction costsairtrafficlevelsorlitigationpotentials.
Decision nodesrectangles represent points wherethedecision makerhas achoice of
D EC IS IO NN OD ES
Section16.5. Decision Networks 627
Airport Site
Air Traffic Deaths
Litigation Noise U
Construction Cost
Figure16.6 Asimpledecisionnetworkfortheairport-sitingproblem.
actions. In this case the Airport Site action can take on a different value for each site
under consideration. The choice influences the cost safety and noise that will result.
Inthis chapter weassume thatwearedealing withasingle decision node. Chapter17
dealswithcasesinwhichmorethanonedecision mustbemade.
Utility nodes diamonds represent the agents utility function.9 The utility node has
U TI LI TY NO DE S
as parents all variables describing the outcome that directly affect utility. Associated
with the utility node is a description of the agents utility as a function of the parent
attributes. The description could be just a tabulation of the function or it might be a
parameterized additiveorlinearfunction oftheattribute values.
A simplified form is also used in many cases. The notation remains identical but the
chancenodesdescribing theoutcomestateareomitted. Instead theutilitynodeisconnected
directlytothecurrent-statenodesandthedecisionnode. Inthiscaseratherthanrepresenting
autilityfunctiononoutcomestatestheutilitynoderepresentstheexpectedutilityassociated
with each action as defined in Equation 16.1 on page 611; that is the node is associated
A CT IO N-U TI LI TY with an action-utility function also known as a Q-function in reinforcement learning as
F UN CT IO N
described in Chapter 21. Figure 16.7 shows the action-utility representation of the airport
sitingproblem.
Noticethatbecausethe Noise Deathsand Cost chance nodesin Figure16.6referto
future states they can neverhave theirvalues set asevidence variables. Thus thesimplified
version that omits these nodes can be used whenever the more general form can be used.
Although the simplified form contains fewer nodes the omission of an explicit description
of the outcome of the siting decision means that it is less flexible with respect to changes in
circumstances. Forexamplein Figure16.6achange inaircraft noiselevelscanbereflected
by a change in the conditional probability table associated with the Noise node whereas a
change in the weight accorded to noise pollution in the utility function can be reflected by
Airport Site
Air Traffic
Litigation U
Construction
Figure16.7 Asimplifiedrepresentationoftheairport-sitingproblem. Chancenodescor-
respondingtooutcomestateshavebeenfactoredout.
a change in the utility table. In the action-utility diagram Figure 16.7 on the other hand
all such changes have to be reflected by changes to the action-utility table. Essentially the
action-utility formulation isacompiledversionoftheoriginal formulation.
Actionsareselected byevaluating thedecision networkfor eachpossible settingofthedeci-
sionnode. Oncethedecision nodeisset itbehaves exactly likeachance nodethathasbeen
setasanevidencevariable. Thealgorithm forevaluating decisionnetworks isthefollowing:
1. Settheevidence variablesforthecurrentstate.
2. Foreachpossible valueofthedecision node:
a Setthedecision nodetothatvalue.
b Calculate theposterior probabilities fortheparent nodes oftheutility node using
astandard probabilistic inference algorithm.
c Calculatetheresulting utilityfortheaction.
3. Returntheactionwiththehighestutility.
This is a straightforward extension of the Bayesian network algorithm and can be incorpo-
rated directly into the agent design given in Figure 13.1 on page 484. We will see in Chap-
ter 17 that the possibility of executing several actions in sequence makes the problem much
moreinteresting.
Inthepreceding analysis wehaveassumedthatallrelevant information oratleastallavail-
able information is provided to the agent before it makes its decision. In practice this is
Section16.6. The Valueof Information 629
hardly ever the case. One of the most important parts of decision making is knowing what
questions to ask. Forexample a doctor cannot expect to be provided with the results of all
possiblediagnostictestsandquestionsatthetimeapatientfirstenterstheconsultingroom.10
Testsare often expensive and sometimes hazardous both directly and because ofassociated
delays. Their importance depends on two factors: whether the test results would lead to a
significantly bettertreatmentplanandhowlikelythevarioustestresultsare.
I NF OR MA TI ON VA LU E This section describes information value theory which enables an agent to choose
T HE OR Y
what information to acquire. We assume that prior to selecting a real action represented
by the decision node the agent can acquire the value of any of the potentially observable
chance variables in the model. Thus information value theory involves a simplified form
of sequential decision makingsimplified because the observation actions affect only the
agents belief state not the external physical state. The value of any particular observation
mustderivefromthepotentialtoaffecttheagentseventualphysicalaction;andthispotential
canbeestimated directlyfromthedecision modelitself.
Supposeanoilcompanyishopingtobuyoneofnindistinguishable blocksofocean-drilling
rights. Letusassumefurtherthatexactlyoneoftheblockscontainsoilworth C dollarswhile
the others are worthless. The asking price of each block is Cn dollars. If the company is
risk-neutral thenitwillbeindifferent betweenbuyingablockandnotbuyingone.
Now suppose that a seismologist offers the company the results of a survey of block
number 3 which indicates definitively whether the block contains oil. How much should
the company be willing to pay for the information? The way to answer this question is to
examinewhatthecompanywoulddoifithadtheinformation:
Withprobability 1nthesurveywillindicate oilinblock 3. Inthis case thecompany
willbuyblock3for Cndollarsandmakeaprofitof C Cn n1 Cndollars.
Withprobabilityn1nthesurveywillshowthattheblockcontainsnooilinwhich
case the company will buy a different block. Now the probability of finding oil in one
oftheotherblockschangesfrom1nto1n1sothecompanymakesanexpected
profitof Cn1 Cn Cnn1dollars.
Nowwecancalculate theexpected profitgiventhesurveyinformation:
Cn.
n n n nn1
Therefore the company should be willing to pay the seismologist up to Cn dollars for the
information: theinformation isworthasmuchastheblockitself.
The value of information derives from the fact that with the information ones course
of action can be changed to suit the actual situation. One can discriminate according to the
situation whereas without the information one has to do whats best on average over the
possible situations. In general the value of a given piece of information is defined to be the
difference inexpected valuebetweenbestactionsbeforeandafterinformation isobtained.
Itissimpletoderiveageneralmathematicalformulaforthevalueofinformation. Weassume
that exact evidence canbeobtained about the value ofsomerandom variable E that is we
j
V AL UE OF PE RF EC T learn E e sothephrase valueofperfectinformation V PIisused.11
I NF OR MA TI ON j j
Let the agents initial evidence be e. Then the value of the current best action α is
definedby
cid:12
E Uαe max P RE SU LTascid:2ae Uscid:2
a
scid:3
andthevalueofthenewbestactionafterthenewevidence E e isobtained willbe
j j
cid:12
E Uα ejee j max P RE SU LTascid:2aee j Uscid:2 .
a
scid:3
But E isarandom variable whosevalue is currently unknown sotodetermine thevalue of
j
discovering E givencurrentinformation ewemustaverageoverallpossiblevaluese that
j jk
wemightdiscoverfor E usingourcurrentbeliefsaboutitsvalue:
j
cid:31
cid:12
V PI E P E e e E Uα e E e E Uαe.
e j j jk ejk j jk
k
To get some intuition for this formula consider the simple case where there are only two
actionsa anda fromwhichtochoose. Theircurrentexpectedutilitiesare U and U . The
cid:2 cid:2
information E e will yield some new expected utilities U and U for the actions but
j jk 1 2
before weobtain E wewillhavesome probability distributions overthe possible values of
j
cid:2 cid:2
U and U whichweassumeareindependent.
Suppose that a and a represent two different routes through a mountain range in
winter. a is anice straight highway through a low pass and a is awinding dirt road over
the top. Just given this information a is clearly preferable because it is quite possible that
1
a is blocked by avalanches whereas it is unlikely that anything blocks a . U is therefore
clearly higher than U . Itispossible toobtain satellite reports E on theactual state ofeach
cid:2 cid:2
road that would give new expectations U and U for the two crossings. The distributions
fortheseexpectations areshownin Figure16.8a. Obviouslyinthiscaseitisnotworththe
expenseofobtainingsatellitereportsbecauseitisunlikelythattheinformationderivedfrom
themwillchangetheplan. Withnochange information hasnovalue.
Nowsuppose thatwearechoosing betweentwodifferentwindingdirtroadsofslightly
different lengths and we are carrying a seriously injured passenger. Then even when U
1
cid:2 cid:2
and U are quite close the distributions of U and U are very broad. There is a significant
possibility thatthesecondroutewillturnouttobeclearwhilethefirstisblocked andinthis
inwhichwebecomesomewhat morecertainabout avariable. We candothatbyintroducing anothervariable
aboutwhichwelearnperfectinformation. Forexample supposeweinitiallyhavebroaduncertaintyabout the
variable Temperature. Then we gain the perfect knowledge Thermometer 37; this gives us imperfect
informationaboutthetrue Temperatureandtheuncertaintyduetomeasurementerrorisencodedinthesensor
model P Thermometer Temperature.See Exercise16.17foranotherexample.
Section16.6. The Valueof Information 631
P U E P U E P U E
j j j
U U U
U U U U U U
a b c
Figure16.8 Threegenericcasesforthevalueofinformation. Ina a willalmostcer-
1
tainlyremainsuperiortoa sotheinformationisnotneeded.Inbthechoiceisunclearand
2
theinformationiscrucial. Incthechoiceisunclearbutbecauseitmakeslittledifference
theinformationislessvaluable.Note: Thefactthat U hasahighpeakincmeansthatits
2
expectedvalueisknownwithhighercertaintythan U .
1
case the difference in utilities will be very high. The V PI formula indicates that it might be
worthwhilegettingthesatellite reports. Suchasituation isshownin Figure16.8b.
Finallysupposethatwearechoosingbetweenthetwodirtroadsinsummertimewhen
blockage byavalanches isunlikely. Inthis case satellite reports might show one route tobe
more scenic than the other because of flowering alpine meadows orperhaps wetterbecause
of errant streams. It is therefore quite likely that we would change our plan if we had the
information. In this case however the difference in value between the two routes is still
likelytobeverysmallsowewillnotbothertoobtainthereports. Thissituation isshownin
Figure16.8c.
In sum information has value to the extent that it is likely to cause a change of plan
andtotheextentthatthenewplanwillbesignificantly betterthantheoldplan.
One might ask whether it is possible for information to be deleterious: can it actually have
negative expected value? Intuitively one should expect this to be impossible. After all one
couldintheworstcasejustignoretheinformationandpretendthatonehasneverreceivedit.
Thisisconfirmedbythefollowingtheoremwhichappliestoanydecision-theoretic agent:
Theexpected valueofinformation isnonnegative:
e E V PI E 0.
j e j
Thetheoremfollowsdirectlyfromthedefinitionof V PIandweleavetheproofasanexercise
Exercise16.18. Itisofcourseatheoremaboutexpectedvaluenotactualvalue. Additional
information can easily lead to a plan that turns out to be worse than the original plan if the
information happens tobemisleading. Forexample amedical testthatgivesafalsepositive
resultmayleadtounnecessarysurgery;butthatdoesnotmeanthatthetestshouldntbedone.
Itisimportanttorememberthat V PIdependsonthecurrentstateofinformation which
is why it is subscripted. It can change as more information is acquired. Forany given piece
of evidence E the value of acquiring it can go down e.g. if another variable strongly
j
constrains the posterior for E or up e.g. if another variable provides a clue on which E
j j
builds enabling anewandbetterplantobedevised. Thus V PIisnotadditive. Thatis
V PI E E cid:7 V PI E V PI E ingeneral .
e j k e j e k
V PIishoweverorderindependent. Thatis
V PI E E V PI E V PI E V PI E V PI E .
e j k e j eej k e k eek j
Order independence distinguishes sensing actions from ordinary actions and simplifies the
problem ofcalculating thevalueofasequenceofsensingactions.
A sensible agent should ask questions in a reasonable order should avoid asking questions
that are irrelevant should take into account the importance of each piece of information in
relation to its cost and should stop asking questions when that is appropriate. All of these
capabilities canbeachievedbyusingthevalueofinformation asaguide.
ligently before acting. For now we assume that with each observable evidence variable
E there is an associated cost Cost E which reflects the cost of obtaining the evidence
j j
through tests consultants questions orwhatever. Theagentrequests whatappears tobethe
most efficient observation interms of utility gain perunit cost. Weassume that the result of
theaction Request E isthatthenextperceptprovides thevalueof E . Ifnoobservation is
j j
worthitscosttheagentselectsarealaction.
The agent algorithm we have described implements a form of information gathering
thatiscalled myopic. Thisisbecause itusesthe V PIformula shortsightedly calculating the
M YO PI C
value of information as if only a single evidence variable will be acquired. Myopic control
is based on the same heuristic idea as greedy search and often works well in practice. For
example it has been shown to outperform expert physicians in selecting diagnostic tests.
function I NF OR MA TI ON-G AT HE RI NG-A GE NTperceptreturnsanaction
persistent: Dadecisionnetwork
integratepercept into D
j thevaluethatmaximizes V PI Ej Cost Ej
if V PI Ej Cost Ej
return R EQ UE ST Ej
elsereturnthebestactionfrom D
Figure16.9 Designofasimpleinformation-gatheringagent. Theagentworksbyrepeat-
edlyselecting the observationwith the highestinformationvalue untilthe cost of the next
observationisgreaterthanitsexpectedbenefit.
Section16.7. Decision-Theoretic Expert Systems 633
However if there is no single evidence variable that will help a lot a myopic agent might
hastily take an action when it would have been better to request two or more variables first
and then take action. A better approach in this situation would be to construct a conditional
plan as described in Section 11.3.2 that asks for variable values and takes different next
stepsdepending ontheanswer.
Onefinalconsideration istheeffectaseries ofquestions willhaveonahumanrespon-
dent. Peoplemayrespondbettertoaseriesofquestionsiftheymakesensesosomeexpert
systems are built to take this into account asking questions in an order that maximizes the
totalutilityofthesystemandhumanratherthananorderthatmaximizesvalueofinformation.
Thefieldofdecisionanalysiswhichevolvedinthe1950sand1960sstudiestheapplication
D EC IS IO NA NA LY SI S
of decision theory to actual decision problems. It is used to help make rational decisions in
important domains where the stakes are high such as business government law military
strategymedicaldiagnosisandpublichealthengineering designandresourcemanagement.
The process involves a careful study of the possible actions and outcomes as well as the
preferences placed on each outcome. It is traditional in decision analysis to talk about two
roles: the decision maker states preferences between outcomes and the decision analyst
D EC IS IO NM AK ER
enumeratesthepossibleactionsandoutcomesandelicitspreferencesfromthedecisionmaker
D EC IS IO NA NA LY ST
to determine the best course of action. Until the early 1980s the main purpose of decision
analysis was to help humans make decisions that actually reflect their own preferences. As
moreandmoredecision processes become automated decision analysis isincreasingly used
toensurethattheautomated processes arebehaving asdesired.
Earlyexpertsystemresearchconcentrated onansweringquestions ratherthanonmak-
ing decisions. Those systems that did recommend actions rather than providing opinions on
matters of fact generally did so using condition-action rules rather than with explicit rep-
resentations of outcomes and preferences. The emergence of Bayesian networks in the late
1980s made it possible to build large-scale systems that generated sound probabilistic infer-
ences from evidence. The addition of decision networks means that expert systems can be
developed that recommend optimal decisions reflecting the preferences of the agent aswell
astheavailable evidence.
A system that incorporates utilities can avoid one of the most common pitfalls associ-
atedwiththeconsultationprocess: confusinglikelihoodandimportance. Acommonstrategy
inearlymedicalexpertsystemsforexamplewastorankpossiblediagnosesinorderoflike-
lihood and report the most likely. Unfortunately this can be disastrous! Forthe majority of
patientsingeneralpracticethetwomostlikelydiagnosesareusually Theresnothingwrong
withyouand Youhaveabadcoldbutifthethirdmostlikelydiagnosisforagivenpatient
is lung cancer thats a serious matter. Obviously a testing or treatment plan should depend
both on probabilities and utilities. Current medical expert systems can take into account the
valueofinformation torecommendtestsandthendescribea differential diagnosis.
Wenowdescribe theknowledge engineering process fordecision-theoretic expert sys-
tems. Asanexample weconsider theproblem ofselecting amedical treatment forakind of
congenital heartdiseaseinchildrensee Lucas1996.
About0.8 ofchildren are born withaheart anomaly the most commonbeing aortic
A OR TI C coarctation aconstriction oftheaorta. Itcanbetreatedwithsurgery angioplasty expand-
C OA RC TA TI ON
ingtheaortawithaballoonplacedinsidethearteryormedication. Theproblemistodecide
whattreatmenttouseandwhentodoit: theyoungertheinfantthegreatertherisksofcertain
treatmentsbutonemustntwaittoolong. Adecision-theoreticexpertsystemforthisproblem
can be created by a team consisting of at least one domain expert a pediatric cardiologist
andoneknowledgeengineer. Theprocess canbebrokendownintothefollowingsteps:
Create a causal model. Determine the possible symptoms disorders treatments and
outcomes. Then draw arcs between them indicating what disorders cause what symptoms
andwhattreatmentsalleviatewhatdisorders. Someofthiswillbewellknowntothedomain
expert and some will come from the literature. Often the model will match well with the
informalgraphical descriptions giveninmedicaltextbooks.
Simplify to a qualitative decision model. Since we are using the model to make
treatment decisions and not for other purposes such as determining the joint probability of
certain symptomdisorder combinations we can often simplify by removing variables that
are not involved in treatment decisions. Sometimes variables will have to be split or joined
to match the experts intuitions. For example the original aortic coarctation model had a
Treatmentvariable withvalues surgery angioplasty andmedication andaseparate variable
for Timing of the treatment. But the expert had a hard time thinking of these separately so
theywerecombined with Treatmenttakingonvaluessuchassurgery in1month. Thisgives
usthemodelof Figure16.10.
Assignprobabilities. Probabilities cancomefrompatientdatabases literature studies
ortheexpertssubjective assessments. Notethatadiagnostic system willreasonfromsymp-
tomsandotherobservations tothedisease orothercause oftheproblems. Thus intheearly
years of building these systems experts were asked for the probability of a cause given an
effect. Ingeneraltheyfoundthisdifficulttodoandwerebetterabletoassesstheprobability
ofaneffectgivenacause. Somodernsystemsusuallyassesscausalknowledgeandencodeit
directly in the Bayesian network structure of the model leaving the diagnostic reasoning to
the Bayesiannetworkinferencealgorithms Shachterand Heckerman 1987.
Assign utilities. When there are a small number of possible outcomes they can be
enumeratedandevaluatedindividuallyusingthemethodsof Section16.3.1. Wewouldcreate
a scale from best to worst outcome and give each a numeric value for example 0 for death
and 1 for complete recovery. We would then place the other outcomes on this scale. This
canbedonebytheexpert butitisbetterifthepatient orin thecaseofinfants thepatients
parentscanbeinvolved becausedifferentpeoplehavedifferentpreferences. Ifthereareex-
ponentially manyoutcomes weneed some waytocombine them using multiattribute utility
functions. Forexamplewemaysaythatthecostsofvariouscomplications areadditive.
Verify and refine the model. To evaluate the system we need a set of correct input
output pairs; a so-called gold standard to compare against. For medical expert systems
G OL DS TA ND AR D
this usually means assembling the best available doctors presenting them with a few cases
Section16.7. Decision-Theoretic Expert Systems 635
Sex
Postcoarctectomy
Syndrome
Tachypnea Tachycardia
Paradoxical
Failure Hypertension
To Thrive
Aortic
Intercostal Aneurysm
Dyspnea
Recession
Paraplegia
Heart Intermediate Late
Age Treatment
Failure Result Result
Hepato-
C VA
megaly
Pulmonary Aortic
Crepitations Dissection
Myocardial
Cardiomegaly
Infarction
U
Figure16.10 Influencediagramforaorticcoarctationcourtesyof Peter Lucas.
and asking them for their diagnosis and recommended treatment plan. We then see how
wellthe system matches theirrecommendations. Ifitdoes poorly wetryto isolate theparts
that are going wrong and fix them. It can be useful to run the system backward. Instead
of presenting the system with symptoms and asking for a diagnosis we can present it with
a diagnosis such as heart failure examine the predicted probability of symptoms such as
tachycardia andcomparewiththemedicalliterature.
S EN SI TI VI TY Perform sensitivity analysis. This important step checks whether the best decision is
A NA LY SI S
sensitivetosmallchangesintheassignedprobabilities andutilitiesbysystematicallyvarying
those parameters and running the evaluation again. If small changes lead to significantly
different decisions then it could be worthwhile to spend more resources to collect better
data. Ifallvariationsleadtothesamedecision thentheagentwillhavemoreconfidencethat
itistherightdecision. Sensitivityanalysis isparticularly important becauseoneofthemain
17
M AK IN G C OM PL EX
D EC IS IO NS
In which we examine methods for deciding what to do today given that we may
decideagaintomorrow.
Inthischapterweaddressthecomputationalissuesinvolvedinmakingdecisionsinastochas-
tic environment. Whereas Chapter 16 was concerned with one-shot or episodic decision
problems in which the utility of each actions outcome was well known we are concerned
S EQ UE NT IA L here withsequential decision problems inwhich theagents utility depends onasequence
D EC IS IO NP RO BL EM
of decisions. Sequential decision problems incorporate utilities uncertainty and sensing
and include search and planning problems as special cases. Section 17.1 explains how se-
quential decision problems are defined and Sections 17.2 and 17.3 explain how they can
be solved to produce optimal behavior that balances the risks and rewards of acting in an
uncertain environment. Section 17.4 extends these ideas to the case of partially observable
environments and Section17.4.3developsacompletedesignfordecision-theoretic agentsin
partially observable environments combining dynamic Bayesian networks from Chapter 15
withdecision networksfrom Chapter16.
The second part of the chapter covers environments with multiple agents. In such en-
vironments the notion of optimal behavior is complicated by the interactions among the
agents. Section 17.5 introduces the main ideas of game theory including the idea that ra-
tional agents might need tobehave randomly. Section 17.6looks athow multiagent systems
canbedesigned sothatmultipleagentscanachieveacommongoal.
Supposethatanagentissituatedinthe43environmentshownin Figure17.1a. Beginning
inthestartstateitmustchooseanactionateachtimestep. Theinteraction withtheenviron-
ment terminates when the agent reaches one of the goal states marked 1 or1. Just as for
search problems the actions available to the agent in each state are given by A CT IO NSs
sometimes abbreviated to As; in the 43 environment the actions in every state are Up
Down Leftand Right. Weassume fornowthattheenvironment is fullyobservable sothat
theagentalwaysknowswhereitis.
645
a b
decisionproblem. b Illustrationofthetransitionmodeloftheenvironment:theintended
outcomeoccurswithprobability0.8butwithprobability0.2theagentmovesatrightangles
totheintendeddirection. Acollisionwithawallresultsinnomovement. Thetwoterminal
stateshavereward1and1respectivelyandallotherstateshavearewardof0.04.
Iftheenvironment weredeterministic asolutionwouldbeeasy: Up Up Right Right
Right. Unfortunately theenvironmentwontalwaysgoalongwiththissolutionbecausethe
actions are unreliable. The particular model of stochastic motion that weadopt isillustrated
in Figure 17.1b. Each action achieves the intended effect with probability 0.8 but the rest
ofthetimetheactionmovestheagentatrightanglestotheintendeddirection. Furthermore
iftheagentbumpsintoawallitstaysinthesamesquare. Forexamplefromthestartsquare
11 theaction Upmovestheagentto12withprobability 0.8butwithprobability 0.1it
movesrightto21andwithprobability 0.1itmovesleftbumpsintothewallandstaysin
11. In such an environment the sequence Up Up Right Right Right goes up around
thebarrierandreachesthegoalstateat43withprobability 0.850.32768. Thereisalsoa
smallchanceofaccidentallyreachingthegoalbygoingtheotherwayaroundwithprobability
0.140.8foragrandtotalof0.32776. Seealso Exercise17.1.
As in Chapter 3 the transition model or just model whenever no confusion can
arise describes the outcome of each action in each state. Here the outcome is stochastic
so we write
Pscid:2sa
to denote the probability of reaching state
scid:2
if action a is done in
states. Wewillassumethattransitionsare Markovianinthesenseof Chapter15thatisthe
cid:2
probabilityofreachings fromsdependsonlyonsandnotonthehistoryofearlierstates. For
now you can think of
Pscid:2sa
as a big three-dimensional table containing probabilities.
Laterin Section17.4.3wewillseethatthetransitionmodelcanberepresentedasadynamic
Bayesiannetworkjustasin Chapter15.
Tocompletethedefinitionofthetaskenvironment wemustspecifytheutilityfunction
for the agent. Because the decision problem is sequential the utility function will depend
on a sequence of statesan environment historyrather than on a single state. Later in
this section we investigate how such utility functions can be specified in general; for now
we simply stipulate that in each state s the agent receives a reward Rs which may be
R EW AR D
positive or negative but must be bounded. For our particular example the reward is 0.04
in all states except the terminal states which have rewards 1 and 1. The utility of an
Section17.1. Sequential Decision Problems 647
environment history is just for now the sum of the rewards received. For example if the
agent reaches the 1 state after 10 steps its total utility will be 0.6. The negative reward of
0.04 gives the agent an incentive to reach 43 quickly so our environment is a stochastic
generalization of the search problems of Chapter 3. Another way of saying this is that the
agentdoesnotenjoylivinginthisenvironment andsowantstoleaveassoonaspossible.
Tosumup: asequentialdecisionproblemforafullyobservablestochasticenvironment
M AR KO VD EC IS IO N witha Markoviantransitionmodelandadditiverewardsiscalleda Markovdecisionprocess
P RO CE SS
or M DPandconsistsofasetofstateswithaninitialstates 0;aset A CT IO NSsofactions
ineachstate;atransition model Pscid:2sa;andarewardfunction Rs.1
Thenextquestion iswhatdoesasolution totheproblem look like? Wehaveseen that
anyfixedactionsequence wontsolvetheproblembecausetheagentmightendupinastate
otherthanthegoal. Thereforeasolutionmustspecifywhattheagentshoulddoforanystate
thattheagentmightreach. Asolutionofthiskindiscalledapolicy. Itistraditionaltodenote
P OL IC Y
a policy by π and πs is the action recommended by the policy π for state s. If the agent
has acomplete policy then no matterwhat theoutcome of anyaction theagent willalways
knowwhattodonext.
Eachtimeagivenpolicyisexecutedstartingfromtheinitialstatethestochastic nature
of the environment may lead to a different environment history. The quality of a policy is
therefore measured by the expected utility of the possible environment histories generated
by that policy. An optimal policy is a policy that yields the highest expected utility. We
O PT IM AL PO LI CY
""
use π to denote an optimal policy. Given π the agent decides what to do by consulting
""
its current percept which tells it the current state s and then executing the action π s. A
policyrepresentstheagentfunctionexplicitlyandisthereforeadescriptionofasimplereflex
agentcomputed fromtheinformation usedforautility-based agent.
An optimal policy for the world of Figure 17.1 is shown in Figure 17.2a. Notice
that because the cost of taking a step is fairly small compared with the penalty for ending
up in 42 by accident the optimal policy for the state 31 is conservative. The policy
recommends taking the long way round rather than taking the shortcut and thereby risking
entering 42.
Thebalanceofriskandrewardchanges depending onthevalue of Rsforthenonter-
minal states. Figure 17.2b shows optimal policies forfourdifferent ranges of Rs. When
Rs 1.6284 life is so painful that the agent heads straight for the nearest exit even if
the exit is worth 1. When 0.4278 Rs 0.0850 life is quite unpleasant; the agent
takes the shortest route to the 1 state and is willing to risk falling into the 1 state by acci-
dent. In particular the agent takes the shortcut from 31. When life is only slightly dreary
0.0221 Rs 0 theoptimal policy takes norisks atall. In41and32 theagent
heads directly away from the 1 state so that it cannot fall in by accident even though this
means banging its head against the wall quite a few times. Finally if Rs 0 then life is
positively enjoyable and the agent avoids both exits. As long as the actions in 41 32
is Rsascid:3. This simplifies the description of some environments but does not change the problem in any
fundamentalwayasshownin Exercise17.4.
1 1
1 1
1 1
1
1 1
0.0221 Rs 0 Rs 0
a b
Figure17.2 a Anoptimalpolicyforthestochasticenvironmentwith Rs 0.04in
thenonterminalstates. b Optimalpoliciesforfourdifferentrangesof Rs.
and33areasshowneverypolicyisoptimalandtheagent obtainsinfinitetotalrewardbe-
causeitneverentersaterminalstate. Surprisingly itturnsoutthattherearesixotheroptimal
policies forvariousrangesof Rs;Exercise17.5asksyoutofindthem.
The careful balancing of risk and reward is a characteristic of M DPs that does not
arise in deterministic search problems; moreover it is a characteristic of many real-world
decision problems. For this reason M DPs have been studied in several fields including
A I operations research economics and control theory. Dozens of algorithms have been
proposed for calculating optimal policies. In sections 17.2 and 17.3 we describe two of the
most important algorithm families. First however we must complete our investigation of
utilities andpoliciesforsequential decision problems.
Inthe M DPexamplein Figure17.1theperformance oftheagentwasmeasuredbyasumof
rewards for the states visited. This choice of performance measure is not arbitrary but it is
not the only possibility for the utility function on environment histories which we write as
U s s ...s . Ouranalysisdrawsonmultiattributeutilitytheory Section16.4and
h 0 1 n
issomewhattechnical; theimpatient readermaywishtoskip tothenextsection.
Thefirstquestion toanswer is whetherthere is a finitehorizon oran infinitehorizon
F IN IT EH OR IZ ON
fordecision making. Afinitehorizon means thatthere isa fixed time N afterwhichnothing
I NF IN IT EH OR IZ ON
mattersthe gameisover sotospeak. Thus U s s ...s U s s ...s
h 0 1 Nk h 0 1 N
forallk 0. Forexamplesupposeanagentstartsat31inthe 43worldof Figure17.1
and suppose that N 3. Then to have any chance of reaching the 1 state the agent must
head directly for it and the optimal action is to go Up. On the other hand if N100
then there is plenty of time to take the safe route by going Left. So with a finite horizon
Section17.1. Sequential Decision Problems 649
the optimal action in a given state could change over time. We say that the optimal policy
N ON ST AT IO NA RY for a finite horizon is nonstationary. With no fixed time limit on the other hand there is
P OL IC Y
no reason to behave differently in the same state at different times. Hence the optimal ac-
tion depends only on the current state and the optimal policy is stationary. Policies forthe
S TA TI ON AR YP OL IC Y
infinite-horizon casearetherefore simplerthanthoseforthefinite-horizon caseandwedeal
mainly with the infinite-horizon case in this chapter. We will see later that for partially ob-
servableenvironmentstheinfinite-horizoncaseisnotsosimple. Notethatinfinitehorizon
does not necessarily mean that all state sequences are infinite; it just means that there is no
fixed deadline. In particular there can be finite state sequences in an infinite-horizon M DP
containing aterminalstate.
Thenext question wemust decide is how to calculate the utility of state sequences. In
theterminologyofmultiattribute utilitytheoryeachstates canbeviewedasanattributeof
i
thestatesequence s s s .... Toobtainasimpleexpression intermsoftheattributes we
will need to make some sort of preference-independence assumption. The most natural as-
S TA TI ON AR Y sumptionisthattheagents preferences betweenstatesequences arestationary. Stationarity
P RE FE RE NC E
cid:2 cid:2 cid:2
forpreferencesmeansthefollowing: iftwostatesequences s s s ...ands s s ...
cid:2
beginwiththesamestatei.e.s s thenthetwosequencesshouldbepreference-ordered
cid:2 cid:2
thesamewayasthesequencess s ...ands s .... In Englishthismeansthatifyou
prefer one future to another starting tomorrow then you should still prefer that future if it
were to start today instead. Stationarity is a fairly innocuous-looking assumption with very
strong consequences: it turns out that under stationarity there are just two coherent ways to
assignutilities tosequences:
1. Additiverewards: Theutilityofastatesequence is
A DD IT IV ER EW AR D
U s s s ... Rs Rs Rs .
h 0 1 2 0 1 2
The 43 world in Figure 17.1 uses additive rewards. Notice that additivity was used
implicitlyinouruseofpathcostfunctions inheuristicsearchalgorithms Chapter3.
D IS CO UN TE D 2. Discountedrewards: Theutilityofastatesequenceis
R EW AR D
U s s s ... Rs γ Rs γ2 Rs
h 0 1 2 0 1 2
wherethediscountfactorγisanumberbetween0and1. Thediscountfactordescribes
D IS CO UN TF AC TO R
the preference of an agent for current rewards over future rewards. When γ is close
to0 rewards inthe distant future are viewed as insignificant. When γ is 1 discounted
rewards are exactly equivalent to additive rewards so additive rewards are a special
case of discounted rewards. Discounting appears to be a good model of both animal
andhumanpreferencesovertime. Adiscountfactorofγisequivalenttoaninterestrate
of1γ1.
For reasons that will shortly become clear we assume discounted rewards in the remainder
ofthechapter although sometimesweallowγ1.
Lurking beneath our choice of infinite horizons is a problem: if the environment does
not contain aterminal state orifthe agent neverreaches one then all environment histories
will be infinitely long and utilities with additive undiscounted rewards will generally be
infinite. Whilewecanagreethatisbetterthancomparingtwostatesequenceswith
utilityismoredifficult. Therearethreesolutions twoofwhichwehaveseenalready:
1. With discounted rewards the utility of an infinite sequence is finite. In fact if γ 1
andrewardsareboundedby R wehave
max
cid:12 cid:12
U s s s ... γt Rs γt R R 1γ 17.1
h 0 1 2 t max max
t0 t0
usingthestandard formulaforthesumofaninfinitegeometricseries.
2. Iftheenvironment contains terminal states and ifthe agent isguaranteed to getto one
eventually then we will never need to compare infinite sequences. A policy that is
guaranteed toreachaterminalstateiscalledaproperpolicy. Withproperpolicies we
P RO PE RP OL IC Y
can use γ1 i.e. additive rewards. The first three policies shown in Figure 17.2b
areproperbutthefourthisimproper. Itgainsinfinitetotalrewardbystayingawayfrom
theterminalstateswhentherewardforthenonterminalstatesispositive. Theexistence
of improper policies can cause the standard algorithms for solving M DPs to fail with
additiverewardsandsoprovides agoodreasonforusingdiscounted rewards.
3. Infinite sequences can be compared in terms of the average reward obtained pertime
A VE RA GE RE WA RD
step. Suppose that square 11 in the 43 world has a reward of 0.1 while the other
nonterminal states have a reward of 0.01. Then a policy that does its best to stay in
11willhavehigheraverage rewardthanonethatstayselsewhere. Average rewardis
a useful criterion for some problems but the analysis of average-reward algorithms is
beyondthescopeofthisbook.
Insumdiscounted rewardspresent thefewestdifficulties inevaluating statesequences.
Having decided that the utility of a given state sequence is the sum of discounted rewards
obtained during the sequence we can compare policies by comparing the expected utilities
obtained when executing them. We assume the agent is in some initial state s and define S
t
a random variable to be the state the agent reaches at time t when executing a particular
policyπ. Obviously S sthestatetheagent isinnow. Theprobability distribution over
0
statesequences S S ...isdeterminedbytheinitialstatesthepolicyπandthetransition
modelfortheenvironment.
Theexpectedutilityobtained byexecutingπ startinginsisgivenby
""
cid:12
Uπs E γt R S 17.2
t
t0
where the expectation is with respect tothe probability distribution overstate sequences de-
terminedbysandπ. Nowoutofallthepoliciestheagentcouldchoosetoexecutestartingin
""
soneormorewillhavehigherexpectedutilitiesthanalltheothers. Welluseπ todenote
s
oneofthesepolicies:
π argmax Uπs. 17.3
s
π
Section17.1. Sequential Decision Problems 651
""
Remember that π is a policy so it recommends an action for every state; its connection
s
with s in particular is that its an optimal policy when s is the starting state. A remarkable
consequence of using discounted utilities with infinite horizons is that the optimal policy is
independent of the starting state. Of course the action sequence wont be independent;
remember that a policy is a function specifying an action for each state. This fact seems
intuitivelyobvious: ifpolicyπ isoptimalstartinginaandpolicyπ isoptimalstartinginb
a b
then when they reach a third state c theres no good reason for them to disagree with each
other orwithπ aboutwhattodonext.2 Sowecansimplywriteπ foranoptimalpolicy.
c
Given this definition the true utility of a state is just
Uπ
sthat is the expected
sum of discounted rewards if the agent executes an optimal policy. We write this as Us
matchingthenotation usedin Chapter16fortheutility ofan outcome. Noticethat Usand
Rs are quite different quantities; Rs is the short term reward for being in s whereas
Us is the long term total reward from s onward. Figure 17.3 shows the utilities for the
43world. Noticethattheutilities arehigherforstatesclosertothe1exitbecause fewer
stepsarerequired toreachtheexit.
Rs 0.04fornonterminalstates.
The utility function Us allows the agent to select actions by using the principle of
maximum expected utility from Chapter 16that is choose the action that maximizes the
expectedutilityofthesubsequent state:
cid:12
π
s argmax
Pscid:2sa Uscid:2
. 17.4
a As
scid:3
Thenexttwosections describealgorithms forfindingoptimalpolicies.
rewardsovertime. Theprooffollowsdirectlyfromtheuniquenessoftheutilityfunctiononstatesasshownin
Section17.2.
In this section we present an algorithm called value iteration for calculating an optimal
V AL UE IT ER AT IO N
policy. Thebasicideaistocalculate theutilityofeachstateandthenusethestateutilities to
selectanoptimalactionineachstate.
Section17.1.2definedtheutilityofbeinginastateastheexpectedsumofdiscountedrewards
from that point onwards. From this it follows that there is a direct relationship between the
utility ofastate and theutility ofitsneighbors: the utility ofastate istheimmediate reward
for that state plus the expected discounted utility of the next state assuming that the agent
choosestheoptimalaction. Thatistheutilityofastateisgivenby
cid:12
Us Rsγ max
Pscid:2sa Uscid:2
. 17.5
a As
scid:3
This is called the Bellman equation after Richard Bellman 1957. The utilities of the
B EL LM AN EQ UA TI ON
statesdefinedby Equation17.2astheexpectedutilityofsubsequentstatesequencesare
solutions ofthe set of Bellman equations. In fact they are the unique solutions as weshow
in Section17.2.3.
Let us look at one of the Bellman equations for the 43 world. The equation for the
state11is
U11 0.04γ max 0.8 U120.1 U210.1 U11 Up
0.9 U110.1 U12 Left
0.9 U110.1 U21 Down
0.8 U210.1 U120.1 U11. Right
Whenwepluginthenumbersfrom Figure17.3wefindthat Upisthebestaction.
The Bellmanequation isthebasisofthevalueiterationalgorithm forsolving M DPs. Ifthere
arenpossiblestatesthentherearen Bellmanequations oneforeachstate. Thenequations
containnunknownstheutilitiesofthestates. Sowewouldliketosolvethesesimultaneous
equations tofindtheutilities. Thereisoneproblem: theequationsarenonlinear becausethe
max operator is not a linear operator. Whereas systems of linear equations can be solved
quicklyusinglinearalgebratechniquessystemsofnonlinearequationsaremoreproblematic.
Onethingtotryisaniterativeapproach. Westartwitharbitraryinitialvaluesfortheutilities
calculate the right-hand side of the equation and plug it into the left-hand sidethereby
updating the utility of each state from the utilities of its neighbors. We repeat this until we
reachanequilibrium. Let U sbetheutilityvalueforstatesattheithiteration. Theiteration
i
stepcalleda Bellmanupdatelookslikethis:
B EL LM AN UP DA TE
cid:12
U s Rsγ max Pscid:2sa U scid:2 17.6
i1 i
a As
scid:3
Section17.2. Value Iteration 653
function V AL UE-I TE RA TI ONmdpcid:2returnsautilityfunction
inputs:mdpan M DPwithstates Sactions Astransitionmodel Pscid:5sa
rewards Rsdiscountγ
cid:2themaximumerrorallowedintheutilityofanystate
localvariables: U Ucid:5vectorsofutilitiesforstatesin Sinitiallyzero
δthemaximumchangeintheutilityofanystateinaniteration
repeat
U Ucid:5;δ0
foreachstates in S do cid:12
Ucid:5s Rs γ max Pscid:5sa Uscid:5
a As
scid:3
if Ucid:5s Us δthenδ Ucid:5s Us
untilδ cid:21γγ
return U
Figure17.4 Thevalueiterationalgorithmforcalculatingutilitiesof states. Thetermina-
tionconditionisfrom Equation17.8.
1
0.8
0.6
0.4
0.2
0
-0.2
setamitse
ytilit U
1e07
43
33 1e06
31
41
100
10
1
Number of iterations
deriuqer
snoitaret I
c 0.001
c 0.01
c 0.1
Discount factor γ
a b
Figure17.5 a Graphshowingtheevolutionoftheutilitiesofselectedstatesusingvalue
iteration. b The number of value iterations k required to guarantee an error of at most
cid:2c R fordifferentvaluesofcasafunctionofthediscountfactorγ.
max
where the update is assumed to be applied simultaneously to all the states at each iteration.
If we apply the Bellman update infinitely often we are guaranteed to reach an equilibrium
see Section 17.2.3 in which case the final utility values must be solutions to the Bellman
equations. Infact theyarealsotheuniquesolutions andthecorresponding policyobtained
using Equation 17.4 is optimal. The algorithm called V AL UE-I TE RA TI ON is shown in
Figure17.4.
Wecan apply value iteration to the 43 world in Figure 17.1a. Starting with initial
valuesofzerotheutilitiesevolveasshownin Figure17.5a. Noticehowthestatesatdiffer-
entdistancesfrom43accumulatenegativerewarduntilapathisfoundto43whereupon
the utilities start to increase. We can think of the value iteration algorithm as propagating
information through thestatespacebymeansoflocalupdates.
Wesaid that value iteration eventually converges toaunique set ofsolutions ofthe Bellman
equations. In this section we explain why this happens. We introduce some useful mathe-
maticalideasalongthewayandweobtainsomemethodsforassessingtheerrorintheutility
function returned whenthealgorithm isterminatedearly;thisisusefulbecause itmeansthat
wedonthavetorunforever. Thissection isquitetechnical.
Thebasicconceptusedinshowingthatvalueiterationconvergesisthenotionofacon-
traction. Roughlyspeakingacontractionisafunctionofoneargumentthatwhenappliedto
C ON TR AC TI ON
twodifferent inputs inturn produces twooutputvalues thatareclosertogether byatleast
some constant factor than the original inputs. Forexample the function divide by two is
a contraction because after we divide any two numbers by two their difference is halved.
Noticethatthedividebytwofunction hasafixedpoint namelyzerothatisunchanged by
the application ofthe function. Fromthis example wecan discern twoimportant properties
ofcontractions:
A contraction has only one fixed point; if there were two fixed points they would not
getclosertogetherwhenthefunction wasapplied soitwouldnotbeacontraction.
When the function is applied to any argument the value must get closer to the fixed
point because the fixedpoint does not move so repeated application of acontraction
alwaysreachesthefixedpointinthelimit.
Nowsupposeweviewthe Bellmanupdate Equation17.6asanoperator B thatisapplied
simultaneously toupdate theutilityofeverystate. Let U denotethevectorofutilities forall
i
thestatesattheithiteration. Thenthe Bellmanupdateequationcanbewrittenas
U B U .
i1 i
Nextweneedawaytomeasuredistancesbetweenutilityvectors. Wewillusethemaxnorm
M AX NO RM
whichmeasuresthelengthofavectorbytheabsolute value ofitsbiggest component:
U max Us.
s
With this definition the distance between two vectors U Ucid:2 is the maximum dif-
ference between any two corresponding elements. The main result of this section is the
cid:2
following: Let U and U beanytwoutilityvectors. Thenwehave
i i
B U B Ucid:2 γ U Ucid:2. 17.7
i i i i
That is the Bellman update is a contraction by a factor of γ on the space of utility vectors.
Exercise17.6providessomeguidanceonprovingthisclaim. Hencefromthepropertiesof
contractions in general it follows that value iteration always converges to a unique solution
ofthe Bellmanequations wheneverγ 1.
Section17.2. Value Iteration 655
We can also use the contraction property to analyze the rate of convergence to a solu-
cid:2
tion. In particular we can replace U in Equation 17.7 with the true utilities U for which
i
B UU. Thenweobtaintheinequality
B U U γ U U.
i i
Soifweview U Uastheerrorintheestimate U weseethattheerrorisreducedbya
i i
factorofatleast γ oneachiteration. Thismeansthatvalueiteration converges exponentially
fast. We can calculate the number of iterations required to reach a specified error bound cid:2
as follows: First recall from Equation 17.1 that the utilities of all states are bounded by
R 1γ. This means that the maximum initial error U U 2 R 1γ.
max 0 max
Suppose we run for N iterations to reach an error of at most cid:2. Then because the error is
reducedbyatleast γ eachtimewerequire γ N 2 R 1γ cid:2. Takinglogswefind
max
Nlog2 R cid:21γlog1γ
max
iterations suffice. Figure17.5bshowshow N varieswithγfordifferent valuesoftheratio
cid:2 R . The good news is that because of the exponentially fast convergence N does not
max
dependmuchontheratiocid:2 R . Thebadnewsisthat N growsrapidlyasγ becomesclose
max
to 1. We can get fast convergence if we make γ small but this effectively gives the agent a
shorthorizonandcouldmissthelong-term effectsoftheagents actions.
The error bound in the preceding paragraph gives some idea of the factors influencing
the run time of the algorithm but is sometimes overly conservative as a method of deciding
when to stop the iteration. For the latter purpose we can use a bound relating the error
to the size of the Bellman update on any given iteration. From the contraction property
Equation17.7itcanbeshownthatiftheupdateissmalli.e.nostatesutilitychangesby
muchthentheerrorcomparedwiththetrueutilityfunction alsoissmall. Moreprecisely
if U U cid:21γγ then U U cid:2. 17.8
i1 i i1
Thisistheterminationcondition usedinthe V AL UE-I TE RA TI ON algorithm of Figure17.4.
Sofar wehaveanalyzed theerrorintheutility function returned bythevalueiteration
algorithm. What the agent really cares about however is how well it will do if it makes its
decisionsonthebasisofthisutilityfunction. Supposethatafteriiterationsofvalueiteration
the agent has an estimate U of the true utility U and obtains the M EU policy π based on
i i
one-step look-ahead using U as in Equation 17.4. Will the resulting behavior be nearly
i
asgoodastheoptimalbehavior? Thisisacrucialquestionforanyrealagentanditturnsout
that the answer is yes. Uπis is the utility obtained if π is executed starting in s and the
i
policy loss Uπi Uisthe mosttheagent canlose byexecuting π instead oftheoptimal
P OL IC YL OS S i
""
policyπ . Thepolicylossofπ isconnected totheerrorin U bythefollowinginequality:
i i
if U U cid:2 then Uπi U 2cid:2γ1γ. 17.9
i
Inpracticeitoftenoccursthatπ becomesoptimallongbefore U hasconverged. Figure17.6
i i
showshowthe maximumerrorin U and thepolicy lossapproach zeroasthevalue iteration
i
processproceedsforthe43environmentwithγ0.9. Thepolicyπ isoptimalwheni4
i
eventhoughthemaximumerrorin U isstill0.46.
i
Now we have everything we need to use value iteration in practice. We know that
it converges to the correct utilities we can bound the error in the utility estimates if we
stop after a finite number of iterations and we can bound the policy loss that results from
executing the corresponding M EU policy. As a final note all of the results in this section
depend on discounting with γ 1. If γ1 and the environment contains terminal states
then a similar set of convergence results and error bounds can be derived whenever certain
technical conditions aresatisfied.
In the previous section we observed that it is possible to get an optimal policy even when
the utility function estimate is inaccurate. If one action is clearly better than all others then
the exact magnitude of the utilities on the states involved need not be precise. This insight
suggestsanalternativewaytofindoptimalpolicies. Thepolicyiterationalgorithmalternates
P OL IC YI TE RA TI ON
thefollowingtwostepsbeginning fromsomeinitialpolicy π :
0
Policy evaluation: given a policy π calculate U Uπi the utility of each state if π
P OL IC YE VA LU AT IO N i i i
weretobeexecuted.
P OL IC Y Policy improvement: Calculate a new M EU policy π using one-step look-ahead
I MP RO VE ME NT i1
basedon U asin Equation17.4.
i
Thealgorithm terminateswhenthepolicyimprovementstepyieldsnochangeintheutilities.
Atthis point we know that the utility function U is a fixed point of the Bellman update so
i
itisasolution tothe Bellmanequations andπ mustbeanoptimalpolicy. Becausethereare
i
onlyfinitelymanypolicies forafinitestatespace andeachiteration canbeshowntoyield a
betterpolicypolicyiteration mustterminate. Thealgorithm isshownin Figure17.7.
The policy improvement step is obviously straightforward but how do we implement
the P OL IC Y-E VA LU AT IO N routine? It turns out that doing so is much simpler than solving
the standard Bellman equations which is what value iteration does because the action in
eachstateisfixedbythepolicy. Attheithiteration thepolicyπ specifiestheactionπ sin
i i
1
0.8
0.6
0.4
0.2
0
ssol
ycilo Prorre
xa M
Max error
Policy loss
Number of iterations
Uπi Uasafunctionofthenumberofiterationsofvalueiteration.
Section17.3. Policy Iteration 657
states. Thismeansthatwehaveasimplifiedversion ofthe Bellmanequation 17.5relating
theutilityofsunderπ totheutilitiesofitsneighbors:
i
cid:12
U s Rsγ
Pscid:2sπ
s U
scid:2
. 17.10
i i i
scid:3
Forexamplesuppose π isthepolicyshownin Figure17.2a. Thenwehaveπ 11 Up
i i
π 12 Upandsoonandthesimplified Bellmanequations are
i
U 11 0.040.8 U 120.1 U 110.1 U 21
i i i i
U 12 0.040.8 U 130.2 U 12
i i i
.
.
.
The important point is that these equations are linear because the maxoperator has been
removed. For n states we have n linear equations with n unknowns which can be solved
exactlyintime On3bystandardlinearalgebramethods.
Forsmallstatespacespolicyevaluationusingexactsolutionmethodsisoftenthemost
efficient approach. For large state spaces On3 time might be prohibitive. Fortunately it
is not necessary to do exact policy evaluation. Instead we can perform some number of
simplified value iteration steps simplified because the policy is fixed to give a reasonably
goodapproximation oftheutilities. Thesimplified Bellmanupdateforthisprocessis
cid:12
U s Rsγ Pscid:2sπ s U scid:2
i1 i i
scid:3
and this is repeated k times to produce the next utility estimate. The resulting algorithm is
M OD IF IE DP OL IC Y calledmodifiedpolicyiteration. Itisoftenmuchmoreefficientthanstandardpolicyiteration
I TE RA TI ON
orvalueiteration.
function P OL IC Y-I TE RA TI ONmdpreturnsapolicy
inputs:mdpan M DPwithstates Sactions Astransitionmodel Pscid:5sa
localvariables: Uavectorofutilitiesforstatesin Sinitiallyzero
πapolicyvectorindexedbystateinitiallyrandom
repeat
U P OL IC Y-E VA LU AT IO Nπ Umdp
unchanged?true
foreachstatescid:12in S do cid:12
if max
Pscid:5sa Uscid:5
""
Pscid:5sπs Uscid:5
thendo
a As
scid:3 cid:12 scid:3
πsargmax Pscid:5sa Uscid:5
a As
scid:3
unchanged?false
untilunchanged?
returnπ
Figure17.7 Thepolicyiterationalgorithmforcalculatinganoptimalpolicy.
The algorithms we have described so far require updating the utility or policy for all
states at once. It turns out that this is not strictly necessary. In fact on each iteration we
can pick any subset ofstates and apply either kind ofupdating policy improvement orsim-
plified value iteration to that subset. This very general algorithm is called asynchronous
A SY NC HR ON OU S policy iteration. Given certain conditions on the initial policy and initial utility function
P OL IC YI TE RA TI ON
asynchronous policy iteration is guaranteed to converge to an optimal policy. The freedom
to choose any states to work on means that we can design much more efficient heuristic
algorithmsfor example algorithms that concentrate on updating the values of states that
arelikely tobereachedbyagoodpolicy. Thismakesalotofsenseinreallife: ifonehasno
intention ofthrowing oneself offacliff oneshould notspend timeworrying about theexact
valueoftheresulting states.
Thedescription of Markov decision processes in Section 17.1assumed that theenvironment
was fully observable. With this assumption the agent always knows which state it is in.
Thiscombinedwiththe Markovassumptionforthetransitionmodelmeansthattheoptimal
policydependsonlyonthecurrentstate. Whentheenvironmentisonlypartiallyobservable
the situation is one might say much less clear. The agent does not necessarily know which
stateitisinsoitcannotexecutetheactionπsrecommendedforthatstate. Furthermorethe
utility ofastatesandtheoptimalactioninsdepend notjustonsbutalsoonhowmuchthe
P AR TI AL LY agent knows whenitisins. Forthese reasons partially observable M DPsor P OM DPs
O BS ER VA BL EM DP
pronouncedpom-dee-peesareusuallyviewedasmuchmoredifficultthanordinary M DPs.
Wecannotavoid P OM DPshoweverbecausetherealworldisone.
To get a handle on P OM DPs we must first define them properly. A P OM DP has the same
elements as an M DPthe transition model Pscid:2sa actions As and reward function
Rsbut like the partially observable search problems of Section 4.4 italso has asensor
model Pes. Hereasin Chapter15thesensormodelspecifiestheprobability ofperceiv-
ing evidence e in state s.3 For example we can convert the 43 world of Figure 17.1 into
a P OM DP by adding a noisy or partial sensor instead of assuming that the agent knows its
location exactly. Suchasensormightmeasure the number ofadjacent walls whichhappens
to be 2 in all the nonterminal squares except for those in the third column where the value
is1;anoisyversionmightgivethewrongvaluewithprobability 0.1.
In Chapters 4 and 11 we studied nondeterministic and partially observable planning
problems andidentified the beliefstatethe setofactual states the agent might beinas a
keyconceptfordescribingandcalculatingsolutions. In P OM DPsthebeliefstatebbecomesa
probability distribution overallpossiblestatesjustasin Chapter15. Forexample theinitial
againthischangeisnotfundamental.
Section17.4. Partially Observable M DPs 659
beliefstateforthe43 PO MD Pcouldbetheuniformdistribution overtheninenonterminal
states i.e. cid:1611 11 111 1100cid:17. We write bs for the probability assigned to the
actualstatesbybeliefstateb. Theagentcancalculateitscurrentbeliefstateastheconditional
probability distribution over the actual states given the sequence of percepts and actions so
far. Thisisessentiallythefilteringtaskdescribedin Chapter15. Thebasicrecursivefiltering
equation 15.5 on page 572 shows how to calculate the new belief state from the previous
belief state and thenew evidence. For P OM DPswealso have anaction to consider but the
result isessentially thesame. If bswasthe previous belief state and theagent does action
aandthenperceivesevidence ethenthenewbeliefstateisgivenby
cid:12
bcid:2 scid:2
""
α Pescid:2
""
Pscid:2sabs
s
where αis anormalizing constant that makes the belief state sum to 1. Byanalogy with the
updateoperatorforfilteringpage572wecanwritethisas
cid:2
b F OR WA RDbae. 17.11
Inthe43 PO MD Psupposetheagentmoves Leftanditssensorreports1adjacentwall;then
its quite likely although not guaranteed because both the motion and the sensor are noisy
thattheagentisnowin31. Exercise17.13asksyoutocalculatetheexactprobabilityvalues
forthenewbeliefstate.
The fundamental insight required to understand P OM DPs is this: the optimal action
depends onlyontheagents current belief state. Thatistheoptimal policycanbedescribed
""
by a mapping π b from belief states to actions. It does not depend on the actual state the
agentisin. Thisisagoodthingbecausetheagentdoesnotknowitsactualstate;allitknows
isthebeliefstate. Hencethedecision cycleofa P OM DPagentcanbebroken downintothe
followingthreesteps:
""
1. Giventhecurrentbeliefstate bexecutetheactionaπ b.
2. Receivepercept e.
3. Setthecurrentbeliefstateto F OR WA RDbaeandrepeat.
Nowwecanthink of P OM DPsasrequiring asearch inbelief-state space justlikethemeth-
ods for sensorless and contingency problems in Chapter 4. The main difference is that the
P OM DPbelief-state space iscontinuous because a P OM DPbelief stateisaprobability dis-
tribution. For example a belief state for the 43 world is a point in an 11-dimensional
continuous space. An action changes the belief state not just the physical state. Hence the
action isevaluated atleastinpartaccording totheinformation theagentacquires asaresult.
P OM DPstherefore include the value of information Section 16.6 as one component of the
decision problem.
Lets look more carefully at the outcome of actions. In particular lets calculate the
cid:2
probability thatanagentinbeliefstatebreachesbeliefstateb afterexecutingactiona. Now
if we knew the action and the subsequent percept then Equation 17.11 would provide a
cid:2
deterministic update to the belief state: b F OR WA RDbae. Of course the subsequent
cid:2
percept isnot yetknown sotheagent mightarrive inoneofseveral possible belief states b
depending on the percept that is received. The probability of perceiving e given that a was
cid:2
performed starting inbelief state b is given by summing overall the actual states s that the
agentmightreach:
cid:12
Peab Peascid:2 b Pscid:2ab
cid:12scid:3
""
Pescid:2 Pscid:2ab
cid:12scid:3
cid:12
""
Pescid:2
""
Pscid:2sabs.
scid:3 s
Let us write the probability of reaching
bcid:2
from b given action a as
Pbcid:2ba.
Then that
givesus
cid:12
Pbcid:2ba
""
Pbcid:2ab
""
Pbcid:2eab Peab
cid:12 e cid:12 cid:12
""
Pbcid:2eab Pescid:2
""
Pscid:2sabs
17.12
e scid:3 s
where Pbcid:2eabis1ifbcid:2
F OR WA RDbaeand0otherwise.
Equation17.12canbeviewedasdefiningatransitionmodelforthebelief-statespace.
Wecanalsodefinearewardfunction forbeliefstatesi.e.theexpectedrewardfortheactual
statestheagentmightbein:
cid:12
ρb bs Rs.
s
Together
Pbcid:2ba
and ρb define an observable M DP on the space of belief states. Fur-
""
thermoreitcanbeshownthatanoptimalpolicyforthis M DPπ bisalsoanoptimalpolicy
fortheoriginal P OM DP.Inotherwords solving a P OM DPonaphysical statespace canbe
reducedtosolvingan M DPonthecorresponding belief-statespace. Thisfactisperhapsless
surprisingifwerememberthatthebeliefstateisalwaysobservabletotheagentbydefinition.
Notice that although we have reduced P OM DPs to M DPs the M DPwe obtain has a
continuous and usually high-dimensional state space. None of the M DP algorithms de-
scribed in Sections 17.2 and 17.3 applies directly to such M DPs. The next two subsec-
tions describe a value iteration algorithm designed specifically for P OM DPs and an online
decision-making algorithm similartothosedeveloped for gamesin Chapter5.
Section 17.2 described a value iteration algorithm that computed one utility value for each
state. With infinitely many belief states we need to be more creative. Consider an optimal
""
policy π and itsapplication in aspecific belief state b: the policy generates an action then
foreachsubsequent percept thebeliefstateisupdated and anewaction isgenerated andso
on. Forthisspecificbthereforethepolicyisexactlyequivalenttoaconditionalplanasde-
finedin Chapter4fornondeterministicandpartiallyobservableproblems. Insteadofthinking
about policies letusthink about conditional plans andhowtheexpected utility ofexecuting
afixedconditional planvarieswiththeinitialbeliefstate. Wemaketwoobservations:
Section17.4. Partially Observable M DPs 661
1. Lettheutilityofexecutingafixedconditionalplanpstartinginphysicalstatesbeα s.
cid:2 p
Thentheexpected utility ofexecuting pinbelief state bisjust bsα sorbα
s p p
if we think of them both as vectors. Hence the expected utility of a fixed conditional
planvarieslinearly withb;thatisitcorresponds toahyperplane inbeliefspace.
2. At any given belief state b the optimal policy will choose to execute the conditional
planwithhighestexpectedutility;andtheexpectedutilityofbundertheoptimalpolicy
isjusttheutilityofthatconditional plan:
Ub Uπ b maxbα .
p
p
""
Iftheoptimalpolicyπ choosestoexecutepstartingatbthenitisreasonabletoexpect
that it might choose to execute p in belief states that are very close to b; in fact if we
bound the depth of the conditional plans then there are only finitely many such plans
and the continuous space of belief states will generally be divided into regions each
corresponding toaparticularconditional planthatisoptimalinthatregion.
Fromthese twoobservations weseethattheutility function Ubonbelief states being the
maximumofacollection ofhyperplanes willbepiecewiselinearandconvex.
Toillustrate this weuseasimple two-state world. Thestates arelabeled 0and1with
R00 and R11. There are two actions: Stay stays put with probability 0.9 and Go
switches to the other state with probability 0.9. Fornow we will assume the discount factor
γ1. Thesensor reports the correct state with probability 0.6. Obviously the agent should
Staywhenitthinksitsinstate1and Gowhenitthinksitsinstate0.
The advantage of a two-state world is that the belief space can be viewed as one-
dimensional because the two probabilities must sum to 1. In Figure 17.8a the x-axis
representsthebeliefstatedefinedbyb1theprobabilityofbeinginstate1. Nowletuscon-
sider the one-step plans Stay and Go each of which receives the reward for the current
statefollowedbythediscounted rewardforthestatereached aftertheaction:
α 0 R0γ0.9 R00.1 R1 0.1
Stay
α 1 R1γ0.9 R10.1 R0 1.9
Stay
α 0 R0γ0.9 R10.1 R0 0.9
Go
α 1 R1γ0.9 R00.1 R1 1.1
Go
Thehyperplaneslinesinthiscaseforbα andbα areshownin Figure17.8aand
Stay Go
their maximum is shown in bold. The bold line therefore represents the utility function for
the finite-horizon problem that allows just one action and in each piece of the piecewise
linear utility function the optimal action is the first action of the corresponding conditional
plan. Inthiscasetheoptimalone-step policyisto Staywhenb1 0.5and Gootherwise.
Oncewehave utilities α sforallthe conditional plans pofdepth 1ineach physical
p
state s we can compute the utilities for conditional plans of depth 2 by considering each
possible first action each possible subsequent percept and then each way of choosing a
depth-1plantoexecuteforeachpercept:
Stay; if Percept0then Stay else Stay
Stay; if Percept0then Stay else Go...
3
2.5
2
1.5
1
0.5
0
ytilit U
3
2.5
2
Stay
1.5
Go
1
0.5
0
Probability of state 1
ytilit U
Probability of state 1
a b
3
2.5
2
1.5
1
0.5
0
ytilit U
7.5
7
6.5
6
5.5
5
4.5
Probability of state 1
ytilit U
Probability of state 1
c d
Figure17.8 a Utility oftwo one-stepplansasa functionoftheinitialbeliefstate b1
forthetwo-stateworld with thecorrespondingutility functionshowninbold. b Utilities
for8 distinct two-step plans. c Utilities forfourundominatedtwo-step plans. d Utility
functionforoptimaleight-stepplans.
There are eight distinct depth-2 plans in all and their utilities are shown in Figure 17.8b.
Notice that four of the plans shown as dashed lines are suboptimal across the entire belief
spacewe say these plans are dominated and they need not be considered further. There
D OM IN AT ED PL AN
are four undominated plans each of which is optimal in a specific region as shown in Fig-
ure17.8c. Theregionspartition thebelief-state space.
Werepeat theprocess fordepth 3andsoon. Ingeneral let pbeadepth-d conditional
planwhoseinitialactionisaandwhosedepth-d1subplan forpercept eisp.e;then
cid:31
cid:12 cid:12
α s Rsγ
Pscid:2sa Pescid:2
α
scid:2
. 17.13
p p.e
scid:3 e
Thisrecursionnaturallygivesusavalueiterationalgorithmwhichissketchedin Figure17.9.
Thestructureofthealgorithmanditserroranalysisaresimilartothoseofthebasicvalueiter-
ation algorithm in Figure 17.4onpage653; themaindifference isthat instead ofcomputing
one utility number for each state P OM DP-V AL UE-I TE RA TI ON maintains a collection of
Section17.4. Partially Observable M DPs 663
function P OM DP-V AL UE-I TE RA TI ONpomdpcid:2returnsautilityfunction
inputs:pomdpa P OM DPwithstates Sactions Astransitionmodel Pscid:5sa
sensormodel Pesrewards Rsdiscountγ
cid:2themaximumerrorallowedintheutilityofanystate
localvariables: U Ucid:5setsofplanspwithassociatedutilityvectorsαp
Ucid:5asetcontainingjusttheemptyplanwithα s Rs
""
repeat
U Ucid:5
Ucid:5thesetofallplansconsistingofanactionandforeachpossiblenextpercept
aplanin U withutilityvectorscomputedaccordingto Equation17.13
Ucid:5 RE MO VE-D OM IN AT ED-P LA NS Ucid:5
until M AX-D IF FE RE NC EU Ucid:5 cid:21γγ
return U
R EM OV E-D OM IN AT ED-P LA NSstepand M AX-D IF FE RE NC Etestaretypicallyimplemented
aslinearprograms.
undominated plans with their utility hyperplanes. The algorithms complexity depends pri-
marilyonhowmanyplansgetgenerated. Given Aactions and Epossibleobservations it
iseasytoshowthatthereare A OEd1distinctdepth-dplans.
Evenforthelowlytwo-state
world with d8 the exact number is 2255. The elimination of dominated plans is essential
forreducing thisdoubly exponential growth: thenumberofundominated planswithd8is
just144. Theutilityfunction forthese144plansisshownin Figure17.8d.
Notice that even though state 0 has lower utility than state 1 the intermediate belief
states have even lower utility because the agent lacks the information needed to choose a
good action. This is why information has value in the sense defined in Section 16.6 and
optimalpoliciesin P OM DPsofteninclude information-gathering actions.
Givensuchautilityfunctionanexecutablepolicycanbeextractedbylookingatwhich
hyperplane is optimal at any given belief state b and executing the first action of the corre-
sponding plan. In Figure 17.8d the corresponding optimal policy is still the same as for
depth-1plans: Staywhenb1 0.5and Gootherwise.
In practice the value iteration algorithm in Figure 17.9 is hopelessly inefficient for
larger problemseven the 43 P OM DPis too hard. The main reason is that given ncon-
ditional plans at level d the algorithm constructs
An E
conditional plans at level d1
beforeeliminatingthedominatedones. Sincethe1970swhenthisalgorithmwasdeveloped
therehavebeenseveraladvancesincludingmoreefficientformsofvalueiterationandvarious
kindsofpolicyiterationalgorithms. Someofthesearediscussedinthenotesattheendofthe
chapter. Forgeneral P OM DPshowever finding optimal policies isvery difficult P SP AC E-
hard in facti.e. very hard indeed. Problems witha few dozen states are often infeasible.
Thenext section describes adifferent approximate method forsolving P OM DPsonebased
onlook-ahead search.
A A A A A
t2 t1 t t1 t2
X X X X X U
t1 t t1 t2 t3 t3
R R R R
t1 t t1 t2
E E E E E
t1 t t1 t2 t3
Figure17.10 Thegenericstructureofadynamicdecisionnetwork.Variableswithknown
valuesareshaded.Thecurrenttimeistandtheagentmustdecidewhattodothatischoose
avaluefor At. Thenetworkhasbeenunrolledintothefutureforthreestepsandrepresents
futurerewardsaswellastheutilityofthestateatthelook-aheadhorizon.
Inthissectionweoutlineasimpleapproachtoagentdesignforpartiallyobservablestochas-
ticenvironments. Thebasicelementsofthedesignarealready familiar:
The transition and sensor models are represented by a dynamic Bayesian network
D BNasdescribed in Chapter15.
Thedynamic Bayesian network is extended withdecision and utility nodes as used in
decision networks in Chapter 16. The resulting model is called a dynamic decision
D YN AM IC DE CI SI ON networkor D DN.
N ET WO RK
A filtering algorithm is used to incorporate each new percept and action and to update
thebeliefstaterepresentation.
Decisions are made by projecting forward possible action sequences and choosing the
bestone.
D BNs are factored representations in the terminology of Chapter 2; they typically have
an exponential complexity advantage over atomic representations and can model quite sub-
stantial real-world problems. Theagentdesign istherefore apractical implementation ofthe
utility-based agentsketched in Chapter2.
In the D BN the single state S becomes a set of state variables X and there may be
t t
multipleevidencevariables E . Wewilluse A torefertotheactionattimetsothetransition
t t
modelbecomes P X X A andthesensormodelbecomes P E X . Wewilluse R to
t1 t t t t t
refertotherewardreceivedattime tand U torefertotheutility ofthestateattime t. Both
t
ofthesearerandomvariables. Withthisnotationadynamicdecisionnetworklookslikethe
oneshownin Figure17.10.
Dynamicdecisionnetworkscanbeusedasinputsforany P OM DPalgorithmincluding
thoseforvalueandpolicyiterationmethods. Inthissectionwefocusonlook-aheadmethods
thatprojectactionsequencesforwardfromthecurrentbeliefstateinmuchthesamewayasdo
the game-playing algorithms of Chapter 5. The network in Figure 17.10 has been projected
three steps into the future; the current and future decisions A and the future observations
Section17.4. Partially Observable M DPs 665
A in P X E
t t 1:t
E . . .
t1
... ... ... ...
A t1 in P X t1 E 1:t1 . . .
... ... ...
E . . .
t2
... ... ...
A t2 in P X t2 E 1:t2 . . .
... ... ...
E t3 . . .
... ... ...
U X . . .
t3
Figure17.11 Partofthelook-aheadsolutionofthe D DNin Figure17.10. Eachdecision
willbetakeninthebeliefstateindicated.
E and rewards R are all unknown. Notice that the network includes nodes for the rewards
for X and X but the utility for X . This is because the agent must maximize the
t1 t2 t3
discounted sum of all future rewards and U X represents the reward for X and all
t3 t3
subsequentrewards. Asin Chapter5weassumethat U isavailableonlyinsomeapproximate
form: ifexactutilityvalueswereavailablelook-aheadbeyonddepth1wouldbeunnecessary.
D DNin Figure17.10. Eachofthetriangularnodesisabeliefstateinwhichtheagentmakes
a decision A for i012.... The round chance nodes correspond to choices by the
ti
environment namely what evidence E arrives. Notice that there are no chance nodes
ti
corresponding to the action outcomes; this is because the belief-state update for an action is
deterministic regardless oftheactualoutcome.
The belief state at each triangular node can be computed by applying a filtering al-
gorithm to the sequence of percepts and actions leading to it. In this way the algorithm
takes into account the fact that for decision A the agent will have available percepts
ti
E ... E even though at time t it does not know what those percepts willbe. In this
t1 ti
wayadecision-theoretic agentautomatically takesintoaccountthevalueofinformation and
willexecuteinformation-gathering actionswhereappropriate.
A decision can be extracted from the search tree by backing up the utility values from
the leaves taking an average at the chance nodes and taking the maximum at the decision
nodes. Thisissimilartothe E XP EC TI MI NI MA X algorithmforgametreeswithchancenodes
except that 1 there can also be rewards at non-leaf states and 2 the decision nodes corre-
spond to belief states rather than actual states. The time complexity of an exhaustive search
todepthdis O Ad Edwhere Aisthenumberofavailable actionsand Eisthenum-
ber of possible percepts. Notice that this is far less than the number of depth-d conditional
plans generated by value iteration. For problems in which the discount factor γ is not too
close to 1 a shallow search is often good enough to give near-optimal decisions. It is also
possible to approximate the averaging step atthe chance nodes by sampling from the set of
possibleperceptsinsteadofsummingoverallpossiblepercepts. Therearevariousotherways
offindinggoodapproximate solutions quickly butwedeferthemto Chapter21.
Decision-theoreticagentsbasedondynamicdecisionnetworkshaveanumberofadvan-
tagescompared withother simpleragentdesigns presented inearlierchapters. Inparticular
theyhandlepartiallyobservableuncertainenvironments andcaneasilyrevisetheirplansto
handle unexpected evidence. Withappropriate sensormodels theycanhandle sensorfailure
andcanplantogatherinformation. Theyexhibitgraceful degradation undertimepressure
and incomplex environments using various approximation techniques. Sowhatismissing?
Onedefectofour D DN-basedalgorithmisitsrelianceonforwardsearchthroughstatespace
ratherthanusingthehierarchicalandotheradvancedplanningtechniquesdescribedin Chap-
ter11. Therehavebeenattemptstoextendthesetechniquesintotheprobabilisticdomainbut
sofarthey haveproved tobeinefficient. Asecond related problem isthebasically proposi-
tionalnature ofthe D DNlanguage. Wewouldliketobeabletoextendsomeoftheideasfor
first-order probabilistic languages to the problem of decision making. Current research has
shownthatthisextension ispossible andhassignificant benefits asdiscussed inthenotesat
theendofthechapter.
This chapter has concentrated on making decisions in uncertain environments. But what if
theuncertainty isduetootheragentsandthedecisionstheymake? Andwhatifthedecisions
of those agents are in turn influenced by our decisions? We addressed this question once
before when westudied gamesin Chapter 5. There however wewereprimarily concerned
with turn-taking games in fully observable environments for which minimax search can be
usedtofindoptimalmoves. Inthissectionwestudytheaspectsofgametheorythatanalyze
G AM ET HE OR Y
games with simultaneous moves and other sources of partial observability. Game theorists
usethetermsperfectinformationandimperfectinformationratherthanfullyandpartially
observable. Gametheorycanbeusedinatleasttwoways:
1. Agentdesign: Gametheorycananalyzetheagentsdecisionsandcomputetheexpected
utility for each decision under the assumption that other agents are acting optimally
according to game theory. Forexample in the game two-finger Morra two players
O and E simultaneously display one or two fingers. Let the total number of fingers
be f. If f is odd O collects f dollars from E; and if f is even E collects f dollars
from O. Gametheory can determine the best strategy against a rational player and the
expectedreturnforeachplayer.4
facilitysuchasarestaurantorabiologicalweaponsplantandthefacilityoperatorchoosesadaytohideallthe
nastystuff.Theinspectorwinsifthedaysaredifferentandthefacilityoperatorwinsiftheyarethesame.
Section17.5. Decisionswith Multiple Agents: Game Theory 667
2. Mechanism design: When an environment is inhabited by many agents it might be
possible to define the rules of the environment i.e. the game that the agents must
playsothatthecollective goodofallagentsismaximizedwheneachagentadoptsthe
game-theoretic solution that maximizes its own utility. For example game theory can
help design the protocols for a collection of Internet traffic routers so that each router
hasanincentive toactinsuch awaythat global throughput ismaximized. Mechanism
designcanalsobeusedtoconstruct intelligent multiagentsystemsthatsolvecomplex
problemsinadistributed fashion.
Westartbyconsidering arestricted setofgames: oneswhere allplayerstakeactionsimulta-
neously and the result of the game is based on this single set of actions. Actually it is not
crucialthattheactionstakeplaceatexactlythesametime;whatmattersisthatnoplayerhas
knowledge ofthe otherplayers choices. Therestriction to asingle move and the very use
of the word game might make this seem trivial but in fact game theory is serious busi-
ness. It is used in decision-making situations including the auctioning of oil drilling rights
and wireless frequency spectrum rights bankruptcy proceedings product development and
pricingdecisionsandnationaldefensesituations involvingbillionsofdollarsandhundreds
ofthousands oflives. Asingle-movegameisdefinedbythreecomponents:
Players oragents who will be making decisions. Two-player games have received the
P LA YE R
mostattention although n-player games for n 2are also common. Wegive players
capitalized nameslike Aliceand Bobor O and E.
Actionsthattheplayerscanchoose. Wewillgiveactionslowercasenameslikeone or
A CT IO N
testify. Theplayersmayormaynothavethesamesetofactionsavailable.
Apayoff function that gives the utility toeach player foreach combination of actions
P AY OF FF UN CT IO N
by all the players. Forsingle-move games the payoff function can be represented by a
matrix a representation known as the strategic form also called normal form. The
S TR AT EG IC FO RM
payoffmatrixfortwo-finger Morraisasfollows:
O:one O:two
E:one E 2 O 2 E 3 O 3
E:two E 3 O 3 E 4 O 4
Forexample the lower-right corner shows that when player O chooses action two and
E alsochoosestwothepayoffis4for E and4for O.
Each player in a game must adopt and then execute a strategy which is the name used in
S TR AT EG Y
gametheoryforapolicy. Apurestrategyisadeterministicpolicy;forasingle-movegame
P UR ES TR AT EG Y
a pure strategy is just a single action. Formany games an agent can do better with a mixed
strategy which is a randomized policy that selects actions according to aprobability distri-
M IX ED ST RA TE GY
bution. The mixed strategy that chooses action a with probability p and action b otherwise
is written p:a;1 p:b. For example a mixed strategy for two-finger Morra might be
0.5:one;0.5:two. A strategy profile is an assignment of a strategy to each player; given
S TR AT EG YP RO FI LE
thestrategyprofilethegamesoutcomeisanumericvalueforeachplayer.
O UT CO ME
Asolutiontoagameisastrategyprofileinwhicheachplayeradoptsarationalstrategy.
S OL UT IO N
We will see that the most important issue in game theory is to define what rational means
when each agent chooses only part of the strategy profile that determines the outcome. It is
important to realize that outcomes are actual results of playing a game while solutions are
theoretical constructs used to analyze a game. We will see that some games have a solution
only in mixed strategies. But that does not mean that a player must literally be adopting a
mixedstrategytoberational.
Consider the following story: Two alleged burglars Alice and Bob are caught red-
handednearthesceneofaburglary andareinterrogated separately. Aprosecutorofferseach
a deal: if you testify against your partner as the leader of a burglary ring youll go free for
being thecooperative one while yourpartner willserve 10 years inprison. However if you
both testify against each other youll both get5years. Alice and Bobalsoknow that ifboth
refuse to testify they will serve only 1 year each for the lesser charge of possessing stolen
P RI SO NE RS property. Now Alice and Bob face the so-called prisoners dilemma: should they testify
D IL EM MA
or refuse? Being rational agents Alice and Bob each want to maximize their own expected
utility. Letsassumethat Aliceiscallouslyunconcernedaboutherpartnersfatesoherutility
decreases in proportion to the number of years she will spend in prison regardless of what
happensto Bob. Bobfeelsexactlythesameway. Tohelpreacharational decision theyboth
construct thefollowingpayoffmatrix:
Alice:testify Alice:refuse
Bob:testify A 5 B 5 A 10 B 0
Bob:refuse A 0 B 10 A 1 B 1
Alice analyzes the payoff matrix as follows: Suppose Bob testifies. Then I get 5 years if I
testify and 10 years if I dont so in that case testifying is better. On the other hand if Bob
refuses then Iget0yearsif Itestifyand1yearif Irefusesointhatcaseaswelltestifying is
better. Soineithercaseitsbetterformetotestify sothatswhat Imustdo.
D OM IN AN T Alice has discovered that testify is a dominant strategy for the game. We say that a
S TR AT EG Y
cid:2
S TR ON G strategysforplayerpstronglydominatesstrategys iftheoutcomeforsisbetterforpthan
D OM IN AT IO N
cid:2
the outcome for s for every choice of strategies by the other players. Strategy s weakly
cid:2 cid:2
dominates s if sis betterthan s on atleast one strategy profile and no worse on anyother.
W EA KD OM IN AT IO N
Adominantstrategyisastrategythatdominatesallothers. Itisirrationaltoplayadominated
strategy and irrational not to play a dominant strategy if one exists. Being rational Alice
choosesthedominantstrategy. Weneedjustabitmoreterminology: wesaythatanoutcome
is Paretooptimal5 ifthere isnootheroutcome thatallplayers would prefer. An outcome is
P AR ET OO PT IM AL
Paretodominatedbyanotheroutcomeifallplayers wouldprefertheotheroutcome.
P AR ET OD OM IN AT ED
If Alice is clever as well as rational she will continue to reason as follows: Bobs
dominant strategy isalsototestify. Therefore hewilltestifyandwewillbothgetfiveyears.
When each player has a dominant strategy the combination of those strategies is called a
D OM IN AN T
S TR AT EG Y dominant strategy equilibrium. In general a strategy profile forms an equilibrium if no
E QU IL IB RI UM
player canbenefit byswitching strategies given that every otherplayer sticks withthesame
E QU IL IB RI UM
Section17.5. Decisionswith Multiple Agents: Game Theory 669
strategy. Anequilibrium isessentially a local optimumin thespace of policies; itisthe top
ofapeak thatslopes downward along every dimension wherea dimension corresponds toa
players strategychoices.
The mathematician John Nash 1928 proved that every game has at least one equi-
librium. The general concept of equilibrium is now called Nash equilibrium in his honor.
Clearly a dominant strategy equilibrium is a Nash equilibrium Exercise 17.16 but some
N AS HE QU IL IB RI UM
gameshave Nashequilibria butnodominantstrategies.
The dilemma in the prisoners dilemma is that the equilibrium outcome is worse for
both players than the outcome they would get if they both refused totestify. Inother words
testifytestifyis Paretodominatedbythe-1-1outcomeofrefuserefuse. Isthereany
way for Alice and Bob to arrive at the -1 -1 outcome? It is certainly an allowable option
for both of them to refuse to testify but is is hard to see how rational agents can get there
giventhedefinitionofthegame. Eitherplayercontemplating playing refuse willrealizethat
he or she would do better by playing testify. That is the attractive power of an equilibrium
point. Gametheorists agree thatbeinga Nashequilibrium is anecessary condition forbeing
asolutionalthough theydisagreewhetheritisasufficient condition.
It is easy enough to get to the refuserefuse solution if we modify the game. For
examplewecouldchangetoarepeatedgameinwhichtheplayersknowthattheywillmeet
again. Ortheagents might havemoral beliefs that encourage cooperation andfairness. That
means they have a different utility function necessitating a different payoff matrix making
it a different game. We will see later that agents with limited computational powers rather
thantheabilitytoreasonabsolutelyrationallycanreachnon-equilibriumoutcomesascanan
agentthatknowsthattheotheragenthaslimitedrationality. Ineachcaseweareconsidering
adifferent gamethantheonedescribed bythepayoffmatrixabove.
Now lets look at a game that has no dominant strategy. Acme a video game console
manufacturer has todecide whether itsnext gamemachine willuse Blu-ray discs or D VDs.
Meanwhile the video game software producer Best needs to decide whether to produce its
nextgameon Blu-rayor D VD.Theprofitsforbothwillbepositiveiftheyagreeandnegative
iftheydisagree asshowninthefollowingpayoffmatrix:
Acme:bluray Acme:dvd
Best:bluray A 9 B 9 A 4 B 1
Best:dvd A 3 B 1 A 5 B 5
There is no dominant strategy equilibrium for this game but there are two Nash equilibria:
bluray bluray and dvd dvd. We know these are Nash equilibria because if either player
unilaterally movestoadifferent strategy thatplayer willbeworse off. Nowtheagents have
a problem: there are multiple acceptable solutions but if each agent aims for a different
solution then both agents will suffer. How can they agree on a solution? One answer is
that both should choose the Pareto-optimal solution bluray bluray; that is we can restrict
thedefinition ofsolution totheunique Pareto-optimal Nashequilibrium provided that one
exists. Every game has at least one Pareto-optimal solution but a game might have several
or they might not be equilibrium points. For example if bluray bluray had payoff 5
5 then there would be two equal Pareto-optimal equilibrium points. To choose between
them the agents can either guess or communicate which can be done either by establishing
a convention that orders the solutions before the game begins or by negotiating to reach a
mutually beneficial solution during the game which would mean including communicative
actions aspartofasequential game. Communication thus arises ingametheory forexactly
thesamereasonsthatitaroseinmultiagentplanningin Section11.4. Gamesinwhichplayers
C OO RD IN AT IO N needtocommunicatelikethisarecalled coordination games.
G AM E
A game can have more than one Nash equilibrium; how do we know that every game
must have at least one? Some games have no pure-strategy Nash equilibria. Consider for
example any pure-strategy profile for two-finger Morra page 666. If the total number of
fingersiseventhen Owillwanttoswitch;ontheotherhandsotospeakifthetotalisodd
then E willwanttoswitch. Therefore nopurestrategy profilecanbeanequilibrium andwe
mustlooktomixedstrategies instead.
Butwhichmixed strategy? In 1928 von Neumann developed amethod forfinding the
optimal mixed strategy for two-player zero-sum gamesgames in which the sum of the
Z ER O-S UM GA ME
payoffsisalwayszero.6 Clearly Morraissuchagame. Fortwo-player zero-sum gameswe
know that the payoffs are equal and opposite so we need consider the payoffs of only one
player whowillbethemaximizerjustasin Chapter5. For Morrawepicktheevenplayer
Etobethemaximizersowecandefinethepayoffmatrixbythevalues U eothepayoff
E
to E if E doeseand O doeso. Forconvenience wecallplayer E herand O him. Von
Neumannsmethodiscalledthethemaximintechnique anditworksasfollows:
M AX IM IN
Suppose we change the rules as follows: first E picks her strategy and reveals it to
O. Then O picks his strategy with knowledge of Es strategy. Finally we evaluate
the expected payoff of the game based on the chosen strategies. This gives us a turn-
taking game to which we can apply the standard minimax algorithm from Chapter 5.
Lets suppose this gives an outcome U . Clearly this game favors O so the true
E O
utility U of the original game from Es point of view is at least U . Forexample
E O
if we just look at pure strategies the minimax game tree has a root value of 3 see
Figure17.12a soweknowthat U 3.
Nowsuppose wechangetherulestoforce O torevealhisstrategyfirstfollowedby E.
Thentheminimaxvalueofthisgameis U andbecausethisgamefavors Eweknow
O E
that U is at most U . With pure strategies the value is 2see Figure 17.12b so
O E
weknow U 2.
Combining these twoarguments wesee that thetrue utility U ofthesolution totheoriginal
gamemustsatisfy
U U U orinthiscase 3 U 2.
E O O E
Topinpointthevalueof Uweneedtoturnouranalysistomixedstrategies. Firstobservethe
following: once the first player has revealed his or her strategy the second player might as
wellchooseapurestrategy. Thereasonissimple: ifthesecondplayerplaysamixedstrategy
p:one;1p:twoitsexpectedutilityisalinearcombination pu 1pu of
one two
Section17.5. Decisionswith Multiple Agents: Game Theory 671
a E -3 b O 2
one two one two
O -3 -3 E 2 4
one two one two one two one two
c E d O
p: one; 1 p: two q: one; 1 q: two
O E
one two one two
2p 31 p 3p 41 p 2q 31 q 3q 41 q
e U f U
4 4
3 3
two two
2 2
one one
1 1
1 1
2 2
3 3
turns playing pure strategies. c and d: Parameterized game trees where the first player
plays a mixed strategy. The payoffs depend on the probability parameter p or q in the
mixedstrategy. eandf: Foranyparticularvalueoftheprobabilityparameterthesecond
player will choose the better of the two actions so the value of the first players mixed
strategyisgivenbytheheavylines. Thefirstplayerwillchoosetheprobabilityparameterfor
themixedstrategyattheintersectionpoint.
theutilitiesofthepurestrategies u andu . Thislinearcombination canneverbebetter
one two
thanthebetterofu andu sothesecondplayercanjustchoosethebetterone.
one two
Withthisobservation inmindtheminimaxtreescanbethought ofashaving infinitely
many branches at the root corresponding to the infinitely many mixed strategies the first
player can choose. Each of these leads to a node with two branches corresponding to the
purestrategiesforthesecondplayer. Wecandepicttheseinfinitetreesfinitelybyhavingone
parameterized choiceattheroot:
If E chooses first the situation isas shown in Figure 17.12c. E chooses the strategy
p:one;1p:twoattherootandthen Ochoosesapurestrategyandhenceamove
giventhevalueofp. If Ochoosesonetheexpectedpayoffto Eis2p31p5p
3; if O chooses two the expected payoff is 3p41 p47p. We can draw
thesetwopayoffsasstraightlinesonagraphwhere prangesfrom0to1onthex-axis
asshownin Figure17.12e. Otheminimizerwillalwayschoosethelowerofthetwo
linesasshownbytheheavylinesinthefigure. Thereforethebestthat E candoatthe
rootistochoose ptobeattheintersection point whichiswhere
5p3 47p p 712.
Theutilityfor E atthispointis U 112.
E O
If O moves first the situation is as shown in Figure 17.12d. O chooses the strategy
q:one;1q:twoattheroot andthen E chooses amovegiventhevalueofq. The
payoffsare2q31q5q3and3q41q47q.7 Again Figure17.12f
showsthatthebest O candoattherootistochoosetheintersection point:
5q3 47q q 712.
Theutilityfor E atthispointis U 112.
O E
Nowweknow that thetrue utility ofthe original gameliesbetween 112 and 112 that
is it is exactly 112! The moral is that it is better to be O than E if you are playing this
game. Furthermore thetrueutility isattained bythemixedstrategy 712:one;512:two
M AX IM IN whichshouldbeplayedbybothplayers. Thisstrategyiscalledthemaximinequilibriumof
E QU IL IB RI UM
the game and is a Nash equilibrium. Note that each component strategy in an equilibrium
mixed strategy has the same expected utility. In this case both one and two have the same
expectedutility 112asthemixedstrategyitself.
Our result for two-finger Morra is an example of the general result by von Neumann:
everytwo-playerzero-sumgamehasamaximinequilibriumwhenyouallowmixedstrategies.
Furthermore every Nash equilibrium in a zero-sum game is a maximin for both players. A
player who adopts the maximin strategy has two guarantees: First no other strategy can do
betteragainst anopponent whoplayswellalthough someotherstrategies mightbebetterat
exploiting an opponent who makes irrational mistakes. Second the player continues to do
justaswellevenifthestrategyisrevealedtotheopponent.
The general algorithm for finding maximin equilibria in zero-sum games is somewhat
moreinvolvedthan Figures17.12eandfmightsuggest. Whentherearenpossibleactions
a mixed strategy is a point in n-dimensional space and the lines become hyperplanes. Its
also possible for some pure strategies for the second player to be dominated by others so
that they are not optimal against any strategy for the first player. After removing all such
strategies which might have to be done repeatedly the optimal choice at the root is the
U Eonetwo U Etwoone 3.Thisalsoexplainswhytheoptimalstrategyisthesameforbothplayers.
Section17.5. Decisionswith Multiple Agents: Game Theory 673
highest or lowest intersection point of the remaining hyperplanes. Finding this choice is
anexampleofalinearprogrammingproblem: maximizinganobjective function subjectto
linear constraints. Such problems can be solved by standard techniques in time polynomial
inthenumberofactionsandinthenumberofbitsusedtospecifytherewardfunctionifyou
wanttogettechnical.
Thequestionremainswhatshouldarationalagentactually doinplayingasinglegame
of Morra? The rational agent will have derived the fact that 712:one;512:two is the
maximin equilibrium strategy and willassume that thisismutual knowledge witharational
opponent. Theagentcouldusea12-sideddieorarandomnumbergeneratortopickrandomly
accordingtothismixedstrategy inwhichcasetheexpected payoffwouldbe-112for E. Or
the agent could just decide to play one or two. In either case the expected payoff remains
-112for E. Curiouslyunilaterallychoosingaparticularactiondoesnotharmonesexpected
payoffbutallowingtheotheragenttoknowthatonehasmadesuchaunilateraldecisiondoes
affecttheexpected payoffbecause thentheopponent canadjusthisstrategy accordingly.
Findingequilibria innon-zero-sum gamesissomewhatmorecomplicated. Thegeneral
approach hastwosteps: 1 Enumerateallpossible subsets ofactions thatmightformmixed
strategies. Forexample first try all strategy profiles where each player uses a single action
then those where each player uses either one or two actions and so on. This is exponential
inthenumberofactions andsoonlyappliestorelatively smallgames. 2 Foreachstrategy
profileenumeratedin1checktoseeifitisanequilibrium. Thisisdonebysolvingasetof
equationsandinequalitiesthataresimilartotheonesusedinthezero-sumcase. Fortwoplay-
ers these equations are linear and can be solved with basic linear programming techniques
butforthreeormoreplayers theyarenonlinearandmaybeverydifficulttosolve.
Sofarwehave looked only atgames that last a single move. The simplest kind of multiple-
movegameistherepeatedgameinwhichplayersfacethesamechoicerepeatedly buteach
R EP EA TE DG AM E
time with knowledge of the history of all players previous choices. A strategy profile for a
repeated gamespecifies anaction choice foreach playerateachtimestep foreverypossible
historyofpreviouschoices. Aswith M DPspayoffsareadditiveovertime.
Letsconsidertherepeatedversionoftheprisonersdilemma. Will Aliceand Bobwork
together and refuse to testify knowing they will meet again? The answer depends on the
details of the engagement. For example suppose Alice and Bob know that they must play
exactly100roundsofprisonersdilemma. Thentheybothknowthatthe100throundwillnot
bearepeated gamethatisitsoutcomecanhavenoeffect onfuture roundsand therefore
theywillbothchoose thedominant strategy testifyinthatround. Butoncethe100thround
is determined the 99th round can have no effect on subsequent rounds so it too will have
adominant strategy equilibrium at testifytestify. By induction both players willchoose
testify oneveryroundearning atotaljailsentence of500yearseach.
We can get different solutions by changing the rules of the interaction. For example
suppose that after each round there is a 99 chance that the players will meet again. Then
the expected number of rounds is still 100 but neither player knows for sure which round
willbethelast. Undertheseconditions morecooperative behaviorispossible. Forexample
one equilibrium strategy is foreach player to refuse unless the other player has ever played
P ER PE TU AL testify. This strategy could be called perpetual punishment. Suppose both players have
P UN IS HM EN T
adoptedthisstrategyandthisismutualknowledge. Thenaslongasneitherplayerhasplayed
testifythenatanypointintimetheexpected futuretotalpayoffforeachplayeris
cid:12
0.99t1 100.
t0
Aplayerwhodeviatesfromthestrategyandchooses testify willgainascoreof0ratherthan
1 on the very next move but from then on both players will play testify and the players
totalexpected futurepayoffbecomes
cid:12
0 0.99t5 495.
t1
Therefore at every step there is no incentive to deviate from refuserefuse. Perpetual
punishment is the mutually assured destruction strategy of the prisoners dilemma: once
either player decides to testify it ensures that both players suffer a great deal. But it works
asadeterrentonlyiftheotherplayerbelievesyouhaveadoptedthisstrategyoratleastthat
youmighthaveadopted it.
Otherstrategies aremoreforgiving. Themostfamous calledtit-for-tat callsforstart-
T IT-F OR-T AT
ingwithrefuse andthenechoing theotherplayers previous moveonallsubsequent moves.
So Alicewouldrefuse aslong as Bobrefuses andwouldtestify themoveafter Bobtestified
but would go back to refusing if Bob did. Although very simple this strategy has proven to
behighlyrobustandeffectiveagainstawidevarietyofstrategies.
We can also get different solutions by changing the agents rather than changing the
rules of engagement. Suppose the agents are finite-state machines with n states and they
are playing a game with m n total steps. The agents are thus incapable of representing
the number of remaining steps and must treat it as an unknown. Therefore they cannot do
the induction and are free to arrive at the more favorable refuse refuse equilibrium. In
thiscaseignorance isblissorrather havingyouropponent believethatyouareignorantis
bliss. Yoursuccess in these repeated games depends onthe other players perception of you
asabullyorasimpleton andnotonyouractualcharacteristics.
Inthegeneralcaseagameconsistsofasequenceofturnsthatneednotbeallthesame. Such
gamesarebestrepresentedbyagametreewhichgametheoristscalltheextensiveform. The
E XT EN SI VE FO RM
tree includes all the same information we saw in Section 5.1: an initial state S a function
0
P LA YE Rs that tells which player has the move a function A CT IO NSs enumerating the
possible actions a function R ES UL Tsa that defines the transition to a new state and a
partial function U TI LI TYsp which is defined only on terminal states to give the payoff
foreachplayer.
To represent stochastic games such as backgammon we add a distinguished player
chance that can take random actions. Chances strategy is part of the definition of the
Section17.5. Decisionswith Multiple Agents: Game Theory 675
game specified as a probability distribution over actions the other players get to choose
their own strategy. To represent games with nondeterministic actions such as billiards we
breaktheactionintotwopieces: theplayers actionitself hasadeterministic result andthen
chance hasaturntoreacttotheaction initsowncapricious way. Torepresent simultaneous
movesasintheprisonersdilemmaortwo-finger Morraweimposeanarbitraryorderonthe
playersbutwehavetheoptionofassertingthattheearlierplayersactionsarenotobservable
to the subsequent players: e.g. Alice must choose refuse or testify first then Bob chooses
but Bob does not know what choice Alice made at that time we can also represent the fact
that the moveis revealed later. However we assume the players always remember all their
ownprevious actions;thisassumption iscalled perfectrecall.
The key idea of extensive form that sets it apart from the game trees of Chapter 5 is
the representation of partial observability. We saw in Section 5.6 that a player in a partially
observable game such as Kriegspiel can create a game tree over the space of belief states.
With that tree wesaw that in somecases aplayer can findasequence of moves astrategy
thatleadstoaforcedcheckmateregardlessofwhatactualstatewestartedinandregardlessof
whatstrategytheopponentuses. Howeverthetechniquesof Chapter5couldnottellaplayer
what to do when there is no guaranteed checkmate. If the players best strategy depends
on the opponents strategy and vice versa then minimax or alphabeta by itself cannot
find a solution. The extensive form does allow us to find solutions because it represents the
belief states game theorists call them information sets of all players at once. From that
I NF OR MA TI ON SE TS
representation wecanfindequilibrium solutions justaswe didwithnormal-form games.
Asasimple example ofasequential game place twoagents inthe43world of Fig-
ure17.1andhavethemmovesimultaneously untiloneagentreachesanexitsquareandgets
the payoff for that square. If we specify that no movement occurs when the two agents try
to move into the same square simultaneously a common problem at many traffic intersec-
tions then certain pure strategies can get stuck forever. Thus agents need a mixedstrategy
toperform wellinthisgame: randomly choosebetweenmoving aheadandstaying put. This
isexactlywhatisdonetoresolvepacketcollisions in Ethernetnetworks.
Next well consider a very simple variant of poker. The deck has only four cards two
aces and two kings. One card is dealt to each player. The first player then has the option
to raise the stakes of the game from 1 point to 2 or to check. If player 1 checks the game
is over. If he raises then player 2 has the option to call accepting that the game is worth 2
points or fold conceding the 1 point. If the game does not end with a fold then the payoff
depends on the cards: it is zero for both players if they have the same card; otherwise the
playerwiththekingpaysthestakestotheplayerwiththeace.
Theextensive-form treeforthisgameisshownin Figure17.13. Nonterminalstatesare
shownascircles withtheplayertomoveinsidethecircle;player0ischance. Eachactionis
depictedasanarrowwithalabelcorrespondingtoaraisecheckcallorfoldorforchance
thefourpossible deals A Kmeansthatplayer1getsanaceandplayer2aking. Terminal
states are rectangles labeled by their payoff to player 1 and player 2. Information sets are
shown as labeled dashed boxes; for example I is the information set where it is player
11
1s turn and he knows he has an ace but does not know what player 2 has. In information
set I it is player 2s turn and she knows that she has an ace and that player 1 has raised
21
r c
f
k
I I 1-1!
11 00! 21
16: A A
r c
f
k
13: A K
I 1-1!
16: K K
r c
f
k
13: K A
I
12
00!
I
1-1!
21
r c
f
k
-11! 1-1!
Figure17.13 Extensiveformofasimplifiedversionofpoker.
but does not know what card player 1 has. Due to the limits of two-dimensional paper this
information setisshownastwoboxesratherthanone.
Onewaytosolveanextensivegameistoconvertittoanormal-form game. Recallthat
thenormalformisamatrixeachrowofwhichislabeledwithapurestrategyforplayer1and
each column byapure strategy forplayer 2. Inan extensive gameapure strategy forplayer
icorresponds toanaction foreachinformation setinvolving thatplayer. Soin Figure17.13
one pure strategy forplayer 1 is raise when in I that is when I have an ace and check
11
when in I when I have a king. In the payoff matrix below this strategy is called rk.
12
Similarly strategy cf for player 2 means call when I have an ace and fold when I have a
king. Since this is a zero-sum game the matrix below gives only the payoff for player 1;
player2alwayshastheopposite payoff:
2:cc 2:cf 2:ff 2:fc
1:rr 0 -16 1 76
1:kr -13 -16 56 23
1:rk 13 0 16 12
1:kk 0 0 0 0
This game is so simple that it has two pure-strategy equilibria shown in bold: cf for player
to normal form and then finding a solution usually a mixed strategy using standard linear
programming methods. That works in theory. But if a player has I information sets and
a actions per set then that player will have a I pure strategies. In other words the size of
the normal-form matrix is exponential in the number of information sets so in practice the
Section17.5. Decisionswith Multiple Agents: Game Theory 677
approach works only forvery small game trees on the order of a dozen states. A game like
Texasholdem pokerhasabout1018 statesmakingthisapproach completelyinfeasible.
What are the alternatives? In Chapter 5 we saw how alphabeta search could handle
games of perfect information with huge game trees by generating the tree incrementally by
pruningsomebranchesandbyheuristicallyevaluatingnonterminalnodes. Butthatapproach
does not work well forgames with imperfect information for two reasons: first it is harder
toprunebecauseweneedtoconsidermixedstrategiesthatcombinemultiplebranchesnota
purestrategythatalwayschoosesthebestbranch. Seconditishardertoheuristicallyevaluate
anonterminal nodebecause wearedealing withinformation setsnotindividual states.
Koller et al. 1996 come to the rescue with an alternative representation of extensive
games called the sequence form that is only linear in the size of the tree rather than ex-
S EQ UE NC EF OR M
ponential. Rather than represent strategies it represents paths through the tree; the number
of paths is equal to the number of terminal nodes. Standard linear programming methods
can again be applied to this representation. The resulting system can solve poker variants
with 25000 states in a minute ortwo. This is an exponential speedup overthe normal-form
approach butstillfallsfarshortofhandling fullpoker with1018 states.
If we cant handle 1018 states perhaps we can simplify the problem by changing the
gametoasimplerform. Forexampleif Iholdanaceandamconsidering thepossibility that
thenextcardwillgivemeapairofacesthen Idontcareaboutthesuitofthenextcard;any
suit will do equally well. This suggests forming an abstraction of the game one in which
A BS TR AC TI ON
suits are ignored. The resulting game tree will be smaller by a factor of 4!24. Suppose I
can solve this smaller game; how will the solution to that game relate to the original game?
Ifnoplayerisgoingforaflushorbluffingsothenthesuits dont mattertoanyplayer and
thesolution fortheabstraction willalsobeasolution fortheoriginal game. However ifany
playeriscontemplatingaflushthentheabstractionwillbeonlyanapproximatesolutionbut
itispossibletocomputeboundsontheerror.
Therearemanyopportunitiesforabstraction. Forexample atthepointinagamewhere
each player has two cards if I hold a pair of queens then the other players hands could be
abstracted into three classes: better only a pair of kings or a pair of aces same pair of
queens or worse everything else. However this abstraction might be too coarse. A better
abstraction would divide worse into say medium pair nines through jacks low pair and
nopair. Theseexamples areabstractions ofstates; itisalso possible toabstract actions. For
exampleinstead ofhavingabetactionforeachintegerfrom 1to1000wecouldrestrict the
bets to 100 101 102 and 103. Or we could cut out one of the rounds of betting altogether.
We can also abstract over chance nodes by considering only a subset of the possible deals.
Thisisequivalenttotherollouttechniqueusedin Goprograms. Puttingalltheseabstractions
together we can reduce the 1018 states of poker to 107 states a size that can be solved with
currenttechniques.
Pokerprograms basedonthisapproach caneasily defeatnoviceandsomeexperienced
human players but are not yet at the level of master players. Part of the problem is that
thesolution theseprograms approximatethe equilibrium solutionis optimalonlyagainst
an opponent who also plays the equilibrium strategy. Against fallible human players it is
important to be able to exploit an opponents deviation from the equilibrium strategy. As
Gautam Raoaka The Counttheworldsleadingonlinepokerplayersaid Billings etal.
2003 You have a very strong program. Once you add opponent modeling to it it will kill
everyone. Howevergoodmodelsofhumanfallability remainelusive.
Inasenseextensivegameformistheoneofthemostcompleterepresentationswehave
seen so far: it can handle partially observable multiagent stochastic sequential dynamic
environmentsmost of the hard cases from the list of environment properties on page 42.
Howevertherearetwolimitationsofgametheory. Firstitdoesnotdealwellwithcontinuous
states and actions although there have been some extensions to the continuous case; for
C OU RN OT examplethetheoryof Cournotcompetitionusesgametheorytosolveproblemswheretwo
C OM PE TI TI ON
companies choose prices for their products from a continuous space. Second game theory
assumes the gameis known. Partsof the gamemay be specified asunobservable tosome of
theplayers butitmustbeknown whatparts areunobservable. Incases inwhichtheplayers
learn the unknown structure of the game over time the model begins to break down. Lets
examineeachsourceofuncertainty andwhethereachcanberepresented ingametheory.
Actions: There is no easy way to represent a game where the players have to discover
what actions are available. Consider the game between computer virus writers and security
experts. Partoftheproblem isanticipating whatactionthe viruswriterswilltrynext.
Strategies: Game theory is very good at representing the idea that the other players
strategies are initially unknownas long as we assume all agents are rational. The theory
itself does not say whatto do when the other players are less than fully rational. The notion
B AY ES NA SH ofa Bayes Nashequilibriumpartiallyaddressesthispoint: itisanequilibrium withrespect
E QU IL IB RI UM
toaplayers priorprobability distribution overtheother players strategiesin otherwords
itexpresses aplayers beliefsabouttheotherplayerslikelystrategies.
Chance: Ifagamedependsontherollofadieitiseasyenoughtomodelachancenode
withuniform distribution overtheoutcomes. Butwhatifitis possible that thedie isunfair?
Wecanrepresent thatwithanotherchance node higherupinthetreewithtwobranches for
die is fair and die is unfair such that the corresponding nodes in each branch are in the
sameinformation setthatistheplayersdontknowifthedieisfairornot. Andwhatifwe
suspect the other opponent does know? Thenweadd another chance node with one branch
representing thecasewheretheopponent doesknowandonewherehedoesnt.
Utilities: What if wedont know ouropponents utilities? Again that can be modeled
with a chance node such that the other agent knows its own utilities in each branch but we
dont. But what if we dont know our own utilities? For example how do I know if it is
rational toorderthe Chefssaladif Idontknowhowmuch Iwilllikeit? Wecanmodelthat
withyetanotherchancenodespecifying anunobservable intrinsic quality ofthesalad.
Thusweseethatgametheoryisgoodatrepresentingmostsourcesofuncertaintybut
at the cost of doubling the size of the tree every time we add another node; a habit which
quickly leads to intractably large trees. Because of these and other problems game theory
hasbeenusedprimarilytoanalyzeenvironmentsthatareatequilibriumratherthantocontrol
agentswithinanenvironment. Nextweshallseehowitcanhelpdesignenvironments.
Section17.6. Mechanism Design 679
In the previous section we asked Given a game what is a rational strategy? In this sec-
tionweask Giventhatagentspickrationalstrategieswhatgameshouldwedesign? More
specificallywewouldliketodesignagamewhosesolutions consistingofeachagentpursu-
ingitsownrational strategy result inthemaximization of someglobal utility function. This
problem is called mechanism design or sometimes inverse game theory. Mechanism de-
M EC HA NI SM DE SI GN
signisastapleofeconomics andpolitical science. Capitalism 101saysthatifeveryonetries
to get rich the total wealth of society will increase. But the examples we will discuss show
thatpropermechanismdesignisnecessarytokeeptheinvisiblehandontrack. Forcollections
ofagentsmechanismdesignallowsustoconstructsmartsystemsoutofacollectionofmore
limitedsystemsevenuncooperative systemsinmuchthesamewaythatteamsofhumans
canachievegoalsbeyondthereachofanyindividual.
Examples of mechanism design include auctioning off cheap airline tickets routing
T CPpacketsbetweencomputers decidinghowmedicalinternswillbeassignedtohospitals
and deciding how robotic soccer players will cooperate with their teammates. Mechanism
design becamemorethananacademic subject inthe1990swhen severalnations facedwith
theproblemofauctioning offlicensestobroadcastinvariousfrequencybandslosthundreds
of millions of dollars in potential revenue as a result of poor mechanism design. Formally
a mechanism consists of 1 a language for describing the set of allowable strategies that
M EC HA NI SM
agentsmayadopt2adistinguishedagentcalledthecenterthatcollectsreportsofstrategy
C EN TE R
choices from the agents in the game and 3 an outcome rule known to all agents that the
centerusestodeterminethepayoffstoeachagent giventheirstrategychoices.
Letsconsider auctionsfirst. Anauction isamechanism forselling somegoods tomembers
A UC TI ON
of a pool of bidders. For simplicity we concentrate on auctions with a single item for sale.
Each bidder i has a utility value v for having the item. In some cases each bidder has a
i
private value for the item. For example the first item sold on e Bay was a broken laser
pointer whichsoldfor14.83toacollectorofbrokenlaser pointers. Thusweknowthatthe
collector has v 14.83 but most other people would have v 14.83. In other cases
i j
such as auctioning drilling rights for an oil tract the item has a common valuethe tract
willproduce someamount ofmoney X and all bidders value adollarequallybut there is
uncertainty astowhattheactual valueof X is. Different bidders havedifferent information
andhencedifferentestimatesoftheitemstruevalue. Ineithercasebiddersendupwiththeir
own v . Given v each bidder gets a chance at the appropriate time ortimes in the auction
i i
to make a bid b . The highest bid b wins the item but the price paid need not be b ;
i max max
thatspartofthemechanism design.
The best-known auction mechanism is the ascending-bid8 or English auction in
A SC EN DI NG-B ID
which the center starts by asking for a minimum or reserve bid b . If some bidder is
E NG LI SH AU CT IO N min
willing to pay that amount the center then asks for b d for some increment d and
min
continues up from there. The auction ends when nobody is willing to bid anymore; then the
lastbidderwinstheitempaying thepricehebid.
How do we know if this is a good mechanism? One goal is to maximize expected
revenue for the seller. Another goal is to maximize a notion of global utility. These goals
overlap tosome extent because one aspect ofmaximizing global utility isto ensure that the
winner of the auction is the agent who values the item the most and thus is willing to pay
themost. Wesayanauction isefficientifthe goods gototheagent whovalues them most.
E FF IC IE NT
Theascending-bidauctionisusuallybothefficientandrevenuemaximizingbutifthereserve
price is set too high the bidder who values it most may not bid and if the reserve is set too
lowthesellerlosesnetrevenue.
Probably the most important things that an auction mechanism can do is encourage a
sufficient numberofbidders toenterthe gameand discourage them from engaging in collu-
sion. Collusionisanunfairorillegalagreementbytwoormorebidderstomanipulateprices.
C OL LU SI ON
Itcanhappeninsecretbackroom dealsortacitly withinthe rulesofthemechanism.
For example in 1999 Germany auctioned ten blocks of cell-phone spectrum with a
simultaneous auction bidsweretakenonalltenblocksatthesametimeusingtherulethat
anybidmustbeaminimumofa10raiseoverthepreviousbidonablock. Therewereonly
two credible bidders and the first Mannesman entered the bid of 20 million deutschmark
onblocks1-5and18.18milliononblocks6-10. Why18.18 M?Oneof T-Mobilesmanagers
said they interpreted Mannesmans first bid as an offer. Both parties could compute that
a 10 raise on 18.18 M is 19.99 M; thus Mannesmans bid was interpreted as saying we
can each get half the blocks for 20 M; lets not spoil it by bidding the prices up higher.
And in fact T-Mobile bid 20 M on blocks 6-10 and that was the end of the bidding. The
German government got less than they expected because the two competitors were able to
use the bidding mechanism to come to a tacit agreement on how not to compete. From
the governments point of view a better result could have been obtained by any of these
changes to the mechanism: a higher reserve price; a sealed-bid first-price auction so that
the competitors could not communicate through their bids; or incentives to bring in a third
bidder. Perhaps the 10 rule was an error in mechanism design because it facilitated the
precisesignaling from Mannesmanto T-Mobile.
In general both the seller and the global utility function benefit if there are more bid-
ders although global utility can suffer if you count the cost of wasted time of bidders that
have no chance of winning. One way to encourage more bidders is to make the mechanism
easier for them. After all if it requires too much research orcomputation on the part of the
bidders they may decide to take their money elsewhere. So it is desirable that the bidders
haveadominantstrategy. Recallthat dominant meansthatthestrategy worksagainst all
other strategies which in turn means that an agent can adopt it without regard for the other
strategies. Anagentwithadominantstrategy canjustbidwithout wastingtimecontemplat-
ing other agents possible strategies. A mechanism where agents have a dominant strategy
is called a strategy-proof mechanism. If as is usually the case that strategy involves the
S TR AT EG Y-P RO OF
biddersrevealingtheirtruevalue v thenitiscalledatruth-revealing ortruthfulauction;
T RU TH-R EV EA LI NG i
R EV EL AT IO N theterm incentivecompatibleisalsoused. Therevelation principlestatesthatanymecha-
P RI NC IP LE
Section17.6. Mechanism Design 681
nismcanbetransformedintoanequivalenttruth-revealing mechanismsopartofmechanism
designisfindingtheseequivalent mechanisms.
It turns out that the ascending-bid auction has most of the desirable properties. The
bidder with the highest value v gets the goods at a price of b d where b is the highest
i o o
bid among all the other agents and d is the auctioneers increment.9 Bidders have a simple
dominantstrategy: keepbiddingaslongasthecurrentcostisbelowyourv . Themechanism
i
isnotquitetruth-revealing because thewinningbidderrevealsonlythathisv b d;we
i o
havealowerboundonv butnotanexactamount.
i
A disadvantage from the point of view of the seller of the ascending-bid auction is
that it can discourage competition. Suppose that in a bid for cell-phone spectrum there is
one advantaged company that everyone agrees would be able to leverage existing customers
and infrastructure and thus can make alarger profit than anyone else. Potential competitors
can see that they have no chance in an ascending-bid auction because the advantaged com-
pany can always bid higher. Thus the competitors may not enter at all and the advantaged
companyendsupwinningatthereserveprice.
Anothernegativepropertyofthe Englishauctionisitshighcommunicationcosts. Either
theauction takesplaceinoneroomorallbiddershavetohave high-speed securecommuni-
cationlines;ineithercasetheyhavetohavethetimeavailabletogothroughseveralroundsof
bidding. Analternative mechanismwhichrequires muchlesscommunication isthesealed-
S EA LE D-B ID bidauction. Eachbiddermakes asingle bidandcommunicates ittotheauctioneer without
A UC TI ON
theotherbidders seeing it. Withthismechanism thereisno longer asimple dominant strat-
egy. If your value is v and you believe that the maximum of all the other agents bids will
i
be b then you should bid b cid:2 for some small cid:2 if that is less than v . Thus your bid
o o i
depends on your estimation of the other agents bids requiring you to do more work. Also
note that the agent with the highest v might not win the auction. This is offset by the fact
i
thattheauctionismorecompetitive reducing thebiastowardanadvantaged bidder.
A small change in the mechanism for sealed-bid auctions produces the sealed-bid
S EA LE D-B ID second-priceauctionalsoknownasa Vickreyauction.10 Insuchauctionsthewinnerpays
S EC ON D-P RI CE
A UC TI ON
the price of the second-highest bid b rather than paying his own bid. This simple modifi-
V IC KR EY AU CT IO N o
cation completely eliminates thecomplexdeliberations required forstandard or first-price
sealed-bidauctions becausethedominantstrategyisnowsimplytobidv ;themechanismis
i
truth-revealing. Notethattheutilityofagentiintermsofhisbidb hisvaluev andthebest
i i
bidamongtheotheragents b is
o
cid:24
v b ifb b
u i o i o
i 0 otherwise.
To see that b v is a dominant strategy note that when v b is positive any bid
i i i o
that wins the auction is optimal and bidding v in particular wins the auction. On the other
i
hand when v b isnegative anybid that loses the auction isoptimal and bidding v in
i o i
bo vi bod.Thechanceofthiscanbemadearbitrarilysmallbydecreasingtheincrementd.
diedofaheartattackthreedayslater.
particularlosestheauction. Sobidding v isoptimalforallpossiblevaluesofb andinfact
i o
v istheonlybidthathasthisproperty. Becauseofitssimplicityandtheminimalcomputation
i
requirements for both seller and bidders the Vickrey auction is widely used in constructing
distributed A I systems. Also Internet search engines conduct over a billion auctions a day
to sell advertisements along with their search results and online auction sites handle 100
billionayearingoodsallusingvariantsofthe Vickreyauction. Notethattheexpectedvalue
to the seller is b which is the same expected return as the limit of the English auction as
o
theincrement dgoestozero. Thisisactually averygeneral result: the revenueequivalence
R EV EN UE
theorem states that with a few minor caveats any auction mechanism where risk-neutral
E QU IV AL EN CE
T HE OR EM
bidders have values v known only to themselves but know a probability distribution from
i
whichthosevaluesaresampledwillyieldthesameexpectedrevenue. Thisprinciplemeans
thatthevariousmechanismsarenotcompetingonthebasisofrevenuegeneration butrather
onotherqualities.
Althoughthesecond-priceauctionistruth-revealing itturnsoutthatextendingtheidea
tomultiplegoods andusinganext-price auction isnottruth-revealing. Many Internet search
engines use a mechanism where they auction k slots for ads on a page. The highest bidder
wins the top spot the second highest gets the second spot and so on. Each winner pays the
price bid by the next-lower bidder with the understanding that payment is made only if the
searcher actually clicks on the ad. The top slots are considered more valuable because they
are more likely to be noticed and clicked on. Imagine that three bidders b b and b have
valuations foraclick of v 200v 180 and v 100 and thatk 2slots are available
where it is known that the top spot is clicked on 5 of the time and the bottom spot 2. If
all bidders bid truthfully then b wins the top slot and pays 180 and has an expected return
1
of2001800.051. Thesecondslotgoestob . Butb canseethatifsheweretobid
anythingintherange101179shewouldconcedethetopslottob winthesecondslotand
2
yieldanexpectedreturnof200100.022. Thusb candoubleherexpectedreturnby
1
biddinglessthanhertruevalueinthiscase. Ingeneralbiddersinthismultislotauctionmust
spendalotofenergyanalyzing thebidsofotherstodeterminetheirbeststrategy; thereisno
simpledominantstrategy. Aggarwal etal.2006showthatthereisauniquetruthfulauction
mechanism for this multislot problem in which the winner of slot j pays the full price for
slot j just for those additional clicks that are available at slot j and not at slot j 1. The
winner pays the price for the lower slot for the remaining clicks. In our example b would
1
bid200truthfully andwouldpay180fortheadditional .05.02.03clicksinthetopslot
but would pay only the cost of the bottom slot 100 for the remaining .02 clicks. Thus the
totalreturnto b wouldbe200180.03200100.022.6.
1
Anotherexample ofwhere auctions can come into play within A I is whenacollection
of agents are deciding whether to cooperate on a joint plan. Hunsberger and Grosz 2000
show that this can be accomplished efficiently with an auction in which the agents bid for
rolesinthejointplan.
Section17.6. Mechanism Design 683
Now lets consider another type of game in which countries set their policy for controlling
airpollution. Eachcountry hasachoice: theycanreduce pollution atacostof-10points for
implementing thenecessary changes ortheycancontinue topollute whichgivesthemanet
utility of-5inadded health costs etc. andalsocontributes -1points toeveryothercountry
because the air is shared across countries. Clearly the dominant strategy for each country
iscontinue topollute butifthereare100countries andeachfollowsthispolicy theneach
country gets a total utility of -104 whereas if every country reduced pollution they would
T RA GE DY OF TH E each have a utility of -10. This situation is called the tragedy of the commons: if nobody
C OM MO NS
has to pay for using a common resource then it tends to be exploited in a way that leads to
a lower total utility for all agents. It is similar to the prisoners dilemma: there is another
solution to the game that is better for all parties but there appears to be no way for rational
agentstoarriveatthatsolution.
The standard approach for dealing with the tragedy of the commons is to change the
mechanism toone that charges each agent forusing the commons. More generally weneed
to ensure that all externalitieseffects on global utility that are not recognized in the in-
E XT ER NA LI TI ES
dividual agents transactionsare made explicit. Setting the prices correctly is the difficult
part. In the limit this approach amounts to creating a mechanism in which each agent is
effectively requiredtomaximizeglobalutility butcando sobymakingalocaldecision. For
this example a carbon tax would be an example of a mechanism that charges for use of the
commonsinawaythatifimplemented wellmaximizesglobalutility.
Asafinalexampleconsidertheproblemofallocatingsomecommongoods. Supposea
citydecidesitwantstoinstallsomefreewireless Internet transceivers. Howeverthenumber
oftransceivers theycanaffordislessthanthenumberofneighborhoods thatwantthem. The
city wants to allocate the goods efficiently to the neighborhoods that would value them the
cid:2
most. That is they want to maximize the global utility V v . The problem is that if
i i
theyjustaskeachneighborhood councilhowmuchdoyouvaluethisfreegift? theywould
allhaveanincentive tolieandreportahighvalue. Itturns outthereisamechanism known
V IC KR EY-C LA RK E- as the Vickrey-Clarke-Groves or V CG mechanism that makes it a dominant strategy for
G RO VE S
eachagent toreport itstrueutility andthatachieves anefficient allocation ofthegoods. The
V CG
trick is that each agent pays a tax equivalent to the loss in global utility that occurs because
oftheagents presenceinthegame. Themechanism workslike this:
1. Thecenteraskseachagenttoreportitsvalueforreceivinganitem. Callthisb .
i
2. Thecenterallocatesthegoodstoasubsetofthebidders. Wecallthissubset Aanduse
thenotation b A to meanthe result to iunder this allocation: b if iisin Athat is i
i i
is a winner and 0 otherwise. The center chooses A to maximize total reported utility
cid:2
B b A.
i i
3. The center calculates for each i the sum of the reported utilities for all the winners
cid:2
except i. Weusethe notation Bi jcid:14ib j A. Thecenteralso computes foreach
itheallocation thatwould maximizetotal global utility ifiwerenotinthegame; call
thatsum Wi.
4. Eachagentipaysataxequalto Wi Bi.
18
L EA RN IN G F RO M
E XA MP LE S
In which we describe agents that can improve their behavior through diligent
studyoftheirownexperiences.
Anagentislearningifitimprovesitsperformanceonfuturetasksaftermakingobservations
L EA RN IN G
about the world. Learning can range from the trivial as exhibited by jotting down a phone
number to the profound as exhibited by Albert Einstein who inferred a new theory of the
universe. Inthis chapter wewillconcentrate on one class of learning problem which seems
restricted but actually has vast applicability: from a collection of inputoutput pairs learn a
function thatpredictstheoutputfornewinputs.
Why would we want an agent to learn? If the design of the agent can be improved
whywouldntthedesigners justprogram inthatimprovement tobeginwith? Therearethree
main reasons. First the designers cannot anticipate all possible situations that the agent
might find itself in. For example a robot designed to navigate mazes must learn the layout
of each new maze it encounters. Second the designers cannot anticipate all changes over
time;aprogramdesignedtopredicttomorrowsstockmarket pricesmustlearntoadaptwhen
conditions change from boom to bust. Third sometimes human programmers have no idea
howtoprogramasolutionthemselves. Forexamplemostpeoplearegoodatrecognizingthe
faces of family members but even the best programmers are unable to program a computer
to accomplish that task except by using learning algorithms. This chapter first gives an
overview of the various forms of learning then describes one popular approach decision-
tree learning in Section 18.3 followed byatheoretical analysis oflearning in Sections 18.4
and 18.5. We look at various learning systems used in practice: linear models nonlinear
modelsinparticular neuralnetworksnonparametricmodelsandsupportvectormachines.
Finallyweshowhowensembles ofmodelscanoutperform asinglemodel.
Anycomponent ofanagent canbeimprovedbylearning from data. Theimprovements and
thetechniques usedtomakethemdependonfourmajorfactors:
Whichcomponentistobeimproved.
693
Whatpriorknowledgetheagentalreadyhas.
Whatrepresentation isusedforthedataandthecomponent.
Whatfeedback isavailabletolearnfrom.
Componentstobelearned
Chapter2described severalagentdesigns. Thecomponents oftheseagentsinclude:
1. Adirectmappingfromconditions onthecurrentstatetoactions.
2. Ameanstoinferrelevantproperties oftheworldfromthepercept sequence.
3. Information about the way the world evolves and about the results of possible actions
theagentcantake.
4. Utilityinformation indicating thedesirability ofworldstates.
5. Action-value information indicating thedesirability ofactions.
6. Goalsthatdescribe classesofstateswhoseachievement maximizestheagents utility.
Eachofthesecomponentscanbelearned. Considerforexampleanagenttrainingtobecome
a taxi driver. Every time the instructor shouts Brake! the agent might learn a condition
action rule for when to brake component 1; the agent also learns every time the instructor
does not shout. By seeing many camera images that it is told contain buses it can learn
to recognize them 2. By trying actions and observing the resultsfor example braking
hard on a wet roadit can learn the effects of its actions 3. Then when it receives no tip
from passengers who have been thoroughly shaken up during the trip it can learn a useful
component ofitsoverallutilityfunction 4.
Representation andpriorknowledge
We have seen several examples of representations for agent components: propositional and
first-order logical sentences for the components in a logical agent; Bayesian networks for
the inferential components of adecision-theoretic agent and so on. Effective learning algo-
rithms have been devised for all of these representations. This chapter and most of current
machine learning research covers inputs that form a factored representationa vector of
attribute valuesand outputs that can be either a continuous numerical value or a discrete
value. Chapter 19 covers functions and prior knowledge composed of first-order logic sen-
tences and Chapter20concentrates on Bayesiannetworks.
There is another way to look at the various types of learning. We say that learning
a possibly incorrect general function or rule from specific inputoutput pairs is called in-
I ND UC TI VE ductive learning. We will see in Chapter 19 that we can also do analytical or deductive
L EA RN IN G
D ED UC TI VE learning: going from a known general rule to a new rule that is logically entailed but is
L EA RN IN G
usefulbecauseitallowsmoreefficientprocessing.
Feedbacktolearnfrom
Therearethree typesoffeedback thatdeterminethethreemaintypesoflearning:
U NS UP ER VI SE D Inunsupervisedlearningtheagentlearnspatternsintheinputeventhoughnoexplicit
L EA RN IN G
feedback issupplied. Themostcommonunsupervised learning taskisclustering: detecting
C LU ST ER IN G
Section18.2. Supervised Learning 695
potentially useful clusters of input examples. For example a taxi agent might gradually
develop a concept of good traffic days and bad traffic days without ever being given
labeledexamplesofeachbyateacher.
R EI NF OR CE ME NT In reinforcement learning the agent learns from a series of reinforcementsrewards
L EA RN IN G
orpunishments. Forexamplethelackofatipattheendofthejourneygivesthetaxiagentan
indication that itdid something wrong. The twopoints fora win at the end of achess game
tellstheagentitdidsomethingright. Itisuptotheagenttodecidewhichoftheactionsprior
tothereinforcement weremostresponsible forit.
S UP ER VI SE D Insupervisedlearningtheagentobservessomeexampleinputoutput pairsandlearns
L EA RN IN G
afunctionthatmapsfrominputtooutput. Incomponent1abovetheinputsarepercepts and
the output are provided by ateacher who says Brake! or Turn left. In component 2 the
inputsarecameraimagesandtheoutputsagaincomefromateacherwhosaysthatsabus.
In3 thetheory ofbraking isafunction from states andbraking actions tostopping distance
in feet. In this case the output value is available directly from the agents percepts after the
fact;theenvironment istheteacher.
S EM I-S UP ER VI SE D In practice these distinction are not always so crisp. In semi-supervised learning we
L EA RN IN G
are given a few labeled examples and must make what we can of a large collection of un-
labeled examples. Even the labels themselves may not be the oracular truths that we hope
for. Imagine thatyou are trying tobuild asystem toguess apersons age from aphoto. You
gather some labeled examples by snapping pictures of people and asking their age. Thats
supervised learning. But in reality some of the people lied about their age. Its not just
that there is random noise in the data; rather the inaccuracies are systematic and to uncover
themisanunsupervised learningprobleminvolving images self-reported agesandtrueun-
knownages. Thusbothnoiseandlackoflabelscreateacontinuum betweensupervised and
unsupervised learning.
Thetaskofsupervised learning isthis:
Givenatrainingsetof N exampleinputoutput pairs
T RA IN IN GS ET
x y x y ...x y
whereeach y wasgenerated byanunknownfunction y fx
j
discoverafunction hthatapproximates thetruefunction f.
Here x and y can be any value; they need not be numbers. The function h isa hypothesis.1
H YP OT HE SI S
Learningisasearchthrough thespaceofpossiblehypotheses foronethatwillperform well
even on new examples beyond the training set. Tomeasure the accuracy of a hypothesis we
give it a test set of examples that are distinct from the training set. We say a hypothesis
T ES TS ET
yj theoutput. Incaseswheretheinputisspecificallyavectorofattributevaluesbeginningwith Section18.3
wewillusexj forthejthexampleandwewilluseitoindexthenattributesofeachexample. Theelementsof
xj arewrittenxj1xj2...xjn.
fx fx fx fx
x x x x
a b c d
Figure18.1 a Examplexfx pairsand a consistent linearhypothesis. b A con-
sistentdegree-7polynomialhypothesisforthesamedataset. c Adifferentdatasetwhich
admits an exact degree-6 polynomial fit or an approximate linear fit. d A simple exact
sinusoidalfittothesamedataset.
generalizes well if it correctly predicts the value of y for novel examples. Sometimes the
G EN ER AL IZ AT IO N
function f is stochasticit is not strictly a function of x and what we have to learn is a
conditional probability distribution P Y x.
When the output y is one of a finite set of values such as sunny cloudy or rainy
the learning problem is called classification and is called Boolean or binary classification
C LA SS IF IC AT IO N
if there are only two values. When y is a number such as tomorrows temperature the
learning problem is called regression. Technically solving a regression problem is finding
R EG RE SS IO N
a conditional expectation or average value of y because the probability that we have found
exactlytherightreal-valued numberfor y is0.
Figure18.1showsafamiliarexample: fittingafunctionofasinglevariabletosomedata
points. Theexamplesarepoints inthe xyplane wherey fx. Wedont knowwhatf
is butwewillapproximate itwithafunction hselected fromahypothesisspace Hwhich
H YP OT HE SI SS PA CE
forthisexamplewewilltaketobethesetofpolynomialssuchasx53x22. Figure18.1a
shows some data with an exact fit by a straight line the polynomial 0.4x 3. The line is
calledaconsistenthypothesisbecauseitagreeswithallthedata. Figure18.1bshowsahigh-
C ON SI ST EN T
degree polynomial that is also consistent with the same data. This illustrates a fundamental
problemininductivelearning: howdowechoosefromamongmultipleconsistenthypotheses?
One answer is to prefer the simplest hypothesis consistent with the data. This principle is
called Ockhamsrazorafterthe14th-century Englishphilosopher Williamof Ockhamwho
O CK HA MS RA ZO R
usedittoarguesharplyagainstallsortsofcomplications. Definingsimplicityisnoteasybut
itseemsclearthatadegree-1polynomial issimplerthanadegree-7polynomial andthusa
shouldbepreferred tob. Wewillmakethisintuitionmoreprecisein Section18.4.3.
data set; in fact it requires a degree-6 polynomial for an exact fit. There are just 7 data
points so a polynomial with 7 parameters does not seem to be finding any pattern in the
data and we do not expect it to generalize well. A straight line that is not consistent with
any ofthe data points but mightgeneralize fairly wellforunseen values of x isalso shown
in c. In general there is a tradeoff between complex hypotheses that fit the training data
well and simpler hypotheses that may generalize better. In Figure 18.1d we expand the
Section18.3. Learning Decision Trees 697
hypothesis space H to allow polynomials over both x and sinx and find that the data in
c can be fitted exactly by asimple function of the form axbcsinx. This shows the
importanceofthechoiceofhypothesisspace. Wesaythatalearningproblemisrealizableif
R EA LI ZA BL E
thehypothesisspacecontainsthetruefunction. Unfortunately wecannotalwaystellwhether
agivenlearning problem isrealizable becausethetruefunction isnotknown.
In some cases an analyst looking at a problem is willing to make more fine-grained
distinctions about thehypothesis space tosayeven beforeseeing anydatanot justthat a
hypothesis is possible or impossible but rather how probable it is. Supervised learning can
""
bedonebychoosing thehypothesis h thatismostprobable giventhedata:
h argmax Phdata.
h H
By Bayesrulethisisequivalentto
h argmax Pdatah Ph.
h H
Then we can say that the prior probability Ph is high for a degree-1 or -2 polynomial
lower for a degree-7 polynomial and especially low for degree-7 polynomials with large
sharpspikes asin Figure18.1b. Weallowunusual-looking functions whenthedatasaywe
reallyneedthembutwediscourage thembygivingthemalowpriorprobability.
Why not let H be the class of all Java programs or Turing machines? Afterall every
computable function can be represented by some Turing machine and that is the best we
can do. One problem with this idea is that it does not take into account the computational
complexity oflearning. Thereisatradeoff between theexpressiveness ofahypothesis space
and the complexity of finding a good hypothesis within that space. For example fitting a
straight line to data is an easy computation; fitting high-degree polynomials is somewhat
harder; and fitting Turing machines is in general undecidable. A second reason to prefer
simple hypothesis spaces is that presumably we will want to use h after we have learned it
and computing hx when h is a linear function is guaranteed to be fast while computing
anarbitrary Turingmachine program isnotevenguaranteed toterminate. Forthese reasons
mostworkonlearninghasfocused onsimplerepresentations.
Wewillseethattheexpressivenesscomplexitytradeoffisnotassimpleasitfirstseems:
itisoftenthecaseaswesawwithfirst-orderlogicin Chapter8thatanexpressive language
makesitpossibleforasimplehypothesistofitthedatawhereasrestrictingtheexpressiveness
of the language means that any consistent hypothesis must be very complex. For example
therulesofchesscanbewritteninapageortwooffirst-order logicbutrequirethousandsof
pageswhenwritteninpropositional logic.
Decision tree induction is one of the simplest and yet most successful forms of machine
learning. Wefirstdescribe therepresentationthe hypothesis spaceand then showhowto
learnagoodhypothesis.
A decision tree represents a function that takes as input a vector of attribute values and
D EC IS IO NT RE E
returns a decisiona single output value. The input and output values can be discrete or
continuous. Fornow wewillconcentrate on problems where the inputs have discrete values
and the output has exactly two possible values; this is Boolean classification where each
exampleinputwillbeclassifiedastrueapositiveexampleorfalseanegativeexample.
P OS IT IV E
A decision tree reaches its decision by performing a sequence of tests. Each internal
N EG AT IV E
node in the tree corresponds to a test of the value of one of the input attributes A and
i
the branches from the node are labeled with the possible values of the attribute A v .
i ik
Each leaf node in the tree specifies avalue to be returned by the function. The decision tree
representation is natural for humans; indeed many How To manuals e.g. for car repair
arewrittenentirelyasasingledecision treestretching overhundreds ofpages.
As an example we will build a decision tree to decide whether to wait for a table at a
restaurant. The aim here is to learn a definition for the goal predicate Will Wait. First we
G OA LP RE DI CA TE
listtheattributes thatwewillconsideraspartoftheinput:
1. Alternate: whetherthereisasuitablealternative restaurant nearby.
2. Bar: whethertherestaurant hasacomfortable barareatowaitin.
3. Fri Sat: trueon Fridaysand Saturdays.
4. Hungry: whetherwearehungry.
5. Patrons: howmanypeopleareintherestaurant valuesare None Someand Full.
6. Price: therestaurants pricerange.
7. Raining: whetheritisrainingoutside.
8. Reservation: whetherwemadeareservation.
9. Type: thekindofrestaurant French Italian Thaiorburger.
10. Wait Estimate: thewaitestimatedbythehost010minutes1030 3060 or60.
Note that every variable has a small set of possible values; the value of Wait Estimate for
example isnot aninteger rather itisone ofthefourdiscrete values 010 1030 3060 or
60. Thedecisiontreeusuallyusedbyoneofus S Rforthisdomainisshownin Figure18.2.
Noticethatthetreeignoresthe Price and Type attributes. Examplesareprocessedbythetree
starting attherootandfollowing theappropriate branch untilaleafisreached. Forinstance
an example with Patrons Full and Wait Estimate010 will be classified as positive
i.e.yeswewillwaitforatable.
A Boolean decision tree is logically equivalent to the assertion that the goal attribute is true
if and only if the input attributes satisfy one of the paths leading to a leaf with value true.
Writingthisoutinpropositional logicwehave
Goal Path Path
where each Path is aconjunction of attribute-value tests required to follow that path. Thus
the whole expression is equivalent to disjunctive normal form see page 283 which means
Section18.3. Learning Decision Trees 699
that any function in propositional logic can be expressed as a decision tree. As an example
therightmostpathin Figure18.2is
Path Patrons Full Wait Estimate010.
For a wide variety of problems the decision tree format yields a nice concise result. But
some functions cannot be represented concisely. Forexample the majority function which
returns true if and only if more than half of the inputs are true requires an exponentially
large decision tree. In other words decision trees are good for some kinds of functions and
bad forothers. Is there any kind ofrepresentation that is efficient for all kinds offunctions?
Unfortunately the answer is no. We can show this in a general way. Consider the set of all
Boolean functions onnattributes. Howmany different functions are inthis set? Thisis just
the number of different truth tables that we can write down because the function is defined
by its truth table. A truth table over n attributes has 2n rows one for each combination of
valuesoftheattributes. Wecanconsidertheanswercolumnofthetableasa2n-bitnumber
thatdefinesthefunction.
Thatmeansthereare22n
differentfunctionsandtherewillbemore
than that number of trees since more than one tree can compute the same function. Thisis
a scary number. Forexample with just the ten Boolean attributes of our restaurant problem
there are 21024 or about 10308 different functions to choose from and for20 attributes there
areover 10300000. Wewillneed someingenious algorithms tofindgood hypotheses insuch
alargespace.
Anexamplefora Booleandecisiontreeconsistsofanxypairwherexisavectorofvalues
fortheinputattributes and y isasingle Booleanoutput value. Atraining setof12examples
Patrons?
None Some Full
No Yes Wait Estimate?
60 30-60 10-30 0-10
No Alternate? Hungry? Yes
No Yes No Yes
Reservation? Fri Sat? Yes Alternate?
No Yes No Yes No Yes
Bar? Yes No Yes Yes Raining?
No Yes No Yes
No Yes No Yes
Figure18.2 Adecisiontreefordecidingwhethertowaitforatable.
Input Attributes Goal
Example
Alt Bar Fri Hun Pat Price Rain Res Type Est Will Wait
x Yes No No Yes Some No Yes French 010 y Yes
x Yes No No Yes Full No No Thai 3060 y No
x No Yes No No Some No No Burger 010 y Yes
x Yes No Yes Yes Full Yes No Thai 1030 y Yes
x Yes No Yes No Full No Yes French 60 y No
x No Yes No Yes Some Yes Yes Italian 010 y Yes
x No Yes No No None Yes No Burger 010 y No
x No No No Yes Some Yes Yes Thai 010 y Yes
x No Yes Yes No Full Yes No Burger 60 y No
x Yes Yes Yes Yes Full No Yes Italian 1030 y No
x No No No No None No No Thai 010 y No
x Yes Yes Yes Yes Full No No Burger 3060 y Yes
Figure18.3 Examplesfortherestaurantdomain.
is shown in Figure 18.3. The positive examples are the ones in which the goal Will Wait is
truex x ...;thenegativeexamplesaretheonesinwhichitisfalse x x ....
We want a tree that is consistent with the examples and is as small as possible. Un-
fortunately no matter how we measure size it is an intractable problem to find the smallest
consistent tree; thereisnowaytoefficiently search through
the22n
trees. Withsomesimple
heuristics howeverwecanfindagoodapproximate solution: asmallbutnotsmallestcon-
sistenttree. The D EC IS IO N-T RE E-L EA RN IN G algorithmadoptsagreedydivide-and-conquer
strategy: always test the most important attribute first. This test divides the problem up into
smaller subproblems that can then be solved recursively. By most important attribute we
meantheonethatmakesthemostdifferencetotheclassificationofanexample. Thatwaywe
hopetogettothecorrectclassificationwithasmallnumberoftestsmeaningthatallpathsin
thetreewillbeshortandthetreeasawholewillbeshallow.
Figure18.4ashowsthat Type isapoorattributebecauseitleavesuswithfourpossible
outcomeseachofwhichhasthesamenumberofpositiveasnegativeexamples. Ontheother
handinbweseethat Patrons isafairlyimportantattributebecauseifthevalueis None or
Somethenweareleftwithexamplesetsforwhichwecananswerdefinitively No and Yes
respectively. Ifthe valueis Fullweareleftwithamixedsetofexamples. Ingeneral after
the first attribute test splits up the examples each outcome is a new decision tree learning
probleminitselfwithfewerexamplesandonelessattribute. Therearefourcasestoconsider
fortheserecursive problems:
1. If the remaining examples are all positive or all negative then we are done: we can
answer Yes or No. Figure 18.4b shows examples ofthis happening in the None and
Some branches.
2. Iftherearesomepositiveandsomenegativeexamplesthenchoosethebestattributeto
splitthem. Figure18.4bshows Hungry beingusedtosplittheremainingexamples.
3. Iftherearenoexamplesleftitmeansthatnoexamplehasbeenobserved forthiscom-
Section18.3. Learning Decision Trees 701
Type? Patrons?
French Italian Thai Burger None Some Full
No Yes Hungry?
No Yes
a b
positivelightboxes and negativedarkboxesexamplesremaining. a Splitting on Type
bringsusnonearertodistinguishingbetweenpositiveandnegativeexamples. b Splitting
on Patronsdoesagoodjobofseparatingpositiveandnegativeexamples. Aftersplittingon
Patrons Hungryisafairlygoodsecondtest.
bination ofattribute values and wereturn adefault value calculated from the plurality
classificationofalltheexamplesthatwereusedinconstructingthenodesparent. These
arepassedalonginthevariable parent examples.
4. If there are no attributes left but both positive and negative examples it means that
theseexampleshaveexactlythesamedescriptionbutdifferentclassifications. Thiscan
happen because there is an error or noise in the data; because the domain is nondeter-
N OI SE
ministic; orbecause wecant observe anattribute thatwoulddistinguish theexamples.
Thebestwecandoisreturntheplurality classification oftheremainingexamples.
The D EC IS IO N-T RE E-L EA RN IN G algorithm is shown in Figure 18.5. Note that the set of
examples iscrucial for constructing thetree butnowheredotheexamplesappearinthetree
itself. A tree consists of just tests on attributes in the interior nodes values of attributes on
the branches and output values on the leaf nodes. Thedetails of the I MP OR TA NC E function
are given in Section 18.3.4. The output of the learning algorithm on our sample training
set is shown in Figure 18.6. The tree is clearly different from the original tree shown in
oflearning the correct function. Thiswould bethewrong conclusion todraw however. The
learningalgorithmlooksattheexamplesnotatthecorrectfunctionandinfactitshypothesis
see Figure 18.6 not only is consistent with all the examples but is considerably simpler
thantheoriginal tree! Thelearning algorithm hasnoreason toinclude testsfor Raining and
Reservation because it can classify all the examples without them. It has also detected an
interesting and previously unsuspected pattern: the first author will wait for Thai food on
weekends. It is also bound tomake some mistakes forcases where ithas seen no examples.
Forexampleithasneverseenacasewherethewaitis010minutesbuttherestaurantisfull.
function D EC IS IO N-T RE E-L EA RN IN Gexamplesattributesparent examples returns
atree
ifexamples isemptythenreturn P LU RA LI TY-V AL UEparent examples
elseifallexamples havethesameclassificationthenreturntheclassification
elseifattributes isemptythenreturn P LU RA LI TY-V AL UEexamples
else
Aargmax
aattributes
I MP OR TA NC Eaexamples
treeanewdecisiontreewithroottest A
foreachvaluevk of Ado
exse : eexamples and e.A vk
subtree D EC IS IO N-T RE E-L EA RN IN Gexsattributes Aexamples
addabranchtotree withlabel A vkandsubtreesubtree
returntree
scribedin Section18.3.4.Thefunction P LU RA LI TY-V AL UEselectsthemostcommonoutput
valueamongasetofexamplesbreakingtiesrandomly.
Patrons?
None Some Full
No Yes Hungry?
No Yes
No Type?
French Italian Thai Burger
Yes No Fri Sat? Yes
No Yes
No Yes
Figure18.6 Thedecisiontreeinducedfromthe12-exampletrainingset.
In that case it says not to wait when Hungry is false but I S Rwould certainly wait. With
moretrainingexamplesthelearningprogram couldcorrectthismistake.
Wenotethereisadangerofover-interpreting thetreethatthealgorithm selects. When
thereareseveralvariables ofsimilarimportance thechoice betweenthemissomewhatarbi-
trary: withslightly different input examples adifferent variable wouldbechosen tosplit on
firstandthewholetreewouldlookcompletely different. Thefunction computed bythetree
wouldstillbesimilarbutthestructure ofthetreecanvary widely.
Wecanevaluate the accuracy ofalearning algorithm witha learningcurve asshown
L EA RN IN GC UR VE
in Figure 18.7. Wehave 100 examples atourdisposal which we split into atraining set and
Section18.3. Learning Decision Trees 703
1
0.9
0.8
0.7
0.6
0.5
0.4
tes
tset
no
tcerroc
noitropor P
Training set size
generatedexamplesintherestaurantdomain.Eachdatapointistheaverageof20trials.
atestset. Welearnahypothesis hwiththetrainingsetandmeasureitsaccuracywiththetest
set. We do this starting with a training set of size 1 and increasing one at a time up to size
99. Foreach size weactually repeat the process ofrandomly splitting 20times andaverage
the results of the 20 trials. The curve shows that as the training set size grows the accuracy
increases. Forthis reason learning curves are also called happygraphs. Inthis graph we
reach95accuracy anditlookslikethecurvemightcontinue toincreasewithmoredata.
The greedy search used in decision tree learning is designed to approximately minimize the
depth of the final tree. The idea is to pick the attribute that goes as far as possible toward
providing an exact classification of the examples. A perfect attribute divides the examples
intosetseachofwhichareallpositiveorallnegativeandthuswillbeleavesofthetree. The
Patrons attribute isnotperfectbutitisfairlygood. Areallyuselessattribute suchas Type
leaves the example sets with roughly the same proportion of positive and negative examples
astheoriginal set.
Allweneedthenisaformalmeasureoffairlygoodandreallyuselessandwecan
implement the I MP OR TA NC E function of Figure 18.5. Wewillusethe notion ofinformation
gain which is defined in terms of entropy the fundamental quantity in information theory
E NT RO PY
Shannonand Weaver1949.
Entropyisameasureoftheuncertaintyofarandomvariable; acquisitionofinformation
corresponds to a reduction in entropy. A random variable with only one valuea coin that
always comesupheadshas nouncertainty andthus itsentropy isdefined aszero; thus we
gain no information by observing its value. Aflipof afair coin is equally likely to come up
heads or tails 0 or 1 and we will soon show that this counts as 1 bit of entropy. The roll
ofafairfour-sided diehas2bitsofentropy because ittakestwobitstodescribe oneoffour
equallyprobablechoices. Nowconsideranunfaircointhatcomesupheads99ofthetime.
Intuitivelythiscoinhaslessuncertaintythanthefaircoinifweguessheadswellbewrong
only1ofthetimesowewouldlikeittohaveanentropymeasurethatisclosetozerobut
positive. Ingeneraltheentropyofarandomvariable V withvaluesv eachwithprobability
k
Pv isdefinedas
k
cid:12 cid:12
1
Entropy: H V Pv log Pv log Pv .
k 2 Pv k 2 k
k
k k
Wecancheckthattheentropy ofafaircoinflipisindeed1bit:
H Fair 0.5log 0.50.5log 0.5 1.
Ifthecoinisloadedtogive99heads weget
H Loaded 0.99log 0.990.01log 0.01 0.08bits.
It will help to define Bq as the entropy of a Boolean random variable that is true with
probability q:
Bqqlog q1qlog 1q.
Thus H Loaded B0.99 0.08. Now lets get back to decision tree learning. If a
training set contains p positive examples and n negative examples then the entropy of the
goalattribute onthewholesetis
cid:13 cid:14
p
H Goal B .
pn
The restaurant training set in Figure 18.3 has p n 6 so the corresponding entropy is
B0.5orexactly1bit. Atestonasingleattribute Amightgiveusonlypartofthis1bit. We
canmeasureexactlyhowmuchbylookingattheentropyremainingaftertheattribute test.
Anattribute Awithddistinctvaluesdividesthetrainingset E intosubsets E ...E .
Each subset E has p positive examples and n negative examples so if we go along that
k k k
branch wewillneed anadditional Bp p n bitsofinformation toanswertheques-
k k k
tion. Arandomlychosenexamplefromthetrainingsethasthekthvaluefortheattributewith
probability p n pnsotheexpectedentropy remainingaftertestingattribute Ais
k k
cid:12d
Remainder A pknk B pk .
pn pknk
k1
Theinformationgainfromtheattribute teston Aistheexpectedreduction inentropy:
I NF OR MA TI ON GA IN
Gain A B p Remainder A.
pn
Infact Gain Aisjustwhatweneedtoimplementthe I MP OR TA NC E function. Returningto
theattributes considered in Figure18.4wehave
""
Gain Patrons 1 2 B0 4 B4 6 B2 0.541bits
12 2 12 4 12 6
Gain Type 1 2 B1 2 B1 4 B2 4 B2 0bits
confirming our intuition that Patrons is a better attribute to split on. In fact Patrons has
themaximumgainofanyoftheattributes andwouldbechosenbythedecision-tree learning
algorithm astheroot.
Section18.3. Learning Decision Trees 705
On some problems the D EC IS IO N-T RE E-L EA RN IN G algorithm will generate a large tree
when there is actually no pattern to be found. Consider the problem of trying to predict
whether the roll of a die will come up as 6 ornot. Suppose that experiments are carried out
with various dice and that the attributes describing each training example include the color
of the die its weight the time when the roll was done and whether the experimenters had
their fingers crossed. If the dice are fair the right thing to learn is a tree with a single node
that says no But the D EC IS IO N-T RE E-L EA RN IN G algorithm will seize on any pattern it
can find in the input. If it turns out that there are 2 rolls of a 7-gram blue die with fingers
crossed and theyboth come out6 then the algorithm mayconstruct apath that predicts 6in
thatcase. Thisproblemiscalled overfitting. Ageneral phenomenon overfittingoccurswith
O VE RF IT TI NG
alltypesoflearnersevenwhenthetargetfunctionisnotatallrandom. In Figure18.1band
cwesawpolynomialfunctionsoverfittingthedata. Overfittingbecomesmorelikelyasthe
hypothesis spaceandthenumberofinputattributes growsandlesslikelyasweincrease the
numberoftrainingexamples.
D EC IS IO NT RE E Fordecisiontreesatechniquecalled decisiontreepruningcombatsoverfitting. Prun-
P RU NI NG
ing works by eliminating nodes that are not clearly relevant. We start with a full tree as
generated by D EC IS IO N-T RE E-L EA RN IN G. We then look at a test node that has only leaf
nodes asdescendants. Ifthetestappears tobeirrelevantdetecting onlynoiseinthedata
then weeliminate the test replacing it witha leaf node. Werepeat this process considering
eachtestwithonlyleafdescendants untileachonehaseitherbeenpruned oraccepted asis.
Thequestionishowdowedetectthatanodeistestinganirrelevantattribute? Suppose
weareatanodeconsistingofppositiveandnnegativeexamples. Iftheattributeisirrelevant
wewouldexpectthatitwouldsplittheexamplesintosubsetsthateachhaveroughlythesame
proportion ofpositiveexamplesasthewholeset ppnandsotheinformationgainwill
beclosetozero.2 Thustheinformation gainisagoodcluetoirrelevance. Now thequestion
ishowlargeagainshouldwerequireinordertosplitonaparticular attribute?
Wecan answerthisquestion byusing astatistical significance test. Suchatest begins
S IG NI FI CA NC ET ES T
byassumingthatthereisnounderlying pattern theso-called nullhypothesis. Thentheac-
N UL LH YP OT HE SI S
tualdataareanalyzed tocalculate theextenttowhichtheydeviate fromaperfect absence of
pattern. Ifthe degree ofdeviation isstatistically unlikely usually taken to meana5 prob-
ability or less then that is considered to be good evidence for the presence of a significant
patterninthedata. Theprobabilities arecalculatedfromstandarddistributions oftheamount
ofdeviation onewouldexpecttoseeinrandomsampling.
In this case the null hypothesis is that the attribute is irrelevant and hence that the
information gain for an infinitely large sample would be zero. We need to calculate the
probability that under the null hypothesis a sample of size vn p would exhibit the
observeddeviationfromtheexpecteddistribution ofpositiveandnegativeexamples. Wecan
measurethedeviation bycomparingtheactualnumbersofpositiveandnegativeexamplesin
See Exercise18.5.
eachsubset p andn withtheexpectednumbers pˆ andnˆ assumingtrueirrelevance:
k k k k
p n p n
pˆ p k k nˆ n k k .
k k
pn pn
Aconvenient measureofthetotaldeviation isgivenby
cid:12d p pˆ 2 n nˆ 2
k k k k
Δ .
pˆ nˆ
k k
k1
Under the null hypothesis the value of Δ is distributed according to the χ2 chi-squared
distribution with v 1 degrees of freedom. We can use a χ2 table or a standard statistical
library routine to see if a particular Δ value confirms or rejects the null hypothesis. For
example consider the restaurant type attribute with four values and thus three degrees of
freedom. AvalueofΔ7.82ormorewouldrejectthenullhypothesisatthe5levelanda
valueofΔ11.35ormorewouldrejectatthe1level. Exercise18.8asksyoutoextendthe
D EC IS IO N-T RE E-L EA RN IN G algorithm toimplement this form of pruning which is known
χ2 asχ2 pruning.
P RU NI NG
Withpruningnoiseintheexamplescanbetolerated. Errorsintheexampleslabele.g.
anexamplex Yesthatshouldbex Nogivealinearincreaseinpredictionerrorwhereas
errorsinthedescriptions ofexamplese.g. Pricewhenitwasactually Pricehave
an asymptotic effect that gets worse as the tree shrinks down to smaller sets. Pruned trees
perform significantly better than unpruned trees when the data contain a large amount of
noise. Alsotheprunedtreesareoftenmuchsmallerandhenceeasiertounderstand.
Onefinalwarning: Youmightthink that χ2 pruning andinformation gainlooksimilar
so why not combine them using an approach called early stoppinghave the decision tree
E AR LY ST OP PI NG
algorithm stopgenerating nodeswhenthereisnogoodattribute tosplitonratherthangoing
to all the trouble of generating nodes and then pruning them away. The problem with early
stopping is that it stops us from recognizing situations where there is no one good attribute
buttherearecombinations ofattributes thatareinformative. Forexampleconsiderthe X OR
function of twobinary attributes. If there are roughly equal numberof examples forallfour
combinations of input values then neither attribute willbeinformative yet thecorrect thing
todo istosplit onone ofthe attributes it doesnt matterwhich one and then atthesecond
level we will get splits that are informative. Early stopping would miss this but generate-
and-then-prune handles itcorrectly.
Inordertoextend decision tree induction toawidervariety ofproblems anumberofissues
must be addressed. We will briefly mention several suggesting that a full understanding is
bestobtained bydoingtheassociated exercises:
Missing data: In many domains not all the attribute values will be known for every
example. The values might have gone unrecorded or they might be too expensive to
obtain. This gives rise to two problems: First given a complete decision tree how
should one classify an example that is missing one of the test attributes? Second how
Section18.3. Learning Decision Trees 707
should one modify the information-gain formula when some examples have unknown
valuesfortheattribute? Thesequestions areaddressed in Exercise18.9.
Multivalued attributes: When an attribute has many possible values the information
gain measure gives an inappropriate indication of the attributes usefulness. In the ex-
treme case an attribute such as Exact Time has a different value for every example
which means each subset of examples is a singleton with a unique classification and
theinformationgainmeasurewouldhaveitshighestvalueforthisattribute. Butchoos-
ingthissplitfirstisunlikely toyieldthebesttree. Onesolution istousethegainratio
G AI NR AT IO
Exercise18.10. Anotherpossibilityistoallowa Booleantestoftheform Av that
k
is picking out just one of the possible values for an attribute leaving the remaining
valuestopossibly betestedlaterinthetree.
Continuous and integer-valued input attributes: Continuous or integer-valued at-
tributessuchas Height and Weighthaveaninfinitesetofpossiblevalues. Ratherthan
generate infinitely many branches decision-tree learning algorithms typically find the
split point that gives the highest information gain. For example at a given node in
S PL IT PO IN T
the tree it might be the case that testing on Weight 160 gives the most informa-
tion. Efficient methods exist for finding good split points: start by sorting the values
of the attribute and then consider only split points that are between two examples in
sortedorderthathavedifferentclassifications whilekeepingtrackoftherunningtotals
of positive and negative examples on each side of the split point. Splitting is the most
expensivepartofreal-world decision treelearningapplications.
Continuous-valued output attributes: If we are trying to predict a numerical output
value such as the price of an apartment then we need a regression tree rather than a
R EG RE SS IO NT RE E
classification tree. A regression tree has at each leaf a linear function of some subset
of numerical attributes rather than a single value. For example the branch for two-
bedroom apartments might end with a linear function of square footage number of
bathrooms and average income for the neighborhood. The learning algorithm must
decide when to stop splitting and begin applying linear regression see Section 18.6
overtheattributes.
A decision-tree learning system for real-world applications must be able to handle all of
these problems. Handling continuous-valued variables isespecially important because both
physical and financial processes provide numerical data. Several commercial packages have
been built that meet these criteria and they have been used to develop thousands of fielded
systems. Inmanyareasofindustryandcommercedecisiontreesareusuallythefirstmethod
triedwhenaclassification methodistobeextracted from adataset. Oneimportant property
ofdecisiontreesisthatitispossibleforahumantounderstandthereasonfortheoutputofthe
learningalgorithm. Indeedthisisalegalrequirementforfinancialdecisionsthataresubject
to anti-discrimination laws. This is a property not shared by some other representations
suchasneuralnetworks.
We want to learn a hypothesis that fits the future data best. To make that precise we need
S TA TI ON AR IT Y to define future data and best. We make the stationarity assumption: that there is a
A SS UM PT IO N
probability distribution overexamples that remains stationary overtime. Each example data
pointbeforeweseeitisarandomvariable E whoseobservedvaluee x y issampled
j j j j
fromthatdistribution andisindependent oftheprevious examples:
P E j E j1 E j2... P E j
andeachexamplehasanidentical priorprobability distribution:
P E j P E j1 P E j2 .
Examplesthatsatisfytheseassumptionsarecalledindependent andidenticallydistributed or
i.i.d.. Ani.i.d.assumption connects thepasttothefuture; withoutsomesuchconnection all
I.I.D.
bets are offthe future could be anything. We will see later that learning can still occur if
thereare slowchangesinthedistribution.
The next step is to define best fit. We define the error rate of a hypothesis as the
E RR OR RA TE
proportionofmistakesitmakestheproportionoftimesthathx cid:7 yforanxyexample.
Now just because a hypothesis h has a low error rate on the training set does not mean that
itwillgeneralize well. Aprofessor knowsthatanexamwillnotaccurately evaluate students
if they have already seen the exam questions. Similarly to get an accurate evaluation of a
hypothesisweneedtotestitonasetofexamplesithasnotseenyet. Thesimplestapproachis
theonewehaveseenalready: randomlysplittheavailabledataintoatrainingsetfromwhich
thelearningalgorithmproduces handatestsetonwhichtheaccuracyofhisevaluated. This
H OL DO UT method sometimes called holdoutcross-validation hasthedisadvantage thatitfails touse
C RO SS-V AL ID AT IO N
alltheavailable data;ifweusehalfthedataforthetestsetthenweareonlytraining onhalf
the data and we may get a poor hypothesis. On the other hand if we reserve only 10 of
the data for the test set then we may by statistical chance get a poor estimate of the actual
accuracy.
Wecansqueezemoreoutofthedataandstillgetanaccurateestimateusingatechnique
K-F OL D calledk-foldcross-validation. Theideaisthateachexampleservesdoubledutyastraining
C RO SS-V AL ID AT IO N
data and test data. First wesplit the data into k equal subsets. Wethen perform k rounds of
learning; on each round 1k of the data is held out as a test set and the remaining examples
are used as training data. The average test set score of the k rounds should then be a better
estimate than a single score. Popular values for k are 5 and 10enough to give an estimate
that is statistically likely to be accurate at a cost of 5 to 10 times longer computation time.
L EA VE-O NE-O UT Theextremeisk nalsoknownasleave-one-outcross-validation or L OO CV.
C RO SS-V AL ID AT IO N
Despite the best efforts of statistical methodologists users frequently invalidate their
L OO CV
results by inadvertently peeking at the test data. Peeking can happen like this: A learning
P EE KI NG
algorithmhasvariousknobsthatcanbetwiddledtotuneitsbehaviorforexamplevarious
different criteria for choosing the next attribute in decision tree learning. The researcher
generateshypotheses forvariousdifferentsettingsofthe knobsmeasurestheirerrorrateson
thetestsetandreportstheerrorrateofthebesthypothesis. Alaspeekinghasoccurred! The
Section18.4. Evaluatingand Choosing the Best Hypothesis 709
reasonisthatthehypothesiswasselectedonthebasisofitstestseterrorratesoinformation
aboutthetestsethasleakedintothelearning algorithm.
Peekingisaconsequenceofusingtest-setperformancetobothchooseahypothesisand
evaluate it. The way to avoid this is to really hold the test set outlock it away until you
are completely done with learning and simply wish to obtain an independent evaluation of
the finalhypothesis. And then if you dont like the results ... you have toobtain and lock
away a completely new test set if you want to go back and find a better hypothesis. If the
testsetislockedawaybutyoustillwanttomeasureperformanceonunseendataasawayof
selectingagoodhypothesisthendividetheavailabledatawithoutthetestsetintoatraining
set and a validation set. The next section shows how to use validation sets to find a good
V AL ID AT IO NS ET
tradeoffbetweenhypothesis complexityandgoodness offit.
In Figure18.1page696weshowedthathigher-degree polynomialscanfitthetrainingdata
betterbutwhenthedegreeistoohightheywilloverfitandperformpoorlyonvalidationdata.
Choosingthedegreeofthepolynomialisaninstanceoftheproblemofmodelselection. You
M OD EL SE LE CT IO N
can think ofthe task offinding the best hypothesis astwotasks: model selection defines the
hypothesis spaceandthenoptimization findsthebesthypothesis withinthatspace.
O PT IM IZ AT IO N
Inthis section weexplain how to select among models that are parameterized by size.
Forexamplewithpolynomialswehavesize1forlinearfunctions size2forquadratics
and so on. Fordecision trees the size could be the number of nodes in the tree. In all cases
wewanttofindthevalueofthesize parameterthatbestbalancesunderfitting andoverfitting
togivethebesttestsetaccuracy.
Analgorithm to perform model selection and optimization is shown in Figure 18.8. It
W RA PP ER
is awrapperthat takes alearning algorithm as an argument D EC IS IO N-T RE E-L EA RN IN G
forexample. Thewrapperenumerates modelsaccording toaparameter size. Foreachsize
it uses cross validation on Learner to compute the average error rate on the training and
testsets. Westartwiththesmallest simplest models whichprobably underfitthedata and
iterate considering more complex models at each step until the models start to overfit. In
there may in general be slight random variation while the validation set error decreases at
first and then increases when the model begins to overfit. The cross-validation procedure
picksthevalueofsize withthelowestvalidationseterror;thebottomofthe U-shapedcurve.
Wethengenerate ahypothesis ofthat sizeusing allthedatawithout holding outanyofit.
Finallyofcourseweshouldevaluatethereturned hypothesis onaseparatetestset.
Thisapproachrequiresthatthelearningalgorithmaccepta parameter sizeanddeliver
ahypothesisofthatsize. Aswesaidfordecisiontreelearningthesizecanbethenumberof
nodes. We can modify D EC IS IO N-T RE E-L EA RN ER so that it takes the number of nodes as
an input builds the tree breadth-first rather than depth-first but at each level it still chooses
thehighestgainattribute firstandstopswhenitreachesthedesirednumberofnodes.
function C RO SS-V AL ID AT IO N-W RA PP ER Learnerkexamplesreturnsahypothesis
localvariables: err Tanarrayindexedbysizestoringtraining-seterrorrates
err Vanarrayindexedbysizestoringvalidation-seterrorrates
forsize 1todo
err Tsizeerr Vsize C RO SS-V AL ID AT IO NLearnersizekexamples
iferr T hasconvergedthendo
best sizethevalueofsize withminimumerr Vsize
return Learnerbest sizeexamples
function C RO SS-V AL ID AT IO NLearnersizekexamplesreturnstwovalues:
averagetrainingseterrorrateaveragevalidationseterrorrate
fold err T 0;fold err V 0
forfold 1to k do
training setvalidation set P AR TI TI ONexamplesfoldk
h Learnersizetraining set
fold err Tfold err T E RR OR-R AT Ehtraining set
fold err V fold err V E RR OR-R AT Ehvalidation set
returnfold err Tkfold err Vk
Figure18.8 Analgorithmtoselectthemodelthathasthelowesterrorrateonvalidation
data by building models of increasing complexity and choosing the one with best empir-
ical error rate on validation data. Here err T means error rate on the training data and
err V means error rate on the validation data. Learnersizeexamples returns a hypoth-
esis whose complexityis set by the parametersize and which is trained on the examples.
P AR TI TI ONexamplesfoldksplitsexamplesintotwosubsets: avalidationsetofsize Nk
andatrainingsetwithalltheotherexamples.Thesplitisdifferentforeachvalueoffold.
So far we have been trying to minimize error rate. This is clearly better than maximizing
error rate but it is not the full story. Consider the problem of classifying email messages
as spam or non-spam. It is worse to classify non-spam as spam and thus potentially miss
an important message then to classify spam as non-spam and thus suffer a few seconds of
annoyance. Soaclassifierwitha1errorrate wherealmost alltheerrorswereclassifying
spam as non-spam would be better than a classifier with only a 0.5 error rate if most of
thoseerrorswereclassifying non-spam asspam. Wesawin Chapter16thatdecision-makers
should maximize expected utility and utility is what learners should maximize as well. In
machine learning it is traditional to express utilities by means of a loss function. The loss
L OS SF UN CT IO N
function Lxyyˆ is defined as the amount of utility lost by predicting hxyˆwhen the
correctansweris fxy:
Lxyyˆ Utilityresultofusing y givenaninputx
Utilityresultofusing yˆgivenaninputx
Section18.4. Evaluatingand Choosing the Best Hypothesis 711
60
50
40
30
20
10
0
etar
rorr E
Validation Set Error
Training Set Error
Tree size
Figure18.9 Errorrateson training data lower dashed line and validationdata upper
solidlinefordifferentsizedecisiontrees. We stopwhenthetrainingseterrorrateasymp-
totesandthenchoosethetreewithminimalerroronthevalidationset;inthiscasethetree
ofsize7nodes.
Thisisthe mostgeneral formulation of theloss function. Oftenasimplified version is used
Lyyˆ that is independent of x. We will use the simplified version for the rest of this
chapter which means we cant say that it is worse to misclassify a letter from Mom than it
is to misclassify a letter from our annoying cousin but we can say it is 10 times worse to
classifynon-spam asspamthanvice-versa:
Lspamnospam 1 Lnospamspam 10.
Notethat Lyyisalwayszero; bydefinition thereisnoloss whenyouguess exactly right.
For functions with discrete outputs we can enumerate a loss value for each possible mis-
classification but we cant enumerate all the possibilities for real-valued data. If fx is
137.035999 wewould be fairly happy with hx 137.036 but just how happy should we
be? Ingeneralsmallerrors arebetterthanlargeones;twofunctions thatimplementthatidea
are the absolute value of the difference called the L loss and the square of the difference
1
called the L loss. If we are content with the idea of minimizing error rate we can use
2
the L loss function which has a loss of 1 for an incorrect answer and is appropriate for
01
discrete-valued outputs:
Absolutevalueloss: L yyˆ yyˆ
1
Squarederrorloss: L yyˆ yyˆ2
2
01loss: L yyˆ 0ify yˆ else1
01
The learning agent can theoretically maximize its expected utility by choosing the hypoth-
esis that minimizes expected loss over all inputoutput pairs it will see. It is meaningless
to talk about this expectation without defining aprior probability distribution P XYover
examples. Let E bethe set of all possible inputoutput examples. Thenthe expected gener-
G EN ER AL IZ AT IO N alization lossforahypothesis hwithrespecttolossfunction Lis
L OS S
cid:12
Gen Loss h Lyhx Pxy
L
xy E
""
andthebesthypothesis h istheonewiththeminimumexpectedgeneralization loss:
""
h argmin Gen Loss h.
L
h H
Because Pxyis notknown thelearning agent canonly estimate generalization loss with
empiricallossonasetofexamples E:
E MP IR IC AL LO SS
cid:12
1
Emp Loss h Lyhx.
L E N
xy E
Theestimatedbesthypothesis
hˆ
isthentheonewithminimumempiricalloss:
hˆ
argmin Emp Loss h.
L E
h H
Therearefourreasonswhy
hˆ
maydifferfromthetruefunction f: unrealizability variance
noise and computational complexity. First f may not be realizablemay not be in Hor
may be present in such a way that other hypotheses are preferred. Second a learning algo-
rithm will return different hypotheses for different sets of examples even if those sets are
drawn from the same true function f and those hypotheses will make different predictions
on new examples. Thehigher the variance among the predictions the higher the probability
ofsignificant error. Notethat evenwhentheproblem isrealizable therewillstill berandom
variance but that variance decreases towards zero as the number of training examples in-
creases. Third f may benondeterministic or noisyit mayreturn different values for fx
N OI SE
eachtimexoccurs. Bydefinition noisecannotbepredicted; inmanycasesitarisesbecause
theobservedlabelsyaretheresultofattributesoftheenvironmentnotlistedinx. Andfinally
when Hiscomplex itcanbecomputationally intractable tosystematically searchthewhole
hypothesis space. The best we can do is a local search hill climbing or greedy search that
exploresonlypartofthespace. Thatgivesusanapproximationerror. Combiningthesources
oferror wereleftwithanestimationofanapproximation ofthetruefunction f.
Traditional methods in statistics and the early years of machine learning concentrated
S MA LL-S CA LE on small-scale learning where the number of training examples ranged from dozens to the
L EA RN IN G
low thousands. Here the generalization error mostly comes from the approximation error of
nothavingthetruef inthehypothesisspaceandfromestimationerrorofnothavingenough
training examples to limit variance. In recent years there has been more emphasis on large-
L AR GE-S CA LE scale learning often with millions of examples. Here the generalization error is dominated
L EA RN IN G
bylimitsofcomputation: thereisenoughdataandarichenoughmodelthatwecouldfindan
h that is very close to the true f but the computation to find it is too complex so we settle
forasub-optimal approximation.
In Section18.4.1wesawhowtodomodelselectionwithcross-validation onmodelsize. An
alternativeapproachistosearchforahypothesisthatdirectlyminimizestheweightedsumof
Section18.5. The Theoryof Learning 713
empiricallossandthecomplexity ofthehypothesis whichwewillcallthetotalcost:
Costh Emp Losshλ Complexityh
hˆ
argmin Costh.
h H
Here λ is a parameter a positive number that serves as a conversion rate between loss and
hypothesis complexity which after all are not measured on the same scale. This approach
combines loss and complexity into one metric allowing us to find the best hypothesis all at
once. Unfortunately we still need to do a cross-validation search to find the hypothesis that
generalizes best but this time it is with different values of λ rather than size. We select the
valueofλthatgivesusthebestvalidation setscore.
Thisprocess ofexplicitly penalizing complex hypotheses iscalled regularization be-
R EG UL AR IZ AT IO N
causeitlooksforafunctionthatismoreregularorlesscomplex. Notethatthecostfunction
requires us to make two choices: the loss function and the complexity measure which is
called a regularization function. The choice of regularization function depends on the hy-
pothesis space. For example a good regularization function for polynomials is the sum of
thesquaresofthecoefficientskeeping thesumsmallwouldguideusawayfromthewiggly
polynomialsin Figure18.1bandc. Wewillshowanexampleofthistypeofregularization
in Section18.6.
Anotherwaytosimplifymodelsistoreducethedimensionsthatthemodelsworkwith.
Aprocess of feature selection can beperformed todiscard attributes thatappear tobeirrel-
F EA TU RE SE LE CT IO N
evant. χ2 pruning isakindoffeatureselection.
It is in fact possible to have the empirical loss and the complexity measured on the
samescale without theconversion factor λ: theycan bothbemeasured inbits. Firstencode
the hypothesis as a Turing machine program and count the number of bits. Then count
the number of bits required to encode the data where a correctly predicted example costs
zero bits and the cost ofan incorrectly predicted example depends on how large the error is.
M IN IM UM
The minimum description length or M DL hypothesis minimizes the total number of bits
D ES CR IP TI ON
L EN GT H
required. This works well in the limit but for smaller problems there is a difficulty in that
the choice of encoding for the programfor example how best to encode a decision tree
as a bit stringaffects the outcome. In Chapter 20 page 805 we describe a probabilistic
interpretation ofthe M DLapproach.
The main unanswered question in learning is this: How can we be sure that our learning
algorithm hasproduced ahypothesis thatwillpredictthecorrectvalueforpreviously unseen
inputs? Informaltermshowdoweknowthatthehypothesis hisclosetothetargetfunction
f if we dont know what f is? These questions have been pondered for several centuries.
In more recent decades other questions have emerged: how many examples do we need
to get a good h? What hypothesis space should we use? If the hypothesis space is very
complex can we even find the best h or do we have to settle for a local maximum in the
spaceofhypotheses? Howcomplexshouldhbe? Howdoweavoidoverfitting? Thissection
examinesthesequestions.
Well start with the question of how many examples are needed for learning. We saw
from the learning curve fordecision tree learning on the restaurant problem Figure 18.7 on
specifictoaparticularlearningalgorithm onaparticularproblem. Aretheresomemoregen-
eralprinciples governing thenumberofexamples needed ingeneral? Questions likethis are
C OM PU TA TI ON AL addressed bycomputationallearningtheory whichliesattheintersection of A Istatistics
L EA RN IN GT HE OR Y
andtheoreticalcomputerscience. Theunderlyingprincipleisthatanyhypothesisthatisseri-
ouslywrongwillalmostcertainly befoundoutwithhighprobability afterasmallnumber
ofexamplesbecauseitwillmakeanincorrectprediction. Thusanyhypothesisthatisconsis-
tentwithasufficientlylargesetoftrainingexamplesisunlikelytobeseriouslywrong: thatis
P RO BA BL Y
it must be probably approximately correct. Any learning algorithm that returns hypotheses
A PP RO XI MA TE LY
C OR RE CT
that areprobably approximately correct iscalled a P AClearningalgorithm; wecanuse this
P AC LE AR NI NG
approach toprovidebounds ontheperformance ofvariouslearning algorithms.
P AC-learning theorems like all theorems are logical consequences of axioms. When
a theorem as opposed to say a political pundit states something about the future based on
the past the axioms have to provide the juice to make that connection. For P AClearning
the juice isprovided bythe stationarity assumption introduced onpage 708 whichsays that
future examples are going to be drawn from the same fixed distribution P EP XY
as past examples. Note that we do not have to know what distribution that is just that it
doesnt change. In addition to keep things simple we will assume that the true function f
isdeterministic andisamemberofthehypothesis class Hthatisbeingconsidered.
The simplest P AC theorems deal with Boolean functions for which the 01 loss is ap-
propriate. The error rate of a hypothesis h defined informally earlier is defined formally
hereastheexpectedgeneralization errorforexamplesdrawnfromthestationarydistribution:
cid:12
errorh Gen Loss h L yhx Pxy .
L01 01
xy
In other words errorh is the probability that h misclassifies a new example. This is the
samequantitybeingmeasuredexperimentally bythelearning curvesshownearlier.
A hypothesis h is called approximately correct if errorh cid:2 where cid:2 is a small
constant. Wewillshow thatwecanfindan N such that afterseeing N examples withhigh
probability all consistent hypotheses will be approximately correct. One can think of an
approximately correcthypothesisasbeingclosetothetruefunctioninhypothesisspace: it
cid:2 liesinside whatiscalled thecid:2-ballaround thetruefunction f. Thehypothesis spaceoutside
-B AL L
thisballiscalled H .
bad
We can calculate the probability that a seriously wrong hypothesis h H is
b bad
consistent with the first N examples as follows. We know that errorh cid:2. Thus the
b
probability that it agrees with a given example is at most 1 cid:2. Since the examples are
independent theboundfor N examplesis
Ph agreeswith N examples 1cid:2 N .
b
Section18.5. The Theoryof Learning 715
The probability that H contains at least one consistent hypothesis is bounded by the sum
bad
oftheindividual probabilities:
P H contains aconsistent hypothesis H 1cid:2 N H1cid:2 N
bad bad
where we have used the fact that H H. We would like to reduce the probability of
bad
thiseventbelowsomesmallnumberδ:
H1cid:2 N δ .
Giventhat1cid:2 ecid:2wecanachievethisifweallowthealgorithm tosee
cid:13 cid:14
N ln ln H 18.1
cid:2 δ
examples. Thusifalearningalgorithm returnsahypothesis thatisconsistentwiththismany
examples then with probability at least 1 δ it has error at most cid:2. In other words it is
probably approximately correct. Thenumberofrequired examples asafunction ofcid:2 andδ
S AM PL E iscalledthesamplecomplexityofthehypothesis space.
C OM PL EX IT Y
As we saw earlier if H is the set of all Boolean functions on n attributes then H
22n . Thus thesample complexity ofthespace growsas 2n. Becausethenumberofpossible
examples is also 2n this suggests that P AC-learning in the class of all Boolean functions
requires seeing all or nearly all of the possible examples. A moments thought reveals the
reason for this: H contains enough hypotheses to classify any given set of examples in all
possibleways. Inparticular foranysetof N examplesthesetofhypothesesconsistent with
those examples contains equal numbers of hypotheses that predict x to be positive and
N1
hypotheses thatpredict x tobenegative.
N1
To obtain real generalization to unseen examples then it seems we need to restrict
the hypothesis space H in some way; but of course if we do restrict the space we might
eliminatethetruefunctionaltogether. Therearethreewaystoescapethisdilemma. Thefirst
which wewillcoverin Chapter19 is tobring priorknowledge tobear onthe problem. The
second which we introduced in Section 18.4.3 is to insist that the algorithm return not just
anyconsistenthypothesisbutpreferablyasimpleoneasisdoneindecisiontreelearning. In
cases where finding simple consistent hypotheses istractable the sample complexity results
are generally better than for analyses based only on consistency. The third escape which
we pursue next is to focus on learnable subsets of the entire hypothesis space of Boolean
functions. This approach relies on the assumption that the restricted language contains a
hypothesis h that is close enough to the true function f; the benefits are that the restricted
hypothesisspaceallowsforeffectivegeneralization andistypicallyeasiertosearch. Wenow
examineonesuchrestricted language inmoredetail.
We now show how to apply P AC learning to a new hypothesis space: decision lists. A
D EC IS IO NL IS TS
decision list consists of a series of tests each of which is a conjunction of literals. If a
test succeeds when applied to an example description the decision list specifies the value
to be returned. If the test fails processing continues with the next test in the list. Decision
lists resemble decision trees but their overall structure is simpler: they branch only in one
No No
Patronsx Some Patronsx Full Fri Satx No
Yes Yes
Yes Yes
Figure18.10 Adecisionlistfortherestaurantproblem.
direction. In contrast the individual tests are more complex. Figure 18.10 shows a decision
listthatrepresents thefollowinghypothesis:
Will Wait Patrons Some Patrons Full Fri Sat.
If we allow tests of arbitrary size then decision lists can represent any Boolean function
Exercise 18.14. On the other hand if we restrict the size of each test to at most k literals
then itis possible forthe learning algorithm to generalize successfully from asmall number
k-D L ofexamples. Wecallthislanguagek-D L. Theexamplein Figure18.10isin2-D L.Itiseasyto
k-D T show Exercise18.14thatk-D Lincludesasasubsetthelanguagek-D Tthesetofalldecision
trees ofdepth at most k. Itis important to remember that the particular language referred to
by k-D L depends on the attributes used to describe the examples. We will use the notation
k-D Lntodenoteak-D Llanguage usingn Booleanattributes.
The first task is to show that k-D L is learnablethat is that any function in k-D L can
be approximated accurately after training on a reasonable number of examples. To do this
we need to calculate the number of hypotheses in the language. Let the language of tests
conjunctions ofatmostk literals using nattributesbe Conjnk. Because adecision list
isconstructed oftestsandbecauseeachtestcanbeattachedtoeithera Yes ora No outcome
orcanbeabsentfromthedecisionlistthereareatmost3 Conjnk
distinctsetsofcomponent
tests. Eachofthesesetsoftestscanbeinanyorder so
k-D Ln 3 Conjnk Conjnk!.
Thenumberofconjunctions ofk literalsfrom nattributes isgivenby
cid:13 cid:14
cid:12k
2n
Conjnk Onk.
i
i0
Henceaftersomeworkweobtain
k-D Ln 2 Onklog2nk .
Wecanplugthisinto Equation 18.1toshowthatthenumberofexamplesneeded for P AC-
learning ak-D Lfunction ispolynomial inn:
cid:13 cid:14
N ln Onklog nk .
cid:2 δ 2
Thereforeanyalgorithmthatreturnsaconsistentdecisionlistwill P AC-learnak-D Lfunction
inareasonable numberofexamplesforsmall k.
The next task is to find an efficient algorithm that returns a consistent decision list.
We will use a greedy algorithm called D EC IS IO N-L IS T-L EA RN IN G that repeatedly finds a
Section18.6. Regressionand Classification with Linear Models 717
function D EC IS IO N-L IS T-L EA RN IN Gexamplesreturnsadecisionlistorfailure
ifexamples isemptythenreturnthetrivialdecisionlist No
tatestthatmatchesanonemptysubsetexamples ofexamples
t
suchthatthemembersofexamples areallpositiveorallnegative
t
ifthereisnosucht thenreturnfailure
iftheexamplesinexamples arepositivetheno Yes elseo No
t
returnadecisionlistwithinitialtestt andoutcomeo andremainingtestsgivenby
D EC IS IO N-L IS T-L EA RN IN Gexamples examples t
Figure18.11 Analgorithmforlearningdecisionlists.
1
0.9
0.8
0.7
0.6
0.5
0.4
tes
tset
no
tcerroc
noitropor P
Decision tree
Decision list
Training set size
Figure18.12 Learningcurvefor D EC IS IO N-L IS T-L EA RN IN Galgorithmontherestaurant
data. Thecurvefor D EC IS IO N-T RE E-L EA RN IN Gisshownforcomparison.
test that agrees exactly with some subset of the training set. Once it finds such a test it
adds it to the decision list under construction and removes the corresponding examples. It
thenconstructs theremainderofthedecision list using justtheremaining examples. Thisis
repeated untiltherearenoexamplesleft. Thealgorithm isshownin Figure18.11.
This algorithm does not specify the method for selecting the next test to add to the
decisionlist. Althoughtheformalresultsgivenearlierdonotdependontheselectionmethod
it would seem reasonable to prefer small tests that match large sets of uniformly classified
examplessothattheoveralldecisionlistwillbeascompactaspossible. Thesimpleststrategy
istofindthesmallesttesttthatmatchesanyuniformlyclassifiedsubsetregardlessofthesize
ofthesubset. Eventhisapproach worksquitewellas Figure 18.12suggests.
Now it is time to move on from decision trees and lists to a different hypothesis space one
that has been used for hundred of years: the class of linear functions of continuous-valued
L IN EA RF UN CT IO N
900
800
700
600
500
400
300
0001
ni
ecirp
esuo H
Loss
w
0
w
1
House size in square feet
a b
C A in July 2009 along with the linear function hypotcid:2hesis that minimizes squared error
loss: y 0.232x246. b Plot of the loss function jw 1xj w
0
yj2 for various
valuesofw w . Notethatthelossfunctionisconvexwithasingleglobalminimum.
inputs. Well start with the simplest case: regression with a univariate linear function oth-
erwise known as fitting a straight line. Section 18.6.2 covers the multivariate case. Sec-
tions 18.6.3 and 18.6.4 show how to turn linear functions into classifiers by applying hard
andsoftthresholds.
Aunivariatelinearfunctionastraightlinewithinputxandoutputyhastheformyw x
1
w where w and w arereal-valued coefficients tobelearned. Weusethe letter w because
we think of the coefficients as weights; the value of y is changed by changing the relative
W EI GH T
weightofonetermoranother. Welldefine wtobethevectorw w anddefine
h xw xw .
w 1 0
representing the size in square feet and the price of a house offered for sale. The task of
findingtheh thatbestfitsthesedataiscalled linearregression. Tofitalinetothedataall
L IN EA RR EG RE SS IO N w
wehavetodoisfindthevaluesoftheweightsw w thatminimizetheempiricalloss. Itis
traditional going back to Gauss3to use the squared loss function L summed overall the
2
training examples:
cid:12 N cid:12 N cid:12 N
Lossh L y h x y h x 2 y w x w 2 .
w 2 j w j j w j j 1 j 0
j1 j1 j1
areobtainedbyminimizingthesumofthesquaresoftheerrors.
Section18.6. Regressionand Classification with Linear Models 719
cid:2
We would like to find w argmin Lossh . The sum N y w x w 2 is
w w j1 j 1 j 0
minimizedwhenitspartialderivativeswithrespectto w andw arezero:
cid:12 N cid:12 N
""
y w x w 2 0and y w x w 2 0. 18.2
j 1 j 0 j 1 j 0
w w
j1 j1
Theseequations haveauniquesolution:
cid:2 cid:2 cid:2
cid:12 cid:12
N x y x y
w cid:2j j cid:2j j ; w y w x N . 18.3
j j
Forthe example in Figure 18.13a the solution is w 0.232 w 246 and the line with
thoseweightsisshownasadashedlineinthefigure.
Many forms of learning involve adjusting weights to minimize a loss so it helps to
haveamentalpicture ofwhatsgoingoninweightspacethe space definedbyallpossible
W EI GH TS PA CE
settings of the weights. Forunivariate linear regression the weight space defined by w and
0
w istwo-dimensional sowecangraphthelossasafunction ofw andw ina3 Dplotsee
Figure18.13b. Weseethatthelossfunction isconvexasdefinedonpage133;thisistrue
for every linear regression problem with an L loss function and implies that there are no
2
local minima. In some sense thats the end of the story for linear models; if we need to fit
linestodataweapply Equation18.3.4
To go beyond linear models we will need to face the fact that the equations defining
minimum loss as in Equation 18.2 will often have no closed-form solution. Instead we
will face a general optimization search problem in a continuous weight space. As indicated
in Section 4.2page 129 such problems can beaddressed by a hill-climbing algorithm that
follows the gradient of the function to be optimized. In this case because we are trying to
minimize the loss we will use gradient descent. We choose any starting point in weight
G RA DI EN TD ES CE NT
spacehere a point in the w w planeand then move to a neighboring point that is
downhillrepeating untilweconverge ontheminimumpossibleloss:
w anypointintheparameterspace
loopuntilconvergence do
foreachw inwdo
i
""
w w α Lossw 18.4
i i
w
i
Theparameter αwhichwecalledthestepsizein Section4.2isusually called thelearning
ratewhenwearetryingtominimizelossinalearningproblem. Itcanbeafixedconstantor
L EA RN IN GR AT E
itcandecayovertimeasthelearning processproceeds.
Forunivariateregression thelossfunctionisaquadratic function sothepartialderiva-
tive will be a linear function. The only calculus you need to know is that x22x and
x
x1. Lets first work out the partial derivativesthe slopesin the simplified case of
x
pendentofx;allresultsrelyonthestationarityassumption;etc.
onlyonetraining example xy:
""
Lossw yh x2
w
w w
i i
""
2yh x yh x
w w
w
i
""
2yh x yw xw 18.5
w 1 0
w
i
applying thistobothw andw weget:
""
Lossw 2yh x; Lossw 2yh xx
w w
w w
Thenpluggingthisbackinto Equation18.4andfoldingthe2intotheunspecifiedlearning
rateαwegetthefollowinglearning rulefortheweights:
w w αyh x; w w αyh xx
These updates make intuitive sense: if h x y i.e. the output of the hypothesis is too
w
large reduce w a bit and reduce w if x was a positive input but increase w if x was a
negativeinput.
Theprecedingequationscoveronetrainingexample. For N trainingexampleswewant
tominimizethesumoftheindividuallossesforeachexample. Thederivativeofasumisthe
sumofthederivatives sowehave:
cid:12 cid:12
w w α y h x ; w w α y h x x .
j j
B AT CH GR AD IE NT These updates constitute the batch gradient descent learning rule for univariate linear re-
D ES CE NT
gression. Convergence to the unique global minimum is guaranteed as long as we pick α
smallenough butmaybeveryslow: wehavetocyclethrough allthetraining dataforevery
stepandtheremaybemanysteps.
S TO CH AS TI C There is another possibility called stochastic gradient descent where we consider
G RA DI EN TD ES CE NT
only a single training point at a time taking a step after each one using Equation 18.5.
Stochastic gradient descent can be used in an online setting where new data are coming in
one at a time or offline where we cycle through the same data as many times as is neces-
sarytakingastepafterconsideringeachsingleexample. Itisoftenfasterthanbatchgradient
descent. Withafixedlearning rate α however itdoes not guarantee convergence; itcan os-
cillatearoundtheminimumwithoutsettlingdown. Insomecasesasweseelateraschedule
ofdecreasing learning ratesasinsimulatedannealing doesguarantee convergence.
M UL TI VA RI AT E Wecaneasilyextendtomultivariatelinearregression problems inwhicheachexamplex
L IN EA RR EG RE SS IO N j
isann-elementvector.5 Ourhypothesis spaceisthesetoffunctions oftheform
cid:12
h x w w x w x w w x .
sw j 0 1 j1 n jn 0 i ji
i
Section18.6. Regressionand Classification with Linear Models 721
Thew termtheinterceptstandsoutasdifferentfromtheothers. Wecanfixthatbyinventing
0
a dummy input attribute x which is defined as always equal to 1. Then h is simply the
j0
dot product of the weights and the input vector or equivalently the matrix product of the
transpose oftheweightsandtheinputvector:
cid:12
h x wx wcid:12 x w x .
sw j j j i ji
i
""
Thebestvectorofweights w minimizessquared-error lossovertheexamples:
cid:12
w argmin L y wx .
w
j
Multivariatelinearregressionisactuallynotmuchmorecomplicatedthantheunivariatecase
wejust covered. Gradient descent willreach theunique minimum oftheloss function; the
updateequation foreachweightw is
cid:12 i
w w α x y h x . 18.6
i i ji j w j
j
It is also possible to solve analytically for the w that minimizes loss. Let y be the vector of
outputs for the training examples and X be the data matrix i.e. the matrix of inputs with
D AT AM AT RI X
onen-dimensional exampleperrow. Thenthesolution
w
""
Xcid:12 X1 Xcid:12
y
minimizesthesquared error.
With univariate linear regression we didnt have to worry about overfitting. But with
multivariate linear regression in high-dimensional spaces it is possible that some dimension
thatisactually irrelevant appearsbychancetobeusefulresulting inoverfitting.
Thusitiscommontouseregularizationonmultivariatelinearfunctionstoavoidover-
fitting. Recall that with regularization we minimize the total cost of a hypothesis counting
boththeempiricallossandthecomplexity ofthehypothesis:
Costh Emp Losshλ Complexityh.
For linear functions the complexity can be specified as a function of the weights. We can
considerafamilyofregularization functions:
cid:12
Complexityh L w w q .
w q i
i
As with loss functions6 with q1 we have L regularization which minimizes the sum of
1
theabsolute values; withq2 L regularization minimizes thesum ofsquares. Whichreg-
2
ularization function should you pick? Thatdepends onthe specific problem but L regular-
1
ization hasanimportant advantage: ittendstoproduce a sparsemodel. Thatisitoften sets
S PA RS EM OD EL
manyweightstozeroeffectivelydeclaringthecorresponding attributestobeirrelevantjust
as D EC IS IO N-T RE E-L EA RN IN G doesalthough byadifferent mechanism. Hypotheses that
discardattributes canbeeasierforahumantounderstand andmaybelesslikelytooverfit.
notbeusedinpairs:youcoulduse L2losswith L1regularizationorviceversa.
w w
w
w
w w
Figure18.14 Why L regularizationtendstoproduceasparsemodel. a With L regu-
larizationboxthe minimalachievablelossconcentriccontoursoftenoccurson anaxis
meaninga weightof zero. b With L regularizationcircle the minimalloss islikely to
2
occuranywhereonthecirclegivingnopreferencetozeroweights.
Figure18.14givesanintuitiveexplanationofwhy L regularizationleadstoweightsof
1
zero while L regularization does not. Note that minimizing Losswλ Complexityw
2
is equivalent to minimizing Lossw subject to the constraint that Complexityw c for
someconstant cthatisrelated to λ. Nowin Figure 18.14a thediamond-shaped box repre-
sents theset of points win two-dimensional weight space that have L complexity less than
1
c; our solution will have to be somewhere inside this box. The concentric ovals represent
contours ofthelossfunction withtheminimumlossatthecenter. Wewanttofindthepoint
intheboxthatisclosest totheminimum;youcanseefromthediagram thatforanarbitrary
positionoftheminimumanditscontours itwillbecommonforthecorneroftheboxtofind
itswayclosesttotheminimumjustbecausethecornersarepointy. Andofcoursethecorners
are the points that have a value of zero in some dimension. In Figure 18.14b weve done
the same for the L complexity measure which represents a circle rather than a diamond.
2
Here you can see that in general there is no reason for the intersection to appear on one of
theaxes; thus L regularization does nottend toproduce zeroweights. Theresult isthat the
2
numberofexamplesrequiredtofindagoodhislinearinthenumberofirrelevantfeaturesfor
L regularization but only logarithmic with L regularization. Empirical evidence onmany
problemssupports thisanalysis.
Anotherwaytolookatitisthat L regularization takesthedimensionalaxesseriously
1
while L treats them as arbitrary. The L function is spherical which makes it rotationally
invariant: Imagine a set of points in a plane measured by their x and y coordinates. Now
imagine rotating the axes by 45o. Youd get a different set of xcid:2 ycid:2 values representing
the same points. If you apply L regularization before and after rotating you get exactly
2
cid:2 cid:2
the same point as the answer although the point would be described with the new xy
coordinates. Thatisappropriate whenthechoiceofaxesreallyisarbitrarywhen itdoesnt
matterwhetheryourtwodimensions aredistancesnorthandeast;ordistancesnorth-east and
Section18.6. Regressionand Classification with Linear Models 723
south-east. With L regularizationyoudgetadifferentanswerbecausethe L functionisnot
rotationally invariant. That is appropriate when the axes are not interchangeable; it doesnt
makesensetorotatenumberofbathrooms 45o towardslotsize.
Linear functions can be used to do classification as well as regression. For example Fig-
ure18.15ashowsdatapointsoftwoclasses: earthquakeswhichareofinteresttoseismolo-
gistsandunderground explosions whichareofinterestto armscontrolexperts. Eachpoint
is defined by two input values x and x that refer to body and surface wave magnitudes
computed from the seismic signal. Given these training data the task of classification is to
learn ahypothesis hthat willtake new x x points andreturn either 0forearthquakes or
1forexplosions.
7.5
7
6.5
6
5.5
5
4.5
4
3.5
3
2.5
x
2
7.5
7
6.5
6
5.5
5
4.5
4
3.5
3
2.5
x
1
x
2
x
1
a b
Figure18.15 a Plotoftwoseismicdataparametersbodywavemagnitudex andsur-
1
face wave magnitudex forearthquakeswhite circles andnuclearexplosionsblackcir-
2
clesoccurringbetween1982and1990in Asiaandthe Middle East Kebeasyetal.1998.
Alsoshownisadecisionboundarybetweentheclasses. b Thesamedomainwithmoredata
points.Theearthquakesandexplosionsarenolongerlinearlyseparable.
D EC IS IO N A decision boundary is a line or a surface in higher dimensions that separates the
B OU ND AR Y
two classes. In Figure 18.15a the decision boundary is a straight line. A linear decision
boundaryiscalledalinearseparatoranddatathatadmitsuchaseparatorarecalled linearly
L IN EA RS EP AR AT OR
L IN EA R separable. Thelinearseparatorinthiscaseisdefinedby
S EP AR AB IL IT Y
x 1.7x 4.9 or 4.91.7x x 0.
Theexplosionswhichwewanttoclassifywithvalue1aretotherightofthislinewithhigher
values of x and lower values of x so they are points for which 4.91.7x x 0
while earthquakes have 4.9 1.7x x 0. Using the convention of a dummy input
x 1wecanwritetheclassification hypothesis as
0
h x 1ifwx 0and0otherwise.
w
Alternatively we can think of h as the result of passing the linear function wx through a
T HR ES HO LD thresholdfunction:
F UN CT IO N
h x Thresholdwxwhere Thresholdz1ifz 0and0otherwise.
w
Thethreshold function isshownin Figure18.17a.
Now that the hypothesis h x has a well-defined mathematical form we can think
w
about choosing the weights w to minimize the loss. In Sections 18.6.1 and 18.6.2 we did
this both in closed form by setting the gradient to zero and solving for the weights and
by gradient descent in weight space. Here we cannot do either of those things because the
gradient is zero almost everywhere in weight space except at those points where wx0
andatthosepointsthegradientisundefined.
There is however a simple weight update rule that converges to a solutionthat is a
linearseparatorthatclassifiesthedataperfectlyprovided thedataarelinearlyseparable. For
asingleexamplexywehave
w w αyh xx 18.7
i i w i
whichisessentiallyidenticaltothe Equation18.6theupdateruleforlinearregression! This
P ER CE PT RO N ruleiscalledtheperceptronlearningruleforreasonsthatwillbecomeclearin Section18.7.
L EA RN IN GR UL E
Because weareconsidering a01classification problem however thebehavior issomewhat
different. Boththetruevalue yandthehypothesisoutputh xareeither0or1sothereare
w
threepossibilities:
Iftheoutputiscorrect i.e.yh xthentheweightsarenotchanged.
w
Ifyis1buth xis0thenw isincreasedwhenthecorresponding inputx ispositive
w i i
and decreased when x is negative. This makes sense because we want to make wx
i
biggersothath xoutputsa1.
w
Ifyis0buth xis1thenw isdecreasedwhenthecorrespondinginputx ispositive
w i i
and increased when x is negative. This makes sense because we want to make wx
i
smallersothat h xoutputsa0.
w
Typically the learning rule is applied one example at a time choosing examples at random
as in stochastic gradient descent. Figure 18.16a shows atraining curve forthis learning
T RA IN IN GC UR VE
rule applied to the earthquakeexplosion data shown in Figure 18.15a. A training curve
measures the classifier performance on a fixed training set as the learning process proceeds
on that same training set. The curve shows the update rule converging to a zero-error linear
separator. Theconvergenceprocessisntexactlyprettybutitalwaysworks. Thisparticular
runtakes657stepstoconvergeforadatasetwith63examplessoeachexampleispresented
roughly 10timesonaverage. Typically thevariation acrossrunsisverylarge.
We have said that the perceptron learning rule converges to a perfect linear separator
when the data points are linearly separable but what if they are not? This situation is all
too common in the real world. For example Figure 18.15b adds back in the data points
left out by Kebeasy et al. 1998 when they plotted the data shown in Figure 18.15a. In
steps: even though it hits the minimum-error solution three errors many times the algo-
rithm keeps changing the weights. In general the perceptron rule may not converge to a
Section18.6. Regressionand Classification with Linear Models 725
1
0.9
0.8
0.7
0.6
0.5
0.4
tcerroc
noitropor P
1
0.9
0.8
0.7
0.6
0.5
0.4
Number of weight updates
tcerroc
noitropor P
1
0.9
0.8
0.7
0.6
0.5
0.4
Number of weight updates
tcerroc
noitropor P
Number of weight updates
a b c
training set for the perceptron learning rule given the earthquakeexplosion data in Fig-
ure 18.15a. b The same plot for the noisy non-separable data in Figure 18.15b; note
thechangeinscale ofthex-axis. c Thesame plotasinbwitha learningrateschedule
αt10001000t.
stable solution for fixed learning rate α but if α decays as O1t where t is the iteration
numberthentherulecanbeshowntoconverge toaminimum-errorsolution whenexamples
are presented in a random sequence.7 It can also be shown that finding the minimum-error
solution is N P-hardsooneexpectsthatmanypresentations oftheexampleswillberequired
for convergence to be achieved. Figure 18.16b shows the training process with a learning
rate schedule αt10001000 t: convergence is not perfect after 100000 iterations
butitismuchbetterthanthefixed-αcase.
We have seen that passing the output of a linear function through the threshold function
creates a linear classifier; yet the hard nature of the threshold causes some problems: the
hypothesis h xisnotdifferentiable andisinfactadiscontinuous function ofitsinputsand
w
itsweights;thismakeslearningwiththeperceptronruleaveryunpredictable adventure. Fur-
thermore the linear classifier always announces acompletely confident prediction of 1or0
even for examples that are very close to the boundary; in many situations we really need
moregradatedpredictions.
Alloftheseissuescanberesolvedtoalargeextentbysofteningthethresholdfunction
approximating the hard threshold with a continuous differentiable function. In Chapter 14
page 522 we saw two functions that look like soft thresholds: the integral of the standard
normal distribution used for the probit model and the logistic function used for the logit
model. Although thetwofunctions areverysimilarinshape thelogistic function
1
Logisticz
1ez
P P
t1 t1
theseconditions.
1
0.8
0.6
-2
1
-8 -6 -4 -2 0 2 4 6 8 -6 -4 -2 0 2 4 6
a b c
that the function is nondifferentiable at z0. b The logistic function Logisticz
1ez
h wx Logisticwxforthedatashownin Figure18.15b.
has more convenient mathematical properties. The function is shown in Figure 18.17b.
Withthelogisticfunction replacing thethreshold function wenowhave
1
h x Logisticwx .
w 1ewx
Anexampleofsuchahypothesisforthetwo-inputearthquakeexplosion problemisshownin
as a probability of belonging to the class labeled 1. The hypothesis forms a soft boundary
in the input space and gives a probability of 0.5 for any input at the center of the boundary
regionandapproaches 0or1aswemoveawayfromtheboundary.
Theprocess offittingtheweights ofthis modeltominimize lossonadatasetiscalled
L OG IS TI C logisticregression. Thereisnoeasyclosed-formsolutiontofindtheoptimalvalueofwwith
R EG RE SS IO N
thismodelbutthegradient descent computation isstraightforward. Becauseourhypotheses
no longer output just 0 or 1 we will use the L loss function; also to keep the formulas
2
cid:2
readable welluseg tostandforthelogisticfunction withg itsderivative.
For a single example xy the derivation of the gradient is the same as for linear
regression Equation 18.5 up to the point where the actual form of his inserted. Forthis
cid:2
derivation wewillneedthechainrule: gfxxg fxfxx. Wehave
C HA IN RU LE
""
Lossw yh x2
w
w w
i i
""
2yh x yh x
w w
w
i
""
2yh xgcid:2 wx wx
w
w
i
2yh xgcid:2 wxx .
w i
Section18.7. Artificial Neural Networks 727
1
0.9
0.8
0.7
0.6
0.5
0.4
elpmaxe
rep
rorre
derauq S
1
0.9
0.8
0.7
0.6
0.5
0.4
Number of weight updates
elpmaxe
rep
rorre
derauq S
1
0.9
0.8
0.7
0.6
0.5
0.4
Number of weight updates
elpmaxe
rep
rorre
derauq S
Number of weight updates
a b c
squarederror. Theplotinacovers5000iterationsrather than1000whilebandcuse
thesamescale.
Thederivative gcid:2 ofthelogistic functionsatisfiesgcid:2 zgz1gz sowehave
gcid:2 wx gwx1gwx h x1h x
w w
sotheweightupdateforminimizingthelossis
w w αyh xh x1h xx . 18.8
i i w w w i
Repeating the experiments of Figure 18.16 with logistic regression instead of the linear
threshold classifier we obtain the results shown in Figure 18.18. In a the linearly sep-
arable case logistic regression is somewhat slower to converge but behaves much more
predictably. In b and c where the data are noisy and nonseparable logistic regression
converges farmorequickly andreliably. Theseadvantages tendtocarryoverintoreal-world
applications and logistic regression has become one of the most popular classification tech-
niquesforproblemsinmedicinemarketingandsurveyanalysiscreditscoringpublichealth
andotherapplications.
We turn now to what seems to be a somewhat unrelated topic: the brain. In fact as we
will see the technical ideas we have discussed so far in this chapter turn out to be useful in
buildingmathematicalmodelsofthebrainsactivity;conversely thinkingaboutthebrainhas
helpedinextendingthescopeofthetechnical ideas.
pothesis that mental activity consists primarily of electrochemical activity in networks of
brain cells called neurons. Figure 1.2on page 11 showed a schematic diagram of atypical
neuron. Inspired by this hypothesis some of the earliest A I work aimed to create artificial
neural networks. Other names for the field include connectionism parallel distributed
N EU RA LN ET WO RK
processing and neural computation. Figure 18.19 shows a simple mathematical model
of the neuron devised by Mc Culloch and Pitts 1943. Roughly speaking it fires when a
linearcombinationofitsinputsexceedssomehardorsoft thresholdthat isitimplements
Bias Weight
a 0 1 w a j gin j
0j
g
in
w Σ j
ij
a a
i j
Input Input Activation Output
Output
Links Function Function Links
Figurecid:218.19 Asimplemathematicalmodelforaneuron. Theunitsoutputactivationis
n
ajg i0wijaiwhereaiistheoutputactivationofunitiandwij istheweightonthe
linkfromunititothisunit.
a linear classifier of the kind described in the preceding section. A neural network is just a
collection of units connected together; the properties of the network are determined by its
topology andtheproperties oftheneurons.
Since 1943 much more detailed and realistic models have been developed both for
neurons and for larger systems in the brain leading to the modern field of computational
C OM PU TA TI ON AL neuroscience. On the other hand researchers in A I and statistics became interested in the
N EU RO SC IE NC E
moreabstractproperties ofneuralnetworks suchastheirabilitytoperformdistributed com-
putation totolerate noisyinputs andtolearn. Although weunderstand nowthatotherkinds
of systemsincluding Bayesian networkshave these properties neural networks remain
one of the most popular and effective forms of learning system and are worthy of study in
theirownright.
Neural networks are composed of nodes or units see Figure 18.19 connected by directed
U NI T
links. Alinkfromunititounitj servestopropagatetheactivationa fromitoj.8 Eachlink
L IN K i
also hasanumeric weightw associated withit which determines the strength andsign of
A CT IV AT IO N ij
theconnection. Justasinlinearregression modelseachunithasadummyinputa 1with
W EI GH T 0
anassociated weightw . Eachunitj firstcomputesaweightedsumofitsinputs:
0j
cid:12n
in w a .
j ij i
i0
A CT IV AT IO N Thenitappliesanactivation functiong tothissumtoderivetheoutput:
F UN CT IO N cid:31
cid:12n
a gin g w a . 18.9
j j ij i
i0
indexed by i so that an external activation ai is given by input xi; but index j will refer to internal units
ratherthanexamples. Throughoutthissectionthemathematicalderivationsconcernasinglegenericexamplex
omittingtheusualsummationsoverexamplestoobtainresultsforthewholedataset.
Section18.7. Artificial Neural Networks 729
Theactivation function g istypically eitherahardthreshold Figure18.17a inwhichcase
theunitiscalledaperceptronoralogisticfunction Figure18.17binwhichcasetheterm
P ER CE PT RO N
S IG MO ID sigmoid perceptron is sometimes used. Both of these nonlinear activation function ensure
P ER CE PT RO N
theimportantpropertythattheentirenetworkofunitscanrepresentanonlinearfunctionsee
Exercise18.22. Asmentionedinthediscussionoflogisticregressionpage725thelogistic
activation functionhastheaddedadvantage ofbeingdifferentiable.
Having decided on the mathematical model for individual neurons the next task is
to connect them together to form a network. There are two fundamentally distinct ways to
F EE D-F OR WA RD do this. A feed-forward network has connections only in one directionthat is it forms a
N ET WO RK
directedacyclicgraph. Everynodereceivesinputfromupstreamnodesanddeliversoutput
todownstream nodes; therearenoloops. Afeed-forward networkrepresents afunction of
itscurrentinput;thusithasnointernalstateotherthantheweightsthemselves. Arecurrent
R EC UR RE NT network on the other hand feeds its outputs back into its own inputs. This means that
N ET WO RK
theactivation levelsofthenetworkformadynamical system thatmayreach astablestateor
exhibitoscillationsorevenchaoticbehavior. Moreovertheresponseofthenetworktoagiven
input depends on its initial state which may depend on previous inputs. Hence recurrent
networks unlike feed-forward networks can support short-term memory. This makes them
more interesting as models of the brain but also more difficult to understand. This section
will concentrate on feed-forward networks; some pointers for further reading on recurrent
networksaregivenattheendofthechapter.
Feed-forwardnetworksareusuallyarrangedin layerssuchthateachunitreceivesinput
L AY ER S
onlyfromunitsintheimmediatelyprecedinglayer. Inthenexttwosubsections wewilllook
at single-layer networks in which every unit connects directly from the networks inputs to
itsoutputs andmultilayernetworks whichhaveoneormore layersofhiddenunitsthatare
H ID DE NU NI T
not connected tothe outputs of the network. Sofarinthis chapter wehave considered only
learningproblemswithasingleoutputvariable ybutneuralnetworksareoftenusedincases
where multiple outputs are appropriate. For example if we want to train a network to add
twoinput bits each a0ora1 wewillneed oneoutput forthesum bit andone forthecarry
bit. Also whenthelearning problem involves classification intomorethan twoclassesfor
examplewhenlearningtocategorizeimagesofhandwritten digitsitiscommontouseone
outputunitforeachclass.
Anetworkwithalltheinputsconnecteddirectlytotheoutputsiscalledasingle-layerneural
P ER CE PT RO N network or a perceptron network. Figure 18.20 shows a simple two-input two-output
N ET WO RK
perceptronnetwork. Withsuchanetworkwemighthopetolearnthetwo-bitadderfunction
forexample. Hereareallthetrainingdatawewillneed:
x x y carry y sum
Thefirstthingtonoticeisthataperceptronnetworkwithmoutputsisreallymseparate
networks because each weight affects only one of the outputs. Thus there will be m sepa-
rate training processes. Furthermore depending on the type of activation function used the
trainingprocesseswillbeeitherthe perceptronlearningrule Equation18.7onpage724
orgradient descentruleforthe logistic regression Equation18.8onpage727.
Ifyoutryeithermethodonthetwo-bit-adderdatasomethinginteresting happens. Unit
unit 4 isnot defective! The problem iswith the sum function itself. Wesawin Section 18.6
thatlinearclassifierswhetherhardorsoftcanrepresent lineardecisionboundariesinthein-
putspace. Thisworksfineforthecarryfunctionwhichisalogical A NDsee Figure18.21a.
Thesum function however is an X OR exclusive O Rof the two inputs. As Figure 18.21c
illustrates thisfunction isnotlinearly separable sothe perceptron cannotlearnit.
The linearly separable functions constitute just a small fraction of all Boolean func-
tions; Exercise 18.20 asksyou toquantify thisfraction. Theinability ofperceptrons tolearn
even such simple functions as X OR was a significant setback to the nascent neural network
w w w
13 13 35
w w w
14 14 36
w w w
23 23 45
w w w
24 24 46
a b
Figure18.20 a Aperceptronnetworkwithtwoinputsandtwooutputunits.b Aneural
networkwithtwoinputsonehiddenlayeroftwounitsandoneoutputunit. Notshownare
thedummyinputsandtheirassociatedweights.
x x x
?
ax andx bx orx cx xorx
Figure18.21 Linearseparabilityinthresholdperceptrons. Blackdotsindicateapointin
theinputspacewherethevalueofthefunctionis1andwhitedotsindicateapointwherethe
valueis0. Theperceptronreturns1ontheregiononthenon-shadedsideoftheline. Inc
nosuchlineexiststhatcorrectlyclassifiestheinputs.
Section18.7. Artificial Neural Networks 731
1
0.9
0.8
0.7
0.6
0.5
0.4
tes
tset
no
tcerroc
noitropor P
1
0.9
0.8
0.7
Perceptron 0.6
Decision tree
0.5
0.4
Training set size
tes
tset
no
tcerroc
noitropor P
Perceptron
Decision tree
Training set size
a b
Figure18.22 Comparingtheperformanceofperceptronsanddecisiontrees. a Percep-
tronsarebetteratlearningthemajorityfunctionof11inputs. b Decisiontreesarebetterat
learningthe Will Wait predicateintherestaurantexample.
community in the 1960s. Perceptrons are far from useless however. Section 18.6.4 noted
that logistic regression i.e. training asigmoid perceptron is eventoday a very popular and
effective tool. Moreover a perceptron can represent some quite complex Boolean func-
tions very compactly. For example the majority function which outputs a 1 only if more
than half ofitsninputs are 1 canberepresented byaperceptron witheach w 1andwith
i
w n2. Adecisiontreewouldneedexponentiallymanynodestorepresentthisfunction.
0
the left we show the curve for learning the majority function with 11 Boolean inputs i.e.
thefunctionoutputsa1if6ormoreinputsare1. Aswewouldexpecttheperceptronlearns
the function quite quickly because the majority function is linearly separable. On the other
handthedecision-tree learnermakesnoprogress because themajorityfunctionisveryhard
althoughnotimpossible torepresentasadecisiontree. Ontherightwehavetherestaurant
example. The solution problem is easily represented as a decision tree but is not linearly
separable. Thebestplanethrough thedatacorrectlyclassifiesonly65.
Mc Culloch and Pitts1943werewellawarethatasinglethreshold unitwouldnotsolveall
their problems. In fact their paper proves that such a unit can represent the basic Boolean
functions A ND O Rand N OT andthen goes ontoargue that anydesired functionality canbe
obtainedbyconnecting largenumbersofunitsintopossibly recurrentnetworksofarbitrary
depth. Theproblem wasthatnobodyknewhowtotrainsuchnetworks.
This turns out to be an easy problem if we think of a network the right way: as a
function h xparameterized bytheweights w. Considerthesimplenetwork shownin Fig-
w
ure 18.20b which hastwoinput units twohidden units and twooutput unit. Inaddition
each unit has a dummy input fixed at 1. Given an input vector xx x the activations
h xx h xx
W 1 2 W 1 2
-4 -2 x 10 2 4 -4 -2 0 x 2 -4 -2 x 10 2 4 -4 -2 0 x 2
a b
Figure18.23 a Theresultofcombiningtwoopposite-facingsoftthresholdfunctionsto
producearidge.b Theresultofcombiningtworidgestoproduceabump.
oftheinputunitsaresettoa a x x . Theoutputatunit5isgivenby
a gw w a w a
gw w gw w a w a w gw 4w a w a
05 35 03 13 1 23 2 45 0 14 1 24 2
gw w gw w x w x w gw 4w x w x .
05 35 03 13 1 23 2 45 0 14 1 24 2
Thus we have the output expressed as a function of the inputs and the weights. A similar
expression holds for unit 6. As long as we can calculate the derivatives of such expressions
with respect to the weights we can use the gradient-descent loss-minimization method to
train the network. Section 18.7.4 shows exactly how to do this. And because the function
representedbyanetworkcanbehighlynonlinearcomposed asitisofnestednonlinearsoft
N ON LI NE AR threshold functionswe canseeneuralnetworksasatoolfor doingnonlinearregression.
R EG RE SS IO N
Before delving into learning rules let us look at the ways in which networks generate
complicated functions. Firstrememberthateachunitinasigmoidnetwork represents asoft
threshold in its input space as shown in Figure 18.17c page 726. With one hidden layer
and one output layer as in Figure 18.20b each output unit computes a soft-thresholded
linear combination of several such functions. For example by adding two opposite-facing
softthresholdfunctionsandthresholdingtheresultwecanobtainaridgefunctionasshown
in Figure 18.23a. Combining twosuch ridges atright angles toeach otheri.e. combining
theoutputsfromfourhiddenunits weobtainabumpasshownin Figure18.23b.
Withmorehiddenunits wecanproduce morebumpsofdifferentsizesinmoreplaces.
Infactwithasinglesufficientlylargehiddenlayeritispossibletorepresentanycontinuous
function oftheinputs witharbitrary accuracy; withtwolayers evendiscontinuous functions
canberepresented.9 Unfortunately forany particular network structure itishardertochar-
acterizeexactlywhichfunctions canberepresented andwhichonescannot.
thenumberofinputs.Forexample2nnhiddenunitsareneededtoencodeall Booleanfunctionsofninputs.
Section18.7. Artificial Neural Networks 733
Firstletusdispensewithoneminorcomplicationarisinginmultilayernetworks: interactions
among the learning problems when the network has multiple outputs. In such cases we
shouldthinkofthenetworkasimplementingavectorfunctionh ratherthanascalarfunction
w
h ; for example the network in Figure 18.20b returns a vector a a . Similarly the
w 5 6
target output willbe avector y. Whereas aperceptron network decomposes into m separate
learningproblemsforanm-outputproblemthisdecompositionfailsinamultilayernetwork.
Forexample both a and a in Figure 18.20b depend on all of the input-layer weights so
updatestothoseweightswilldependonerrorsinbotha anda . Fortunatelythisdependency
is very simple in the case of any loss function that is additive across the components of the
errorvector yh x. Forthe L losswehaveforanyweightw
w 2
cid:12 cid:12
""
Lossw yh x2 y a 2 y a 2 18.10
w k k k k
w w w w
k k
where the index k ranges over nodes in the output layer. Each term in the final summation
is just the gradient of the loss for the kth output computed as if the other outputs did not
exist. Hence we can decompose an m-output learning problem into m learning problems
providedweremembertoaddupthegradientcontributionsfromeachofthemwhenupdating
theweights.
The major complication comes from the addition of hidden layers to the network.
Whereas the error y h at the output layer is clear the error at the hidden layers seems
w
mysterious because the training data do not say what value the hidden nodes should have.
Fortunately it turns out that we can back-propagate the error from the output layer to the
B AC K-P RO PA GA TI ON
hiddenlayers. Theback-propagationprocessemergesdirectlyfromaderivationoftheoverall
errorgradient. Firstwewilldescribetheprocesswithanintuitivejustification; thenwewill
showthederivation.
At the output layer the weight-update rule is identical to Equation 18.8. We have
multiple output units so let Err be the kth component of the error vector yh . Wewill
k w
also find it convenient to define a modified error Δ Err
gcid:2
in so that the weight-
k k k
updaterulebecomes
w w αa Δ . 18.11
jk jk j k
Toupdate the connections between the input units and the hidden units weneed to define a
quantity analogous to the error term for output nodes. Here is where we do the error back-
propagation. Theideaisthathiddennodej isresponsibleforsomefractionoftheerror Δ
k
in each of the output nodes to which itconnects. Thus the Δ values are divided according
k
tothestrength oftheconnection betweenthehidden nodeand theoutput node andareprop-
agated back to provide the Δ values for the hidden layer. The propagation rule for the Δ
j
valuesisthefollowing:
cid:12
cid:2
Δ g in w Δ . 18.12
j j jk k
k
function B AC K-P RO P-L EA RN IN Gexamplesnetworkreturnsaneuralnetwork
inputs:examplesasetofexampleseachwithinputvectorxandoutputvectory
networkamultilayernetworkwith Llayersweightswijactivationfunctiong
localvariables: Δavectoroferrorsindexedbynetworknode
repeat
foreachweightwij innetwork do
wijasmallrandomnumber
foreachexamplexyinexamples do
Propagatetheinputsforwardtocomputetheoutputs
foreachnodeiintheinputlayerdo
aixi
forcid:32to Ldo
foreachnocid:2dejinlayercid:3do
inj
i
wij ai
ajginj
Propagatedeltasbackwardfromoutputlayertoinputlayer
foreachnodej intheoutputlayerdo
Δjgcid:5inj yj aj
forcid:3 L1to1do
foreachnodeiinlcid:2ayercid:3do
Δigcid:5ini
j
wij Δj
Updateeveryweightinnetworkusingdeltas
foreachweightwij innetwork do
wijwij α ai Δj
untilsomestoppingcriterionissatisfied
returnnetwork
Figure18.24 Theback-propagationalgorithmforlearninginmultilayernetworks.
Nowtheweight-updaterulefortheweightsbetweentheinputsandthehiddenlayerisessen-
tiallyidenticaltotheupdaterulefortheoutput layer:
w w αa Δ .
ij ij i j
Theback-propagation processcanbesummarizedasfollows:
ComputetheΔvaluesfortheoutputunits usingtheobserved error.
Starting with output layer repeat the following foreach layer in the network until the
earliesthiddenlayerisreached:
PropagatetheΔvaluesbacktothepreviouslayer.
Updatetheweightsbetweenthetwolayers.
Thedetailed algorithm isshownin Figure18.24.
For the mathematically inclined we will now derive the back-propagation equations
from first principles. The derivation is quite similar to the gradient calculation for logistic
Section18.7. Artificial Neural Networks 735
regression leading upto Equation 18.8 onpage 727 except that wehavetouse thechain
rulemorethanonce.
Following Equation 18.10 we compute just the gradient for Loss y a 2 at
k k k
the kthoutput. Thegradient ofthis loss with respect toweights connecting the hidden layer
to the output layer will be zero except for weights w that connect to the kth output unit.
jk
Forthoseweights wehave
Loss a gin
k 2y a k 2y a k
k k k k
w w w
jk jk jk
cid:12
in
2y a gcid:2 in k 2y a gcid:2 in w a
k k k k k k jk j
w w
jk jk
j
2y a gcid:2 in a a Δ
k k k j j k
withΔ definedasbefore. Toobtainthegradientwithrespecttothe w weightsconnecting
k ij
theinputlayertothehidden layer wehavetoexpandoutthe activations a andreapply the
j
chainrule. Wewillshowthederivation ingorydetail because itisinteresting toseehowthe
derivativeoperatorpropagates backthrough thenetwork:
Loss a gin
k 2y a k 2y a k
k k k k
w w w
ij ij ij
cid:12
in
2y a gcid:2 in k 2Δ w a
k k k k jk j
w w
ij ij
j
a gin
2Δ w j 2Δ w j
k jk k jk
w w
ij ij
in
2Δ w gcid:2 in j
k jk j
w
ij cid:31
cid:12
""
2Δ w gcid:2 in w a
k jk j ij i
w
ij
i
2Δ w gcid:2 in a a Δ
k jk j i i j
whereΔ isdefinedasbefore. Thusweobtaintheupdaterulesobtainedearlierfromintuitive
j
considerations. Itisalsoclearthattheprocess canbecontinued fornetworkswithmorethan
onehiddenlayer whichjustifiesthegeneralalgorithm givenin Figure18.24.
Having made it through or skipped over all the mathematics lets see how a single-
hidden-layer network performs on the restaurant problem. First we need to determine the
structure of the network. We have 10 attributes describing each example so we will need
Shouldtheybefullyconnected? Thereisnogoodtheorythatwilltellustheanswer. Seethe
nextsection. Asalwayswecanusecross-validation: tryseveraldifferentstructures andsee
whichoneworksbest. Itturnsoutthatanetworkwithonehiddenlayercontainingfournodes
is about right for this problem. In Figure 18.25 we show two curves. The first is a training
curve showing the mean squared error on a given training set of 100 restaurant examples
14
12
10
8
6
4
2
0
tes
gniniart
no
rorre
lato T
1
0.9
0.8
0.7
0.6
0.5
0.4
Number of epochs
tes
tset
no
tcerroc
noitropor P Decision tree
Multilayer network
Training set size
a b
modified over several epochs for a given set of examples in the restaurant domain. b
Comparativelearningcurvesshowingthatdecision-treelearningdoesslightlybetteronthe
restaurantproblemthanback-propagationinamultilayernetwork.
duringtheweight-updatingprocess. Thisdemonstratesthatthenetworkdoesindeedconverge
to a perfect fit to the training data. The second curve is the standard learning curve for the
restaurant data. The neural network does learn well although not quite as fast as decision-
tree learning; this is perhaps not surprising because the data were generated from a simple
decision treeinthefirstplace.
Neural networks are capable of farmore complex learning tasks of course although it
must be said that a certain amount of twiddling is needed to get the network structure right
andtoachieveconvergence tosomethingclosetotheglobaloptimuminweightspace. There
are literally tens of thousands of published applications of neural networks. Section 18.11.1
looksatonesuchapplication inmoredepth.
Sofarwehaveconsidered theproblem oflearning weights givenafixednetwork structure;
just as with Bayesian networks we also need to understand how to find the best network
structure. Ifwechooseanetworkthatistoobigitwillbeabletomemorizealltheexamples
by forming a large lookup table but will not necessarily generalize well to inputs that have
notbeenseenbefore.10 Inotherwordslikeallstatisticalmodelsneuralnetworksaresubject
tooverfitting whenthere aretoo manyparameters inthe model. Wesawthis in Figure 18.1
page 696 where the high-parameter models in b and c fit all the data but might not
generalize aswellasthelow-parametermodelsinaandd.
Ifwesticktofullyconnectednetworkstheonlychoicestobemadeconcernthenumber
restrictionkeepstheactivationvaluesinthelinearregionofthesigmoidfunctiongxwherexisclosetozero.
Thisinturnmeansthatthenetworkbehaveslikealinearfunction Exercise18.22withfarfewerparameters.
Section18.8. Nonparametric Models 737
of hidden layers and their sizes. The usual approach is to try several and keep the best. The
cross-validation techniques of Chapter 18 are needed if we are to avoid peeking at the test
set. Thatiswechoosethenetworkarchitecture thatgivesthehighestprediction accuracyon
thevalidation sets.
If we want to consider networks that are not fully connected then we need to find
someeffectivesearchmethodthroughtheverylargespaceofpossibleconnectiontopologies.
O PT IM AL BR AI N The optimal brain damage algorithm begins with a fully connected network and removes
D AM AG E
connections from it. After the network is trained for the first time an information-theoretic
approach identifies an optimal selection of connections that can be dropped. The network
is then retrained and if its performance has not decreased then the process is repeated. In
additiontoremovingconnections itisalsopossible toremoveunitsthatarenotcontributing
muchtotheresult.
Severalalgorithmshavebeenproposedforgrowingalargernetworkfromasmallerone.
One the tiling algorithm resembles decision-list learning. The idea is to start with a single
T IL IN G
unit that does its best to produce the correct output on as many of the training examples as
possible. Subsequentunitsareaddedtotakecareoftheexamplesthatthefirstunitgotwrong.
Thealgorithm addsonlyasmanyunitsasareneededtocoveralltheexamples.
Linearregression and neural networks use thetraining data toestimate afixedset ofparam-
eters w. Thatdefinesourhypothesis h xandatthatpointwecanthrowawaythetraining
w
data because they are all summarized by w. A learning model that summarizes data with a
set of parameters of fixed size independent of the number of training examples is called a
parametricmodel.
P AR AM ET RI CM OD EL
No matter how much data you throw at a parametric model it wont change its mind
abouthowmanyparametersitneeds. Whendatasetsaresmallitmakessensetohaveastrong
restrictionontheallowablehypotheses toavoidoverfitting. Butwhentherearethousandsor
millionsorbillionsofexamplestolearnfromitseemslike abetterideatoletthedataspeak
for themselves rather than forcing them to speak through a tiny vector of parameters. If the
data say that the correct answer is a very wiggly function we shouldnt restrict ourselves to
linearorslightly wigglyfunctions.
N ON PA RA ME TR IC Anonparametricmodelisonethatcannotbecharacterizedbyaboundedsetofparam-
M OD EL
eters. Forexample suppose that each hypothesis wegenerate simply retains within itself all
ofthetraining examples andusesallofthemtopredict thenextexample. Suchahypothesis
family would benonparametric because the effective number of parameters isunbounded
I NS TA NC E-B AS ED it grows with the number of examples. This approach is called instance-based learning or
L EA RN IN G
memory-basedlearning. Thesimplestinstance-basedlearningmethodistablelookup: take
T AB LE LO OK UP
allthetrainingexamplesputtheminalookuptableandthenwhenaskedforhxseeifxis
inthe table; ifitis return thecorresponding y. Theproblem withthismethod is thatitdoes
notgeneralize well: whenxisnotinthetableallitcandoisreturnsomedefault value.
x x
x x
k1 k5
Figure18.26 a Ak-nearest-neighbormodelshowingtheextentoftheexplosionclassfor
thedatain Figure18.15withk1. Overfittingisapparent. b With k5theoverfitting
problemgoesawayforthisdataset.
Wecanimproveontablelookupwithaslightvariation: givenaqueryx findthekexamples
q
N EA RE ST that are nearest to x . This is called k-nearest neighbors lookup. Well use the notation
N EI GH BO RS q
N Nkx todenotethesetofk nearestneighbors.
q
To do classification first find N Nkx then take the plurality vote of the neighbors
q
which is the majority vote in the case of binary classification. To avoid ties k is always
chosen to be an odd number. To do regression we can take the mean or median of the k
neighbors orwecansolvealinearregression problem onthe neighbors.
In Figure 18.26 we show the decision boundary of k-nearest-neighbors classification
for k 1 and 5 on the earthquake data set from Figure 18.15. Nonparametric methods are
stillsubjecttounderfittingandoverfittingjustlikeparametricmethods. Inthiscase1-nearest
neighborsisoverfitting;itreactstoomuchtotheblackoutlierintheupperrightandthewhite
outlier at 5.4 3.7. The 5-nearest-neighbors decision boundary is good; higher k would
underfit. Asusual cross-validation canbeusedtoselectthebestvalueofk.
The very word nearest implies a distance metric. How do we measure the distance
from a query point x to an example point x ? Typically distances are measured with a
q j
M IN KO WS KI Minkowskidistanceor Lp normdefinedas
D IS TA NC E cid:12
Lpx x x x p1p .
j q ji qi
i
With p2this is Euclidean distance and withp1itis Manhattan distance. With Boolean
attribute values the number of attributes on which the two points differ is called the Ham-
mingdistance. Often p2isused ifthedimensions aremeasuring similarproperties such
H AM MI NG DI ST AN CE
asthewidth height and depth ofparts onaconveyor belt and Manhattan distance isused if
they aredissimilar such as age weight and gender ofapatient. Note that ifweuse the raw
numbers from each dimension then the total distance will be affected by a change in scale
in any dimension. That is if we change dimension i from measurements in centimeters to
Section18.8. Nonparametric Models 739
mileswhilekeeping theotherdimensions thesamewellgetdifferent nearest neighbors. To
avoidthisitiscommontoapplynormalizationtothemeasurementsineachdimension. One
N OR MA LI ZA TI ON
simple approach is to compute the mean μ and standard deviation σ of the values in each
i i
dimension and rescale them so that x becomes x μ σ . A more complex metric
ji ji i i
M AH AL AN OB IS knownasthe Mahalanobisdistancetakesintoaccountthecovariance betweendimensions.
D IS TA NC E
In low-dimensional spaces with plenty of data nearest neighbors works very well: we
are likely to have enough nearby data points to get a good answer. But as the number of
dimensions rises weencounter aproblem: the nearest neighbors inhigh-dimensional spaces
areusually notverynear! Consider k-nearest-neighbors onadata setof N points uniformly
distributed throughout the interior of an n-dimensional unit hypercube. Well define the k-
neighborhood ofapointasthesmallesthypercube thatcontains thek-nearest neighbors. Let
cid:3betheaveragesidelengthofaneighborhood. Thenthevolumeoftheneighborhoodwhich
contains k points is cid:3n and the volume of the full cube which contains N points is 1. So
onaverage cid:3nk N. Takingnthrootsofbothsideswegetcid:3 k N1n.
To be concrete let k10 and N1000000. In two dimensions n2; a unit
square the average neighborhood has cid:30.003 a small fraction of the unit square and
in3dimensions cid:3isjust2oftheedgelengthoftheunitcube. Butbythetimewegetto17
dimensions cid:3ishalftheedgelengthoftheunithypercube andin200dimensions itis94.
C UR SE OF Thisproblem hasbeencalledthecurseofdimensionality.
D IM EN SI ON AL IT Y
Anotherwaytolookatit: considerthepointsthatfallwithinathinshellmakingupthe
outer 1 of the unit hypercube. These are outliers; in general it will be hard to find a good
value forthembecause wewillbeextrapolating ratherthaninterpolating. Inonedimension
these outliers are only 2 of the points on the unit line those points where x .01 or
x .99 but in 200 dimensions over 98 of the points fall within this thin shellalmost
allthepoints areoutliers. Youcanseeanexample ofapoornearest-neighbors fitonoutliers
ifyoulookaheadto Figure18.28b.
The N Nkx function isconceptually trivial: givenasetof N examplesandaquery
q
x iteratethroughtheexamplesmeasurethedistanceto x fromeachoneandkeepthebest
q q
k. Ifwearesatisfiedwithanimplementation thattakes O Nexecutiontimethenthatisthe
end of the story. But instance-based methods are designed for large data sets so we would
like an algorithm with sublinear run time. Elementary analysis of algorithms tells us that
exact table lookup is O N with a sequential table Olog N with a binary tree and O1
with a hash table. We will now see that binary trees and hash tables are also applicable for
findingnearest neighbors.
Abalancedbinarytreeoverdatawithanarbitrarynumberofdimensionsiscalledak-dtree
K-D TR EE
for k-dimensional tree. In our notation the number of dimensions is n so they would be
n-d trees. The construction of a k-d tree is similar to the construction of a one-dimensional
balancedbinarytree. Westartwithasetofexamplesandattherootnodewesplitthemalong
theithdimension bytesting whetherx m. Wechose thevaluemtobethemedian ofthe
i
examplesalongtheithdimension;thushalftheexampleswillbeintheleftbranchofthetree
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
doohrobhgien
fo
htgnel
egd E
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Number of dimensions
llehs
roiretxe
ni
stniop
fo
noitropor P
Number of dimensions
a b
Figure18.27 Thecurseofdimensionality:a Thelengthoftheaverageneighborhoodfor
10-nearest-neighborsinaunithypercubewith1000000pointsasafunctionofthenumber
of dimensions. b The proportion of points that fall within a thin shell consisting of the
outer1ofthehypercubeasafunctionofthenumberofdimensions.Sampledfrom10000
randomlydistributedpoints.
and half inthe right. Wethen recursively makeatree fortheleft and right sets ofexamples
stopping when there are fewerthan two examples left. To choose a dimension to split on at
each node of the tree one can simply select dimension i mod n at level i of the tree. Note
thatwemayneedtosplitonanygivendimensionseveraltimesasweproceeddownthetree.
Anotherstrategyistosplitonthedimension thathasthewidestspreadofvalues.
Exact lookup from a k-d tree is just like lookup from a binary tree with the slight
complicationthatyouneedtopayattentiontowhichdimensionyouaretestingateachnode.
But nearest neighbor lookup is more complicated. As we go down the branches splitting
the examples in half in some cases we can discard the other half of the examples. But not
always. Sometimes the point we are querying for falls very close to the dividing boundary.
The query point itself might be on the left hand side of the boundary but one or more of
the k nearest neighbors might actually be on the right-hand side. We have to test for this
possibility by computing the distance of the query point to the dividing boundary and then
searching bothsides ifwecant findk examplesontheleftthatarecloserthanthisdistance.
Because ofthisproblem k-dtreesareappropriate onlywhen therearemanymoreexamples
than dimensions preferably at least 2n examples. Thus k-d trees work well with up to 10
dimensionswiththousandsofexamplesorupto20dimensionswithmillionsofexamples. If
wedonthaveenoughexamples lookupisnofasterthanalinearscanoftheentiredataset.
Hash tables have the potential to provide even faster lookup than binary trees. But how can
wefindnearestneighbors usingahashtable whenhashcodesrelyonanexactmatch? Hash
codes randomly distribute values among the bins but we want to have near points grouped
L OC AL IT Y-S EN SI TI VE togetherinthesamebin;wewantalocality-sensitive hash L SH.
H AS H
Section18.8. Nonparametric Models 741
We cant use hashes to solve N Nkx exactly but with a clever use of randomized
q
algorithms we can find an approximate solution. First we define the approximate near-
A PP RO XI MA TE neighborsproblem: given adata setofexample points and aquery point x find withhigh
N EA R-N EI GH BO RS q
probability an example point orpoints that isnear x . Tobe moreprecise werequire that
q
if there is a point x that is within a radius r of x then with high probability the algorithm
j q
willfindapointx jcid:3 thatiswithindistancecrofq. Ifthereisnopointwithinradiusrthenthe
algorithm isallowed toreport failure. Thevalues of cand high probability areparameters
ofthealgorithm.
To solve approximate near neighbors we will need a hash function gx that has the
propertythatforanytwopoints x
j
andx jcid:3theprobabilitythattheyhavethesamehashcode
is small if their distance is more than cr and is high if their distance is less than r. For
simplicity we will treat each point as a bit string. Any features that are not Boolean can be
encoded intoasetof Booleanfeatures.
The intuition we rely on is that if two points are close together in an n-dimensional
spacethentheywillnecessarilybeclosewhenprojecteddownontoaone-dimensionalspace
aline. Infact wecandiscretize thelineinto binshash bucketsso that withhigh prob-
ability nearpoints project downto exactly the same bin. Points that are faraway from each
otherwilltendtoproject downinto different binsformostprojections but therewillalways
be a few projections that coincidentally project far-apart points into the same bin. Thus the
bin forpoint x contains manybut not allpoints thatare nearto x as wellassome points
q q
thatarefaraway.
Thetrickof L SHistocreatemultiplerandomprojectionsandcombinethem. Arandom
projection is just a random subset of the bit-string representation. We choose cid:3 different
randomprojectionsandcreate cid:3hashtablesg x...g x. Wethenenteralltheexamples
intoeachhashtable. Thenwhengivenaquerypointx wefetchthesetofpointsinbing q
q k
foreach kandunion these setstogether intoasetofcandidate points C. Thenwecompute
theactualdistancetox foreachofthepointsin C andreturnthekclosestpoints. Withhigh
q
probability eachofthepointsthatarenearto x willshowupinatleastoneofthebins and
q
although some far-away points will show up as well we can ignore those. With large real-
world problems such as finding the near neighbors in a data set of 13 million Web images
using512dimensions Torralbaetal.2008locality-sensitivehashingneedstoexamineonly
a few thousand images out of 13 million to find nearest neighbors; a thousand-fold speedup
overexhaustiveork-dtreeapproaches.
Now well look at nonparametric approaches to regression rather than classification. Fig-
ure 18.28 shows an example of some different models. In a wehave perhaps the simplest
method of all known informally as connect-the-dots and superciliously as piecewise-
linear nonparametric regression. This model creates a function hx that when given a
query x solves the ordinary linear regression problem with just two points: the training
q
examples immediately to the left and right of x . When noise is low this trivial method is
q
actuallynottoobadwhichiswhyitisastandardfeatureofchartingsoftwareinspreadsheets.
a b
c d
Figure18.28 Nonparametricregressionmodels:aconnectthedotsb3-nearestneigh-
borsaveragec3-nearest-neighborslinearregression dlocallyweightedregressionwith
aquadratickernelofwidthk10.
Butwhenthedataarenoisy theresulting functionisspiky anddoesnotgeneralize well.
N EA RE ST- k-nearest-neighbors regression Figure 18.28b improves on connect-the-dots. In-
N EI GH BO RS
R EG RE SS IO N
stead of using just the two examples to the left and right of a query point x we use the
q
k nearest neighbors here 3. A larger value of k tends to smooth out the magnitude of
thespikes although theresulting function hasdiscontinuities. Inb wehave thek-nearest-
cid:2
neighborsaverage: hxisthemeanvalueofthekpoints y k. Noticethatattheoutlying
j
pointsnearx0andx14theestimatesarepoorbecausealltheevidencecomesfromone
sidetheinteriorandignoresthetrend. Incwehavek-nearest-neighbor linearregression
whichfindsthebestlinethroughthekexamples. Thisdoesabetterjobofcapturingtrendsat
theoutliers butisstilldiscontinuous. Inbothbandcwereleftwiththequestionofhow
tochoose agoodvaluefork. Theanswerasusual iscross-validation.
L OC AL LY WE IG HT ED Locallyweightedregression Figure18.28dgivesustheadvantagesofnearestneigh-
R EG RE SS IO N
borswithoutthediscontinuities. Toavoiddiscontinuities inhxweneedtoavoiddisconti-
Section18.8. Nonparametric Models 743
1
0.5
0
-10 -5 0 5 10
k10centeredonthequerypointx0.
nuitiesinthesetofexamplesweusetoestimatehx. Theideaoflocallyweightedregression
isthatateachquerypoint x theexamplesthatareclosetox areweightedheavily andthe
q q
examplesthatarefartherawayareweightedlessheavilyornotatall. Thedecreaseinweight
overdistanceisalwaysgradual notsudden.
We decide how much to weight each example with a function known as a kernel. A
K ER NE L
kernelfunctionlookslikeabump;in Figure18.29weseethespecifickernelusedtogenerate
andreacheszeroatadistanceof5. Canwechoosejustanyfunctionforakernel? No. First
notethatweinvokeakernelfunction Kwith K Distancex x wherex isaquerypoint
j q q
that is a given distance from x and we want to know how much to weight that distance.
j
So K should be symmetric around 0 and have a maximum at 0. The area under the kernel
mustremainbounded aswegoto. Othershapes suchas Gaussians havebeenusedfor
kernels but the latest research suggests that the choice of shape doesnt matter much. We
do have to be careful about the width of the kernel. Again this is a parameter of the model
thatisbestchosen bycross-validation. Justasinchoosing thek fornearest neighbors ifthe
kernels aretoowidewellgetunderfitting andiftheyaretoo narrow wellgetoverfitting. In
Figure18.29d thevalueofk10givesasmoothcurvethatlooksaboutrightbut maybe
it does not pay enough attention to the outlier at x6; a narrower kernel width would be
moreresponsivetoindividual points.
Doing locally weighted regression with kernels is now straightforward. For a given
querypointx wesolvethefollowingweightedregression problem usinggradientdescent:
q cid:12
w argmin K Distancex x y wx 2
q j j j
w
j
where Distance is any of the distance metrics discussed for nearest neighbors. Then the
answerishx
wx
.
q q
Notethatweneedtosolveanewregressionproblemforeveryquerypointthatswhat
it means to be local. In ordinary linear regression we solved the regression problem once
globally and then used thesameh forany query point. Mitigating against thisextra work
w
is the fact that each regression problem will be easier to solve because it involves only the
examples withnonzero weightthe examples whosekernels overlap thequery point. When
kernelwidthsaresmallthismaybejustafewpoints.
Mostnonparametricmodelshavetheadvantagethatitiseasytodoleave-one-outcross-
validation without having to recompute everything. With a k-nearest-neighbors model for
instance whengivenatestexamplexyweretrievetheknearestneighborsoncecompute
the per-example loss Lyhx from them and record that as the leave-one-out result for
everyexamplethatisnotoneoftheneighbors. Thenweretrievethek1nearestneighbors
and record distinct results for leaving out each of the k neighbors. With N examples the
wholeprocessis Oknot Ok N.
S UP PO RT VE CT OR Thesupportvectormachineor S VMframeworkiscurrently themostpopularapproach for
M AC HI NE
off-the-shelf supervised learning: ifyoudonthaveanyspecialized priorknowledge about
a domain then the S VM is an excellent method to try first. There are three properties that
make S VMsattractive:
1. S VMsconstructamaximummarginseparatoradecisionboundarywiththelargest
possibledistance toexamplepoints. Thishelpsthemgeneralize well.
2. S VMs create a linear separating hyperplane but they have the ability to embed the
dataintoahigher-dimensional space using theso-called kerneltrick. Oftendatathat
arenot linearly separable in the original input space are easily separable inthe higher-
dimensional space. The high-dimensional linear separator is actually nonlinear in the
original space. Thismeansthehypothesis space isgreatly expanded overmethods that
usestrictly linearrepresentations.
3. S VMsareanonparametricmethodtheyretaintrainingexamplesandpotentiallyneed
to store them all. On the other hand in practice they often end up retaining only a
smallfractionofthenumberofexamplessometimes asfewas asmallconstant times
the numberof dimensions. Thus S VMscombine the advantages of nonparametric and
parametric models: they have the flexibility to represent complex functions but they
areresistant tooverfitting.
You could say that S VMs are successful because of one key insight and one neat trick. We
willcovereachinturn. In Figure18.30awehaveabinaryclassificationproblemwiththree
candidate decision boundaries each a linear separator. Each of them is consistent with all
the examples so from the point of view of 01 loss each would be equally good. Logistic
regression would find some separating line; the exact location of the line depends on all the
example points. The key insight of S VMs is that some examples are more important than
others andthatpayingattention tothemcanleadtobettergeneralization.
Consider the lowest of the three separating lines in a. It comes very close to 5 of the
black examples. Although itclassifies allthe examples correctly andthus minimizes loss it
Section18.9. Support Vector Machines 745
a b
Figure18.30 Supportvectormachineclassification: a Twoclassesofpointsblackand
white circles and three candidate linear separators. b The maximum margin separator
heavy line is at the midpoint of the margin area between dashed lines. The support
vectorspointswithlargecirclesaretheexamplesclosesttotheseparator.
should make you nervous that so many examples are close to the line; it may be that other
blackexampleswillturnouttofallontheothersideoftheline.
S VMsaddressthisissue: Insteadofminimizingexpectedempiricallossonthetraining
data S VMs attempt to minimize expected generalization loss. We dont know where the
as-yet-unseen points may fall but under the probabilistic assumption that they are drawn
from the same distribution as the previously seen examples there are some arguments from
computationallearningtheory Section18.5suggestingthatweminimizegeneralizationloss
by choosing the separator that is farthest away from the examples we have seen so far. We
M AX IM UM MA RG IN callthisseparator shownin Figure18.30b the maximummargin separator. Themargin
S EP AR AT OR
is the width of the area bounded by dashed lines in the figuretwice the distance from the
M AR GI N
separatortothenearestexamplepoint.
Now how do we find this separator? Before showing the equations some notation:
Traditionally S VMsuse the convention that class labels are 1and -1 instead ofthe 1 and
0wehave been using so far. Also where weput the intercept into the weight vector wand
a corresponding dummy 1 value into x S VMs do not do that; they keep the intercept
j0
as a separate parameter b. With that in mind the separator is defined as the set of points
x : wxb0. We could search the space of w and b with gradient descent to find the
parameters thatmaximizethemarginwhilecorrectly classifying alltheexamples.
However it turns out there is another approach to solving this problem. We wont
show the details but will just say that there is an alternative representation called the dual
representation inwhichtheoptimalsolution isfoundbysolving
cid:12 cid:12
1
argmax α α α y y x x 18.13
j j k j k j k
2
α
j jk
cid:2
Q UA DR AT IC subject to the constraints α 0 and α y 0. This is a quadratic programming
P RO GR AM MI NG j j j j
optimization problem forwhichthere aregoodsoftware packages. Oncewehave found the
cid:2
vector α we can get back to w with the equation w α x or we can stay in the dual
j j j
representation. Therearethreeimportantpropertiesof Equation18.13. Firsttheexpression
isconvex;ithasasingleglobalmaximumthatcanbefoundefficiently. Secondthedataenter
theexpressiononlyintheformofdotproductsofpairsofpoints. Thissecondpropertyisalso
trueoftheequation fortheseparatoritself;oncetheoptimalα havebeencalculated itis
j
""
cid:12
hx sign α y xx b . 18.14
j j j
j
Afinalimportant property isthattheweights α associated witheachdatapointarezeroex-
j
ceptforthe supportvectorsthe points closest totheseparator. Theyarecalledsupport
S UP PO RT VE CT OR
vectors because they hold up the separating plane. Because there are usually manyfewer
supportvectorsthanexamples S VMsgainsomeoftheadvantages ofparametricmodels.
Whatiftheexamples arenotlinearly separable? Figure18.31a showsaninput space
defined by attributes xx x with positive examples y 1 inside a circular region
andnegativeexamplesy1outside. Clearlythereisnolinearseparatorforthisproblem.
Nowsupposewere-expresstheinputdatai.e.wemapeachinputvectorxtoanewvector
offeature values Fx. Inparticular letususethethreefeatures
""
f x2 f x2 f 2x x . 18.15
We will see shortly where these came from but for now just look at what happens. Fig-
ure18.31bshowsthedatainthenewthree-dimensionalspacedefinedbythethreefeatures;
the data are linearly separable in this space! This phenomenon is actually fairly general: if
dataaremappedintoaspace ofsufficiently high dimension thentheywillalmostalwaysbe
linearlyseparableifyoulookatasetofpointsfromenough directions youllfindawayto
makethemlineup. Hereweusedonlythreedimensions;11 Exercise18.16asksyoutoshow
thatfourdimensions sufficeforlinearly separating acircle anywhereintheplanenotjustat
theoriginandfivedimensionssufficetolinearlyseparate anyellipse. Ingeneralwithsome
specialcasesexceptedifwehave N datapointsthentheywillalwaysbeseparableinspaces
of N 1dimensions ormore Exercise18.25.
Now we would not usually expect to find a linear separator in the input space x but
wecanfindlinearseparatorsinthehigh-dimensional featurespace Fxsimplybyreplacing
x x in Equation18.13with Fx Fx . Thisbyitselfisnotremarkablereplacing xby
j k j k
Fxinanylearningalgorithmhastherequiredeffectbutthedotproducthassomespecial
properties. It turns out that Fx Fx can often becomputed without firstcomputing F
j k
Section18.9. Support Vector Machines 747
1.5
1
0.5
0
-0.5
-1
-1.5
-1.5 -1 -0.5 0 0.5 1 1.5
x 2
2xx
3
2
1
0
-1
-2 2.5
-3 2
0
2
x2 1.5 0.5 1 2
x
1
a b
cles and negative examples as white circles. The true decision boundary x2 x2 1
is also shown. b The same data after mapping into a three-dimensional input space
x2x2 2x x . Thecirculardecisionboundaryinabecomesalineardecisionboundary
inthreedimensions.Figure18.30bgivesacloseupoftheseparatorinb.
foreachpoint. Inourthree-dimensional featurespacedefinedby Equation18.15alittlebit
ofalgebra showsthat
Fx Fx x x 2 .
j k j k
""
Thats why the 2 is in f . The expression x x 2 is called a kernel function12 and
K ER NE LF UN CT IO N 3 j k
is usually written as Kx x . The kernel function can be applied to pairs of input data to
j k
evaluate dotproducts insomecorresponding featurespace. Sowecanfindlinearseparators
inthehigher-dimensional featurespace Fxsimplybyreplacing x x in Equation18.13
j k
withakernelfunction Kx x . Thuswecanlearninthehigher-dimensional spacebutwe
j k
computeonlykernelfunctions ratherthanthefulllistoffeatures foreachdatapoint.
Thenextstepistoseethattheresnothingspecialaboutthekernel Kx x x x 2.
j k j k
It corresponds to a particular higher-dimensional feature space but other kernel functions
correspond to other feature spaces. A venerable result in mathematics Mercers theo-
rem 1909 tells us that any reasonable13 kernel function corresponds to some feature
M ER CE RS TH EO RE M
space. These feature spaces can be very large even for innocuous-looking kernels. For ex-
P OL YN OM IA L ample the polynomial kernel Kx x 1 x x d corresponds to a feature space
K ER NE L j k j k
whosedimension isexponential ind.
S VMkernelsaredistancemetricsbutnotallare.
This then is the clever kernel trick: Plugging these kernels into Equation 18.13
K ER NE LT RI CK
optimal linear separators can be found efficiently in feature spaces with billions of or in
somecases infinitely manydimensions. Theresulting linearseparators whenmappedback
to the original input space can correspond to arbitrarily wiggly nonlinear decision bound-
ariesbetweenthepositiveandnegativeexamples.
In the case of inherently noisy data we may not want a linear separator in some high-
dimensional space. Rather wed like a decision surface in a lower-dimensional space that
does not cleanly separate the classes but reflects the reality of the noisy data. That is pos-
sible withthe soft margin classifier which allows examples to fall on the wrong side of the
S OF TM AR GI N
decision boundary but assigns them a penalty proportional to the distance required to move
thembackonthecorrectside.
The kernel method can be applied not only with learning algorithms that find optimal
linear separators but also with any other algorithm that can be reformulated to work only
with dot products of pairs of data points as in Equations 18.13 and 18.14. Once this is
done the dot product is replaced by a kernel function and we have a kernelized version
K ER NE LI ZA TI ON
of the algorithm. This can be done easily for k-nearest-neighbors and perceptron learning
Section18.7.2amongothers.
So far we have looked at learning methods in which a single hypothesis chosen from a
E NS EM BL E hypothesis space is used to make predictions. The idea of ensemble learning methods is
L EA RN IN G
to select a collection or ensemble of hypotheses from the hypothesis space and combine
their predictions. For example during cross-validation we might generate twenty different
decision treesandhavethemvoteonthebestclassification foranewexample.
The motivation for ensemble learning is simple. Consider an ensemble of K5 hy-
pothesesandsupposethatwecombinetheirpredictionsusingsimplemajorityvoting. Forthe
ensembletomisclassify anewexampleatleastthreeofthefivehypotheses havetomisclas-
sifyit. Thehopeisthatthisismuchlesslikelythanamisclassificationbyasinglehypothesis.
Suppose we assume that each hypothesis h in the ensemble has an error of pthat is the
k
probabilitythatarandomlychosenexampleismisclassifiedbyh isp. Furthermoresuppose
k
weassumethattheerrorsmadebyeachhypothesisareindependent. Inthatcaseifpissmall
then the probability of a large number of misclassifications occurring is minuscule. For ex-
ampleasimplecalculation Exercise18.18showsthatusinganensembleoffivehypotheses
reduces an error rate of 1 in 10 down to an error rate of less than 1 in 100. Now obviously
the assumption of independence isunreasonable because hypotheses arelikely tobemisled
in the same way by any misleading aspects of the training data. But ifthe hypotheses are at
leastalittlebitdifferenttherebyreducingthecorrelationbetweentheirerrorsthenensemble
learning canbeveryuseful.
Another way to think about the ensemble idea is as a generic way of enlarging the
hypothesisspace. Thatisthinkoftheensembleitselfasahypothesisandthenewhypothesis
Section18.10. Ensemble Learning 749
""
""
""
""
""
""
""
""
""
""
""
""
""
""
""
""
""
Figure18.32 Illustrationoftheincreasedexpressivepowerobtainedby ensemblelearn-
ing. We take three linear threshold hypotheses each of which classifies positively on the
unshaded side and classify as positive any example classified positively by all three. The
resultingtriangularregionisahypothesisnotexpressibleintheoriginalhypothesisspace.
spaceasthesetofallpossibleensemblesconstructablefromhypothesesintheoriginalspace.
Figure18.32showshowthiscanresultinamoreexpressivehypothesis space. Iftheoriginal
hypothesis space allows for a simple and efficient learning algorithm then the ensemble
methodprovidesawaytolearnamuchmoreexpressiveclassofhypotheseswithoutincurring
muchadditional computational oralgorithmic complexity.
Themostwidelyusedensemblemethodiscalledboosting. Tounderstandhowitworks
B OO ST IN G
W EI GH TE DT RA IN IN G we need first to explain the idea of a weighted training set. In such a training set each
S ET
example has an associated weight w 0. The higher the weight of an example the higher
j
is the importance attached to it during the learning of a hypothesis. It is straightforward to
modifythelearning algorithmswehaveseensofartooperate withweightedtraining sets.14
Boosting starts with w 1foralltheexamples i.e. anormal training set. From this
j
setitgeneratesthefirsthypothesis h . Thishypothesiswillclassifysomeofthetrainingex-
1
amplescorrectly andsomeincorrectly. Wewouldlikethenexthypothesis todobetteronthe
misclassifiedexamplessoweincreasetheirweightswhiledecreasingtheweightsofthecor-
rectly classified examples. From this new weighted training set wegenerate hypothesis h .
2
Theprocesscontinuesinthiswayuntilwehavegenerated K hypotheseswhere K isaninput
totheboostingalgorithm. Thefinalensemblehypothesisisaweighted-majoritycombination
ofallthe Khypotheseseachweightedaccordingtohowwellitperformedonthetrainingset.
Figure18.33showshowthealgorithmworksconceptually. Therearemanyvariantsoftheba-
sicboostingideawithdifferentwaysofadjustingtheweightsandcombiningthehypotheses.
Onespecificalgorithmcalled A DA BO OS Tisshownin Figure18.34. A DA BO OS Thasavery
important property: if the input learning algorithm L is a weak learning algorithmwhich
W EA KL EA RN IN G
thejthexampleappearswj timesusingrandomizationtohandlefractionalweights.
h h h h
h
Figure18.33 Howtheboostingalgorithmworks. Eachshadedrectanglecorrespondsto
anexample; the heightof therectanglecorrespondsto theweight. Thechecksandcrosses
indicatewhethertheexamplewasclassifiedcorrectlybythecurrenthypothesis. Thesizeof
thedecisiontreeindicatestheweightofthathypothesisinthefinalensemble.
means that L always returns a hypothesis with accuracy on the training set that is slightly
betterthanrandomguessingi.e.50cid:2for Booleanclassificationthen A DA BO OS Twill
return a hypothesis that classifies the training data perfectly for large enough K. Thus the
algorithm boosts the accuracy of the original learning algorithm on the training data. This
result holds no matter how inexpressive the original hypothesis space and no matter how
complexthefunction beinglearned.
Letusseehowwellboostingdoesontherestaurantdata. Wewillchooseasouroriginal
hypothesis spacetheclassofdecision stumpswhicharedecision treeswithjustonetestat
D EC IS IO NS TU MP
the root. The lower curve in Figure 18.35a shows that unboosted decision stumps are not
veryeffectiveforthisdatasetreachingapredictionperformanceofonly81on100training
examples. When boosting is applied with K5 the performance is better reaching 93
after100examples.
Aninteresting thing happens astheensemble size K increases. Figure18.35b shows
the training set performance on 100 examples as a function of K. Notice that the error
reaches zero when K is 20; that is a weighted-majority combination of 20 decision stumps
sufficestofitthe100examplesexactly. Asmorestumpsareaddedtotheensemble theerror
remains at zero. The graph also shows that the test set performance continues to increase
long after the training set error has reached zero. At K 20 the test performance is 0.95
or 0.05 error and the performance increases to 0.98 as late as K 137 before gradually
dropping to0.95.
Thisfindingwhichisquiterobustacrossdatasetsandhypothesisspacescameasquite
a surprise when it was first noticed. Ockhams razor tells us not to make hypotheses more
Section18.10. Ensemble Learning 751
function A DA BO OS Texamples L Kreturnsaweighted-majorityhypothesis
inputs:examplessetof N labeledexamplesx 1y 1...x Ny N
Lalearningalgorithm
Kthenumberofhypothesesintheensemble
localvariables: wavectorof N exampleweightsinitially1 N
havectorof K hypotheses
zavectorof K hypothesisweights
fork 1to K do
hk Lexamplesw
error0
forj 1to N do
ifhkxjcid:7 yj thenerrorerror wj
forj 1to N do
ifhkxjyj thenwjwj error1error
w N OR MA LI ZEw
zklog1errorerror
return W EI GH TE D-M AJ OR IT Yhz
Figure18.34 The A DA BO OS Tvariantoftheboostingmethodforensemblelearning.The
algorithmgenerateshypothesesbysuccessivelyreweightingthetrainingexamples.Thefunc-
tion W EI GH TE D-M AJ OR IT Y generates a hypothesis that returns the output value with the
highestvotefromthehypothesesinhwithvotesweightedbyz.
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5
tes
tset
no
tcerroc
noitropor P
1
0.95
0.9
0.85
0.8
0.75
Boosted decision stumps
Decision stump 0.7
0.65
0.6
Training set size
ycarucca
tsetgniniar T
Training error
Test error
Number of hypotheses K
a b
Figure18.35 a Graphshowingtheperformanceofboosteddecisionstumpswith K5
versusunboosteddecisionstumpsonthe restaurantdata. b Theproportioncorrectonthe
trainingset and the test set as a functionof K the numberof hypothesesin the ensemble.
Noticethatthetestsetaccuracyimprovesslightlyevenafterthetrainingaccuracyreaches1
i.e.aftertheensemblefitsthedataexactly.
complex than necessary but the graph tells us that the predictions improve as the ensemble
hypothesis getsmorecomplex! Various explanations have been proposed forthis. Oneview
is that boosting approximates Bayesian learning see Chapter 20 which can be shown to
be an optimal learning algorithm and the approximation improves as more hypotheses are
added. Another possible explanation is that the addition of further hypotheses enables the
ensembletobemoredefiniteinitsdistinctionbetweenpositiveandnegativeexampleswhich
helpsitwhenitcomestoclassifying newexamples.
Sofareverything wehavedoneinthischapterhasreliedontheassumption thatthedataare
i.i.d. independentandidenticallydistributed. Ontheonehandthatisasensibleassumption:
ifthefuturebearsnoresemblancetothepastthenhowcanwepredictanything? Ontheother
handitistoostronganassumption: itisrarethatourinputshavecapturedalltheinformation
thatwouldmakethefuturetrulyindependent ofthepast.
Inthissectionweexaminewhattodowhenthedataarenoti.i.d.;whentheycanchange
overtime. Inthiscaseitmatterswhenwemakeapredictionsowewilladopttheperspective
calledonlinelearning: anagentreceivesaninputx fromnaturepredictsthecorresponding
O NL IN EL EA RN IN G j
y and then is told the correct answer. Then the process repeats with x and so on. One
j j1
might think this task is hopelessif nature is adversarial all the predictions may be wrong.
Itturnsoutthattherearesomeguarantees wecanmake.
Let us consider the situation where our input consists of predictions from a panel of
experts. Forexample each dayasetof K pundits predicts whetherthestock marketwillgo
upordown and ourtask istopool those predictions and makeourown. Onewaytodo this
is to keep track of how welleach expert performs and choose to believe them in proportion
R AN DO MI ZE D
W EI GH TE D totheirpastperformance. Thisiscalledthe randomizedweightedmajorityalgorithm. We
M AJ OR IT Y
A LG OR IT HM candescribed itmoreformally:
1. Initializeasetofweightsw ...w allto1.
2. Receivethepredictions yˆ ...yˆ fromtheexperts.
""
3. Randomlychooseanexpertk inproportion toitsweight: Pkw k kcid:3w kcid:3.
4. Predictyˆ k.
5. Receivethecorrectanswer y.
6. Foreachexpert k suchthatyˆ cid:7 yupdatew βw
k k k
Hereβ isanumber 0 β 1thattellshowmuchtopenalizeanexpertforeachmistake.
We measure the success of this algorithm in terms of regret which is defined as the
R EG RE T
number of additional mistakes we make compared to the expert who in hindsight had the
""
bestprediction record. Let M bethenumberofmistakesmadebythebestexpert. Thenthe
numberofmistakes Mmadebytherandom weightedmajorityalgorithm isbounded by15
""
M ln1βln K
M .
1β
Section18.11. Practical Machine Learning 753
This bound holds for any sequence of examples even ones chosen by adversaries trying to
do their worst. To be specific when there are K10 experts if we choose β12 then
""
our number of mistakes is bounded by 1.39 M 4.6 and if β34 by 1.15 M 9.2. In
generalifβiscloseto1thenweareresponsivetochangeoverthelongrun;ifthebestexpert
changes wewillpick up onitbefore too long. However wepay apenalty atthe beginning
when we start with all experts trusted equally; we may accept the advice of the bad experts
fortoolong. Whenβ iscloserto0thesetwofactorsarereversed. Notethatwecanchooseβ
""
N O-R EG RE T togetasymptotically closeto M inthelongrun; thisiscalled no-regret learningbecause
L EA RN IN G
theaverageamountofregretpertrialtendsto0asthenumber oftrialsincreases.
Online learning is helpful when the data may be changing rapidly over time. It is also
useful forapplications thatinvolve alargecollection ofdatathatisconstantly growing even
ifchangesaregradual. Forexamplewithadatabaseofmillionsof Webimagesyouwouldnt
wanttotrainsayalinearregressionmodelonallthedata andthenretrainfromscratchevery
timeanewimageisadded. Itwouldbemorepracticaltohaveanonlinealgorithmthatallows
images to be added incrementally. For most learning algorithms based on minimizing loss
thereisanonlineversionbasedonminimizingregret. Itisa bonusthatmanyoftheseonline
algorithms comewithguaranteed bounds onregret.
Tosomeobservers itissurprising thattherearesuchtight bounds onhowwellwecan
do compared to a panel of experts. Toothers the really surprising thing is that when panels
of human experts congregatepredicting stock market prices sports outcomes or political
conteststhe viewing public is so willing to listen to them pontificate and so unwilling to
quantify theirerrorrates.
Wehaveintroducedawiderangeofmachinelearningtechniqueseachillustratedwithsimple
learningtasks. Inthissectionweconsidertwoaspectsofpracticalmachinelearning. Thefirst
involvesfindingalgorithmscapableoflearningtorecognizehandwrittendigitsandsqueezing
every last drop of predictive performance out of them. The second involves anything but
pointingoutthatobtaining cleaningandrepresentingthedatacanbeatleastasimportantas
algorithm engineering.
Recognizing handwritten digits is an important problem with many applications including
automated sorting of mail by postal code automated reading of checks and tax returns and
dataentryforhand-held computers. Itisanareawhererapid progresshasbeenmadeinpart
because ofbetterlearning algorithms andinpartbecauseof theavailability ofbettertraining
sets. The United States National Institute of Science and Technology N IS Thas archived a
database of 60000 labeled digits each 2020400 pixels with 8-bit grayscale values. It
hasbecomeoneofthestandardbenchmarkproblemsforcomparingnewlearningalgorithms.
Someexampledigitsareshownin Figure18.36.
Figure18.36 Examplesfromthe N IS Tdatabaseofhandwrittendigits.Toprow:examples
ofdigits09thatareeasytoidentify.Bottomrow:moredifficultexamplesofthesamedigits.
Manydifferent learning approaches have been tried. Oneofthe first and probably the
simplest is the 3-nearest-neighbor classifier which also has the advantage of requiring no
training time. As a memory-based algorithm however it must store all 60000 images and
itsruntimeperformance isslow. Itachievedatesterrorrateof2.4.
A single-hidden-layer neural network was designed for this problem with 400 input
unitsoneperpixeland10outputunitsoneperclass. Usingcross-validation itwasfound
thatroughly300hiddenunitsgavethebestperformance. Withfullinterconnections between
layers therewereatotalof123300weights. Thisnetworkachieveda1.6errorrate.
Aseries of specialized neuralnetworks called Le Netwere devised totake advantage
ofthestructure oftheproblemthat theinput consists ofpixelsinatwodimensional array
and that small changes in the position or slant of an image are unimportant. Each network
hadaninputlayerof3232unitsontowhichthe2020pixelswerecenteredsothateach
inputunitispresentedwithalocalneighborhood ofpixels. Thiswasfollowedbythreelayers
of hidden units. Each layer consisted of several planes of nn arrays where n is smaller
thanthepreviouslayersothatthenetworkisdown-samplingtheinputandwheretheweights
ofeveryunitinaplaneareconstrained tobeidentical sothattheplaneisacting asafeature
detector: itcanpickoutafeaturesuchasalongverticallineorashortsemi-circulararc. The
output layerhad10units. Manyversions ofthisarchitecture weretried; arepresentative one
had hidden layers with768 192 and 30units respectively. Thetraining setwasaugmented
byapplyingaffinetransformations totheactualinputs: shiftingslightlyrotatingandscaling
theimages. Ofcourse thetransformations havetobesmall orelse a6willbetransformed
intoa9! Thebesterrorrateachievedby Le Netwas0.9.
A boosted neural network combined three copies of the Le Net architecture with the
second one trained on a mix of patterns that the first one got 50 wrong and the third one
trainedonpatternsforwhichthefirsttwodisagreed. During testingthethreenetsvotedwith
themajorityruling. Thetesterrorratewas0.7.
Asupportvectormachinesee Section18.9with25000supportvectorsachievedan
error rate of 1.1. This is remarkable because the S VM technique like the simple nearest-
neighbor approach required almostnothought oriterated experimentation onthepartofthe
developer yetitstillcameclosetotheperformance of Le Netwhichhadhadyearsofdevel-
opment. Indeed the support vector machine makes no use of the structure of the problem
andwouldperform justaswellifthepixelswerepresented in apermutedorder.
Section18.11. Practical Machine Learning 755
V IR TU AL SU PP OR T A virtual support vector machine starts with a regular S VM and then improves it
V EC TO RM AC HI NE
withatechniquethatisdesignedtotakeadvantageofthestructureoftheproblem. Insteadof
allowingproducts ofallpixelpairsthisapproach concentrates onkernels formedfrompairs
ofnearby pixels. Italsoaugments thetraining setwithtransformations ofthe examples just
as Le Netdid. Avirtual S VMachievedthebesterrorraterecorded todate0.56.
Shapematchingisatechniquefromcomputervisionusedtoaligncorrespondingparts
of two different images of objects Belongie et al. 2002. The idea is to pick out a set
of points in each of the two images and then compute for each point in the first image
which point in the second image itcorresponds to. From this alignment wethen compute a
transformation between the images. The transformation gives us a measure of the distance
betweentheimages. Thisdistancemeasureisbettermotivatedthanjustcountingthenumber
of differing pixels and it turns out that a 3nearest neighbor algorithm using this distance
measure performs very well. Training on only 20000 of the 60000 digits and using 100
sample points per image extracted from a Canny edge detector a shape matching classifier
achieved0.63testerror.
Humansareestimatedtohaveanerrorrateofabout0.2onthisproblem. Thisfigure
is somewhat suspect because humans have not been tested as extensively as have machine
learning algorithms. On a similar data set of digits from the United States Postal Service
humanerrorswereat2.5.
The following figure summarizes the error rates run time performance memory re-
quirements and amount oftraining timeforthe seven algorithms wehave discussed. Italso
adds another measure the percentage of digits that must be rejected to achieve 0.5 error.
For example if the S VM is allowed to reject 1.8 of the inputsthat is pass them on for
someone else to make the final judgmentthen its error rate on the remaining 98.2 of the
inputsisreduced from1.1to0.5.
The following table summarizes the error rate and some of the other characteristics of
theseventechniques wehavediscussed.
N N Hidden Le Net Le Net S VM S VM Match
Errorratepct. 2.4 1.6 0.9 0.7 1.1 0.56 0.63
Runtimemillisecdigit 1000 10 30 50 2000 200
Memoryrequirements Mbyte 12 .49 .012 .21 11
Trainingtimedays 0 7 14 30 10
rejectedtoreach0.5error 8.1 3.2 1.8 0.5 1.8
In a textbook weneed to deal with simple toy data to get the ideas across: a small data set
usually in two dimensions. But in practical applications of machine learning the data set
is usually large multidimensional and messy. The data are not handed to the analyst in a
prepackaged setofxyvalues;rathertheanalystneedstogooutandacquiretherightdata.
There is a task to be accomplished and most of the engineering problem is deciding what
data are necessary to accomplish the task; a smaller part is choosing and implementing an
19
K NO WL ED GE I N
L EA RN IN G
Inwhichweexaminetheproblem oflearning whenyouknowsomething already.
Inalloftheapproaches tolearning described intheprevious chapter theideaistoconstruct
afunction thathastheinputoutput behaviorobserved inthedata. Ineachcasethelearning
methodscanbeunderstoodassearchingahypothesisspacetofindasuitablefunctionstarting
from only a very basic assumption about the form of the function such as second-degree
polynomial ordecision tree and perhaps a preference for simpler hypotheses. Doing this
amounts to saying that before you can learn something new you must first forget almost
everything you know. In this chapter we study learning methods that can take advantage
of prior knowledge about the world. In most cases the prior knowledge is represented
P RI OR KN OW LE DG E
as general first-order logical theories; thus for the first time we bring together the work on
knowledgerepresentation andlearning.
withtheobservedexamples. Herewespecializethisdefinitiontothecasewherethehypoth-
esisisrepresentedbyasetoflogicalsentences. Exampledescriptionsandclassificationswill
also be logical sentences and a new example can be classified by inferring a classification
sentence from the hypothesis and the example description. This approach allows for incre-
mentalconstructionofhypothesesonesentenceatatime. Italsoallowsforpriorknowledge
because sentences that are already known can assist in the classification of new examples.
Thelogical formulation oflearning mayseemlikealotofextraworkatfirstbutitturns out
toclarify manyoftheissues inlearning. Itenables ustogowellbeyond the simplelearning
methodsof Chapter18byusingthefullpoweroflogicalinferenceintheserviceoflearning.
Recallfrom Chapter18therestaurant learningproblem: learningarulefordecidingwhether
towaitforatable. Examplesweredescribedbyattributessuchas Alternate Bar Fri Sat
768
Section19.1. A Logical Formulationof Learning 769
and so on. In a logical setting an example is described by a logical sentence; the attributes
become unary predicates. Let us generically call the ith example X . For instance the first
i
examplefrom Figure18.3page700isdescribed bythesentences
Alternate X Bar X Fri Sat X Hungry X ...
Wewillusethenotation D X torefertothedescriptionof X where D canbeanylogical
i i i i
expression taking a single argument. The classification of the example is given by a literal
usingthegoalpredicate inthiscase
Will Wait X or Will Wait X .
Thecompletetrainingsetcanthusbeexpressedastheconjunctionofalltheexampledescrip-
tionsandgoalliterals.
The aim of inductive learning in general is to find a hypothesis that classifies the ex-
amples well and generalizes well to new examples. Herewe are concerned withhypotheses
expressed inlogic;eachhypothesis h willhavetheform
j
x Goalx C x
j
where C x is a candidate definitionsome expression involving the attribute predicates.
j
Forexampleadecisiontreecanbeinterpretedasalogicalexpressionofthisform. Thusthe
tree in Figure 18.6 page 702 expresses the following logical definition which we will call
h forfuturereference:
r
r Will Waitr Patronsr Some
Patronsr Full Hungryr Typer French
Patronsr Full Hungryr Typer Thai 19.1
Fri Satr
Patronsr Full Hungryr Typer Burger.
Eachhypothesis predicts thatacertain setofexamplesnamely thosethat satisfy itscandi-
date definitionwill be examples of the goal predicate. This set is called the extension of
E XT EN SI ON
the predicate. Two hypotheses with different extensions are therefore logically inconsistent
with each other because they disagree on their predictions for at least one example. If they
havethesameextension theyarelogically equivalent.
Thehypothesisspace Histhesetofallhypothesesh ...h thatthelearningalgo-
rithmisdesigned toentertain. Forexample the D EC IS IO N-T RE E-L EA RN IN G algorithm can
entertain anydecision treehypothesis defined intermsoftheattributes provided; itshypoth-
esis space therefore consists of all these decision trees. Presumably the learning algorithm
believesthatoneofthehypotheses iscorrect;thatisitbelievesthesentence
h h h ...h . 19.2
As the examples arrive hypotheses that are not consistent with the examples can be ruled
out. Letusexaminethisnotion ofconsistency morecarefully. Obviously ifhypothesis h is
j
consistentwiththeentiretrainingsetithastobeconsistentwitheachexampleinthetraining
set. What would it mean for it to be inconsistent with an example? There are two possible
waysthatthiscanhappen:
Anexample canbeafalsenegative forthehypothesis ifthehypothesis saysitshould
F AL SE NE GA TI VE
benegativebutinfactitispositive. Forinstance thenewexample X described by
13
Patrons X Full Hungry X ...Will Wait X
wouldbeafalsenegativeforthehypothesis h givenearlier. From h andtheexample
r r
description we can deduce both Will Wait X which is what the example says
13
and Will Wait X which is what the hypothesis predicts. The hypothesis and the
13
examplearethereforelogically inconsistent.
Anexample can be a false positive forthe hypothesis if the hypothesis says it should
F AL SE PO SI TI VE
bepositivebutinfactitisnegative.1
If an example is afalse positive or false negative fora hypothesis then the example and the
hypothesis arelogically inconsistent witheachother. Assumingthattheexampleisacorrect
observation offactthenthehypothesis canberuledout. Logically thisisexactlyanalogous
to the resolution rule of inference see Chapter 9 where the disjunction of hypotheses cor-
responds toaclause andtheexample corresponds toaliteral thatresolves against oneofthe
literalsintheclause. Anordinarylogicalinferencesystemthereforecouldinprinciplelearn
from the example by eliminating one or more hypotheses. Suppose for example that the
exampleisdenotedbythesentence I andthehypothesisspaceish h h h . Thenif
I isinconsistentwithh andh thelogicalinferencesystemcandeducethenewhypothesis
spaceh h .
We therefore can characterize inductive learning in a logical setting as a process of
gradually eliminating hypotheses that are inconsistent with the examples narrowing down
the possibilities. Because the hypothesis space is usually vast oreveninfinite inthe case of
first-order logic we do not recommend trying to build a learning system using resolution-
basedtheoremprovingandacompleteenumerationofthehypothesisspace. Insteadwewill
describe twoapproaches thatfindlogically consistent hypotheses withmuchlesseffort.
C UR RE NT-B ES T- The idea behind current-best-hypothesis search is to maintain a single hypothesis and to
H YP OT HE SI S
adjust it as new examples arrive in order to maintain consistency. The basic algorithm was
described by John Stuart Mill1843 andmaywellhaveappeared evenearlier.
Suppose we have some hypothesis such as h of which we have grown quite fond.
r
As long as each new example is consistent we need do nothing. Then along comes a false
negativeexample X . Whatdowedo? Figure 19.1a shows h schematically asaregion:
everythinginsidetherectangleispartoftheextensionofh . Theexamplesthathaveactually
r
been seen so far are shown as or and we see that h correctly categorizes all the
r
examples as positive or negative examples of Will Wait. In Figure 19.1b a new example
circledisafalsenegative: thehypothesissaysitshouldbenegativebutitisactuallypositive.
Theextensionofthehypothesismustbeincreasedtoincludeit. Thisiscalledgeneralization;
G EN ER AL IZ AT IO N
onepossiblegeneralization isshownin Figure19.1c. Thenin Figure19.1dweseeafalse
positive: the hypothesis says the new example circled should be positive but it actually is
tests.Aresultisafalsepositiveifitindicatesthatthepatienthasthediseasewheninfactnodiseaseispresent.
Section19.1. A Logical Formulationof Learning 771
""
""
""
""
""
""
""
""
""
""
""
""
a b c d e
Figure19.1 a Aconsistenthypothesis. b Afalsenegative. c Thehypothesisisgen-
eralized.d Afalsepositive.e Thehypothesisisspecialized.
function C UR RE NT-B ES T-L EA RN IN Gexampleshreturnsahypothesisorfail
ifexamples isemptythen
returnh
e F IR STexamples
ife isconsistentwithh then
return C UR RE NT-B ES T-L EA RN IN GR ES Texamplesh
elseife isafalsepositiveforh then
foreachhcid:5 in specializationsofh consistentwithexamples seensofardo
hcid:5cid:5 CU RR EN T-B ES T-L EA RN IN GR ES Texampleshcid:5
ifhcid:5cid:5 cid:7 fail thenreturnhcid:5cid:5
elseife isafalsenegativeforh then
foreachhcid:5 ingeneralizationsofh consistentwithexamples seensofardo
hcid:5cid:5 CU RR EN T-B ES T-L EA RN IN GR ES Texampleshcid:5
ifhcid:5cid:5 cid:7 fail thenreturnhcid:5cid:5
returnfail
tent hypothesis that fits all the examples and backtracks when no consistent specializa-
tiongeneralizationcan be found. To start the algorithm any hypothesis can be passed in;
itwillbespecializedorgneralizedasneeded.
negative. Theextension ofthehypothesis mustbedecreased toexclude theexample. Thisis
calledspecialization; in Figure19.1eweseeonepossible specialization ofthe hypothesis.
S PE CI AL IZ AT IO N
Themoregeneral than andmore specific than relations between hypotheses provide the
logicalstructure onthehypothesis spacethatmakesefficientsearchpossible.
Wecannowspecifythe C UR RE NT-B ES T-L EA RN IN G algorithmshownin Figure19.2.
Noticethateachtimeweconsidergeneralizingorspecializingthehypothesiswemustcheck
forconsistency withtheotherexamples because anarbitrary increasedecrease intheexten-
sionmightincludeexclude previously seennegativepositive examples.
Wehave defined generalization and specialization as operations that change the exten-
sion of a hypothesis. Now we need to determine exactly how they can be implemented as
syntactic operations that change the candidate definition associated with the hypothesis so
thataprogramcancarrythemout. Thisisdonebyfirstnotingthatgeneralizationandspecial-
ization are also logical relationships between hypotheses. If hypothesis h with definition
1
C isageneralization ofhypothesis h withdefinition C thenwemusthave
x C x C x.
Therefore in order to construct a generalization of h we simply need to find a defini-
2
tion C that is logically implied by C . This is easily done. For example if C x is
Alternatex Patronsx Some then one possible generalization is given by C x
1
D RO PP IN G Patronsx Some. This is called dropping conditions. Intuitively it generates a weaker
C ON DI TI ON S
definitionandthereforeallowsalargersetofpositiveexamples. Thereareanumberofother
generalization operations depending on the language being operated on. Similarly we can
specialize ahypothesis byadding extra conditions toitscandidate definition orbyremoving
disjuncts from adisjunctive definition. Letusseehowthis worksontherestaurant example
usingthedatain Figure18.3.
Thefirstexample X ispositive. Theattribute Alternate X istruesolettheinitial
hypothesis be
h : x Will Waitx Alternatex.
1
Thesecondexample X isnegative. h predictsittobepositivesoitisafalsepositive.
Thereforeweneedtospecialize h . Thiscanbedonebyaddinganextraconditionthat
1
willruleout X whilecontinuing toclassify X aspositive. Onepossibility is
h : x Will Waitx Alternatex Patronsx Some.
2
Thethirdexample X ispositive. h predictsittobenegativesoitisafalsenegative.
Thereforeweneedtogeneralize h . Wedropthe Alternate condition yielding
2
h : x Will Waitx Patronsx Some.
3
Thefourthexample X ispositive. h predictsittobenegativesoitisafalsenegative.
We therefore need to generalize h . We cannot drop the Patrons condition because
3
that would yield an all-inclusive hypothesis that would be inconsistent with X . One
2
possibility istoaddadisjunct:
h : x Will Waitx Patronsx Some
4
Patronsx Full Fri Satx.
Alreadythehypothesis isstartingtolookreasonable. Obviously thereareotherpossibilities
consistent withthefirstfourexamples;herearetwoofthem:
hcid:2 : x Will Waitx Wait Estimatex30-60.
4
hcid:2cid:2 : x Will Waitx Patronsx Some
4
Patronsx Full Wait Estimatex10-30.
The C UR RE NT-B ES T-L EA RN IN G algorithmisdescribednondeterministically becauseatany
pointtheremaybeseveralpossiblespecializationsorgeneralizationsthatcanbeapplied. The
Section19.1. A Logical Formulationof Learning 773
function V ER SI ON-S PA CE-L EA RN IN Gexamplesreturnsaversionspace
localvariables: Vtheversionspace:thesetofallhypotheses
V thesetofallhypotheses
foreachexamplee inexamples do
if V isnotemptythen V V ER SI ON-S PA CE-U PD AT EVe
return V
function V ER SI ON-S PA CE-U PD AT EVereturnsanupdatedversionspace
V h V : hisconsistentwithe
Figure19.3 Theversionspacelearningalgorithm.Itfindsasubsetof V thatisconsistent
withalltheexamples.
choicesthataremadewillnotnecessarily leadtothesimplesthypothesis andmayleadtoan
unrecoverable situation wherenosimplemodificationofthe hypothesis isconsistent withall
ofthedata. Insuchcases theprogrammustbacktrack toaprevious choicepoint.
The C UR RE NT-B ES T-L EA RN IN G algorithm and its variants have been used in many
machine learning systems starting with Patrick Winstons 1970 arch-learning program.
Withalargenumberofexamplesandalargespace howeversomedifficulties arise:
1. Checkingallthepreviousexamplesoveragainforeachmodificationisveryexpensive.
2. Thesearchprocessmayinvolveagreatdealofbacktracking. Aswesawin Chapter18
hypothesis spacecanbeadoublyexponentially largeplace.
Backtracking arises because the current-best-hypothesis approach has to choose a particular
hypothesis as its best guess even though it does not have enough data yet to be sure of the
choice. What we can do instead is to keep around all and only those hypotheses that are
consistent with all the data so far. Each new example will either have no effect or will get
rid of some of the hypotheses. Recall that the original hypothesis space can be viewed as a
disjunctive sentence
h h h ...h .
Asvarioushypothesesarefoundtobeinconsistentwiththeexamplesthisdisjunctionshrinks
retaining only those hypotheses not ruled out. Assuming that the original hypothesis space
does in fact contain the right answer the reduced disjunction must still contain the right an-
swerbecauseonlyincorrecthypotheseshavebeenremoved. Thesetofhypothesesremaining
iscalledtheversionspaceandthelearningalgorithm sketched in Figure19.3iscalledthe
V ER SI ON SP AC E
C AN DI DA TE versionspacelearningalgorithm alsothe candidateelimination algorithm.
E LI MI NA TI ON
One important property of this approach is that it is incremental: one never has to
go back and reexamine the old examples. All remaining hypotheses are guaranteed to be
consistent with them already. But there is an obvious problem. We already said that the
This region all inconsistent
G 1 G 2 G 3 . . . G m
More general
More specific
S 1 S 2 . . . S n
This region all inconsistent
Figure19.4 Theversionspacecontainsallhypothesesconsistentwiththeexamples.
hypothesisspaceisenormoussohowcanwepossiblywritedownthisenormousdisjunction?
The following simple analogy is very helpful. How do you represent all the real num-
bers between 1 and 2? After all there are an infinite number of them! The answer is to use
anintervalrepresentation thatjustspecifiestheboundaries oftheset: 12. Itworksbecause
wehaveanordering ontherealnumbers.
Wealsohaveanorderingonthehypothesisspacenamelygeneralizationspecialization.
This is a partial ordering which means that each boundary will not be a point but rather a
set of hypotheses called a boundary set. The great thing is that we can represent the entire
B OU ND AR YS ET
version space using just twoboundary sets: amostgeneral boundary the G-set andamost
G-S ET
specific boundary the S-set. Everything in between isguaranteed to be consistent with the
S-S ET
examples. Beforeweprovethisletusrecap:
The current version space is the set of hypotheses consistent with all the examples so
far. Itisrepresented bythe S-setand G-seteachofwhichis asetofhypotheses.
Every member of the S-set is consistent with all observations so far and there are no
consistent hypotheses thataremorespecific.
Every member of the G-set is consistent with all observations so far and there are no
consistent hypotheses thataremoregeneral.
Wewanttheinitialversionspacebeforeanyexampleshavebeenseentorepresentallpossi-
blehypotheses. Wedothisbysettingthe G-settocontain True thehypothesis thatcontains
everything andthe S-settocontain False thehypothesis whoseextension isempty.
Figure19.4showsthegeneralstructureoftheboundary-set representationoftheversion
space. Toshowthattherepresentation issufficientweneed thefollowingtwoproperties:
Section19.1. A Logical Formulationof Learning 775
1. Everyconsistenthypothesisotherthanthoseintheboundarysetsismorespecificthan
somememberofthe G-setandmoregeneral thansomememberof the S-set. Thatis
there are no stragglers left outside. This follows directly from the definitions of S
and G. If there were a straggler h then it would have to be no more specific than any
member of G in which case it belongs in G; or no more general than any member of
Sinwhichcaseitbelongsin S.
2. Everyhypothesis morespecific than somememberofthe G-setand moregeneral than
somememberofthe S-setisaconsistent hypothesis. Thatis there arenoholes be-
tween the boundaries. Any h between S and G must reject all the negative examples
rejectedbyeachmemberof Gbecauseitismorespecificandmustacceptallthepos-
itiveexamplesacceptedbyanymemberof S becauseitismoregeneral. Thushmust
agree with all the examples and therefore cannot be inconsistent. Figure 19.5 shows
the situation: there are no known examples outside S but inside G so any hypothesis
inthegapmustbeconsistent.
We have therefore shown that if S and G are maintained according to their definitions then
they provide a satisfactory representation ofthe version space. Theonly remaining problem
is how to update S and G for a new example the job of the V ER SI ON-S PA CE-U PD AT E
function. This may appear rather complicated at first but from the definitions and with the
helpof Figure19.4itisnottoohardtoreconstruct thealgorithm.
""
G
1
""
""
S G 2
1
""
""
""
""
""
""
betweenthetwosetsofboundaries.
Weneedtoworryaboutthemembers S and G ofthe S-and G-sets. Foreachonethe
i i
newexamplemaybeafalsepositiveorafalsenegative.
1. Falsepositive for S : Thismeans S istoo general but there are no consistent special-
i i
izationsof S bydefinition sowethrowitoutofthe S-set.
i
2. Falsenegativefor S : Thismeans S istoospecificsowereplaceitbyallitsimmediate
i i
generalizations provided theyaremorespecificthansomememberof G.
3. Falsepositivefor G : Thismeans G istoogeneralsowereplaceitbyallitsimmediate
i i
specializations provided theyaremoregeneralthansomememberof S.
4. Falsenegative for G : Thismeans G istoospecific butthere areno consistent gener-
i i
alizations of G bydefinition sowethrowitoutofthe G-set.
i
Wecontinuetheseoperations foreachnewexampleuntiloneofthreethingshappens:
1. Wehave exactly one hypothesis left inthe version space in which case wereturn itas
theunique hypothesis.
2. The version space collapseseither S or G becomes empty indicating that there are
noconsistent hypotheses forthetraining set. Thisisthesamecaseasthefailure ofthe
simpleversionofthedecision treealgorithm.
3. We run out of examples and have several hypotheses remaining in the version space.
This means the version space represents a disjunction of hypotheses. For any new
exampleifallthedisjunctsagreethenwecanreturntheirclassificationoftheexample.
Iftheydisagree onepossibility istotakethemajorityvote.
Weleaveasanexercisetheapplication ofthe V ER SI ON-S PA CE-L EA RN IN G algorithm tothe
restaurant data.
Therearetwoprincipal drawbackstotheversion-space approach:
Ifthedomaincontainsnoiseorinsufficientattributesforexactclassificationtheversion
spacewillalwayscollapse.
Ifweallowunlimiteddisjunction inthehypothesis spacethe S-setwillalwayscontain
a single most-specific hypothesis namely the disjunction of the descriptions of the
positiveexamplesseentodate. Similarlythe G-setwillcontainjustthenegationofthe
disjunction ofthedescriptions ofthenegativeexamples.
For some hypothesis spaces the number of elements in the S-set or G-set may grow
exponentiallyinthenumberofattributeseventhoughefficientlearningalgorithmsexist
forthosehypothesis spaces.
To date no completely successful solution has been found for the problem of noise. The
problem ofdisjunction canbeaddressed byallowing onlylimitedformsofdisjunction orby
G EN ER AL IZ AT IO N including a generalization hierarchy of more general predicates. For example instead of
H IE RA RC HY
using thedisjunction Wait Estimatex30-60 Wait Estimatex60 wemight use the
single literal Long Waitx. The set of generalization and specialization operations can be
easilyextendedtohandlethis.
The pure version space algorithm was first applied in the Meta-D EN DR AL system
which was designed to learn rules for predicting how molecules would break into pieces in
a mass spectrometer Buchanan and Mitchell 1978. Meta-D EN DR AL was able to generate
rulesthatweresufficientlynoveltowarrantpublicationinajournalofanalyticalchemistry
the first real scientific knowledge generated by a computer program. It was also used in the
elegant L EXsystem Mitchelletal.1983whichwasabletolearntosolvesymbolicintegra-
tion problems by studying its own successes and failures. Although version space methods
are probably not practical in most real-world learning problems mainly because of noise
theyprovideagooddealofinsight intothelogicalstructure ofhypothesis space.
Section19.2. Knowledgein Learning 777
Prior
knowledge
Knowledge-based
Observations Hypotheses Predictions
inductive learning
knowledgeovertime.
Theprecedingsectiondescribedthesimplestsettingforinductivelearning. Tounderstandthe
role of prior knowledge we need to talk about the logical relationships among hypotheses
exampledescriptions andclassifications. Let Descriptions denotetheconjunction ofallthe
exampledescriptions inthetrainingsetandlet Classifications denotetheconjunction ofall
theexampleclassifications. Thena Hypothesis thatexplains theobservations mustsatisfy
thefollowingproperty recallthat meanslogically entails:
Hypothesis Descriptions Classifications . 19.3
E NT AI LM EN T Wecall this kind of relationship an entailment constraint inwhich Hypothesis is theun-
C ON ST RA IN T
known. Pure inductive learning means solving this constraint where Hypothesis is drawn
from some predefined hypothesis space. For example if we consider a decision tree as a
logicalformulasee Equation19.1onpage769thenadecisiontreethatisconsistentwith
all the examples will satisfy Equation 19.3. If weplace no restrictions on the logical form
ofthehypothesis ofcoursethen Hypothesis Classifications alsosatisfiestheconstraint.
Ockhams razor tells us to prefer small consistent hypotheses so we try to do better than
simplymemorizingtheexamples.
Thissimpleknowledge-freepictureofinductivelearningpersisteduntiltheearly1980s.
Themodernapproachistodesignagentsthatalreadyknowsomethingandaretryingtolearn
somemore. Thismaynotsoundlikeaterrificallydeepinsightbutitmakesquiteadifference
to the way we design agents. It might also have some relevance to our theories about how
scienceitselfworks. Thegeneralideaisshownschematically in Figure19.6.
Anautonomous learning agent thatusesbackground knowledge mustsomehow obtain
the background knowledge in the first place in order for it to be used in the new learning
episodes. This method must itself be a learning process. The agents life history will there-
fore be characterized by cumulative or incremental development. Presumably the agent
could start out with nothing performing inductions in vacuo like a good little pure induc-
tion program. But once it has eaten from the Tree of Knowledge it can no longer pursue
such naive speculations and should use its background knowledge to learn more and more
effectively. Thequestion isthenhowtoactuallydothis.
Letusconsidersomecommonsenseexamplesoflearningwithbackgroundknowledge. Many
apparently rational cases of inferential behavior in the face of observations clearly do not
followthesimpleprinciples ofpureinduction.
Sometimes one leaps to general conclusions after only one observation. Gary Larson
once drew a cartoon in which a bespectacled caveman Zog is roasting his lizard on
the end of a pointed stick. He is watched by an amazed crowd of his less intellectual
contemporarieswhohavebeenusingtheirbarehandstoholdtheirvictualsoverthefire.
Thisenlightening experience isenough toconvince thewatchers ofageneral principle
ofpainless cooking.
Orconsiderthecaseofthetravelerto Brazilmeetingherfirst Brazilian. Onhearinghim
speak Portuguese she immediately concludes that Brazilians speak Portuguese yeton
discovering that his name is Fernando she does not conclude that all Brazilians are
called Fernando. Similar examples appear in science. For example when a freshman
physics student measures the density and conductance of a sample of copper at a par-
ticular temperature she is quite confident in generalizing those values to all pieces of
copper. Yetwhenshemeasuresitsmassshedoesnotevenconsiderthehypothesisthat
allpieces ofcopperhave that mass. Ontheotherhand itwouldbequite reasonable to
makesuchageneralization overallpennies.
Finally consider the case of a pharmacologically ignorant but diagnostically sophisti-
cated medical student observing a consulting session between a patient and an expert
internist. After a series of questions and answers the expert tells the patient to take a
course of a particular antibiotic. The medical student infers the general rule that that
particularantibiotic iseffectiveforaparticulartypeof infection.
These are all cases in which the use of background knowledge allows much faster learning
thanonemightexpectfromapureinduction program.
In each of the preceding examples one can appeal to prior knowledge to try to justify the
generalizations chosen. Wewillnowlookatwhatkindsofentailment constraints areoperat-
ingineachcase. Theconstraints willinvolve the Background knowledge inaddition tothe
Hypothesis andtheobserved Descriptions and Classifications.
In the case of lizard toasting the cavemen generalize by explaining the success of the
pointed stick: it supports the lizard while keeping the hand away from the fire. From this
explanationtheycaninferageneralrule: thatanylongrigidsharpobjectcanbeusedtotoast
small soft-bodied edibles. Thiskind ofgeneralization process hasbeencalled explanation-
E XP LA NA TI ON-
based learning or E BL.Notice that the general rule follows logically from the background
B AS ED
L EA RN IN G
knowledgepossessedbythecavemen. Hencetheentailmentconstraintssatisfiedby E BLare
thefollowing:
Hypothesis Descriptions Classifications
Background Hypothesis .
Section19.2. Knowledgein Learning 779
Because E BL uses Equation 19.3 it was initially thought to be a way to learn from ex-
amples. But because it requires that the background knowledge be sufficient to explain the
Hypothesis which in turn explains the observations the agent does not actually learn any-
thing factually new from the example. Theagent could have derived the example from what
it already knew although that might have required an unreasonable amount of computation.
E BL is now viewed as a method for converting first-principles theories into useful special-
purpose knowledge. Wedescribealgorithms for E BLin Section19.3.
The situation of our traveler in Brazil is quite different for she cannot necessarily ex-
plain why Fernando speaks the way he does unless she knows her papal bulls. Moreover
the same generalization would be forthcoming from a traveler entirely ignorant of colonial
history. The relevant prior knowledge in this case is that within any given country most
people tend to speak the same language; on the other hand Fernando is not assumed to be
thenameofall Brazilians because thiskindofregularity doesnotholdfornames. Similarly
thefreshman physics student alsowould behardputtoexplain theparticular values that she
discoversfortheconductance anddensityofcopper. Shedoesknowhoweverthatthemate-
rial of which an object is composed and its temperature together determine its conductance.
Ineachcasethepriorknowledge Background concernstherelevanceofasetoffeaturesto
R EL EV AN CE
thegoal predicate. Thisknowledge together withtheobservations allowstheagent toinfer
anewgeneralrulethatexplains theobservations:
Hypothesis Descriptions Classifications
19.4
Background Descriptions Classifications Hypothesis .
R EL EV AN CE-B AS ED Wecallthiskindofgeneralization relevance-based learningor R BLalthough thenameis
L EA RN IN G
not standard. Notice that whereas R BLdoes make use ofthe content ofthe observations it
doesnotproducehypothesesthatgobeyondthelogicalcontentofthebackgroundknowledge
and the observations. It is a deductive form of learning and cannot by itself account for the
creation ofnewknowledgestarting fromscratch.
In the case of the medical student watching the expert we assume that the students
prior knowledge is sufficient to infer the patients disease D from the symptoms. This is
not however enough to explain the fact that the doctor prescribes aparticular medicine M.
The student needs to propose another rule namely that M generally is effective against D.
Giventhisruleandthestudentspriorknowledgethestudentcannowexplainwhytheexpert
prescribes M in this particular case. We can generalize this example to come up with the
entailment constraint
Background Hypothesis Descriptions Classifications . 19.5
Thatisthebackground knowledge andthenewhypothesis combinetoexplain theexamples.
Aswithpureinductivelearningthelearningalgorithmshouldproposehypothesesthatareas
simple as possible consistent with this constraint. Algorithms that satisfy constraint 19.5
K NO WL ED GE-B AS ED
arecalled knowledge-basedinductivelearningor K BI Lalgorithms.
I ND UC TI VE
L EA RN IN G
K BI L algorithms which are described in detail in Section 19.5 have been studied
I ND UC TI VE LO GI C mainly in the field of inductive logic programming or I LP. In I LP systems prior knowl-
P RO GR AM MI NG
edgeplaystwokeyrolesinreducing thecomplexity oflearning:
1. Becauseanyhypothesis generated mustbeconsistent withthepriorknowledge aswell
as with the new observations the effective hypothesis space size is reduced to include
onlythosetheoriesthatareconsistent withwhatisalready known.
2. For any given set of observations the size of the hypothesis required to construct an
explanation for the observations can be much reduced because the prior knowledge
will be available to help out the new rules in explaining the observations. The smaller
thehypothesis theeasieritistofind.
In addition to allowing the use of prior knowledge in induction I LP systems can formulate
hypotheses in general first-order logic rather than in the restricted attribute-based language
of Chapter18. Thismeans thatthey canlearn inenvironments that cannot be understood by
simplersystems.
Explanation-based learning is a method for extracting general rules from individual obser-
vations. As an example consider the problem of differentiating and simplifying algebraic
expressions Exercise 9.17. If we differentiate an expression such as X2 with respect to
X we obtain 2 X. We use a capital letter for the arithmetic unknown X to distinguish it
from the logical variable x. In a logical reasoning system the goal might be expressed as
A SK Derivative X2 Xd K Bwithsolutiond 2 X.
Anyonewhoknowsdifferentialcalculuscanseethissolutionbyinspectionasaresult
ofpracticeinsolvingsuchproblems. Astudentencounteringsuchproblemsforthefirsttime
or a program with no experience will have a much more difficult job. Application of the
standard rules of differentiation eventually yields the expression 1 2 X21 and
eventually this simplifies to 2 X. In the authors logic programming implementation this
takes 136 proof steps of which 99 are on dead-end branches in the proof. After such an
experience we would like the program to solve the same problem much more quickly the
nexttimeitarises.
The technique of memoization has long been used in computer science to speed up
M EM OI ZA TI ON
programs by saving the results of computation. The basic idea of memo functions is to
accumulate a database of inputoutput pairs; when the function is called it first checks the
database to see whether it can avoid solving the problem from scratch. Explanation-based
learning takes this a good deal further by creating general rules that cover an entire class
of cases. In the case of differentiation memoization would remember that the derivative of
X2 withrespect to X is2 Xbutwouldleavetheagenttocalculate thederivativeof Z2 with
respect to Z from scratch. We would like to be able to extract the general rule that for any
arithmetic unknown u the derivative of u2 with respect to u is 2u. An even more general
rule for un can also be produced but the current example suffices to make the point. In
logicaltermsthisisexpressed bytherule
Arithmetic Unknownu Derivativeu2u2u.
Section19.3. Explanation-Based Learning 781
Ifthe knowledge base contains such arule then anynew case that isaninstance ofthis rule
canbesolvedimmediately.
Thisisofcoursemerelyatrivialexampleofaverygeneral phenomenon. Oncesome-
thing is understood it can be generalized and reused in other circumstances. It becomes an
obvious stepandcanthenbeusedasabuilding blockinsolving problems still morecom-
plex. Alfred North Whitehead 1911 co-author with Bertrand Russell of Principia Mathe-
matica wrote Civilization advances by extending the number of important operations that
wecandowithoutthinking aboutthemperhapshimselfapplying E BLtohisunderstanding
of events such as Zogs discovery. If you have understood the basic idea of the differenti-
ation example then your brain is already busily trying to extract the general principles of
explanation-based learning fromit. Noticethatyouhadnt already invented E BLbefore you
saw the example. Like the cavemen watching Zog you and we needed an example before
we could generate the basic principles. This is because explaining why something is a good
ideaismucheasierthancomingupwiththeideainthefirstplace.
Thebasic idea behind E BLisfirst toconstruct anexplanation ofthe observation using prior
knowledge and then to establish adefinition of the class of cases forwhich thesame expla-
nation structure can be used. This definition provides the basis fora rule covering all of the
casesintheclass. Theexplanation canbealogicalproof butmoregenerally itcanbeany
reasoning orproblem-solving process whose steps arewell defined. The keyis tobe able to
identify thenecessary conditions forthosesamestepstoapplytoanothercase.
We will use for our reasoning system the simple backward-chaining theorem prover
described in Chapter9. Theprooftreefor Derivative X2 X2 X istoolargetouseasan
example so we will use a simpler problem to illustrate the generalization method. Suppose
ourproblem istosimplify 10 X. Theknowledgebaseincludes thefollowingrules:
Rewriteuv Simplifyvw Simplifyuw.
Primitiveu Simplifyuu.
Arithmetic Unknownu Primitiveu.
Numberu Primitiveu.
Rewrite1uu.
Rewrite0uu.
.
.
.
The proof that the answer is X is shown in the top half of Figure 19.7. The E BL method
actually constructs twoprooftreessimultaneously. Thesecondprooftreeusesa variabilized
goal inwhich the constants from the original goal are replaced by variables. Asthe original
proof proceeds the variabilized proof proceeds in step using exactly the same rule applica-
tions. This could cause some of the variables to become instantiated. Forexample in order
tousetherule Rewrite1uuthevariablexinthesubgoal Rewritexyzvmust
cid:2
bebound to1. Similarly y mustbebound to0inthesubgoal Rewriteyzv inorderto
usetherule Rewrite0uu. Oncewehavethe generalized proof tree wetake theleaves
Simplify1 0 Xw
Rewrite10 Xv Simplify0 Xw
Yesv 0 X
Rewrite0 Xv' Simplify Xw
Yesv' X w X
Primitive X
Arithmetic Unknown X
Simplifyx yzw Yes
Rewritex yzv Simplifyyzw
Yesx 1 v yz
Rewriteyzv' Simplifyzw
Yesy 0 v' z w z
Primitivez
Arithmetic Unknownz
Yes
Figure19.7 Prooftreesforthesimplificationproblem. Thefirsttreeshowstheprooffor
theoriginalprobleminstancefromwhichwecanderive
Arithmetic Unknownz Simplify10zz.
Thesecondtreeshowstheproofforaprobleminstancewithallconstantsreplacedbyvari-
ablesfromwhichwecanderiveavarietyofotherrules.
withthenecessary bindings andformageneral ruleforthe goalpredicate:
Rewrite10z0z Rewrite0zz Arithmetic Unknownz
Simplify10zz.
Noticethatthefirsttwoconditions ontheleft-hand sideare true regardless ofthevalue ofz.
Wecanthereforedropthemfromtheruleyielding
Arithmetic Unknownz Simplify10zz.
Ingeneral conditions canbedropped fromthefinalruleiftheyimposenoconstraints onthe
variables on the right-hand side of the rule because the resulting rule will still be true and
will be more efficient. Notice that we cannot drop the condition Arithmetic Unknownz
because not all possible values of z are arithmetic unknowns. Values other than arithmetic
unknownsmightrequiredifferent formsofsimplification: forexampleif z were23then
thecorrectsimplification of1023wouldbe6andnot23.
Torecapthebasic E BLprocessworksasfollows:
1. Givenanexampleconstructaproofthatthegoalpredicateappliestotheexampleusing
theavailable background knowledge.
Section19.3. Explanation-Based Learning 783
2. In parallel construct a generalized proof tree for the variabilized goal using the same
inferencestepsasintheoriginalproof.
3. Construct a new rule whose left-hand side consists of the leaves of the proof tree and
whose right-hand side is the variabilized goal after applying the necessary bindings
fromthegeneralized proof.
4. Dropanyconditions fromtheleft-hand sidethataretrueregardlessofthevaluesofthe
variablesinthegoal.
Thegeneralized prooftreein Figure19.7actuallyyieldsmorethanonegeneralized rule. For
example if we terminate or prune the growth of the right-hand branch in the proof tree
whenitreachesthe Primitive stepwegettherule
Primitivez Simplify10zz.
Thisruleisasvalidas but moregeneral than therule using Arithmetic Unknownbecause
itcoverscaseswhere z isanumber. Wecanextractastillmoregeneralrulebypruningafter
thestep Simplifyyzwyielding therule
Simplifyyzw Simplify1yzw.
Ingeneralarulecanbeextractedfromanypartialsubtreeofthegeneralizedprooftree. Now
wehaveaproblem: whichoftheserulesdowechoose?
The choice of which rule to generate comes down to the question of efficiency. There
arethreefactorsinvolved intheanalysisofefficiencygainsfrom E BL:
1. Adding large numbers of rules can slow down the reasoning process because the in-
ferencemechanism muststillcheckthoserulesevenincases wheretheydonotyielda
solution. Inotherwordsitincreases the branchingfactorinthesearchspace.
2. Tocompensate for the slowdown in reasoning the derived rules must offer significant
increases inspeedforthecasesthattheydocover. Theseincreases comeaboutmainly
because the derived rules avoid dead ends that would otherwise be taken but also be-
causetheyshortentheproofitself.
3. Derivedrulesshouldbeasgeneralaspossible sothattheyapplytothelargestpossible
setofcases.
Acommonapproachtoensuringthatderivedrulesareefficientistoinsistontheoperational-
ityofeachsubgoal intherule. Asubgoal isoperational ifitiseasytosolve. Forexample
O PE RA TI ON AL IT Y
the subgoal Primitivez is easy to solve requiring at most two steps whereas the subgoal
Simplifyy zw could lead to an arbitrary amount of inference depending on the values
of y and z. If a test for operationality is carried out at each step in the construction of the
generalizedproofthenwecanprunetherestofabranchassoonasanoperationalsubgoalis
found keepingjusttheoperational subgoalasaconjunct ofthenewrule.
Unfortunately there is usually a tradeoff between operationality and generality. More
specific subgoals are generally easier to solve but cover fewer cases. Also operationality
is a matter of degree: one or two steps is definitely operational but what about 10 or 100?
Finally the cost of solving a given subgoal depends on what other rules are available in the
knowledge base. It can go up or down as more rules are added. Thus E BL systems really
face a very complex optimization problem in trying to maximize the efficiency of a given
initialknowledge base. Itissometimespossible toderiveamathematicalmodeloftheeffect
on overall efficiency of adding a given rule and to use this model to select the best rule to
add. The analysis can become very complicated however especially when recursive rules
are involved. One promising approach is to address the problem of efficiency empirically
simplybyaddingseveralrulesandseeingwhichonesareusefulandactuallyspeedthingsup.
Empirical analysis of efficiency is actually at the heart of E BL. What we have been
calling loosely the efficiency of agiven knowledge base isactually the average-case com-
plexity on a distribution of problems. By generalizing from past example problems E BL
makes the knowledge base more efficient for the kind of problems that it is reasonable to
expect. This works as long as the distribution of past examples is roughly the same as for
future examplesthe same assumption used for P AC-learning in Section 18.5. If the E BL
system is carefully engineered it is possible to obtain significant speedups. For example a
very large Prolog-based natural language system designed for speech-to-speech translation
between Swedish and English was able to achieve real-time performance only by the appli-
cationof E BLtotheparsingprocess Samuelssonand Rayner 1991.
Ourtravelerin Brazilseemstobeabletomakeaconfidentgeneralizationconcerningthelan-
guagespokenbyother Brazilians. Theinferenceissanctionedbyherbackgroundknowledge
namely that people in a given country usually speak the same language. We can express
thisinfirst-orderlogicasfollows:2
Nationalityxn Nationalityyn Languagexl Languageyl.19.6
Literal translation: If x and y have the samenationality n and x speaks language l then y
alsospeaksit. Itisnotdifficulttoshowthatfromthissentence andtheobservation that
Nationality Fernando Brazil Language Fernando Portuguese
thefollowingconclusion isentailedsee Exercise19.1:
Nationalityx Brazil Languagex Portuguese.
Sentencessuchas19.6express astrict formofrelevance: givennationality language
isfullydetermined. Putanotherway: languageisafunctionofnationality. Thesesentences
F UN CT IO NA L arecalled functionaldependenciesordeterminations. Theyoccursocommonly incertain
D EP EN DE NC Y
kinds of applications e.g. defining database designs that a special syntax is used to write
D ET ER MI NA TI ON
them. Weadoptthenotation of Davies1985:
Nationalityxn Languagexl.
beamendedforcountriessuchas Switzerlandand India.
Section19.4. Learning Using Relevance Information 785
As usual this is simply a syntactic sugaring but it makes it clear that the determination is
really a relationship between the predicates: nationality determines language. The relevant
properties determining conductance anddensity canbeexpressed similarly:
Materialxm Temperaturext Conductancexρ;
Materialxm Temperaturext Densityxd.
Thecorrespondinggeneralizationsfollowlogicallyfromthedeterminationsandobservations.
Although the determinations sanction general conclusions concerning all Brazilians or all
pieces of copper at a given temperature they cannot of course yield a general predictive
theory for all nationalities or for all temperatures and materials from a single example.
Theirmaineffectcanbeseenaslimitingthespaceofhypothesesthatthelearningagentneed
consider. In predicting conductance for example one need consider only material and tem-
perature and can ignore mass ownership day of the week the current president and so on.
Hypotheses can certainly include terms that are in turn determined by material and temper-
ature such as molecular structure thermal energy or free-electron density. Determinations
specifyasufficientbasisvocabularyfromwhichtoconstructhypothesesconcerningthetarget
predicate. This statement can be proven by showing that a given determination is logically
equivalent toastatement thatthecorrect definition ofthetarget predicate isoneofthesetof
alldefinitionsexpressible usingthepredicates ontheleft-hand sideofthedetermination.
Intuitively it is clear that a reduction in the hypothesis space size should make it eas-
ier to learn the target predicate. Using the basic results of computational learning theory
Section 18.5 we can quantify the possible gains. First recall that for Boolean functions
log H examples are required to converge to a reasonable hypothesis where H is the
size of the hypothesis space. If the learner has n Boolean features with which to construct
hypotheses then in the absence of further restrictions H O22n so the number of ex-
amples is O2n. Ifthe determination contains dpredicates intheleft-hand side the learner
willrequireonly O2dexamplesareduction of O2nd.
As we stated in the introduction to this chapter prior knowledge is useful in learning; but
it too has to be learned. In order to provide a complete story of relevance-based learning
we must therefore provide a learning algorithm for determinations. The learning algorithm
wenowpresentisbasedonastraightforward attempttofindthesimplestdetermination con-
sistent with the observations. A determination P Q says that if any examples match on
P then they must also match on Q. A determination is therefore consistent with a set of
examples if every pair that matches on the predicates on the left-hand side also matches on
the goal predicate. For example suppose we have the following examples of conductance
measurements onmaterialsamples:
function M IN IM AL-C ON SI ST EN T-D ET EAreturnsasetofattributes
inputs:Easetofexamples
Aasetofattributesofsizen
fori 0ton do
foreachsubset Aiof Aofsizei do
if C ON SI ST EN T-D ET?Ai Ethenreturn Ai
function C ON SI ST EN T-D ET?A Ereturnsatruthvalue
inputs:Aasetofattributes
Easetofexamples
localvariables: Hahashtable
foreachexamplee in E do
ifsomeexamplein H hasthesamevaluesase fortheattributes A
butadifferentclassificationthenreturnfalse
storetheclassofe in Hindexedbythevaluesforattributes Aoftheexamplee
returntrue
Figure19.8 Analgorithmforfindingaminimalconsistentdetermination.
Sample Mass Temperature Material Size Conductance
S1 12 26 Copper 3 0.59
S1 12 100 Copper 3 0.57
S2 24 26 Copper 6 0.59
S3 12 26 Lead 2 0.05
S3 12 100 Lead 2 0.04
S4 24 26 Lead 4 0.05
The minimal consistent determination is Material Temperature Conductance. There
is a nonminimal but consistent determination namely Mass Size Temperature
Conductance. Thisisconsistentwiththeexamplesbecausemassandsizedeterminedensity
and inourdata set wedonot have twodifferent materials withthe samedensity. Asusual
wewouldneedalargersamplesetinordertoeliminateanearlycorrecthypothesis.
There are several possible algorithms for finding minimal consistent determinations.
Themostobviousapproachistoconductasearchthroughthespaceofdeterminationscheck-
ingalldeterminations withonepredicate twopredicates andsoon until aconsistent deter-
mination isfound. Wewillassumeasimple attribute-based representation like thatusedfor
decision tree learning in Chapter 18. A determination d will be represented by the set of
attributesontheleft-handsidebecausethetargetpredicateisassumedtobefixed. Thebasic
algorithm isoutlinedin Figure19.8.
The time complexity of this algorithm depends on the size of the smallest consistent
determination. Suppose thisdetermination has pattributes outofthe ntotalattrcid:20ibcid:21utes. Then
thealgorithmwillnotfindituntilsearchingthesubsetsof Aofsizep. Thereare n Onp
p
Section19.4. Learning Using Relevance Information 787
1
0.9
0.8
0.7
0.6
0.5
0.4
tes
tset
no
tcerroc
noitropor P
R BD TL
D TL
Training set size
R BD TL on randomly generated data for a target function that depends on only 5 of 16
attributes.
such subsets; hence the algorithm is exponential inthe size ofthe minimal determination. It
turns out that the problem is N P-complete so we cannot expect to do better in the general
case. Inmostdomains however there willbesufficient localstructure see Chapter14fora
definitionoflocallystructured domainsthat pwillbesmall.
Givenanalgorithm forlearningdeterminations alearning agenthasawaytoconstruct
aminimalhypothesiswithinwhichtolearnthetargetpredicate. Forexamplewecancombine
M IN IM AL-C ON SI ST EN T-D ET withthe D EC IS IO N-T RE E-L EA RN IN G algorithm. Thisyields
a relevance-based decision-tree learning algorithm R BD TL that first identifies a minimal
set of relevant attributes and then passes this set to the decision tree algorithm for learning.
Unlike D EC IS IO N-T RE E-L EA RN IN G R BD TL simultaneously learns andusesrelevance in-
formationinordertominimizeitshypothesisspace. Weexpectthat R BD TLwilllearnfaster
than D EC IS IO N-T RE E-L EA RN IN Gandthisisinfactthecase. Figure19.9showsthelearning
performance for the two algorithms on randomly generated data for a function that depends
ononly 5of16 attributes. Obviously incases whereall the available attributes are relevant
R BD TL willshownoadvantage.
Thissectionhasonlyscratched thesurfaceofthefieldof declarative biaswhichaims
D EC LA RA TI VE BI AS
tounderstand how priorknowledge can beused to identify the appropriate hypothesis space
withinwhichtosearchforthecorrecttargetdefinition. Therearemanyunansweredquestions:
Howcanthealgorithms beextendedtohandlenoise?
Canwehandle continuous-valued variables?
Howcanotherkindsofpriorknowledgebeusedbesidesdeterminations?
How can the algorithms be generalized to cover any first-order theory rather than just
anattribute-based representation?
Someofthesequestions areaddressed inthenextsection.
Inductivelogicprogramming I LPcombinesinductivemethodswiththepoweroffirst-order
representations concentrating in particular on the representation of hypotheses as logic pro-
grams.3 It has gained popularity for three reasons. First I LP offers a rigorous approach to
the general knowledge-based inductive learning problem. Second it offers complete algo-
rithms for inducing general first-order theories from examples which can therefore learn
successfully in domains where attribute-based algorithms are hard to apply. An example is
in learning how protein structures fold Figure 19.10. The three-dimensional configuration
of a protein molecule cannot be represented reasonably by a set of attributes because the
configuration inherently refers to relationships between objects not to attributes of a single
object. First-order logic is an appropriate language for describing the relationships. Third
inductive logic programming produces hypotheses that are relatively easy for humans to
read. For example the English translation in Figure 19.10 can be scrutinized and criticized
byworkingbiologists. Thismeansthatinductive logicprogrammingsystemscanparticipate
inthescientificcycleofexperimentation hypothesisgenerationdebateandrefutation. Such
participation wouldnotbepossible forsystemsthatgenerate black-box classifiers suchas
neuralnetworks.
Recallfrom Equation19.5thatthegeneralknowledge-basedinductionproblemistosolve
theentailmentconstraint
Background Hypothesis Descriptions Classifications
fortheunknown Hypothesis given the Background knowledge and examples described by
Descriptions and Classifications. To illustrate this we will use the problem of learning
family relationships from examples. The descriptions will consist of an extended family
tree described in terms of Mother Father and Married relations and Male and Female
properties. As an example we will use the family tree from Exercise 8.14 shown here in
Figure19.11. Thecorresponding descriptions areasfollows:
Father Philip Charles Father Philip Anne ...
Mother Mum Margaret Mother Mum Elizabeth ...
Married Diana Charles Married Elizabeth Philip ...
Male Philip Male Charles ...
Female Beatrice Female Margaret ...
Thesentencesin Classifications dependonthetargetconceptbeinglearned. Wemightwant
to learn Grandparent Brother In Law or Ancestor for example. For Grandparent the
including Hornclausesconjunctivenormalformunificationandresolution.
Section19.5. Inductive Logic Programming 789
completesetof Classifications contains 2020400conjuncts oftheform
Grandparent Mum Charles Grandparent Elizabeth Beatrice ...
Grandparent Mum Harry Grandparent Spencer Peter ...
Wecouldofcourselearnfromasubsetofthiscompleteset.
The object of an inductive learning program is to come up with a set of sentences for
the Hypothesis suchthattheentailmentconstraintissatisfied. Supposeforthemomentthat
the agent hasno background knowledge: Background isempty. Thenonepossible solution
H:5111-113
H:119-37 H:679-88
H:371-84
H:461-64
H:18-17
H:566-70
H:226-33
E:296-98
E:157-59
H:493-108
H:241-64
H:799-106 H:340-50
2mhr - Four-helical up-and-down bundle 1omd - E F-Hand
a b
four-helical up-and-down bundle concept in the domain of protein folding. Each
example structure is coded into a logical expression of about 100 conjuncts such as
Total Length D2mhr118 Number Helices D2mhr6....Fromthesedescriptionsand
from classifications such as Fold F OU R-H EL IC AL-U P-A ND-D OW N-B UN DL ED2mhr
the I LPsystem P RO GO LMuggleton1995learnedthefollowingrule:
Fold F OU R-H EL IC AL-U P-A ND-D OW N-B UN DL Ep
Helixph 1 Lengthh 1 HI GH Positionph 1n
1n3 Adjacentph h Helixph .
Thiskindofrulecouldnotbelearnedorevenrepresentedbyanattribute-basedmechanism
suchaswesaw inpreviouschapters. Therulecanbetranslatedinto Englishas Proteinp
hasfoldclass Four-helicalup-and-down-bundleifitcontainsalonghelixh atasecondary
1
structurepositionbetween1and3andh isnexttoasecondhelix.
1
for Hypothesis isthefollowing:
Grandparentxy z Motherxz Motherzy
z Motherxz Fatherzy
z Fatherxz Motherzy
z Fatherxz Fatherzy.
Noticethatanattribute-based learningalgorithmsuchas D EC IS IO N-T RE E-L EA RN IN Gwill
getnowhere insolving thisproblem. Inordertoexpress Grandparent asanattribute i.e. a
unarypredicate wewouldneedtomake pairsofpeopleintoobjects:
Grandparentcid:16 Mum Charlescid:17...
Thenwegetstuckintryingtorepresenttheexampledescriptions. Theonlypossibleattributes
arehorriblethingssuchas
First Element Is Mother Of Elizabethcid:16 Mum Charlescid:17.
The definition of Grandparent in terms of these attributes simply becomes a large disjunc-
tionofspecificcasesthatdoesnotgeneralizetonewexamplesatall. Attribute-basedlearning
algorithmsareincapable oflearningrelational predicates. Thusoneoftheprincipal advan-
tages of I LP algorithms is their applicability to a much wider range of problems including
relational problems.
Thereader will certainly have noticed that a little bit ofbackground knowledge would
help in the representation of the Grandparent definition. For example if Background in-
cludedthesentence
Parentxy Motherxy Fatherxy
thenthedefinitionof Grandparent wouldbereducedto
Grandparentxy z Parentxz Parentzy.
This shows how background knowledge can dramatically reduce the size of hypotheses re-
quiredtoexplaintheobservations.
It is also possible for I LP algorithms to create new predicates in order to facilitate the
expression of explanatory hypotheses. Given the example data shown earlier it is entirely
reasonable for the I LP program to propose an additional predicate which we would call
George Mum
Spencer Kydd Elizabeth Philip Margaret
Diana Charles Anne Mark Andrew Sarah Edward Sophie
William Harry Peter Zara Beatrice Eugenie Louise James
Figure19.11 Atypicalfamilytree.
Section19.5. Inductive Logic Programming 791
Parent in order to simplify the definitions of the target predicates. Algorithms that can
C ON ST RU CT IV E generate new predicates are called constructive inductionalgorithms. Clearly constructive
I ND UC TI ON
induction is a necessary part of the picture of cumulative learning. It has been one of the
hardestproblemsinmachinelearningbutsome I LPtechniquesprovideeffectivemechanisms
forachieving it.
In the rest of this chapter we will study the two principal approaches to I LP. The first
uses a generalization of decision tree methods and the second uses techniques based on
inverting aresolution proof.
Thefirstapproachto I LPworksbystartingwithaverygeneral ruleandgraduallyspecializing
it so that it fits the data. This is essentially what happens in decision-tree learning where a
decision tree is gradually grown until it is consistent with the observations. To do I LP we
use first-order literals instead ofattributes and the hypothesis isaset ofclauses instead of a
decision tree. Thissection describes F OI L Quinlan 1990oneofthefirst I LPprograms.
Suppose we are trying to learn a definition of the Grandfatherxy predicate using
the same family data as before. As with decision-tree learning we can divide the examples
intopositiveandnegativeexamples. Positiveexamplesare
cid:16 George Annecid:17 cid:16 Philip Petercid:17 cid:16 Spencer Harrycid:17 ...
andnegativeexamplesare
cid:16 George Elizabethcid:17 cid:16 Harry Zaracid:17 cid:16 Charles Philipcid:17 ...
Noticethateachexample isapair ofobjects because Grandfather isabinary predicate. In
allthereare12positiveexamplesinthefamilytreeand388 negativeexamplesalltheother
pairsofpeople.
F OI Lconstructsasetofclauseseachwith Grandfatherxyasthehead. Theclauses
must classify the 12 positive examples as instances of the Grandfatherxy relationship
whilerulingoutthe388negativeexamples. Theclausesare Hornclauseswiththeextension
thatnegatedliterals areallowedinthebody ofaclause andareinterpreted usingnegation as
failure asin Prolog. Theinitialclausehasanemptybody:
Grandfatherxy.
Thisclause classifies every example as positive so itneeds to bespecialized. Wedo this by
addingliteralsoneatatimetotheleft-hand side. Herearethreepotential additions:
Fatherxy Grandfatherxy.
Parentxz Grandfatherxy.
Fatherxz Grandfatherxy.
Noticethatweareassumingthataclausedefining Parent isalreadypartofthebackground
knowledge. Thefirstofthesethreeclausesincorrectly classifiesallofthe12positiveexam-
ples as negative and can thus beignored. Thesecond and third agree withallof the positive
examples butthesecondisincorrect onalargerfraction of thenegativeexamplestwiceas
manybecause itallowsmothersaswellasfathers. Hencewepreferthethirdclause.
Now we need to specialize this clause further to rule out the cases in which x is the
fatherofsomezbutz isnotaparentofy. Addingthesingleliteral Parentzygives
Fatherxz Parentzy Grandfatherxy
which correctly classifies all the examples. F OI L will find and choose this literal thereby
solving the learning task. In general the solution is a set of Horn clauses each of which
implies the target predicate. For example if we didnt have the Parent predicate in our
vocabulary thenthesolution mightbe
Fatherxz Fatherzy Grandfatherxy
Fatherxz Motherzy Grandfatherxy.
Notethateachoftheseclausescoverssomeofthepositiveexamplesthattogethertheycover
all the positive examples and that N EW-C LA US E is designed in such a way that no clause
willincorrectly coveranegativeexample. Ingeneral F OI L willhavetosearchthrough many
unsuccessful clauses beforefindingacorrectsolution.
Thisexample isavery simpleillustration ofhow F OI L operates. Asketch of thecom-
plete algorithm is shown in Figure 19.12. Essentially the algorithm repeatedly constructs a
clauseliteralbyliteraluntilitagreeswithsomesubset ofthepositiveexamplesandnoneof
the negative examples. Then the positive examples covered by the clause are removed from
the training set and the process continues until no positive examples remain. The two main
subroutinestobeexplainedare N EW-L IT ER AL Swhichconstructsallpossiblenewliteralsto
addtotheclause and C HO OS E-L IT ER ALwhichselectsaliteraltoadd.
N EW-L IT ER AL S takes a clause and constructs all possible useful literals that could
beaddedtotheclause. Letususeasanexampletheclause
Fatherxz Grandfatherxy.
Therearethreekindsofliteralsthatcanbeadded:
1. Literalsusingpredicates: theliteralcanbenegatedorunnegatedanyexistingpredicate
includingthegoalpredicatecanbeusedandtheargumentsmustallbevariables. Any
variablecanbeusedforanyargumentofthepredicatewithonerestriction: eachliteral
mustincludeatleastonevariablefromanearlierliteralorfromtheheadoftheclause.
Literals such as Motherzu Marriedzz Maley and Grandfathervx are
allowed whereas Marrieduv is not. Notice that the use of the predicate from the
headoftheclauseallows F OI L tolearnrecursive definitions.
2. Equality and inequality literals: these relate variables already appearing in the clause.
Forexample we might add z cid:7 x. These literals can also include user-specified con-
stants. Forlearning arithmetic wemightuse0and1andforlearning listfunctions we
mightusetheemptylist.
3. Arithmetic comparisons: when dealing with functions of continuous variables literals
such as x y and y z can be added. As in decision-tree learning a constant
threshold valuecanbechosen tomaximizethediscriminatory powerofthetest.
Theresultingbranchingfactorinthissearchspaceisverylargesee Exercise19.6but F OI L
can also use type information to reduce it. Forexample if the domain included numbers as
Section19.5. Inductive Logic Programming 793
function F OI Lexamplestargetreturnsasetof Hornclauses
inputs:examplessetofexamples
targetaliteralforthegoalpredicate
localvariables: clausessetofclausesinitiallyempty
whileexamples containspositiveexamplesdo
clause N EW-C LA US Eexamplestarget
removepositiveexamplescoveredbyclause fromexamples
addclause toclauses
returnclauses
function N EW-C LA US Eexamplestargetreturnsa Hornclause
localvariables: clauseaclausewithtarget asheadandanemptybody
laliteraltobeaddedtotheclause
extended examplesasetofexampleswithvaluesfornewvariables
extended examplesexamples
whileextended examples containsnegativeexamplesdo
l C HO OS E-L IT ER AL NE W-L IT ER AL Sclauseextended examples
appendl tothebodyofclause
extended examplessetofexamplescreatedbyapplying E XT EN D-E XA MP LE
toeachexampleinextended examples
returnclause
function E XT EN D-E XA MP LEexampleliteralreturnsasetofexamples
ifexample satisfiesliteral
thenreturnthesetofexamplescreatedbyextendingexample with
eachpossibleconstantvalueforeachnewvariableinliteral
elsereturntheemptyset
fromexamples. N EW-L IT ER AL Sand C HO OS E-L IT ER ALareexplainedinthetext.
wellaspeopletyperestrictionswouldprevent N EW-L IT ER AL S fromgeneratingliteralssuch
as Parentxnwherexisapersonandnisanumber.
C HO OS E-L IT ER ALusesaheuristicsomewhatsimilartoinformationgainseepage704
to decide which literal to add. The exact details are not important here and a number of
different variations have been tried. One interesting additional feature of F OI L is the use of
Ockhamsrazortoeliminatesomehypotheses. Ifaclausebecomeslongeraccordingtosome
metric than the total length of the positive examples that the clause explains that clause is
notconsideredasapotentialhypothesis. Thistechniqueprovidesawaytoavoidovercomplex
clausesthatfitnoiseinthedata.
F OI L andits relatives havebeen used tolearn awidevariety ofdefinitions. Oneofthe
mostimpressivedemonstrations Quinlanand Cameron-Jones1993involvedsolvingalong
sequence of exercises on list-processing functions from Bratkos 1986 Prolog textbook. In
each case the program wasable tolearn acorrect definition ofthefunction from asmallset
ofexamples usingthepreviously learnedfunctions asbackground knowledge.
The second major approach to I LP involves inverting the normal deductive proof process.
I NV ER SE Inverse resolution is based on the observation that if the example Classifications follow
R ES OL UT IO N
from Background Hypothesis Descriptionsthenonemustbeabletoprovethisfactby
resolution becauseresolutioniscomplete. Ifwecanrun theproofbackwardthenwecan
find a Hypothesis such that the proof goes through. The key then is to find away to invert
theresolution process.
Wewillshowabackwardproofprocessforinverseresolution thatconsistsofindividual
backward steps. Recall that an ordinary resolution step takes two clauses C and C and
resolves them to produce the resolvent C. An inverse resolution step takes a resolvent C
and produces two clauses C and C such that C is the result of resolving C and C .
Alternatively it may take a resolvent C and clause C and produce a clause C such that C
istheresultofresolving C and C .
The early steps in an inverse resolution process are shown in Figure 19.13 where we
focus on the positive example Grandparent George Anne. Theprocess begins at the end
of the proof shown at the bottom of the figure. We take the resolvent C to be empty
clausei.e. acontradiction and C tobe Grandparent George Annewhichisthenega-
2
tion of the goal example. The first inverse step takes C and C and generates the clause
2
Grandparent George Anne for C . The next step takes this clause as C and the clause
1
Parent Elizabeth Anneas C andgenerates theclause
2
Parent Elizabethy Grandparent Georgey
as C . Thefinalstep treats this clause as the resolvent. With Parent George Elizabethas
1
C onepossible clause C isthehypothesis
Parentxz Parentzy Grandparentxy.
Nowwehavearesolutionproofthatthehypothesisdescriptionsandbackgroundknowledge
entailtheclassification Grandparent George Anne.
Clearly inverse resolution involves a search. Each inverse resolution step is nonde-
terministic because for any C there can be many or even an infinite number of clauses
C and C that resolve to C. For example instead of choosing Parent Elizabethy
Grandparent Georgey for C in the last step of Figure 19.13 the inverse resolution step
1
mighthavechosenanyofthefollowingsentences:
Parent Elizabeth Anne Grandparent George Anne.
Parentz Anne Grandparent George Anne.
Parentzy Grandparent Georgey.
.
.
.
See Exercises 19.4 and 19.5. Furthermore the clauses that participate in each step can be
chosenfromthe Background knowledgefromtheexample Descriptionsfromthenegated
Section19.5. Inductive Logic Programming 795
Classificationsorfromhypothesizedclausesthathavealreadybeengeneratedintheinverse
resolution tree. Thelarge number ofpossibilities means alarge branching factor and there-
foreaninefficientsearchwithoutadditionalcontrols. Anumberofapproaches totamingthe
searchhavebeentriedinimplemented I LPsystems:
1. Redundant choices can be eliminatedfor example by generating only the most spe-
cifichypothesespossibleandbyrequiringthatallthehypothesizedclausesbeconsistent
witheachother andwiththeobservations. Thislastcriterionwouldruleouttheclause
Parentzy Grandparent Georgeylistedbefore.
2. The proof strategy can be restricted. For example we saw in Chapter 9 that linear
resolutionisacompleterestrictedstrategy. Linearresolutionproducesprooftreesthat
have a linear branching structurethe whole tree follows one line with only single
clausesbranching offthatlineasin Figure19.13.
3. Therepresentationlanguagecanberestrictedforexamplebyeliminatingfunctionsym-
bols or by allowing only Horn clauses. For instance P RO GO L operates with Horn
I NV ER SE clausesusinginverseentailment. Theideaistochangetheentailmentconstraint
E NT AI LM EN T
Background Hypothesis Descriptions Classifications
tothelogically equivalent form
Background Descriptions Classifications Hypothesis.
From this one can use a process similar to the normal Prolog Horn-clause deduction
withnegation-as-failure toderive Hypothesis. Becauseitisrestricted to Hornclauses
thisisanincomplete method but itcanbemoreefficientthan full resolution. Itisalso
possibletoapplycompleteinference withinverseentailment Inoue2001.
4. Inferencecanbedonewithmodelcheckingratherthantheoremproving. The P RO GO L
system Muggleton 1995 uses a form of model checking to limit the search. That
Parent Elizabethy Grandparent Georgey Parent Elizabeth Anne
y Anne
Grandparent George Anne Grandparent George Anne
""
Parent George Elizabeth
x George z Elizabeth
Parentzy Grandparentxy Parentxz
""
""
generated by inverse resolution steps from the clause to the right and the clause below.
The unshaded clauses are from the Descriptions and Classifications including negated
Classifications.
is like answer set programming it generates possible values for logical variables and
checksforconsistency.
5. Inferencecanbedonewithgroundpropositional clausesratherthaninfirst-orderlogic.
The L IN USsystem Lavraucand Duzeroski1994worksbytranslatingfirst-orderthe-
ories into propositional logic solving them with a propositional learning system and
then translating back. Working with propositional formulas can be more efficient on
someproblems aswesawwith S AT PL AN in Chapter10.
Aninverse resolution procedure that inverts a complete resolution strategy is in principle a
complete algorithm for learning first-order theories. That is if some unknown Hypothesis
generates a set of examples then an inverse resolution procedure can generate Hypothesis
from the examples. This observation suggests an interesting possibility: Suppose that the
available examples include avariety oftrajectories offalling bodies. Wouldaninverse reso-
lutionprogrambetheoretically capableofinferringthelawofgravity? Theanswerisclearly
yesbecausethelawofgravityallowsonetoexplaintheexamplesgivensuitablebackground
mathematics. Similarlyonecanimaginethatelectromagnetismquantummechanicsandthe
theoryofrelativityarealsowithinthescopeof I LPprograms. Ofcoursetheyarealsowithin
the scope of a monkey with a typewriter; we still need better heuristics and new ways to
structure thesearchspace.
Onethingthatinverseresolution systems willdoforyouisinventnewpredicates. This
abilityisoftenseenassomewhatmagicalbecausecomputersareoftenthoughtofasmerely
working with what they are given. In fact new predicates fall directly out of the inverse
resolution step. Thesimplestcasearises inhypothesizing twonewclauses C and C given
aclause C. Theresolutionof C and C eliminatesaliteralthatthetwoclausesshare;hence
itisquite possible thattheeliminated literalcontained apredicate thatdoes notappearin C.
Thus whenworking backward one possibility is togenerate anewpredicate from which to
reconstruct themissingliteral.
Figure19.14showsanexampleinwhichthenewpredicate P isgeneratedintheprocess
oflearning adefinition for Ancestor. Oncegenerated P canbeusedinlaterinverseresolu-
tionsteps. Forexamplealaterstepmighthypothesizethat Motherxy Pxy. Thus
thenewpredicate P hasitsmeaningconstrainedbythegenerationofhypotheses thatinvolve
it. Another example might lead to the constraint Fatherxy Pxy. In other words
the predicate P is what we usually think of as the Parent relationship. As we mentioned
earlier the invention of new predicates can significantly reduce the size of the definition of
thegoalpredicate. Hencebyincludingtheabilitytoinventnewpredicatesinverseresolution
systemscanoftensolvelearning problemsthatareinfeasible withothertechniques.
Someof the deepest revolutions in science come from the invention of new predicates
and functionsfor example Galileos invention of acceleration or Joules invention of ther-
mal energy. Once these terms are available the discovery of new laws becomes relatively
easy. The difficult part lies in realizing that some new entity with a specific relationship
to existing entities will allow an entire body of observations to be explained with a much
Section19.6. Summary 797
Fatherxy Pxy
x George
""
Father Georgey Ancestor Georgey
P Georgey Ancestor Georgey
Figure19.14 Aninverseresolutionstepthatgeneratesanewpredicate P.
simplerandmoreeleganttheorythanpreviously existed.
Asyet I LPsystemshavenotmadediscoveriesonthelevelof Galileoor Joulebuttheir
discoveries have been deemed publishable in the scientific literature. For example in the
Journalof Molecular Biology Turcotteetal.2001describetheautomateddiscoveryofrules
forprotein folding by the I LP program P RO GO L. Many of the rules discovered by P RO GO L
could havebeenderivedfrom knownprinciples butmosthadnotbeenpreviously published
as part of a standard biological database. See Figure 19.10 for an example.. In related
work Srinivasan et al. 1994 dealt with the problem of discovering molecular-structure-
basedrulesforthemutagenicityofnitroaromaticcompounds. Thesecompoundsarefoundin
automobileexhaustfumes. For80ofthecompoundsinastandarddatabaseitispossibleto
identify fourimportantfeatures andlinearregression on thesefeaturesoutperforms I LP.For
theremaining20thefeaturesalonearenotpredictive and I LPidentifiesrelationships that
allow it to outperform linear regression neural nets and decision trees. Most impressively
Kingetal.2009endowedarobotwiththeabilitytoperformmolecularbiologyexperiments
and extended I LP techniques to include experiment design thereby creating an autonomous
scientistthatactuallydiscoverednewknowledgeaboutthefunctionalgenomicsofyeast. For
alltheseexamplesitappearsthattheabilitybothtorepresentrelationsandtousebackground
knowledgecontribute to I LPshighperformance. Thefactthattherulesfoundby I LPcanbe
interpreted by humans contributes to the acceptance of these techniques in biology journals
ratherthanjustcomputersciencejournals.
I LPhas made contributions to other sciences besides biology. One of the most impor-
tant is natural language processing where I LP has been used to extract complex relational
information fromtext. Theseresultsaresummarizedin Chapter23.
This chapter has investigated various ways in which prior knowledge can help an agent to
learn from new experiences. Because much prior knowledge is expressed in terms of rela-
tional models rather than attribute-based models we have also covered systems that allow
learning ofrelational models. Theimportantpointsare:
The use of prior knowledge in learning leads to a picture of cumulative learning in
whichlearning agentsimprovetheirlearning abilityastheyacquiremoreknowledge.
Priorknowledge helps learning byeliminating otherwise consistent hypotheses andby
20
L EA RN IN G
P RO BA BI LI ST IC M OD EL S
Inwhichweviewlearning asaformofuncertain reasoning fromobservations.
Chapter13pointedouttheprevalenceofuncertaintyinrealenvironments. Agentscanhandle
uncertainty byusingthemethodsofprobability anddecision theory butfirsttheymustlearn
their probabilistic theories of the world from experience. This chapter explains how they
can do that by formulating the learning task itself as a process of probabilistic inference
Section20.1. Wewillseethata Bayesianviewoflearningisextremelypowerfulproviding
general solutions to the problems of noise overfitting and optimal prediction. It also takes
intoaccountthefactthataless-than-omniscientagentcanneverbecertainaboutwhichtheory
oftheworldiscorrectyetmuststillmakedecisions byusingsometheoryoftheworld.
Wedescribemethodsforlearningprobability modelsprimarily Bayesiannetworks
in Sections 20.2 and 20.3. Some of the material in this chapter is fairly mathematical al-
thoughthegenerallessonscanbeunderstoodwithoutplungingintothedetails. Itmaybenefit
thereadertoreview Chapters13and14andpeekat Appendix A.
The key concepts in this chapter just as in Chapter 18 are data and hypotheses. Here the
dataareevidencethatisinstantiationsofsomeoralloftherandomvariablesdescribingthe
domain. The hypotheses in this chapter are probabilistic theories of how the domain works
including logicaltheories asaspecial case.
Consider a simple example. Our favorite Surprise candy comes in two flavors: cherry
yumandlimeugh. Themanufacturerhasapeculiarsenseofhumorandwrapseachpiece
of candy in the same opaque wrapper regardless of flavor. The candy is sold in very large
bagsofwhichthereareknowntobefivekindsagain indistinguishable fromtheoutside:
h : 100cherry
1
h : 75cherry25lime
2
h : 50cherry50lime
3
h : 25cherry75lime
4
h : 100lime .
5
802
Section20.1. Statistical Learning 803
Given a new bag of candy the random variable H for hypothesis denotes the type of the
bag with possible values h through h . H is not directly observable of course. As the
pieces of candy are opened and inspected data are revealed D D ... D where each
D is a random variable with possible values cherry and lime. The basic task faced by the
i
agent is to predict the flavor of the next piece of candy.1 Despite its apparent triviality this
scenario serves to introduce many of the major issues. The agent really does need to infer a
theoryofitsworldalbeitaverysimpleone.
Bayesianlearningsimplycalculatestheprobability ofeachhypothesisgiventhedata
B AY ES IA NL EA RN IN G
and makes predictions on that basis. That is the predictions are made by using all the hy-
pothesesweightedbytheirprobabilities ratherthanbyusingjustasinglebesthypothesis.
In this way learning is reduced to probabilistic inference. Let D represent all the data with
observed valued;thentheprobability ofeachhypothesis isobtainedby Bayesrule:
Ph d α Pdh Ph . 20.1
i i i
Nowsuppose wewanttomakeaprediction aboutanunknownquantity X. Thenwehave
cid:12 cid:12
P Xd P Xdh Ph d P Xh Ph d 20.2
i i i i
i i
where we have assumed that each hypothesis determines a probability distribution over X.
This equation shows that predictions are weighted averages overthe predictions of the indi-
vidual hypotheses. The hypotheses themselves are essentially intermediaries between the
rawdataandthepredictions. Thekeyquantitiesinthe Bayesianapproacharethe hypothesis
prior Ph andthelikelihoodofthedataundereachhypothesis Pdh .
H YP OT HE SI SP RI OR i i
For our candy example we will assume for the time being that the prior distribution
L IK EL IH OO D
over h ...h is given by cid:160.10.20.40.20.1cid:17 as advertised by the manufacturer. The
likelihood of the data is calculated under the assumption that the observations are i.i.d. see
page708sothat
cid:25
Pdh Pd h . 20.3
i j i
j
For example suppose the bag is really an all-lime bag h and the first 10 candies are all
5
lime;then Pdh is0.510because halfthecandies inanh bagarelime.2 Figure20.1a
shows how the posterior probabilities of the five hypotheses change as the sequence of 10
lime candies is observed. Notice that the probabilities start out at their prior values so h
3
is initially the most likely choice and remains so after 1 lime candy is unwrapped. After 2
limecandies areunwrapped h ismostlikely; after3ormore h thedreadedall-limebag
is the most likely. After 10 in a row we are fairly certain of our fate. Figure 20.1b shows
thepredicted probability thatthenextcandyislimebased on Equation20.2. Aswewould
expectitincreases monotonically toward1.
urnsandballslesscompellingthancandy;furthermorecandylendsitselftoothertaskssuchasdecidingwhether
totradethebagwithafriendsee Exercise20.2.
itismorecorrectbutlesshygienictorewrapeachcandyafterinspectionandreturnittothebag.
1
0.8
0.6
0.4
0.2
0
sisehtopyh
fo
ytilibaborp
roiretso P
1
Ph d
1
Ph d
Ph2 d 0.9 3
Ph d
Ph4 d 0.8
5
0.7
0.6
0.5
0.4
Number of observations in d
emil
si ydnac
txen
taht
ytilibabor P
Number of observations in d
a b
number of observations N ranges from 1 to 10 and each observation is of a lime candy.
b Bayesianprediction Pd N1limed 1...d Nfrom Equation20.2.
The example shows that the Bayesian prediction eventually agrees with the true hy-
pothesis. This is characteristic of Bayesian learning. For any fixed prior that does not rule
out the true hypothesis the posterior probability of any false hypothesis will under certain
technical conditions eventually vanish. Thishappens simplybecause theprobability ofgen-
erating uncharacteristic data indefinitely is vanishingly small. This point is analogous to
one made in the discussion of P AC learning in Chapter 18. More important the Bayesian
prediction is optimal whetherthe data setbesmallorlarge. Giventhe hypothesis prior any
otherprediction isexpected tobecorrectlessoften.
The optimality of Bayesian learning comes at a price of course. For real learning
problems the hypothesis space is usually very large orinfinite as wesaw in Chapter 18. In
somecases thesummationin Equation 20.2orintegration inthecontinuous casecanbe
carriedouttractably butinmostcaseswemustresorttoapproximate orsimplifiedmethods.
Averycommonapproximationonethatisusuallyadoptedinscienceistomakepre-
dictionsbasedonasinglemostprobablehypothesisthat isanh thatmaximizes Ph d.
i i
M AX IM UM A Thisisoftencalledamaximumaposteriorior M APpronouncedem-ay-peehypothesis.
P OS TE RI OR I
Predictions madeaccording toan M APhypothesis h areapproximately Bayesian tothe
M AP
extentthat P Xd P Xh . Inourcandyexampleh h afterthreelimecan-
M AP M AP 5
diesinarowsothe M APlearnerthenpredicts thatthefourth candyislimewithprobability
1.0a much more dangerous prediction than the Bayesian prediction of 0.8 shown in Fig-
ure20.1b. Asmoredataarrivethe M APand Bayesian predictions becomecloser because
thecompetitors tothe M APhypothesis becomelessandlessprobable.
Although our example doesnt show it finding M AP hypotheses is often much easier
than Bayesianlearningbecauseitrequiressolvinganoptimizationprobleminsteadofalarge
summationorintegration problem. Wewillseeexamplesof thislaterinthechapter.
Section20.1. Statistical Learning 805
In both Bayesian learning and M AP learning the hypothesis prior Ph plays an im-
i
portant role. We saw in Chapter 18 that overfitting can occur when the hypothesis space
is too expressive so that it contains many hypotheses that fit the data set well. Rather than
placing an arbitrary limit on the hypotheses to be considered Bayesian and M AP learning
methods use the prior to penalize complexity. Typically more complex hypotheses have a
lower prior probabilityin part because there are usually many more complex hypotheses
than simple hypotheses. Onthe otherhand more complex hypotheses have agreater capac-
ity to fit the data. In the extreme case a lookup table can reproduce the data exactly with
probability 1. Hence the hypothesis priorembodies atradeoff between thecomplexity ofa
hypothesis anditsdegreeoffittothedata.
Wecanseetheeffectofthistradeoffmostclearlyinthelogicalcasewhere H contains
onlydeterministic hypotheses. Inthatcase Pdh is1ifh isconsistent and 0otherwise.
i i
Looking at Equation 20.1 we see that h will then be the simplest logical theory that
M AP
is consistent with the data. Therefore maximum a posteriori learning provides a natural
embodimentof Ockhamsrazor.
Another insight into the tradeoff between complexity and degree of fit is obtained by
taking the logarithm of Equation 20.1. Choosing h to maximize Pdh Ph is
M AP i i
equivalent tominimizing
log Pdh log Ph .
Using the connection between information encoding and probability that we introduced in
Chapter18.3.4weseethatthe log Ph termequalsthenumberofbitsrequiredtospec-
ifythehypothesish . Furthermore log Pdh istheadditionalnumberofbitsrequired
i 2 i
to specify the data given the hypothesis. To see this consider that no bits are required
if the hypothesis predicts the data exactlyas with h and the string of lime candiesand
5
log 10. Hence M AP learning is choosing the hypothesis that provides maximum com-
2
pression ofthedata. Thesametaskisaddressed moredirectly bythe minimumdescription
lengthor M DLlearningmethod. Whereas M APlearningexpressessimplicitybyassigning
higherprobabilities tosimplerhypotheses M DLexpresses itdirectly bycounting thebitsin
abinaryencoding ofthehypotheses anddata.
A final simplification is provided by assuming a uniform prior over the space of hy-
potheses. In that case M AP learning reduces to choosing an h that maximizes Pdh .
i i
M AX IM UM- Thisiscalledamaximum-likelihood M Lhypothesis h . Maximum-likelihood learning
L IK EL IH OO D M L
is very common in statistics a discipline in which many researchers distrust the subjective
natureofhypothesis priors. Itisareasonable approach whenthereisnoreasontopreferone
hypothesis overanother apriorifor example whenallhypotheses are equally complex. It
provides a good approximation to Bayesian and M AP learning when the data set is large
because the data swamps the prior distribution over hypotheses but it has problems as we
shallseewithsmalldatasets.
Thegeneraltaskoflearningaprobability modelgivendata thatareassumedtobegenerated
from that model is called density estimation. The term applied originally to probability
D EN SI TY ES TI MA TI ON
densityfunctions forcontinuous variables butisusednow fordiscretedistributions too.
This section covers the simplest case where we have complete data. Data are com-
C OM PL ET ED AT A
plete when each data point contains values forevery variable in the probability model being
P AR AM ET ER learned. We focus on parameter learningfinding the numerical parameters for a proba-
L EA RN IN G
bility model whose structure is fixed. For example we might be interested in learning the
conditional probabilities in a Bayesian network with a given structure. We will also look
brieflyattheproblem oflearning structure andatnonparametric densityestimation.
Supposewebuyabagoflimeandcherrycandyfromanewmanufacturerwhoselimecherry
proportions are completely unknown; the fraction could be anywhere between 0 and 1. In
that case we have a continuum of hypotheses. The parameter in this case which we call
θ is the proportion of cherry candies and the hypothesis is h . The proportion of limes is
θ
just 1θ. If we assume that all proportions are equally likely a priori then a maximum-
likelihood approach is reasonable. If we model the situation with a Bayesian network we
needjustonerandomvariable Flavor theflavorofarandomlychosencandyfromthebag.
Ithasvaluescherry andlimewheretheprobabilityofcherry isθsee Figure20.2a. Now
suppose weunwrap N candies ofwhich care cherries and cid:3 N care limes. According
to Equation20.3 thelikelihood ofthisparticular datasetis
cid:25 N
Pdh Pd h θc1θcid:3 .
θ j θ
j1
The maximum-likelihood hypothesis is given by the value of θ that maximizes this expres-
sion. Thesamevalueisobtained bymaximizingtheloglikelihood
L OG LI KE LI HO OD
cid:12 N
Ldh log Pdh log Pd h clogθcid:3log1θ.
θ θ j θ
j1
Bytaking logarithms wereduce theproduct toasum overthe data whichisusually easier
tomaximize. Tofindthemaximum-likelihood valueofθwedifferentiate Lwithrespect to
θ andsettheresulting expression tozero:
d Ldh c cid:3 c c
θ 0 θ .
dθ θ 1θ ccid:3 N
In English then the maximum-likelihood hypothesis h asserts that the actual proportion
M L
ofcherries inthebagisequaltotheobserved proportion inthecandiesunwrapped sofar!
It appears that we have done a lot of work to discover the obvious. In fact though
wehavelaidoutonestandardmethodformaximum-likelihood parameterlearningamethod
withbroadapplicability:
Section20.2. Learningwith Complete Data 807
P Fcherry
θ
P Fcherry Flavor
θ
F P Wred F
θ
Flavor cherry 1
θ
lime 2
Wrapper
a b
Figure20.2 a Bayesiannetworkmodelforthecaseofcandieswithanunknownpropor-
tionofcherriesandlimes. b Modelforthecasewherethewrappercolordependsproba-
bilisticallyonthecandyflavor.
1. Writedownanexpressionforthelikelihoodofthedataasafunctionoftheparameters.
2. Writedownthederivativeoftheloglikelihood withrespecttoeachparameter.
3. Findtheparametervaluessuchthatthederivativesarezero.
The trickiest step is usually the last. In our example it was trivial but we will see that in
manycasesweneedtoresorttoiterativesolutionalgorithmsorothernumericaloptimization
techniques as described in Chapter 4. The example also illustrates a significant problem
with maximum-likelihood learning in general: when the data set is small enough that some
eventshavenotyetbeenobservedforinstancenocherrycandiesthemaximum-likelihood
hypothesis assigns zero probability to those events. Various tricks are used to avoid this
problemsuchasinitializing thecountsforeacheventto1insteadof0.
Letus look atanother example. Suppose this new candy manufacturer wants togive a
littlehinttotheconsumerandusescandywrapperscoloredredandgreen. The Wrapper for
eachcandyisselectedprobabilistically accordingtosomeunknownconditionaldistribution
depending on the flavor. The corresponding probability model is shown in Figure 20.2b.
Notice that it has three parameters: θ θ and θ . With these parameters the likelihood of
seeing say a cherry candy in a green wrapper can be obtained from the standard semantics
for Bayesiannetworkspage513:
P Flavor cherry Wrapper greenh
θθ1θ2
P Flavor cherryh P Wrapper green Flavor cherryh
θθ1θ2 θθ1θ2
θ1θ .
1
Nowweunwrap N candies ofwhichcarecherries and cid:3arelimes. Thewrappercounts are
asfollows: r ofthecherrieshaveredwrappersand g havegreenwhiler ofthelimeshave
c c cid:3
redandg havegreen. Thelikelihood ofthedataisgivenby
cid:3
Pdh θc1θcid:3θrc1θ gc θrcid:31θ gcid:3 .
θθ1θ2 1 1 2 2
Thislooksprettyhorrible buttakinglogarithmshelps:
Lclogθcid:3log1θrclogθ 1gclog1θ 1rcid:3logθ 2gcid:3log1θ 2.
Thebenefitoftakinglogsisclear: theloglikelihoodisthesumofthreetermseachofwhich
containsasingleparameter. Whenwetakederivativeswithrespecttoeachparameterandset
themtozerowegetthreeindependent equations eachcontaining justoneparameter:
L c cid:3 0 θ c
θ θ 1θ ccid:3
L rc gc 0 θ rc
θ1 θ1 1θ1 1 rcgc
L rcid:3 gcid:3 0 θ rcid:3 .
θ2 θ2 1θ2 2 rcid:3gcid:3
The solution for θ is the same as before. The solution for θ the probability that a cherry
1
candy has a red wrapper is the observed fraction of cherry candies with red wrappers and
similarlyforθ .
2
Theseresultsareverycomforting anditiseasytoseethattheycanbeextended toany
Bayesiannetworkwhoseconditionalprobabilitiesarerepresentedastables. Themostimpor-
tant point is that with complete data the maximum-likelihood parameter learning problem
fora Bayesiannetworkdecomposesintoseparatelearningproblemsoneforeachparameter.
See Exercise20.6forthenontabulatedcasewhereeachparameteraffectsseveralconditional
probabilities. Thesecond point isthat theparametervalues foravariable given itsparents
are just the observed frequencies of the variable values for each setting of the parent values.
Asbeforewemustbecarefultoavoidzeroeswhenthedataset issmall.
Probably the most common Bayesian network model used in machine learning is the naive
Bayesmodelfirstintroduced onpage499. Inthis model theclass variable C which isto
bepredicted istherootandtheattribute variables X aretheleaves. Themodelisnaive
i
because it assumes that the attributes are conditionally independent of each other given the
class. The model in Figure 20.2b is a naive Bayes model with class Flavor and just one
attribute Wrapper. Assuming Booleanvariables theparameters are
θ PCtrueθ P X true Ctrueθ P X true Cfalse.
i1 i i2 i
The maximum-likelihood parameter values are found in exactly the same way as for Fig-
ure20.2b. Oncethemodelhasbeentrainedinthiswayitcanbeusedtoclassifynewexam-
plesforwhichtheclassvariable C isunobserved. Withobservedattributevalues x ...x
theprobability ofeachclassisgivenby
cid:25
P Cx ...x α P C Px C.
i
A deterministic prediction can be obtained by choosing the most likely class. Figure 20.3
shows the learning curve for this method when it is applied to the restaurant problem from
presumably because the true hypothesiswhich is a decision treeis not representable ex-
actly using anaive Bayesmodel. Naive Bayeslearning turnsout todosurprisingly wellina
wide range of applications; the boosted version Exercise 20.4 is one of the most effective
Section20.2. Learningwith Complete Data 809
1
0.9
0.8
0.7
0.6
0.5
0.4
tes
tset
no
tcerroc
noitropor P
Decision tree
Naive Bayes
Training set size
Figure20.3 Thelearningcurvefornaive Bayeslearningappliedtotherestaurantproblem
from Chapter18;thelearningcurvefordecision-treelearningisshownforcomparison.
general-purpose learning algorithms. Naive Bayes learning scales well to very large prob-
lems: withn Boolean attributes there are just 2n1parameters and nosearch is required
tofindh the maximum-likelihood naive Bayeshypothesis. Finally naive Bayeslearning
M L
systems have no difficulty with noisy or missing data and can give probabilistic predictions
whenappropriate.
Continuous probability models such as the linear Gaussian model were introduced in Sec-
tion14.3. Becausecontinuousvariablesareubiquitousinreal-worldapplications itisimpor-
tanttoknowhowtolearntheparametersofcontinuous models fromdata. Theprinciples for
maximum-likelihood learningareidentical inthecontinuous anddiscretecases.
Let us begin with a very simple case: learning the parameters of a Gaussian density
function onasinglevariable. Thatisthedataaregenerated asfollows:
Px
1 ex 2 σμ 22
.
2πσ
The parameters of this model are the mean μ and the standard deviation σ. Notice that the
normalizing constant depends on σ so we cannot ignore it. Let the observed values be
x ...x . Thentheloglikelihood is
L
cid:12 N
log
1 exj 2 σ2μ2
""
Nlog
2πlogσ
cid:12 N x
j
μ2
.
2πσ 2σ2
j1 j1
Settingthederivatives tozeroasusualweobtain
cid:2 P
L 1 N x μ 0 μ jxj
μ σ2 j1 cid:2j cid:9 NP
20.4
L N 1 N x μ2 0 σ
jxjμ2
.
σ σ σ3 j1 j N
Thatisthemaximum-likelihood valueofthemeanisthesampleaverageandthemaximum-
likelihood value of the standard deviation is the square root of the sample variance. Again
thesearecomforting resultsthatconfirmcommonsense practice.
1
0.8
Py x
3.5
3
2.5
0.4
2
1.5
1
y
x
a b
Figure20.4 a Alinear Gaussianmodeldescribedas yθ xθ plus Gaussiannoise
withfixedvariance.b Asetof50datapointsgeneratedfromthismodel.
Nowconsideralinear Gaussianmodelwithonecontinuous parent X andacontinuous
child Y. As explained on page 520 Y has a Gaussian distribution whose mean depends
linearly on the value of X and whose standard deviation is fixed. To learn the conditional
distribution P Y Xwecanmaximizetheconditional likelihood
Pyx
1 eyθ1 2σx 2θ22
. 20.5
2πσ
Heretheparametersareθ θ andσ. Thedataareacollectionofx y pairsasillustrated
in Figure20.4. Usingtheusualmethods Exercise20.5wecanfindthemaximum-likelihood
values of the parameters. The point here is different. If we consider just the parameters θ
1
and θ that define thelinear relationship between x and y it becomes clearthat maximizing
2
the log likelihood with respect to these parameters is the same as minimizing the numerator
y θ x θ 2 in the exponent of Equation 20.5. This is the L loss the squared er-
ror between the actual value y and the prediction θ xθ . This is the quantity minimized
by the standard linear regression procedure described in Section 18.6. Now we can under-
standwhy: minimizingthesumofsquarederrorsgivesthemaximum-likelihood straight-line
modelprovided thatthedataaregenerated with Gaussiannoiseoffixedvariance.
Maximum-likelihood learning gives rise to some very simple procedures but it has some
serious deficiencies with small data sets. For example after seeing one cherry candy the
maximum-likelihood hypothesis is that the bag is 100 cherry i.e. θ1.0. Unless ones
hypothesis prior is that bags must be either all cherry or all lime this is not a reasonable
conclusion. It is more likely that the bag is a mixture of lime and cherry. The Bayesian
approach to parameter learning starts by defining a prior probability distribution over the
possible hypotheses. We call this the hypothesis prior. Then as data arrives the posterior
H YP OT HE SI SP RI OR
probability distribution isupdated.
Section20.2. Learningwith Complete Data 811
2.5
2
1.5
1
0.5
0
θ
""
Θ P
6
55
5
22 4
3
11
2
1
0
Parameterθ
θ
""
Θ P
3010
62
31
Parameterθ
a b
Figure20.5 Examplesofthebetaabdistributionfordifferentvaluesofab.
The candy example in Figure 20.2a has one parameter θ: the probability that a ran-
domly selected piece of candy is cherry-flavored. In the Bayesian view θ is the unknown
value of a random variable Θ that defines the hypothesis space; the hypothesis prior is just
thepriordistribution PΘ. Thus PΘθisthepriorprobabilitythatthebaghasafraction
θ ofcherrycandies.
If the parameter θ can be any value between 0 and 1 then PΘ must be a continuous
distributionthatisnonzeroonlybetween0and1andthatintegratesto1. Theuniformdensity
Pθ Uniform01θ is one candidate. See Chapter 13. It turns out that the uniform
density isamemberofthefamilyofbetadistributions. Eachbetadistribution isdefinedby
B ET AD IS TR IB UT IO N
twohyperparameters3 aandbsuchthat
H YP ER PA RA ME TE R
betaabθ
αθa11θb1
20.6
forθintherange01. Thenormalizationconstantαwhichmakesthedistributionintegrate
to1depends onaandb. See Exercise 20.7. Figure20.5showswhatthedistribution looks
like forvarious values of aand b. The meanvalue of the distribution is aab so larger
values of a suggest a belief that Θ is closer to 1 than to 0. Larger values of ab make the
distribution more peaked suggesting greater certainty about the value of Θ. Thus the beta
familyprovides ausefulrangeofpossibilities forthehypothesis prior.
Besides its flexibility the beta family has another wonderful property: if Θ has aprior
betaab then after a data point is observed the posterior distribution for Θ is also a beta
distribution. In other words beta is closed under update. The beta family is called the
conjugate prior for the family of distributions for a Boolean variable.4 Lets see how this
C ON JU GA TE PR IO R
works. Supposeweobserveacherrycandy;thenwehave
andthe Normal Wishartfamilyfortheparametersofa Gaussiandistribution.See Bernardoand Smith1994.
Θ
Flavor Flavor Flavor
Wrapper Wrapper Wrapper
Θ Θ
Figure20.6 A Bayesiannetworkthatcorrespondstoa Bayesianlearningprocess. Poste-
riordistributionsfortheparametervariables ΘΘ andΘ canbeinferredfromtheirprior
distributionsandtheevidenceinthe Flavoriand Wrapperi variables.
Pθ D cherry α PD cherryθ Pθ
αcid:2 θbetaabθ αcid:2 θθa11θb1
""
αcid:2 θa1θb1
betaa1bθ.
Thus after seeing acherry candy wesimply increment the aparameter toget the posterior;
similarly after seeing a lime candy weincrement the b parameter. Thus we can view the a
and bhyperparameters as virtual counts inthesense thataprior betaab behaves exactly
V IR TU AL CO UN TS
asifwehad started outwithauniform prior beta11 andseen a1actual cherry candies
andb1actuallimecandies.
Byexaminingasequenceofbetadistributions forincreasingvaluesofaandbkeeping
the proportions fixed we can see vividly how the posterior distribution over the parameter
Θ changes as data arrive. Forexample suppose the actual bag ofcandy is75 cherry. Fig-
ure 20.5b shows the sequence beta31 beta62 beta3010. Clearly the distribution
isconvergingtoanarrowpeakaroundthetruevalueof Θ. Forlargedatasetsthen Bayesian
learningatleastinthiscaseconvergestothesameanswerasmaximum-likelihoodlearning.
Nowletusconsideramorecomplicated case. Thenetwork in Figure 20.2bhasthree
parameters θθ andθ whereθ istheprobability ofaredwrapperonacherrycandy and
θ is the probability of a red wrapper on a lime candy. The Bayesian hypothesis prior must
2
cover all three parametersthat is we need to specify PΘΘ Θ . Usually we assume
P AR AM ET ER parameterindependence:
I ND EP EN DE NC E
PΘΘ Θ PΘ PΘ PΘ .
Section20.2. Learningwith Complete Data 813
With this assumption each parameter can have its ownbeta distribution that is updated sep-
arately as data arrive. Figure 20.6 shows how we can incorporate the hypothesis prior and
any data into one Bayesian network. The nodes ΘΘ Θ have no parents. But each time
wemakeanobservation ofawrapperandcorresponding flavorofapieceofcandy weadda
node Flavor whichisdependent ontheflavorparameter Θ:
i
P Flavor cherryΘθ θ .
i
Wealsoaddanode Wrapper whichisdependent onΘ andΘ :
i 1 2
P Wrapper red Flavor cherryΘ θ θ
i i 1 1 1
P Wrapper red Flavor limeΘ θ θ .
i i 2 2 2
Now the entire Bayesian learning process can be formulated as an inference problem. We
add new evidence nodes then query the unknown nodes in this case ΘΘ Θ . This for-
mulation of learning and prediction makes it clear that Bayesian learning requires no extra
principles oflearning. Furthermore thereis inessence justonelearning algorithm the
inference algorithm for Bayesian networks. Ofcourse thenatureofthesenetworks issome-
whatdifferent from those of Chapter14because ofthe potentially huge numberofevidence
variables representing the training set and the prevalence of continuous-valued parameter
variables.
Sofar wehaveassumed thatthestructure ofthe Bayesnetisgivenandwearejusttrying to
learn the parameters. The structure of the network represents basic causal knowledge about
the domain that is often easy for an expert or even a naive user to supply. In some cases
however the causal model may be unavailable or subject to disputefor example certain
corporations have long claimed that smoking does not cause cancerso it is important to
understand how the structure of a Bayes net can be learned from data. This section gives a
briefsketchofthemainideas.
The most obvious approach is to search for a good model. We can start with a model
containing no links and begin adding parents for each node fitting the parameters with the
methods we have just covered and measuring the accuracy of the resulting model. Alterna-
tively we can start with an initial guess at the structure and use hill-climbing or simulated
annealing search to make modifications retuning the parameters after each change in the
structure. Modifications can include reversing adding or deleting links. We must not in-
troduce cycles in the process so many algorithms assume that an ordering is given for the
variables and that a node can have parents only among those nodes that come earlier in the
ordering just asintheconstruction process in Chapter14. Forfullgenerality wealsoneed
tosearchoverpossible orderings.
There are two alternative methods fordeciding when a good structure has been found.
Thefirstistotestwhethertheconditionalindependenceassertionsimplicitinthestructureare
actually satisfied inthe data. Forexample the useof anaive Bayesmodel fortherestaurant
problem assumesthat
P Fri Sat Bar Will Wait P Fri Sat Will Wait P Bar Will Wait
andwecancheckinthedatathatthesameequation holdsbetweenthecorresponding condi-
tional frequencies. But even if the structure describes the true causal nature of the domain
statistical fluctuations in the data set mean that the equation will never be satisfied exactly
so we need to perform a suitable statistical test to see if there is sufficient evidence that the
independence hypothesis is violated. The complexity of the resulting network will depend
on the threshold used forthis testthe stricter the independence test the more links will be
addedandthegreaterthedangerofoverfitting.
An approach more consistent with the ideas in this chapter is to assess the degree to
which the proposed model explains the data in a probabilistic sense. We must be careful
how we measure this however. If we just try to find the maximum-likelihood hypothesis
wewill end up with afully connected network because adding more parents to anode can-
not decrease the likelihood Exercise 20.8. We are forced to penalize model complexity in
some way. The M AP or M DL approach simply subtracts a penalty from the likelihood of
each structure after parameter tuning before comparing different structures. The Bayesian
approach places ajoint prioroverstructures and parameters. There are usually fartoo many
structures to sum over superexponential in the number of variables so most practitioners
use M CM Ctosampleoverstructures.
Penalizingcomplexitywhetherby M APor Bayesianmethodsintroducesanimportant
connection between the optimal structure and the nature of the representation for the condi-
tional distributions in the network. With tabular distributions the complexity penalty for a
nodes distribution growsexponentially with thenumber of parents but with say noisy-O R
distributions itgrows only linearly. This means that learning with noisy-O R orother com-
pactlyparameterizedmodelstendstoproducelearnedstructureswithmoreparentsthandoes
learning withtabulardistributions.
Itispossibletolearnaprobabilitymodelwithoutmakinganyassumptionsaboutitsstructure
and parameterization by adopting the nonparametric methods of Section 18.8. The task of
N ON PA RA ME TR IC nonparametric density estimation is typically done in continuous domains such as that
D EN SI TY ES TI MA TI ON
shown in Figure 20.7a. Thefigure shows aprobability density function on aspace defined
by two continuous variables. In Figure 20.7b we see a sample of data points from this
densityfunction. Thequestion iscanwerecoverthemodelfromthesamples?
First we will consider k-nearest-neighbors models. In Chapter 18 we saw nearest-
neighbor models for classification and regression; here we see them for density estimation.
Givenasampleofdatapointstoestimatetheunknownprobability densityataquerypointx
wecansimplymeasurethedensityofthedatapointsintheneighborhoodofx. Figure20.7b
shows two query points small squares. For each query point we have drawn the smallest
circle that encloses 10neighborsthe 10-nearest-neighborhood. Wecansee thatthe central
circle is large meaning there is a low density there and the circle on the right is small
meaningthereisahighdensitythere. In Figure20.8weshowthreeplotsofdensityestimation
using k-nearest-neighbors for different values of k. It seems clear that b is about right
whileaistoospikyk istoosmallandcistoosmoothk istoobig.
Section20.2. Learningwith Complete Data 815
Density
18
14
0.9
12
8
4
1
0.3
a b
Figure20.7 a A3 Dplotofthemixtureof Gaussiansfrom Figure20.11a. b A128-
pointsampleofpointsfromthemixturetogetherwithtwoquerypointssmallsquaresand
their10-nearest-neighborhoodsmediumandlargecircles.
Density Density Density
a b c
ure 20.7b for k3 10 and 40 respectively. k 3 is too spiky 40 is too smooth and
10isjustaboutright.Thebestvalueforkcanbechosenbycross-validation.
Density Density Density
a b c
Figure20.9 Kerneldensityestimationforthedatain Figure20.7busing Gaussianker-
nelswithw0.020.07and0.20respectively.w0.07isaboutright.
Another possibility is to use kernel functions as we did for locally weighted regres-
sion. Toapplyakernelmodeltodensityestimationassumethateachdatapointgeneratesits
ownlittledensityfunction usinga Gaussiankernel. Theestimateddensityataquerypointx
isthentheaverage densityasgivenbyeachkernelfunction:
cid:12 N
1
Px Kxx .
j
N
j1
Wewillassumespherical Gaussianswithstandard deviation w alongeachaxis:
Kxx j
1 e D 2x wx 2j2
""
w2 2πd
where d is the number of dimensions in x and D is the Euclidean distance function. We
still have the problem of choosing a suitable value for kernel width w; Figure 20.9 shows
valuesthataretoosmalljustright andtoolarge. Agoodvalueofwcanbechosenbyusing
cross-validation.
The preceding section dealt with the fully observable case. Many real-world problems have
hiddenvariables sometimes called latent variables which are not observable in the data
L AT EN TV AR IA BL E
that are available for learning. For example medical records often include the observed
symptoms the physicians diagnosis the treatment applied and perhaps the outcome of the
treatment but they seldom contain a direct observation of the disease itself! Note that the
diagnosisisnotthedisease;itisacausalconsequenceoftheobservedsymptomswhicharein
turncausedbythedisease. Onemightask Ifthediseaseisnotobservedwhynotconstruct
a model without it? The answer appears in Figure 20.10 which shows a small fictitious
diagnostic modelforheartdisease. Therearethreeobservablepredisposing factorsandthree
observable symptoms which are too depressing to name. Assume that each variable has
three possible values e.g. none moderate and severe. Removing the hidden variable
from the network in a yields the network in b; the total number of parameters increases
from 78 to 708. Thus latent variables can dramatically reduce the number of parameters
required tospecify a Bayesiannetwork. Thisinturncandramatically reduce theamountof
dataneededtolearntheparameters.
Hidden variables are important but they do complicate the learning problem. In Fig-
ure 20.10a for example it is not obvious how to learn the conditional distribution for
Heart Diseasegivenitsparents becausewedonotknowthevalueof Heart Disease ineach
case; the same problem arises in learning the distributions for the symptoms. This section
E XP EC TA TI ON describes an algorithm called expectationmaximization or E M that solves this problem
M AX IM IZ AT IO N
in avery general way. Wewill show three examples and then provide a general description.
The algorithm seems like magic at first but once the intuition has been developed one can
findapplications for E Minahugerangeoflearning problems.
Section20.3. Learningwith Hidden Variables: The E MAlgorithm 817
Smoking Diet Exercise Smoking Diet Exercise
Symptom Symptom Symptom Symptom Symptom Symptom
a b
Figure20.10 a Asimple diagnosticnetworkforheartdisease whichisassumedtobe
a hidden variable. Each variable has three possible values and is labeled with the number
of independent parameters in its conditional distribution; the total number is 78. b The
equivalent network with Heart Disease removed. Note that the symptom variables are no
longerconditionallyindependentgiventheirparents.Thisnetworkrequires708parameters.
U NS UP ER VI SE D Unsupervised clustering is the problem of discerning multiple categories in a collection of
C LU ST ER IN G
objects. Theproblemisunsupervisedbecausethecategorylabelsarenotgiven. Forexample
suppose we record the spectra of a hundred thousand stars; are there different types of stars
revealed by the spectra and if so how many types and what are their characteristics? We
are all familiar with terms such as red giant and white dwarf but the stars do not carry
these labels on their hatsastronomers had to perform unsupervised clustering to identify
these categories. Other examples include the identification of species genera orders and
so on in the Linnæan taxonomy and the creation of natural kinds for ordinary objects see
Chapter12.
Unsupervised clustering beginswithdata. Figure20.11b shows500datapointseach
ofwhichspecifies thevalues oftwocontinuous attributes. Thedatapoints mightcorrespond
to stars and the attributes might correspond to spectral intensities at two particular frequen-
cies. Nextweneedtounderstandwhatkindofprobability distribution mighthavegenerated
M IX TU RE the data. Clustering presumes that the data are generated from a mixture distribution P.
D IS TR IB UT IO N
Such a distribution has k components each of which is a distribution in its own right. A
C OM PO NE NT
datapointisgeneratedbyfirstchoosingacomponentandthengenerating asamplefromthat
component. Lettherandom variable C denotethecomponent withvalues1...k;thenthe
mixturedistribution isgivenby
cid:12k
Px P Ci Px Ci
i1
where x refers to the values of the attributes for a data point. Forcontinuous data a natural
choiceforthecomponentdistributionsisthemultivariate Gaussianwhichgivestheso-called
M IX TU RE OF mixtureof Gaussiansfamilyofdistributions. Theparameters ofamixtureof Gaussians are
G AU SS IA NS
a b c
Figure20.11 a A Gaussianmixturemodelwiththreecomponents;theweightsleft-to-
rightare0.20.3and0.5.b500datapointssampledfromthemodelina.c Themodel
reconstructedby E Mfromthedatainb.
w P Ci the weight of each component μ the mean of each component and Σ
i i i
the covariance of each component. Figure 20.11a shows a mixture of three Gaussians;
this mixture is in fact the source of the data in b as well as being the model shown in
Figure20.7aonpage815.
The unsupervised clustering problem then is to recover a mixture model like the one
in Figure20.11afromrawdatalikethatin Figure20.11b. Clearlyifweknewwhichcom-
ponentgenerated eachdatapointthenitwouldbeeasytorecoverthecomponent Gaussians:
wecouldjustselectallthedatapointsfromagivencomponentandthenapplyamultivariate
versionof Equation20.4page809forfittingtheparametersofa Gaussiantoasetofdata.
On the other hand if we knew the parameters of each component then we could at least in
a probabilistic sense assign each data point to a component. The problem is that we know
neithertheassignments northeparameters.
The basic idea of E M in this context is to pretend that we know the parameters of the
modelandthentoinfertheprobabilitythateachdatapointbelongstoeachcomponent. After
thatwerefitthecomponentstothedatawhereeachcomponentisfittedtotheentiredataset
with each point weighted by the probability that it belongs to that component. The process
iterates until convergence. Essentially wearecompleting thedata byinferring probability
distributionsoverthehiddenvariableswhichcomponenteachdatapointbelongstobased
onthecurrentmodel. Forthemixtureof Gaussians weinitialize themixture-model parame-
tersarbitrarily andtheniteratethefollowingtwosteps:
1. E-step: Compute the probabilities p P Cix the probability that datum x
ij j j
wasgeneratedbycomponenti. By Bayesrulewehavep α Px Ci P Ci.
ij j
The term Px Ci is just the probability at x of the ith Gaussian and the term
j j cid:2
P Ci is just the weight parameter for the ith Gaussian. Define n p the
i j ij
effectivenumberofdatapointscurrently assigned tocomponenti.
2. M-step: Computethenewmeancovariance andcomponentweightsusingthefollow-
ingstepsinsequence:
Section20.3. Learningwith Hidden Variables: The E MAlgorithm 819
cid:12
μ p x n
i ij j i
j
cid:12
Σ p x μ x μ cid:12 n
i ij j i j i i
j
w n N
i i
where N is the total number of data points. The E-step or expectation step can be viewed
ascomputingtheexpectedvaluesp ofthehiddenindicatorvariables Z where Z is1if
I ND IC AT OR VA RI AB LE ij ij ij
datumx wasgeneratedbytheithcomponentand0otherwise. The M-stepormaximization
j
step finds the new values of the parameters that maximize the log likelihood of the data
giventheexpectedvaluesofthehidden indicatorvariables.
Thefinalmodelthat E Mlearnswhenitisappliedtothedatain Figure20.11aisshown
in Figure 20.11c; it is virtually indistinguishable from the original model from which the
data were generated. Figure 20.12a plots the log likelihood of the data according to the
currentmodelas E Mprogresses.
There are two points to notice. First the log likelihood for the final learned model
slightly exceeds that of the original model from which the data were generated. This might
seem surprising but it simply reflects the fact that the data were generated randomly and
might not provide an exact reflection of the underlying model. The second point is that E M
increases theloglikelihood ofthedataateveryiteration. Thisfactcanbeprovedingeneral.
Furthermore under certain conditions that hold in ost cases E M can be proven to reach
a local maximum in likelihood. In rare cases it could reach a saddle point or even a local
minimum. Inthis sense E Mresembles agradient-based hill-climbing algorithm butnotice
thatithasnostepsizeparameter.
700
600
500
400
300
200
100
0
-100
-200
Ldoohilekil-go L
Iteration number
Ldoohilekil-go L
Iteration number
a b
Figure20.12 Graphsshowingtheloglikelihoodofthedata L asafunctionofthe E M
iteration.Thehorizontallineshowstheloglikelihoodaccordingtothetruemodel.a Graph
for the Gaussian mixture model in Figure 20.11. b Graph for the Bayesian network in
Figure20.13a.
P Bag1
θ
Bag C
Bag P Fcherry B
θ
θ
Flavor Wrapper Hole X
a b
Figure20.13 a Amixturemodelforcandy. Theproportionsofdifferentflavorswrap-
perspresenceofholesdependonthebagwhichisnotobserved. b Bayesiannetworkfor
a Gaussianmixture. Themeanandcovarianceoftheobservablevariables Xdependonthe
component C.
Things do not always go as well as Figure 20.12a might suggest. It can happen for
examplethatone Gaussiancomponentshrinkssothatitcoversjustasingledatapoint. Then
its variance will go to zero and its likelihood will go to infinity! Another problem is that
twocomponents canmergeacquiringidenticalmeansandvariancesandsharingtheirdata
points. These kinds of degenerate local maxima are serious problems especially in high
dimensions. One solution is to place priors on the model parameters and to apply the M AP
version of E M. Another is to restart a component with new random parameters if it gets too
smallortooclosetoanothercomponent. Sensibleinitialization alsohelps.
To learn a Bayesian network with hidden variables we apply the same insights that worked
formixturesof Gaussians. Figure20.13represents asituation inwhichtherearetwobagsof
candies that have been mixed together. Candies are described by three features: in addition
to the Flavor and the Wrapper some candies have a Hole in the middle and some do not.
The distribution of candies in each bag is described by a naive Bayes model: the features
are independent given the bag but the conditional probability distribution for each feature
depends on the bag. The parameters are as follows: θ is the prior probability that a candy
comes from Bag 1; θ and θ are the probabilities that the flavor is cherry given that the
F1 F2
candy comes from Bag 1or Bag 2 respectively; θ and θ give the probabilities that the
W1 W2
wrapperisred;and θ andθ givetheprobabilities thatthecandy hasahole. Noticethat
H1 H2
the overall model is a mixture model. In fact we can also model the mixture of Gaussians
as a Bayesian network as shown in Figure 20.13b. In the figure the bag is a hidden
variable because once thecandies have been mixedtogether weno longer know which bag
each candy came from. In such a case can we recover the descriptions of the two bags by
Section20.3. Learningwith Hidden Variables: The E MAlgorithm 821
observingcandiesfromthemixture? Letusworkthroughaniterationof E Mforthisproblem.
Firstletslookatthedata. Wegenerated 1000samplesfrom amodelwhosetrueparameters
areasfollows:
θ0.5 θ θ θ 0.8 θ θ θ 0.3. 20.7
F1 W1 H1 F2 W2 H2
That is the candies are equally likely to come from either bag; the first is mostly cherries
with red wrappers and holes; the second is mostly limes with green wrappers and no holes.
Thecountsfortheeightpossible kindsofcandyareasfollows:
W red W green
H1 H0 H1 H0
F cherry 273 93 104 90
F lime 79 100 94 167
Westartbyinitializing theparameters. Fornumerical simplicity wearbitrarily choose5
θ00.6 θ0 θ0 θ0 0.6 θ0 θ0 θ0 0.4. 20.8
F1 W1 H1 F2 W2 H2
First let us work on the θ parameter. In the fully observable case we would estimate this
directly fromthe observed counts ofcandies frombags1and2. Because thebagisahidden
variable we calculate the expected counts instead. The expected count Nˆ Bag1 is the
sumoverallcandies oftheprobability thatthecandycame frombag1:
cid:12 N
θ1 Nˆ Bag1 N P Bag1flavor wrapper holes N .
j j j
j1
Theseprobabilities canbecomputed byanyinference algorithm for Bayesian networks. For
a naive Bayes model such as the one in our example we can do the inference by hand
using Bayesruleandapplying conditional independence:
cid:12 N
θ1
.
N
j1
i Pflavorj Bagi Pwrapperj Bagi Pholesj Bagi P Bagi
Applying thisformula to say the273 red-wrapped cherry candies withholes weget acon-
tribution of
F1 W1 H1 0.22797.
F1 W1 H1 F2 W2 H2
Continuingwiththeothersevenkindsofcandyinthetableofcountsweobtainθ10.6124.
Nowletusconsidertheotherparameters suchas θ . Inthefullyobservable casewe
F1
wouldestimatethisdirectlyfromtheobservedcountsofcherryandlimecandiesfrombag1.
Theexpected countofcherrycandiesfrombag1isgivenby
cid:12
P Bag1 Flavor cherrywrapper holes .
j j j
j:Flavorjcherry
Again these probabilities can be calculated by any Bayes net algorithm. Completing this
process weobtainthenewvaluesofalltheparameters:
θ10.6124 θ1 0.6684 θ1 0.6483 θ1 0.6558
F1 W1 H1 20.9
1 1 1
θ 0.3887θ 0.3817 θ 0.3827.
F2 W2 H2
The log likelihood of the data increases from about 2044 initially to about 2021 after
the first iteration as shown in Figure 20.12b. That is the update improves the likelihood
itself by a factor of about e23 1010. By the tenth iteration the learned model is a better
fitthantheoriginal model L 1982.214. Thereafter progress becomes veryslow. This
is not uncommon with E M and many practical systems combine E M with a gradient-based
algorithm suchas Newton Raphson see Chapter4forthelastphaseoflearning.
The general lesson from this example is that the parameter updates for Bayesian net-
work learning with hidden variables are directly available from the results of inference on
each example. Moreover only local posterior probabilities are needed for each parame-
ter. Here local means that the C PT for each variable X can be learned from posterior
i
probabilities involving just X and its parents U . Defining θ to be the C PT parameter
i i ijk
P X x U u theupdateisgivenbythenormalizedexpected countsasfollows:
i ij i ik
θ Nˆ X x U u Nˆ U u .
ijk i ij i ik i ik
Theexpectedcountsareobtainedbysummingovertheexamplescomputingtheprobabilities
P X x U u for each by using any Bayes net inference algorithm. For the exact
i ij i ik
algorithmsincluding variableeliminationalltheseprobabilities areobtainabledirectlyas
aby-product ofstandard inference withnoneed forextracomputations specific tolearning.
Moreover theinformation needed forlearning isavailable locallyforeachparameter.
Our final application of E M involves learning the transition probabilities in hidden Markov
models H MMs. Recall from Section 15.3 that a hidden Markov model can be represented
by a dynamic Bayes net with a single discrete state variable as illustrated in Figure 20.14.
Each data point consists of an observation sequence of finite length so the problem is to
learn the transition probabilities from a set of observation sequences or from just one long
sequence.
We have already worked out how to learn Bayes nets but there is one complication:
in Bayes nets each parameter is distinct; in a hidden Markov model on the other hand the
individualtransitionprobabilitiesfromstateitostatejattimetθ P X j X i
ijt t1 t
are repeated across timethat is θ θ for all t. To estimate the transition probability
ijt ij
from state i to state j we simply calculate the expected proportion of times that the system
undergoes atransition tostate j wheninstatei:
cid:12 cid:12
θ Nˆ X j X i Nˆ X i.
ij t1 t t
t t
Theexpectedcountsarecomputedbyan H MMinferencealgorithm. Theforwardbackward
algorithm shown in Figure 15.4 can be modified very easily to compute the necessary prob-
abilities. One important point is that the probabilities required are obtained by smoothing
Section20.3. Learningwith Hidden Variables: The E MAlgorithm 823
P R 0 R t0 P 0 R .71 P R 0 R t0 P 0 R .71 R t1 P 0 R .72 R t2 P 0 R .73 R t3 P 0 R .74
Rain Rain Rain Rain Rain Rain Rain
Umbrella 1 Umbrella 1 Umbrella 2 Umbrella 3 Umbrella 4
R 1 P U 1 R 1 P U 1 R 2 P U 2 R 3 P U 3 R 4 P U 4
t 0.9 t 0.9 t 0.9 t 0.9 t 0.9
f 0.2 f 0.2 f 0.2 f 0.2 f 0.2
modelrepeatof Figure15.16.
rather than filtering; that is we need to pay attention to subsequent evidence in estimating
theprobability thataparticulartransition occurred. The evidence inamurdercaseisusually
obtained afterthecrimei.e.thetransition fromstate itostatejhastakenplace.
We have seen several instances of the E M algorithm. Each involves computing expected
values ofhidden variables foreach example and then recomputing the parameters using the
expected values as if they were observed values. Let x be all the observed values in all the
examples let Z denote all the hidden variables for all the examples and let θ be all the
parameters fortheprobability model. Thenthe E Malgorithm is
cid:12
θi1 argmax P Zzxθi Lx Zzθ.
θ
z
Thisequationisthe E Malgorithminanutshell. The E-stepisthecomputationofthesumma-
tionwhichistheexpectationoftheloglikelihoodofthecompleteddatawithrespecttothe
distribution P Zzxθiwhichistheposterioroverthehiddenvariablesgiventhedata.
The M-step is the maximization of this expected log likelihood with respect to the parame-
ters. Formixturesof Gaussiansthehiddenvariablesarethe Z swhere Z is1ifexamplej
ij ij
wasgeneratedbycomponent i. For Bayesnets Z isthevalueofunobserved variable X in
ij i
examplej. For H MMs Z isthestateofthesequence inexamplej attimet. Startingfrom
jt
the general form itis possible toderive an E M algorithm for aspecific application once the
appropriate hiddenvariables havebeenidentified.
As soon as we understand the general idea of E M it becomes easy to derive all sorts
of variants and improvements. For example in many cases the E-stepthe computation of
posteriors over the hidden variablesis intractable as in large Bayes nets. It turns out that
one can use an approximate E-step and still obtain an effective learning algorithm. With a
samplingalgorithm suchas M CM Csee Section14.5thelearning processisveryintuitive:
each state configuration of hidden and observed variables visited by M CM C is treated ex-
actlyasifitwereacompleteobservation. Thustheparameters canbeupdated directly after
each M CM Ctransition. Otherformsofapproximate inference suchasvariational andloopy
methods havealsoprovedeffectiveforlearning verylarge networks.
In Section 20.2.5 we discussed the problem of learning Bayes net structures with complete
data. When unobserved variables may be influencing the data that are observed things get
moredifficult. Inthesimplestcaseahumanexpertmighttellthelearningalgorithmthatcer-
tainhidden variables exist leaving ittothealgorithm tofindaplacefortheminthenetwork
structure. Forexampleanalgorithmmighttrytolearnthestructureshownin Figure20.10a
onpage817giventheinformationthat Heart Disease athree-valuedvariableshouldbein-
cludedinthemodel. Asinthecomplete-datacasetheoverallalgorithmhasanouterloopthat
searchesoverstructuresandaninnerloopthatfitsthenetworkparametersgiventhestructure.
If the learning algorithm is not told which hidden variables exist then there are two
choices: either pretend that the data is really completewhich may force the algorithm to
learn aparameter-intensive model such astheonein Figure 20.10bor invent newhidden
variablesinordertosimplifythemodel. Thelatterapproachcanbeimplementedbyincluding
newmodificationchoicesinthestructuresearch: inadditiontomodifyinglinksthealgorithm
canaddordeleteahiddenvariableorchangeitsarity. Ofcoursethealgorithmwillnotknow
that the new variable it has invented is called Heart Disease; nor will it have meaningful
namesforthevalues. Fortunately newlyinventedhiddenvariableswillusuallybeconnected
topreexistingvariablessoahumanexpertcanofteninspectthelocalconditionaldistributions
involving thenewvariableandascertain itsmeaning.
Asinthecomplete-datacasepuremaximum-likelihoodstructurelearningwillresultin
acompletely connected network moreover one withno hidden variables so some form of
complexity penaltyisrequired. Wecanalsoapply M CM Ctosamplemanypossible network
structures thereby approximating Bayesian learning. For example wecanlearnmixtures of
Gaussianswithanunknownnumberofcomponentsbysamplingoverthenumber;theapprox-
imateposteriordistributionforthenumberof Gaussiansis givenbythesamplingfrequencies
ofthe M CM Cprocess.
For the complete-data case the inner loop to learn the parameters is very fastjust a
matter of extracting conditional frequencies from the data set. When there are hidden vari-
ables the inner loop may involve many iterations of E M ora gradient-based algorithm and
eachiterationinvolvesthecalculationofposteriorsina Bayesnetwhichisitselfan N P-hard
problem. To date this approach has proved impractical for learning complex models. One
possibleimprovementistheso-called structural E Malgorithm whichoperatesinmuchthe
S TR UC TU RA LE M
same way as ordinary parametric E M except that the algorithm can update the structure
as well as the parameters. Just as ordinary E M uses the current parameters to compute the
expected counts in the E-step and then applies those counts in the M-step to choose new
parametersstructural E Musesthecurrentstructuretocomputeexpectedcountsandthenap-
pliesthose counts inthe M-steptoevaluate thelikelihood forpotential newstructures. This
contrasts with the outer-loopinner-loop method which computes new expected counts for
each potential structure. In this way structural E M may make several structural alterations
tothenetworkwithoutoncerecomputingtheexpectedcountsandiscapableoflearningnon-
trivial Bayes net structures. Nonetheless much work remains to be done before we can say
thatthestructure-learning problem issolved.
21
R EI NF OR CE ME NT
L EA RN IN G
In which we examine how an agent can learn from success and failure from re-
wardandpunishment.
Chapters1819and20coveredmethodsthatlearnfunctionslogicaltheoriesandprobability
modelsfrom examples. Inthischapter wewillstudyhowagents canlearn whattodointhe
absenceoflabeledexamplesofwhattodo.
Consider for example the problem of learning to play chess. A supervised learning
agent needs tobe told the correct moveforeach position it encounters but such feedback is
seldom available. In the absence of feedback from a teacher an agent can learn a transition
modelforitsownmovesandcan perhaps learn topredict theopponents moves butwithout
somefeedbackaboutwhatisgoodandwhatisbadtheagentwillhavenogroundsfordecid-
ing which moveto make. Theagent needs toknow that something good has happened when
it accidentally checkmates the opponent and that something bad has happened when it is
checkmatedor vice versa if the game is suicide chess. This kind of feedback is called a
rewardorreinforcement. Ingameslikechessthereinforcementisreceivedonlyattheend
R EI NF OR CE ME NT
of the game. In other environments the rewards come more frequently. In ping-pong each
point scored can be considered a reward; when learning to crawl any forward motion is an
achievement. Our framework for agents regards the reward as part of the input percept but
the agent must be hardwired to recognize that part as a reward rather than as just another
sensory input. Thus animalsseemtobehardwired torecognize painandhungerasnegative
rewards and pleasure andfood intake aspositive rewards. Reinforcement has been carefully
studiedbyanimalpsychologists forover60years.
Rewards were introduced in Chapter 17 where they served to define optimal policies
in Markov decision processes M DPs. An optimal policy is a policy that maximizes the
expectedtotalreward. Thetaskofreinforcementlearningistouseobservedrewardstolearn
an optimal ornearly optimal policy for the environment. Whereas in Chapter 17 the agent
hasacompletemodeloftheenvironment andknowstherewardfunctionhereweassumeno
830
Section21.1. Introduction 831
priorknowledge ofeither. Imagineplaying anew gamewhoserules youdont know; aftera
hundred orso moves your opponent announces You lose. This is reinforcement learning
inanutshell.
In many complex domains reinforcement learning is the only feasible way to train a
program toperform athighlevels. Forexampleingameplaying itisveryhardforahuman
toprovideaccurateandconsistentevaluationsoflargenumbersofpositions whichwouldbe
needed to train an evaluation function directly from examples. Instead the program can be
told when it has won or lost and it can use this information to learn an evaluation function
thatgivesreasonablyaccurateestimatesoftheprobabilityofwinningfromanygivenposition.
Similarlyitisextremelydifficulttoprogramanagenttoflyahelicopter;yetgivenappropriate
negativerewardsforcrashing wobblingordeviatingfrom asetcourseanagentcanlearnto
flybyitself.
Reinforcementlearningmightbeconsidered toencompassallof A I:anagentisplaced
in an environment and must learn to behave successfully therein. To keep the chapter man-
ageable wewillconcentrate onsimpleenvironments andsimpleagentdesigns. Forthemost
part we will assume a fully observable environment so that the current state is supplied by
each percept. On the other hand we will assume that the agent does not know how the en-
vironment worksorwhatitsactions doandwewillallow forprobabilistic action outcomes.
Thus the agent faces an unknown Markov decision process. We will consider three of the
agentdesignsfirstintroduced in Chapter2:
Autility-basedagentlearnsautilityfunctiononstatesandusesittoselectactionsthat
maximizetheexpectedoutcomeutility.
A Q-learning agent learns an action-utility function or Q-function giving the ex-
Q-L EA RN IN G
pectedutilityoftakingagivenactioninagivenstate.
Q-F UN CT IO N
Areflexagentlearnsapolicythatmapsdirectly fromstatestoactions.
Autility-based agentmustalsohaveamodel oftheenvironment inordertomakedecisions
because itmustknowthestates towhichitsactions willlead. Forexample inordertomake
useofabackgammonevaluationfunction abackgammonprogrammustknowwhatitslegal
moves are and how they affect the board position. Only in this way can it apply the utility
function to the outcome states. A Q-learning agent on the other hand can compare the
expectedutilitiesforitsavailable choiceswithoutneedingtoknowtheiroutcomes soitdoes
not need a model of the environment. On the other hand because they do not know where
theiractionslead Q-learningagentscannotlookahead;thiscanseriouslyrestricttheirability
tolearn asweshallsee.
Webegin in Section 21.2 with passive learning where the agents policy is fixed and
P AS SI VE LE AR NI NG
thetaskistolearntheutilitiesofstatesorstateaction pairs;thiscouldalsoinvolvelearning
amodeloftheenvironment. Section21.3covers active learning wheretheagent mustalso
A CT IV EL EA RN IN G
learn what to do. The principal issue is exploration: an agent must experience as much as
E XP LO RA TI ON
possible ofitsenvironment inordertolearn howtobehave in it. Section 21.4discusses how
an agent can use inductive learning to learn much faster from its experiences. Section 21.5
covers methods forlearning direct policy representations in reflexagents. Anunderstanding
of Markovdecisionprocesses Chapter17isessentialforthischapter.
Tokeep things simple westart with the case of a passive learning agent using a state-based
representation in a fully observable environment. In passive learning the agents policy π
is fixed: in state s it always executes the action πs. Its goal is simply to learn how good
the policy isthat is to learn the utility function Uπs. We will use as our example the
43 world introduced in Chapter 17. Figure 21.1 shows a policy for that world and the
corresponding utilities. Clearly the passive learning task is similar to the policy evaluation
task part of the policy iteration algorithm described in Section 17.3. The main difference
is that the passive learning agent does not know the transition model
Pscid:2sa
which
cid:2
specifies the probability of reaching state s from state s after doing action a; nor does it
knowtherewardfunction Rswhichspecifiestherewardforeachstate.
a b
rewardsof Rs 0.04inthenonterminalstatesandnodiscounting. b Theutilitiesof
thestatesinthe43worldgivenpolicyπ.
Theagentexecutesasetoftrialsintheenvironmentusingitspolicyπ. Ineachtrialthe
T RI AL
agent starts in state 11 and experiences a sequence of state transitions until it reaches one
oftheterminal states 42or43. Itspercepts supply boththecurrent stateandthereward
receivedinthatstate. Typicaltrialsmightlooklikethis:
11 cid:212 cid:213 cid:212 cid:213 cid:223 cid:233 cid:243
-.04 -.04 -.04 -.04 -.04 -.04 -.04 1
11 cid:212 cid:213 cid:223 cid:233 cid:232 cid:233 cid:243
-.04 -.04 -.04 -.04 -.04 -.04 -.04 1
11 cid:221 cid:231 cid:232 cid:242 .
-.04 -.04 -.04 -.04 -1
Notethat each state percept is subscripted withthe reward received. Theobject istouse the
informationaboutrewardstolearntheexpectedutility Uπsassociatedwitheachnontermi-
nal state s. Theutility is defined to be the expected sum of discounted rewards obtained if
Section21.2. Passive Reinforcement Learning 833
policyπ isfollowed. Asin Equation17.2onpage650wewrite
""
cid:12
Uπs E γt R S 21.1
t
t0
where Rsistherewardforastate S arandomvariableisthestatereachedattimetwhen
t
executing policy π and S s. Wewillinclude a discountfactor γ inall of ourequations
0
butforthe43worldwewillsetγ1.
D IR EC TU TI LI TY A simple method for direct utility estimation was invented in the late 1950s in the area of
E ST IM AT IO N
A DA PT IV EC ON TR OL adaptive control theory by Widrow and Hoff 1960. The idea is that the utility of a state
T HE OR Y
is the expected total reward from that state onward called the expected reward-to-go and
R EW AR D-T O-G O
eachtrialprovides a sampleofthisquantity foreachstatevisited. Forexample thefirsttrial
in the set of three given earlier provides a sample total reward of 0.72 for state 11 two
samples of0.76 and 0.84 for12 twosamples of0.80 and 0.88 for13 and so on. Thus
attheendofeachsequencethealgorithmcalculatestheobservedreward-to-goforeachstate
andupdatestheestimatedutilityforthatstateaccordingly justbykeeping arunningaverage
foreachstateinatable. Inthelimitofinfinitelymanytrialsthesampleaveragewillconverge
tothetrueexpectation in Equation21.1.
It is clear that direct utility estimation is just an instance of supervised learning where
each example has the state as input and the observed reward-to-go as output. This means
that we have reduced reinforcement learning to a standard inductive learning problem as
discussed in Chapter18. Section 21.4discusses theuseofmorepowerful kinds ofrepresen-
tations for the utility function. Learning techniques for those representations can be applied
directlytotheobserveddata.
Direct utility estimation succeeds in reducing the reinforcement learning problem to
an inductive learning problem about which much is known. Unfortunately it misses a very
important source of information namely the fact that the utilities of states are not indepen-
dent! Theutility ofeachstateequals itsownrewardplus theexpected utility ofitssuccessor
states. That is the utility values obey the Bellman equations for a fixed policy see also
Equation17.10:
cid:12
Uπs Rsγ Pscid:2sπs Uπscid:2 . 21.2
scid:3
Byignoringtheconnections betweenstatesdirectutility estimation missesopportunities for
learning. For example the second of the three trials given earlier reaches the state 32
which has not previously been visited. The next transition reaches 33 which is known
from the first trial to have a high utility. The Bellman equation suggests immediately that
32isalsolikelytohaveahighutility because itleadsto33butdirectutilityestimation
learns nothing until the end of the trial. More broadly we can view direct utility estimation
as searching for U in a hypothesis space that is much larger than it needs to be in that it
includes many functions that violate the Bellman equations. For this reason the algorithm
oftenconverges veryslowly.
function P AS SI VE-A DP-A GE NTperceptreturnsanaction
inputs:perceptaperceptindicatingthecurrentstatescid:5andrewardsignalrcid:5
persistent: πafixedpolicy
mdpan M DPwithmodel Prewards Rdiscountγ
Uatableofutilitiesinitiallyempty
Nsaatableoffrequenciesforstateactionpairsinitiallyzero
Nscid:3saatableofoutcomefrequenciesgivenstateactionpairsinitiallyzero
sathepreviousstateandactioninitiallynull
ifscid:5isnewthen Uscid:5rcid:5;Rscid:5rcid:5
ifs isnotnullthen
increment Nsasaand Nscid:3sascid:5sa
foreacht suchthat Nscid:3satsaisnonzerodo
Ptsa Nscid:3satsa Nsasa
U P OL IC Y-E VA LU AT IO Nπ Umdp
ifscid:5.T ER MI NA L?thensanullelsesascid:5πscid:5
returna
Figure21.2 Apassivereinforcementlearningagentbasedonadaptivedynamicprogram-
ming. The P OL IC Y-E VA LU AT IO N function solves the fixed-policy Bellman equations as
describedonpage657.
A DA PT IV ED YN AM IC An adaptive dynamic programming or A DP agent takes advantage of the constraints
P RO GR AM MI NG
among the utilities of states by learning the transition model that connects them and solv-
ing the corresponding Markov decision process using a dynamic programming method. For
apassivelearningagentthismeanspluggingthelearnedtransitionmodel Pscid:2sπsand
the observed rewards Rs into the Bellman equations 21.2 to calculate the utilities of the
states. As we remarked in our discussion of policy iteration in Chapter 17 these equations
are linear no maximization involved so they can be solved using any linear algebra pack-
age. Alternatively we can adopt the approach of modified policy iteration see page 657
using a simplified value iteration process to update the utility estimates after each change to
the learned model. Because the model usually changes only slightly with each observation
the value iteration process can use the previous utility estimates as initial values and should
converge quitequickly.
The process of learning the model itself is easy because the environment is fully ob-
servable. Thismeansthatwehaveasupervisedlearningtaskwheretheinputisastateaction
pair and the output is the resulting state. In the simplest case we can represent the tran-
sition model as a table of probabilities. We keep track of how often each action outcome
occurs and estimate the transition probability
Pscid:2sa
from the frequency with which
scid:2
is reached when executing a in s. Forexample in the three trials given on page 832 Right
is executed three times in 13 and two out of three times the resulting state is 23 so
P2313 Rightisestimatedtobe23.
Section21.2. Passive Reinforcement Learning 835
1
0.8
0.6
0.4
0.2
0
setamitse
ytilit U
0.6
43
33 0.5
13
0.4
11
32
0.3
0.2
0.1
0
Number of trials
ytilitu
ni
rorre
S MR
Number of trials
a b
Figure21.3 Thepassive A DPlearningcurvesforthe43worldgiventheoptimalpolicy
shownin Figure21.1. a Theutilityestimatesforaselectedsubsetofstates asa function
ofthenumberoftrials. Noticethelargechangesoccurringaroundthe78thtrialthisisthe
first time that the agent falls into the 1 terminal state at 42. b The root-mean-square
errorsee Appendix Aintheestimatefor U11averagedover20runsof100trialseach.
The full agent program for a passive A DP agent is shown in Figure 21.2. Its perfor-
mance on the 43 world is shown in Figure 21.3. In terms of how quickly its value es-
timates improve the A DP agent is limited only by its ability to learn the transition model.
In this sense it provides a standard against which to measure other reinforcement learning
algorithms. Itishowever intractable forlarge statespaces. Inbackgammon forexample it
wouldinvolve solvingroughly 1050 equations in1050 unknowns.
Areaderfamiliarwiththe Bayesianlearning ideasof Chapter20willhavenoticed that
the algorithm in Figure 21.2 is using maximum-likelihood estimation to learn the transition
model; moreover by choosing a policy based solely on the estimated model it is acting as
if the model were correct. This is not necessarily a good idea! For example a taxi agent
that didnt know about how traffic lights might ignore a red light once or twice without no
ill effects and then formulate a policy to ignore red lights from then on. Instead it might
be a good idea to choose a policy that while not optimal for the model estimated by maxi-
mumlikelihood worksreasonablywellforthewholerangeof modelsthathaveareasonable
chanceofbeingthetruemodel. Therearetwomathematicalapproaches thathavethisflavor.
B AY ES IA N
The first approach Bayesian reinforcement learning assumes a prior probability
R EI NF OR CE ME NT
L EA RN IN G Phforeachhypothesis haboutwhatthetruemodelis;theposteriorprobability Pheis
obtainedintheusualwayby Bayesrulegiventheobservationstodate. Theniftheagenthas
decided tostop learning the optimal policy istheone that gives thehighest expected utility.
Let uπ be the expected utility averaged over all possible start states obtained by executing
h
policyπ inmodelh. Thenwehave
cid:12
π argmax Pheuπ .
h
π
h
In somespecial cases this policy can even be computed! If the agent willcontinue learning
in the future however then finding an optimal policy becomes considerably more difficult
because the agent must consider the effects of future observations on its beliefs about the
transition model. Theproblem becomes a P OM DPwhosebelief states aredistributions over
models. This concept provides an analytical foundation for understanding the exploration
problem described in Section21.3.
R OB US TC ON TR OL Thesecond approach derived from robustcontroltheory allowsforasetofpossible
T HE OR Y
models Handdefinesanoptimalrobustpolicyasonethatgivesthebestoutcomeintheworst
caseover H:
π argmaxminuπ .
h
π h
Often theset H willbethesetofmodels thatexceedsomelikelihood threshold on Phe
so the robust and Bayesian approaches are related. Sometimes the robust solution can be
computed efficiently. There are moreover reinforcement learning algorithms that tend to
produce robustsolutions althoughwedonotcoverthemhere.
Solving the underlying M DP as in the preceding section is not the only way to bring the
Bellman equations to bear on the learning problem. Another way is to use the observed
transitions to adjust the utilities of the observed states so that they agree with the constraint
equations. Consider for example the transition from 13 to 23 in the second trial on
and Uπ230.92. Nowifthis transition occurred allthe timewewould expect theutili-
tiestoobeytheequation
Uπ13 0.04 Uπ23
so Uπ13wouldbe0.88. Thusitscurrentestimateof0.84mightbealittlelowandshould
cid:2
be increased. More generally when a transition occurs from state s to state s weapply the
followingupdateto Uπs:
Uπs Uπsα Rsγ Uπscid:2 Uπs. 21.3
Hereαisthelearningrateparameter. Becausethisupdateruleusesthedifferenceinutilities
T EM PO RA L- betweensuccessivestates itisoftencalledthetemporal-difference or T Dequation.
D IF FE RE NC E
All temporal-difference methods work by adjusting the utility estimates towards the
idealequilibrium thatholdslocally whentheutility estimates arecorrect. Inthecaseofpas-
sivelearning theequilibrium isgiven by Equation 21.2. Now Equation 21.3 does in fact
cause theagenttoreachtheequilibrium givenby Equation 21.2 butthere issomesubtlety
cid:2
involved. First notice that the update involves only the observed successor s whereas the
actualequilibriumconditionsinvolveallpossiblenextstates. Onemightthinkthatthiscauses
animproperly largechangein Uπswhenaveryraretransition occurs;butinfactbecause
rare transitions occur only rarely the average value of Uπs will converge to the correct
value. Furthermore if we change α from a fixed parameter to a function that decreases as
thenumberoftimesastate hasbeen visited increases then Uπsitself willconverge tothe
Section21.2. Passive Reinforcement Learning 837
function P AS SI VE-T D-A GE NTperceptreturnsanaction
inputs:perceptaperceptindicatingthecurrentstatescid:5andrewardsignalrcid:5
persistent: πafixedpolicy
Uatableofutilitiesinitiallyempty
Nsatableoffrequenciesforstatesinitiallyzero
sarthepreviousstateactionandrewardinitiallynull
ifscid:5isnewthen Uscid:5rcid:5
ifs isnotnullthen
increment Nss
Us Us α Nssr γ Uscid:5 Us
ifscid:5.T ER MI NA L?thensarnullelsesarscid:5πscid:5rcid:5
returna
Figure21.4 Apassivereinforcementlearningagentthatlearnsutilityestimatesusingtem-
poraldifferences.Thestep-sizefunctionαnischosentoensureconvergenceasdescribed
inthetext.
correct value.1 Thisgivesustheagentprogram shownin Figure21.4. Figure21.5illustrates
theperformance ofthepassive T Dagentonthe 43world. Itdoesnotlearnquiteasfastas
the A DPagent and shows muchhigher variability but itismuch simplerand requires much
lesscomputationperobservation. Noticethat T Ddoesnotneedatransitionmodeltoperform
itsupdates. Theenvironment suppliestheconnection betweenneighboring statesintheform
ofobserved transitions.
The A DPapproach andthe T Dapproach areactually closely related. Bothtrytomake
local adjustments tothe utility estimates inordertomakeeach state agree withitssucces-
sors. One difference is that T D adjusts a state to agree with its observed successor Equa-
tion 21.3 whereas A DP adjusts the state to agree with all of the successors that might
occur weighted by their probabilities Equation 21.2. This difference disappears when
the effects of T D adjustments are averaged over a large number of transitions because the
frequencyofeachsuccessorinthesetoftransitionsisapproximatelyproportionaltoitsprob-
ability. A more important difference is that whereas T D makes a single adjustment per ob-
served transition A DP makes as many as it needs to restore consistency between the utility
estimates U and the environment model P. Although the observed transition makes only a
local change in P its effects might need to be propagated throughout U. Thus T D can be
viewedasacrudebutefficientfirstapproximation to A DP.
Each adjustment made by A DP could be seen from the T D point of view as a re-
sult of a pseudoexperience generated by simulating the current environment model. It
is possible to extend the T D approach to use an environment model to generate several
pseudoexperiencestransitionsthatthe T Dagentcanimaginemighthappengivenitscurrent
model. Foreachobservedtransition the T Dagentcangenerate alargenumberofimaginary
satisfiestheconditions.
1
0.8
0.6
0.4
0.2
0
setamitse
ytilit U
0.6
43
33 0.5
13
11 0.4
21
0.3
0.2
0.1
0
Number of trials
ytilitu
ni
rorre
S MR
Number of trials
a b
Figure21.5 The T D learningcurvesforthe 43 world. a Theutility estimates fora
selectedsubsetofstatesasafunctionofthenumberoftrials. b Theroot-mean-squareerror
intheestimatefor U11averagedover20runsof500trialseach. Onlythefirst100trials
areshowntoenablecomparisonwith Figure21.3.
transitions. Inthiswaytheresultingutilityestimateswillapproximatemoreandmoreclosely
thoseof A DPofcourse attheexpenseofincreased computation time.
Ina similar vein wecan generate more efficient versions of A DPby directly approxi-
mating the algorithms forvalue iteration orpolicy iteration. Even though the value iteration
algorithm is efficient it is intractable if we have say 10100 states. However many of the
necessary adjustments to the state values on each iteration will be extremely tiny. One pos-
sible approach to generating reasonably good answers quickly is to bound the number of
adjustmentsmadeaftereachobservedtransition. Onecanalsouseaheuristictorankthepos-
P RI OR IT IZ ED sibleadjustmentssoastocarryoutonlythemostsignificant ones. Theprioritizedsweeping
S WE EP IN G
heuristic preferstomakeadjustments tostateswhose likelysuccessors havejustundergone a
large adjustment in their own utility estimates. Using heuristics like this approximate A DP
algorithmsusuallycanlearnroughlyasfastasfull A DPintermsofthenumberoftrainingse-
quences butcanbeseveralordersofmagnitudemoreefficientintermsofcomputation. See
Exercise 21.3. This enables them to handle state spaces that are far too large for full A DP.
Approximate A DPalgorithms haveanadditional advantage: intheearlystages oflearning a
new environment the environment model P often will be far from correct so there is little
pointincalculatinganexactutilityfunctiontomatchit. Anapproximationalgorithmcanuse
aminimumadjustmentsizethatdecreasesastheenvironment modelbecomesmoreaccurate.
This eliminates the very long value iterations that can occur early in learning due to large
changes inthemodel.
Section21.3. Active Reinforcement Learning 839
Apassivelearningagenthasafixedpolicythatdeterminesitsbehavior. Anactiveagentmust
decide whatactions totake. Letusbeginwiththeadaptive dynamicprogramming agent and
considerhowitmustbemodifiedtohandlethisnewfreedom.
First the agent will need to learn a complete model with outcome probabilities for all
actions ratherthanjust themodelforthefixedpolicy. Thesimplelearning mechanism used
by P AS SI VE-A DP-A GE NT will do just fine for this. Next we need to take into account the
fact thatthe agent hasachoice ofactions. Theutilities itneeds tolearn arethose defined by
theoptimalpolicy;theyobeythe Bellmanequationsgivenonpage652whichwerepeathere
forconvenience:
cid:12
Us Rsγ max
Pscid:2sa Uscid:2
. 21.4
a
scid:3
These equations can be solved to obtain the utility function U using the value iteration or
policyiterationalgorithmsfrom Chapter17. Thefinalissueiswhattodoateachstep. Having
obtained a utility function U that is optimal for the learned model the agent can extract an
optimal action by one-step look-ahead to maximize the expected utility; alternatively if it
uses policy iteration the optimal policy is already available so it should simply execute the
actiontheoptimalpolicyrecommends. Orshouldit?
recommendation of the optimal policy for the learned model at each step. The agent does
not learn the true utilities or the true optimal policy! What happens instead is that in the
39th trial it finds a policy that reaches the 1 reward along the lower route via 21 31
32 and 33. See Figure 21.6b. After experimenting with minor variations from the
276th trial onward it sticks to that policy never learning the utilities of the other states and
neverfindingtheoptimalroutevia1213and23. Wecallthisagentthegreedyagent.
G RE ED YA GE NT
Repeatedexperimentsshowthatthegreedyagentveryseldomconvergestotheoptimalpolicy
forthisenvironment andsometimesconverges toreallyhorrendous policies.
Howcanitbethatchoosingtheoptimalactionleadstosuboptimalresults? Theanswer
is that the learned model is not the same as the true environment; what is optimal in the
learned modelcantherefore besuboptimal inthetrueenvironment. Unfortunately theagent
does not know what the true environment is so it cannot compute the optimal action forthe
trueenvironment. Whatthenistobedone?
What the greedy agent has overlooked is that actions do more than provide rewards
according tothecurrentlearnedmodel;theyalsocontribute tolearning thetruemodelbyaf-
fectingthepercepts thatarereceived. Byimprovingthemodeltheagentwillreceivegreater
rewards in the future.2 An agent therefore must make a tradeoff between exploitation to
E XP LO IT AT IO N
maximize its rewardas reflected inits current utility estimatesand exploration to maxi-
E XP LO RA TI ON
2
1.5
1
0.5
0
ssol
ycilop
rorre
S MR
R MS error
Policy loss
1
Number of trials
a b
Figure21.6 Performanceofa greedy A DP agentthatexecutesthe actionrecommended
bytheoptimalpolicyforthelearnedmodel. a R MSerrorintheutilityestimatesaveraged
over the nine nonterminal squares. b The suboptimal policy to which the greedy agent
convergesinthisparticularsequenceoftrials.
mizeitslong-term well-being. Pureexploitation risksgettingstuckinarut. Pureexploration
toimproveonesknowledgeisofnouseifoneneverputsthatknowledgeintopractice. Inthe
real world one constantly has to decide between continuing in a comfortable existence and
striking outintotheunknown inthehopes ofdiscovering anewandbetterlife. With greater
understanding lessexploration isnecessary.
Canwebe alittle more precise than this? Isthere an optimal exploration policy? This
questionhasbeenstudiedindepthinthesubfieldofstatisticaldecisiontheorythatdealswith
so-called banditproblems. Seesidebar.
B AN DI TP RO BL EM
Although bandit problems are extremely difficult to solve exactly to obtain an optimal
exploration method it is nonetheless possible to come up with a reasonable scheme that
will eventually lead to optimal behavior by the agent. Technically any such scheme needs
to be greedy in the limit of infinite exploration or G LI E. A G LI E scheme must try each
G LI E
action in each state an unbounded number of times to avoid having a finite probability that
an optimal action ismissed because of an unusually bad series of outcomes. An A DPagent
usingsuchaschemewilleventuallylearnthetrueenvironment model. A GL IEschememust
alsoeventuallybecomegreedysothattheagentsactionsbecomeoptimalwithrespecttothe
learnedandhencethetruemodel.
Thereareseveral G LI Eschemes;oneofthesimplest istohave theagentchoose aran-
dom action a fraction 1t of the time and to follow the greedy policy otherwise. While this
does eventually converge to an optimal policy it can be extremely slow. A more sensible
approach would give some weight to actions that the agent has not tried very often while
tending to avoid actions that are believed to be of low utility. This can be implemented by
altering the constraint equation 21.4 so that it assigns a higher utility estimate to relatively
Section21.3. Active Reinforcement Learning 841
E XP LO RA TI ON A ND B AN DI TS
In Las Vegas a one-armed bandit is a slot machine. A gambler can insert a coin
pullthelever and collect thewinnings ifany. Ann-armedbandithasnlevers.
The gambler must choose which lever to play on each successive cointhe one
thathaspaidoffbestormaybeonethathasnotbeentried?
Then-armedbandit problem isaformal modelforreal problems inmanyvi-
tally important areas such as deciding on the annual budget for A I research and
development. Each arm corresponds to an action such as allocating 20 million
forthedevelopmentofnew A Itextbooksandthepayofffrompullingthearmcor-
responds to the benefits obtained from taking the action immense. Exploration
whether it is exploration of a new research field orexploration of a new shopping
mallisrisky isexpensive andhasuncertain payoffs; ontheotherhand failureto
exploreatallmeansthatoneneverdiscovers anyactionsthatareworthwhile.
Toformulateabanditproblemproperlyonemustdefineexactlywhatismeant
by optimal behavior. Most definitions in the literature assume that the aim is to
maximizetheexpectedtotalrewardobtained overtheagentslifetime. Thesedefi-
nitionsrequirethattheexpectationbetakenoverthepossibleworldsthattheagent
couldbeinaswellasoverthepossibleresultsofeachactionsequenceinanygiven
world. Here aworld isdefinedbythetransition model
Pscid:2sa.
Thus inor-
derto act optimally the agent needs aprior distribution over the possible models.
Theresulting optimization problemsareusuallywildlyintractable.
Insomecasesforexamplewhenthepayoffofeachmachineisindependent
and discounted rewards are usedit is possible to calculate a Gittins index for
each slot machine Gittins 1989. The index is a function only of the number of
timestheslotmachinehasbeenplayedandhowmuchithaspaidoff. Theindexfor
eachmachineindicateshowworthwhileitistoinvestmore;generallyspeakingthe
higher the expected return and the higher the uncertainty in the utility of a given
choice the better. Choosing the machine with the highest index value gives an
optimalexplorationpolicy. Unfortunatelynowayhasbeenfoundtoextend Gittins
indicestosequential decision problems.
One can use the theory of n-armed bandits to argue for the reasonableness
of the selection strategy in genetic algorithms. See Chapter 4. If you consider
each arm in an n-armed bandit problem to be a possible string of genes and the
investment of a coin in one arm to be the reproduction of those genes then it can
beproventhatgeneticalgorithmsallocatecoinsoptimallygivenanappropriateset
ofindependence assumptions.
unexplored stateaction pairs. Essentially thisamounts toanoptimisticprioroverthepossi-
bleenvironments andcauses theagenttobehave initially as ifthere werewonderful rewards
scattered alloverthe place. Letus use Ustodenote the optimistic estimate of the utility
i.e.theexpected reward-to-go ofthestate sandlet Nsabethenumberoftimesaction
a has been tried in state s. Suppose we are using value iteration in an A DP learning agent;
thenweneedtorewritetheupdateequation Equation17.6 onpage652toincorporate the
optimisticestimate. Thefollowingequation doesthis:
cid:13 cid:14
cid:2
Us Rsγ max f Pscid:2sa Uscid:2 Nsa . 21.5
a scid:3
E XP LO RA TI ON Here fun is called the exploration function. It determines how greed preference for
F UN CT IO N
high values of u is traded off against curiosity preference for actions that have not been
tried often and have low n. The function fun should be increasing in u and decreasing
inn. Obviously therearemanypossible functions thatfitthese conditions. Oneparticularly
simpledefinitionis
cid:24
R ifn N
fun e
u otherwise
where R isanoptimisticestimateofthebestpossiblerewardobtainableinanystateand N
e
is a fixed parameter. This will have the effect of making the agent try each actionstate pair
atleast N times.
e
The fact that U rather than U appears on the right-hand side of Equation 21.5 is
veryimportant. Asexploration proceedsthestatesandactionsnearthestartstatemightwell
be tried a large number of times. If we used U the more pessimistic utility estimate then
the agent would soon become disinclined to explore further afield. The use of U means
that the benefits of exploration are propagated back from the edges of unexplored regions
so that actions that lead toward unexplored regions are weighted more highly rather than
just actions that are themselves unfamiliar. Theeffect of this exploration policy can be seen
clearlyin Figure21.7whichshowsarapidconvergence towardoptimalperformance unlike
thatofthegreedyapproach. Averynearlyoptimalpolicyisfoundafterjust18trials. Notice
that the utility estimates themselves do not converge as quickly. This is because the agent
stops exploring the unrewarding parts of the state space fairly soon visiting them only by
accidentthereafter. Howeveritmakesperfectsensefortheagentnottocareabouttheexact
utilities ofstatesthatitknowsareundesirable andcanbeavoided.
Nowthatwehavean active A DPagent letusconsider how toconstruct anactivetemporal-
difference learning agent. The most obvious change from the passive case is that the agent
is no longer equipped with a fixed policy so if it learns a utility function U it will need to
learn a model in order to be able to choose an action based on U via one-step look-ahead.
Themodelacquisition problemforthe T Dagentisidenticaltothatforthe A DPagent. What
ofthe T Dupdateruleitself? Perhapssurprisingly theupdaterule21.3remainsunchanged.
Thismightseem odd forthefollowing reason: Supposetheagent takesastep thatnormally
Section21.3. Active Reinforcement Learning 843
2.2
2
1.8
1.6
1.4
1.2
1
0.8
0.6
setamitse
ytilit U
11 1.4
12
13 1.2
23
32 1
33
43 0.8
0.6
0.4
0.2
0
Number of trials
ssol
ycilop
rorre
S MR
R MS error
Policy loss
Number of trials
a b
Figure21.7 Performanceoftheexploratory A DPagent. using R 2and Ne 5. a
Utility estimates for selected states overtime. b The R MS error in utility values and the
associatedpolicyloss.
leadstoagooddestinationbutbecauseofnondeterminism intheenvironmenttheagentends
upinacatastrophicstate. The T Dupdaterulewilltakethisasseriouslyasiftheoutcomehad
been the normal result of the action whereas one might suppose that because the outcome
was a fluke the agent should not worry about it too much. In fact of course the unlikely
outcome will occur only infrequently in a large set of training sequences; hence in the long
run its effects will be weighted proportionally to its probability as we would hope. Once
againitcanbeshownthatthe T Dalgorithm willconverge tothesamevaluesas A DPasthe
numberoftrainingsequences tendstoinfinity.
There is an alternative T D method called Q-learning which learns an action-utility
representation instead of learning utilities. We will use the notation Qsa to denote the
valueofdoingactionainstates. Q-valuesaredirectlyrelated toutilityvaluesasfollows:
Us max Qsa. 21.6
a
Q-functions may seem like just another way of storing utility information but they have a
very important property: a T D agent that learns a Q-function does not need a model of the
form
Pscid:2sa
either for learning or for action selection. For this reason Q-learning is
called a model-free method. As with utilities we can write a constraint equation that must
holdatequilibrium whenthe Q-valuesarecorrect:
M OD EL-F RE E
cid:12
Qsa Rsγ
Pscid:2samax Qscid:2 acid:2
. 21.7
acid:3
scid:3
As in the A DP learning agent we can use this equation directly as an update equation for
an iteration process that calculates exact Q-values given an estimated model. This does
however require that a model also be learned because the equation uses
Pscid:2sa.
The
temporal-difference approach on the other hand requires no model of state transitionsall
function Q-L EA RN IN G-A GE NTperceptreturnsanaction
inputs:perceptaperceptindicatingthecurrentstatescid:5andrewardsignalrcid:5
persistent: Qatableofactionvaluesindexedbystateandactioninitiallyzero
Nsaatableoffrequenciesforstateactionpairsinitiallyzero
sarthepreviousstateactionandrewardinitiallynull
if T ER MI NA L?sthen Qs Nonercid:5
ifs isnotnullthen
increment Nsasa
Qsa Qsa α Nsasar γ maxacid:3 Qscid:5acid:5 Qsa
sarscid:5argmax
acid:3
f Qscid:5acid:5 Nsascid:5acid:5rcid:5
returna
Figure21.8 Anexploratory Q-learningagent. Itisanactivelearnerthatlearnsthevalue
Qsaof each actionin each situation. Ituses the same explorationfunctionf as the ex-
ploratory A DPagentbutavoidshavingtolearnthetransitionmodelbecausethe Q-valueof
astatecanberelateddirectlytothoseofitsneighbors.
itneedsarethe Qvalues. Theupdateequation for T DQ-learningis
Qsa Qsaα Rsγ max Qscid:2 acid:2 Qsa 21.8
acid:3
cid:2
whichiscalculated wheneveraction aisexecutedinstatesleading tostates.
The complete agent design for an exploratory Q-learning agent using T D is shown in
exploratory A DP agenthence the need to keep statistics on actions taken the table N. If
asimplerexploration policy isusedsay acting randomly onsome fraction ofsteps where
thefractiondecreases overtimethenwecandispense withthestatistics.
Q-learninghasacloserelativecalled S AR SAfor State-Action-Reward-State-Action.
S AR SA
Theupdaterulefor S AR SAisverysimilarto Equation21.8:
Qsa Qsaα Rsγ Qscid:2 acid:2 Qsa 21.9
cid:2 cid:2
where a is the action actually taken in state s. The rule is applied at the end of each
cid:2 cid:2
s a r s a quintuplethence the name. The difference from Q-learning is quite subtle:
whereas Q-learning backs up the best Q-value from the state reached in the observed transi-
tion S AR SAwaitsuntilanaction isactually takenand backsupthe Q-valueforthataction.
Now for a greedy agent that always takes the action with best Q-value the two algorithms
are identical. When exploration is happening however they differ significantly. Because
Q-learning usesthebest Q-value itpaysnoattention tothe actual policybeing followedit
isanoff-policylearningalgorithmwhereas S AR SAisanon-policyalgorithm. Q-learningis
O FF-P OL IC Y
moreflexiblethan S AR SAinthesensethata Q-learningagentcanlearnhowtobehavewell
O N-P OL IC Y
evenwhenguidedbyarandomoradversarial exploration policy. Ontheotherhand S AR SA
ismorerealistic: forexampleiftheoverallpolicyisevenpartlycontrolledbyotheragentsit
isbettertolearna Q-functionforwhatwillactuallyhappen ratherthanwhattheagentwould
liketohappen.
Section21.4. Generalization in Reinforcement Learning 845
Both Q-learning and S AR SA learn the optimal policy for the 43 world but do so
at a much slower rate than the A DP agent. This is because the local updates do not enforce
consistencyamongallthe Q-valuesviathemodel. Thecomparisonraisesageneralquestion:
is it better to learn a model and a utility function or to learn an action-utility function with
no model? In other words what is the best way to represent the agent function? This is
an issue at the foundations of artificial intelligence. As we stated in Chapter 1 one of the
key historical characteristics of much of A I research is its often unstated adherence to the
knowledge-based approach. This amounts to an assumption that the best way to represent
the agent function is to build a representation of some aspects of the environment in which
theagentissituated.
Some researchers both inside and outside A I have claimed that the availability of
model-free methods such as Q-learning means thatthe knowledge-based approach isunnec-
essary. Thereishoweverlittletogoonbutintuition. Ourintuitionforwhatitsworthisthat
as the environment becomes more complex the advantages of a knowledge-based approach
become more apparent. This is borne out even in games such as chess checkers draughts
and backgammon see next section where efforts to learn an evaluation function by means
ofamodelhavemetwithmoresuccessthan Q-learning methods.
Sofar wehave assumed that the utility functions and Q-functions learned by the agents are
represented in tabular form with one output value for each input tuple. Such an approach
worksreasonably wellforsmall statespaces but thetimeto convergence andfor A DPthe
timeperiterationincreaserapidlyasthespacegetslarger. Withcarefullycontrolled approx-
imate A DP methods it might be possible to handle 10000 states or more. This suffices for
two-dimensional maze-like environments but more realistic worlds are out of the question.
Backgammon and chess are tiny subsets of the real world yet their state spaces contain on
the order of 1020 and 1040 states respectively. It would be absurd to suppose that one must
visitallthesestatesmanytimesinordertolearnhowtoplay thegame!
F UN CT IO N One way to handle such problems is to use function approximation which simply
A PP RO XI MA TI ON
means using any sort of representation for the Q-function other than a lookup table. The
representation isviewed asapproximate because itmight not bethe case that the true utility
function or Q-function canberepresented inthechosen form. Forexample in Chapter5we
described an evaluation function for chess that is represented as aweighted linear function
ofasetoffeaturesorbasisfunctionsf ...f :
B AS IS FU NC TI ON 1 n
Uˆ s θ f sθ f sθ f s.
θ 1 1 2 2 n n
A reinforcement learning algorithm can learn values for the parameters θθ ...θ such
θ
values in a table this function approximator is characterized by say n20 parameters
an enormous compression. Although no one knows the true utility function for chess no
one believes that it can be represented exactly in 20 numbers. If the approximation is good
enough however theagentmightstillplayexcellent chess.3 Function approximation makes
itpracticaltorepresentutilityfunctionsforverylargestatespacesbutthatisnotitsprincipal
benefit. The compression achieved by a function approximator allows the learning agent to
generalize from states it has visited to states it has not visited. That is the most important
aspectoffunctionapproximationisnotthatitrequireslessspacebutthatitallowsforinduc-
tive generalization over input states. To give you some idea of the power of this effect: by
examiningonlyoneinevery1012 ofthepossiblebackgammonstatesitispossibletolearna
utilityfunction thatallowsaprogramtoplayaswellasanyhuman Tesauro1992.
Ontheflip side of course there isthe problem that there could fail tobe any function
in the chosen hypothesis space that approximates the true utility function sufficiently well.
As in all inductive learning there is a tradeoff between the size of the hypothesis space and
thetimeittakestolearnthefunction. Alargerhypothesis spaceincreases thelikelihood that
agoodapproximation canbefound butalsomeansthatconvergence islikelytobedelayed.
Letusbeginwiththesimplestcasewhichisdirectutilityestimation. See Section21.2.
With function approximation this is an instance of supervised learning. Forexample sup-
posewerepresenttheutilitiesforthe43worldusingasimplelinearfunction. Thefeatures
ofthesquares arejusttheir xandy coordinates sowehave
Uˆ xy θ θ xθ y . 21.10
θ 0 1 2
Thusifθ θ θ 0.50.20.1then Uˆ 110.8. Givenacollection oftrials weob-
tainasetofsamplevaluesof Uˆ xyandwecanfindthebestfitinthesenseofminimizing
θ
thesquarederror usingstandard linearregression. See Chapter18.
For reinforcement learning it makes more sense to use an online learning algorithm
that updates the parameters after each trial. Suppose we run a trial and the total reward
obtained starting at 11 is 0.4. This suggests that Uˆ 11 currently 0.8 is too large and
θ
must be reduced. How should the parameters be adjusted to achieve this? As with neural-
network learning we write an error function and compute its gradient with respect to the
parameters. If u s is the observed total reward from state s onward in the jth trial then
j
theerrorisdefined ashalf thesquared difference ofthepredicted totalandtheactual total:
E s Uˆ su s22. Therateofchangeoftheerrorwithrespecttoeachparameter
j θ j
θ is E θ sotomovetheparameterinthedirection ofdecreasing theerrorwewant
i j i
E s Uˆ s
θ θ α j θ αu s Uˆ s θ . 21.11
i i i j θ
θ θ
i i
This is called the Widrow Hoff rule or the delta rule for online least-squares. For the
W ID RO WH OF FR UL E
linearfunctionapproximator Uˆ sin Equation21.10wegetthreesimpleupdaterules:
D EL TA RU LE θ
θ θ αu s Uˆ s
θ θ αu s Uˆ sx
θ θ αu s Uˆ sy .
itcanberepresentedbyaprogramthatsolvesthegameexactlyeverytimeitiscalled. Weareinterestedonlyin
functionapproximatorsthatuseareasonable amountofcomputation. Itmightinfactbebettertolearnavery
simplefunctionapproximatorandcombineitwithacertainamountoflook-aheadsearch.Thetradeoffsinvolved
arecurrentlynotwellunderstood.
Section21.4. Generalization in Reinforcement Learning 847
We can apply these rules to the example where Uˆ 11 is 0.8 and u 11 is 0.4. θ θ
θ j 0 1
andθ arealldecreased by 0.4αwhichreduces theerrorfor11. Noticethat changing the
2
parametersθinresponsetoanobservedtransitionbetweentwostatesalsochangesthevalues
of Uˆ for every other state! This is what we mean by saying that function approximation
θ
allowsareinforcement learnertogeneralize fromitsexperiences.
We expect that the agent will learn faster if it uses a function approximator provided
that the hypothesis space is not too large but includes some functions that are a reasonably
good fit to the true utility function. Exercise 21.5 asks you to evaluate the performance of
directutilityestimation bothwithandwithoutfunction approximation. Theimprovementin
the43worldisnoticeablebutnotdramaticbecausethisisaverysmallstatespacetobegin
with. Theimprovementismuchgreaterina 1010worldwitha1rewardat1010. This
world is well suited for a linear utility function because the true utility function is smooth
and nearly linear. See Exercise 21.8. If we put the 1 reward at 55 the true utility is
more like a pyramid and the function approximator in Equation 21.10 will fail miserably.
All is not lost however! Remember that what matters for linear function approximation
is that the function be linear in the parametersthe features themselves can be arbitrary
nocid:10nlinearfunctionsofthestatevariables. Hencewecanincludeatermsuchasθ 3f 3xy
θ xx 2yy 2 thatmeasuresthedistancetothegoal.
Wecanapplytheseideasequallywelltotemporal-differencelearners. Allweneeddois
adjusttheparameters totrytoreduce thetemporaldifference betweensuccessive states. The
new versions of the T D and Q-learning equations 21.3 on page 836 and 21.8 on page 844
aregivenby
Uˆ s
θ θ α Rsγ Uˆ scid:2 Uˆ s θ 21.12
i i θ θ
θ
i
forutilitiesand
Qˆ sa
θ θ α Rsγ max Qˆ scid:2 acid:2 Qˆ sa θ 21.13
i i θ θ
acid:3 θ
i
for Q-values. Forpassive T Dlearningtheupdaterulecanbeshowntoconvergetotheclosest
possible4 approximation tothetruefunction whenthefunction approximator is linear inthe
parameters. With active learning and nonlinear functions such as neural networks all bets
are off: There are some very simple cases in which the parameters can go off to infinity
even though there are good solutions in the hypothesis space. There are more sophisticated
algorithms thatcan avoid these problems but atpresent reinforcement learning withgeneral
function approximators remainsadelicate art.
Function approximation can also be very helpful for learning a model of the environ-
ment. Rememberthatlearning amodelforan observable environment isasupervised learn-
ingproblembecausethenextperceptgivestheoutcomestate. Anyofthesupervisedlearning
methodsin Chapter18canbeusedwithsuitableadjustmentsforthefactthatweneedtopre-
dictacompletestatedescriptionratherthanjusta Booleanclassificationorasinglerealvalue.
For a partially observable environment the learning problem is much more difficult. If we
knowwhatthehiddenvariablesareandhowtheyarecausallyrelatedtoeachotherandtothe
observablevariablesthenwecanfixthestructureofadynamic Bayesiannetworkandusethe
E Malgorithm tolearn the parameters as wasdescribed in Chapter20. Inventing thehidden
variables and learning the model structure are still open problems. Somepractical examples
aredescribed in Section21.6.
The final approach we will consider for reinforcement learning problems is called policy
search. In some ways policy search is the simplest of all the methods in this chapter: the
P OL IC YS EA RC H
ideaistokeeptwiddling thepolicyaslongasitsperformance improvesthenstop.
Letus begin withthe policies themselves. Rememberthat apolicy π isafunction that
mapsstatestoactions. Weareinterestedprimarilyin parameterized representations ofπ that
have far fewer parameters than there are states in the state space just as in the preceding
section. For example we could represent π by a collection of parameterized Q-functions
oneforeachaction andtaketheactionwiththehighestpredicted value:
πs max Qˆ sa. 21.14
θ
a
Each Q-function could be a linear function of the parameters θ as in Equation 21.10
or it could be a nonlinear function such as a neural network. Policy search will then ad-
just the parameters θ to improve the policy. Notice that if the policy is represented by Q-
functions then policy search results in a process that learns Q-functions. This process is
not the same as Q-learning! In Q-learning with function approximation the algorithm finds
a value of θ such that Qˆ is close to Q the optimal Q-function. Policy search on the
θ
other hand finds a value of θ that results in good performance; the values found by the two
methods may differ very substantially. For example the approximate Q-function defined
by Qˆ sa Q sa10 gives optimal performance even though it is not at all close to
θ
""
Q . Anotherclearinstance ofthe difference isthecase where πsiscalculated using say
depth-10 look-ahead search with anapproximate utility function Uˆ . Avalue of θ that gives
θ
goodresultsmaybealongwayfrommaking
Uˆ
resemblethetrueutilityfunction.
θ
One problem with policy representations of the kind given in Equation 21.14 is that
thepolicy isadiscontinuous function oftheparameters whentheactions arediscrete. Fora
continuousactionspacethepolicycanbeasmoothfunctionoftheparameters. Thatisthere
willbevaluesofθsuchthataninfinitesimalchangeinθcausesthepolicytoswitchfromone
action to another. This means that the value of the policy may also change discontinuously
whichmakesgradient-basedsearchdifficult. Forthisreasonpolicysearchmethodsoftenuse
astochasticpolicyrepresentation π sawhichspecifiestheprobabilityofselectingaction
S TO CH AS TI CP OL IC Y θ
ainstates. Onepopularrepresentation isthe softmaxfunction:
S OF TM AX FU NC TI ON
cid:12
π sa e Qˆ θsa e Qˆ θsacid:3 .
θ
acid:3
Softmax becomes nearly deterministic if one action is much better than the others but it
always gives adifferentiable function of θ; hence the value of the policy which depends in
Section21.5. Policy Search 849
a continuous fashion on the action selection probabilities is a differentiable function of θ.
Softmaxisageneralization ofthelogistic functionpage725tomultiplevariables.
Nowletuslookatmethodsforimprovingthepolicy. Westartwiththesimplestcase: a
deterministic policy and a deterministic environment. Let ρθ be the policy value i.e. the
P OL IC YV AL UE
expectedreward-to-gowhenπ isexecuted. Ifwecanderiveanexpressionforρθinclosed
θ
formthenwehaveastandardoptimizationproblemasdescribedin Chapter4. Wecanfollow
the policy gradient vector ρθ provided ρθ is differentiable. Alternatively if ρθ is
P OL IC YG RA DI EN T θ
not available in closed form we can evaluate π simply by executing it and observing the
θ
accumulatedreward. Wecanfollowtheempiricalgradientbyhillclimbingi.e.evaluating
the change in policy value for small increments in each parameter. With the usual caveats
thisprocesswillconverge toalocaloptimuminpolicyspace.
When the environment or the policy is stochastic things get more difficult. Suppose
we are trying to do hill climbing which requires comparing ρθ and ρθ Δθ for some
small Δθ. The problem is that the total reward on each trial may vary widely so estimates
of the policy value from a small number of trials will be quite unreliable; trying to compare
twosuch estimates willbeeven moreunreliable. Onesolution issimply to runlots oftrials
measuring the sample variance and using it to determine that enough trials have been run
to get a reliable indication of the direction of improvement for ρθ. Unfortunately this is
impractical formanyrealproblemswhereeachtrialmaybeexpensive time-consuming and
perhapsevendangerous.
Forthecaseofastochastic policyπ saitispossibletoobtainanunbiasedestimate
θ
of the gradient at θ ρθ directly from the results of trials executed at θ. Forsimplicity
θ
wewillderive thisestimate forthesimple caseofanonsequential environment inwhich the
reward Ra is obtained immediately after doing action a in the start state s . In this case
0
thepolicyvalueisjusttheexpected valueoftherewardand wehave
cid:12 cid:12
ρθ π s a Ra π s a Ra.
θ θ θ 0 θ θ 0
a a
Now we perform a simple trick so that this summation can be approximated by samples
generated from the probability distribution defined by π s a. Suppose that we have N
θ 0
trialsinallandtheactiontakenonthejthtrialisa . Then
j
cid:12 π s a Ra 1 cid:12 N π s a Ra
ρθ π s a θ θ 0 θ θ 0 j j .
θ θ 0
π s a N π s a
θ 0 θ 0 j
a j1
Thus the true gradient of the policy value is approximated by a sum of terms involving
the gradient of the action-selection probability in each trial. For the sequential case this
generalizes to
ρθ θ θ j j
θ
N π sa
θ j
j1
for each state s visited where a is executed in s on the jth trial and R s is the total
j j
reward received from state s onwards in the jth trial. The resulting algorithm is called
R EI NF OR CE Williams 1992; it is usually much more effective than hill climbing using
lotsoftrialsateachvalueofθ. Itisstillmuchslowerthannecessary however.
Consider the following task: given two blackjack5 programs determine which is best.
One way to do this is to have each play against a standard dealer for a certain number of
handsandthentomeasuretheirrespectivewinnings. Theproblemwiththisaswehaveseen
isthatthewinningsofeachprogram fluctuatewidelydepending onwhetheritreceivesgood
or bad cards. An obvious solution is to generate a certain number of hands in advance and
have each program play the same set of hands. In this way we eliminate the measurement
C OR RE LA TE D error due to differences in the cards received. This idea called correlated sampling un-
S AM PL IN G
derlies a policy-search algorithm called P EG AS US Ng and Jordan 2000. The algorithm is
applicable to domains for which a simulator is available so that the random outcomes of
actions canberepeated. Thealgorithm worksbygenerating inadvance N sequences ofran-
domnumbers eachofwhichcanbeusedtorunatrialofanypolicy. Policysearchiscarried
outbyevaluatingeachcandidatepolicyusingthesamesetofrandomsequencestodetermine
theactionoutcomes. Itcanbeshownthatthenumberofrandomsequencesrequiredtoensure
thatthevalueofeverypolicyiswellestimated depends onlyonthecomplexity ofthepolicy
spaceandnotatallonthecomplexityoftheunderlying domain.
Wenowturntoexamplesoflarge-scale applications ofreinforcement learning. Weconsider
applicationsingameplayingwherethetransitionmodelisknownandthegoalistolearnthe
utilityfunction andinrobotics wherethemodelisusually unknown.
Thefirstsignificant application ofreinforcement learning wasalsothefirstsignificant learn-
ing program of any kindthe checkers program written by Arthur Samuel 1959 1967.
Samuel first used a weighted linear function for the evaluation of positions using up to 16
termsatanyonetime. Heappliedaversionof Equation21.12toupdatetheweights. There
weresomesignificantdifferenceshoweverbetweenhisprogramandcurrentmethods. First
heupdatedtheweightsusingthedifferencebetweenthecurrentstateandthebacked-upvalue
generated byfulllook-ahead inthesearchtree. Thisworksfinebecause itamountstoview-
ing the state space at a different granularity. A second difference was that the program did
notuseanyobservedrewards! Thatisthevaluesofterminalstatesreachedinself-playwere
ignored. Thismeansthatitistheoretically possiblefor Samuelsprogramnottoconverge or
to converge on a strategy designed to lose rather than to win. Hemanaged to avoid this fate
by insisting that the weight for material advantage should always be positive. Remarkably
this was sufficient to direct the program into areas of weight space corresponding to good
checkers play.
Gerry Tesauros backgammon program T D-G AM MO N 1992 forcefully illustrates the
potential of reinforcement learning techniques. In earlier work Tesauro and Sejnowski
1989 Tesauro tried learning a neural network representation of Qsa directly from ex-
Section21.6. Applications of Reinforcement Learning 851
θ
x
Figure21.9 Setupfortheproblemofbalancingalongpoleontopofamovingcart. The
cartcanbejerkedleftorrightbyacontrollerthatobserves xθxandθ.
amples of moves labeled with relative values by a human expert. This approach proved
extremely tedious forthe expert. Itresulted inaprogram called N EU RO GA MM ONthatwas
strong by computer standards but not competitive with human experts. The T D-G AM MO N
project was an attempt to learn from self-play alone. The only reward signal was given at
the end of each game. The evaluation function was represented by a fully connected neural
network with a single hidden layer containing 40 nodes. Simply by repeated application of
Equation 21.12 T D-G AM MO N learned toplayconsiderably betterthan N EU RO GA MM ON
eventhoughtheinputrepresentation contained justtheraw boardpositionwithnocomputed
features. Thistookabout200000traininggamesandtwoweeksofcomputertime. Although
that may seem like a lot of games it is only a vanishingly small fraction of the state space.
Whenprecomputedfeatureswereaddedtotheinputrepresentationanetworkwith80hidden
nodes wasable after300000 training games toreach astandard ofplay comparable tothat
of the top three human players worldwide. Kit Woolsey a top player and analyst said that
Thereisnoquestion inmymindthatitspositional judgment isfarbetterthanmine.
The setup for the famous cartpole balancing problem also known as the inverted pendu-
C AR TP OL E
I NV ER TE D lum is shown in Figure 21.9. The problem is to control the position x of the cart so that
P EN DU LU M
the pole stays roughly upright θ π2 while staying within the limits of the cart track
as shown. Several thousand papers in reinforcement learning and control theory have been
published on this seemingly simple problem. The cartpole problem differs from the prob-
lemsdescribedearlierinthatthestatevariables
xθxandθ
arecontinuous. Theactionsare
B AN G-B AN G usuallydiscrete: jerkleftorjerkrighttheso-called bang-bangcontrolregime.
C ON TR OL
The earliest work on learning for this problem was carried out by Michie and Cham-
bers1968. Their B OX ES algorithm wasabletobalancethepoleforoveranhourafteronly
about30trials. Moreoverunlikemanysubsequentsystems B OX ESwasimplementedwitha
realcartandpolenotasimulation. Thealgorithmfirstdiscretized thefour-dimensional state
spaceintoboxeshence thename. Itthenrantrialsuntilthe polefelloverorthecarthitthe
endofthetrack. Negativereinforcement wasassociated withthefinalactioninthefinalbox
and then propagated back through the sequence. It was found that the discretization caused
some problems when the apparatus was initialized in a position different from those used in
training suggesting that generalization was not perfect. Improved generalization and faster
learning canbeobtained usinganalgorithm that adaptively partitions thestatespaceaccord-
ingtotheobservedvariationintherewardorbyusingacontinuous-state nonlinearfunction
approximatorsuchasaneuralnetwork. Nowadaysbalancing atripleinvertedpendulum isa
commonexerciseafeatfarbeyondthecapabilities ofmosthumans.
Still more impressive is the application of reinforcement learning to helicopter flight
Figure 21.10. This work has generally used policy search Bagnell and Schneider 2001
as well as the P EG AS US algorithm with simulation based on a learned transition model Ng
etal.2004. Furtherdetailsaregivenin Chapter25.
Figure21.10 Superimposedtime-lapse imagesof an autonomoushelicopterperforming
a very difficult nose-in circle maneuver. The helicopter is under the control of a policy
developedbythe P EG AS US policy-searchalgorithm. A simulatormodelwas developedby
observingtheeffectsofvariouscontrolmanipulationsontherealhelicopter;thenthealgo-
rithmwasrunonthesimulatormodelovernight.Avarietyofcontrollersweredevelopedfor
differentmaneuvers. In all cases performance far exceeded that of an expert human pilot
usingremotecontrol.Imagecourtesyof Andrew Ng.
22
N AT UR AL L AN GU AG E
P RO CE SS IN G
In which we see how to make use of the copious knowledge that is expressed in
naturallanguage.
Homosapiensissetapartfromotherspeciesbythecapacityforlanguage. Somewherearound
100000yearsagohumanslearnedhowtospeakandabout7000yearsagolearnedtowrite.
Althoughchimpanzees dolphins andotheranimalshaveshownvocabularies ofhundreds of
signsonlyhumanscanreliablycommunicateanunboundednumberofqualitativelydifferent
messagesonanytopicusingdiscretesigns.
Of course there are other attributes that are uniquely human: no other species wears
clothes creates representational art or watches three hours of television a day. But when
Alan Turing proposed his Test see Section 1.1.1 he based it on language not art or T V.
There are two main reasons why we want our computer agents to be able to process natural
languages: firsttocommunicatewithhumansatopicwetakeupin Chapter23andsecond
toacquire information fromwrittenlanguage thefocusofthischapter.
There are over a trillion pages of information on the Web almost all of it in natural
K NO WL ED GE language. An agent that wants to do knowledge acquisition needs to understand at least
A CQ UI SI TI ON
partially the ambiguous messy languages that humans use. We examine the problem from
the point of view of specific information-seeking tasks: text classification information re-
trievalandinformationextraction. Onecommonfactorinaddressingthesetasksistheuseof
languagemodels: modelsthatpredicttheprobability distribution oflanguage expressions.
L AN GU AG EM OD EL
Formallanguagessuchastheprogramminglanguages Javaor Pythonhavepreciselydefined
L AN GU AG E language models. A language can be defined as a set of strings; print2 2 is a
legal program inthe language Python whereas 22 printisnot. Since there are an
infinitenumberoflegalprogramstheycannotbeenumerated; insteadtheyarespecifiedbya
set of rules called a grammar. Formallanguages also have rules that define the meaning or
G RA MM AR
S EM AN TI CS semanticsofaprogram;forexampletherulessaythatthemeaning of 2 2is4and
themeaningof10isthatanerrorissignaled.
860
Section22.1. Language Models 861
Natural languages such as English or Spanish cannot be characterized as a definitive
set of sentences. Everyone agrees that Not to be invited is sad is a sentence of English
butpeople disagree onthegrammaticality of Tobenotinvited issad. Therefore itismore
fruitful todefineanatural language model asaprobability distribution oversentences rather
than a definitive set. That is rather than asking if a string of words is or is not a memberof
thesetdefiningthelanguage weinsteadaskfor P Swordswhatistheprobability that
arandom sentencewouldbewords.
Naturallanguagesarealsoambiguous. Hesawherduckcanmeaneitherthathesaw
A MB IG UI TY
a waterfowl belonging to her or that he saw her move to evade something. Thus again we
cannot speak ofasingle meaning forasentence but rather of aprobability distribution over
possible meanings.
Finally natural languages are difficult to deal with because they are very large and
constantly changing. Thus our language models are at best an approximation. We start
withthesimplestpossible approximations andmoveupfromthere.
Ultimatelyawrittentextiscomposedofcharactersletters digitspunctuation andspaces
C HA RA CT ER S
in English and more exotic characters in some other languages. Thus one of the simplest
languagemodelsisaprobability distribution oversequencesofcharacters. Asin Chapter15
we write Pc for the probability of a sequence of N characters c through c . In one
1:N 1 N
Webcollection Pthe0.027and Pzgq0.000000002. Asequenceofwrittensym-
bolsoflength niscalledann-gramfrom the Greekrootforwritingorletters withspecial
case unigram for1-gram bigram for2-gram and trigram for3-gram. A model ofthe
N probability distribution of n-lettersequences is thus called an n-gram model. Butbe care-
-G RA MM OD EL
ful: we can have n-gram models over sequences of words syllables or other units; not just
overcharacters.
Ann-gram modelisdefinedasa Markovchainoforder n1. Recallfrompage 568
that in a Markov chain the probability of character c depends only on the immediately pre-
i
ceding characters not on any other characters. So in a trigram model Markov chain of
order2wehave
Pc ic 1:i1 Pc ic i2:i1.
We can define the probability of a sequence of characters Pc under the trigram model
1:N
byfirstfactoring withthechainruleandthenusingthe Markovassumption:
cid:25 N cid:25 N
Pc 1:N Pc ic 1:i1 Pc ic i2:i1.
i1 i1
Foratrigramcharactermodelinalanguagewith100characters P C i C i2:i1hasamillion
entries andcanbeaccurately estimated bycounting charactersequences inabodyoftextof
C OR PU S
Latinwordforbody.
Whatcanwedowithn-gramcharactermodels? Onetaskforwhichtheyarewellsuited
L AN GU AG E islanguageidentification: givenatextdeterminewhatnaturallanguageitiswrittenin. This
I DE NT IF IC AT IO N
is arelatively easy task; evenwith short texts such as Hello world or Wie geht es dir it
iseasy toidentify thefirstas English andthesecond as German. Computersystems identify
languages with greater than 99 accuracy; occasionally closely related languages such as
Swedishand Norwegianareconfused.
One approach to language identification is to first build a trigram character model of
each candidate language Pc ic i2:i1cid:3 wherethe variable cid:3ranges overlanguages. For
each cid:3 the model is built by counting trigrams in acorpus of that language. About 100000
characters of each language are needed. That gives us a model of P Text Language but
wewanttoselectthemostprobablelanguagegiventhetextsoweapply Bayesrulefollowed
bythe Markovassumption togetthemostprobable language:
cid:3 argmax Pcid:3c
1:N
cid:3
argmax Pcid:3 Pc cid:3
1:N
cid:3
cid:25 N
argmax Pcid:3 Pc ic i2:i1cid:3
cid:3
i1
Thetrigram model canbe learned from acorpus but whatabout thepriorprobability Pcid:3?
Wemayhave someestimate ofthese values; forexample ifweareselecting arandom Web
pageweknowthat Englishisthemostlikelylanguageandthattheprobabilityof Macedonian
will be less than 1. The exact number weselect forthese priors is not critical because the
trigrammodelusuallyselectsonelanguagethatisseveralordersofmagnitudemoreprobable
thananyother.
Other tasks for character models include spelling correction genre classification and
named-entity recognition. Genre classification means deciding if a text is a news story a
legal document a scientific article etc. While many features help make this classification
counts of punctuation and other character n-gram features go a long way Kessler et al.
1997. Named-entity recognition is the task of finding names of things in a document and
deciding whatclasstheybelong to. Forexample inthetext Mr. Sopersteen wasprescribed
aciphexweshouldrecognizethat Mr. Sopersteenisthenameofapersonandaciphexis
thenameofadrug. Character-level modelsaregoodforthistaskbecause theycanassociate
thecharactersequence ex exfollowedbyaspacewithadrugnameandsteen with
apersonnameandtherebyidentifywordsthattheyhaveneverseenbefore.
The major complication of n-gram models is that the training corpus provides only an esti-
mateofthetrueprobability distribution. Forcommoncharacter sequences suchas thany
Englishcorpuswillgiveagoodestimate: about1.5ofalltrigrams. Ontheotherhand ht
is very uncommonno dictionary words start with ht. It is likely that the sequence would
have acount ofzero inatraining corpus ofstandard English. Does that meanweshould as-
sign P th0? Ifwedidthenthetext Theprogram issuesanhttprequest wouldhave
Section22.1. Language Models 863
an Englishprobabilityofzerowhichseemswrong. Wehaveaproblemingeneralization: we
want ourlanguage models to generalize well totexts they havent seen yet. Just because we
haveneverseen httpbefore doesnotmeanthatourmodelshould claimthatit isimpossi-
ble. Thus we will adjust our language model so that sequences that have a count of zero in
thetraining corpuswillbeassignedasmallnonzeroprobability andtheothercountswillbe
adjusted downward slightly sothat theprobability still sumsto1. Theprocess od adjusting
theprobability oflow-frequency countsiscalled smoothing.
S MO OT HI NG
Thesimplesttypeofsmoothingwassuggestedby Pierre-Simon Laplaceinthe18thcen-
tury: hesaidthatinthelackoffurtherinformationifarandom Booleanvariable X hasbeen
falseinallnobservationssofarthentheestimatefor P Xtrueshouldbe1n2. That
isheassumesthatwithtwomoretrials onemightbetrueand onefalse. Laplacesmoothing
alsocalledadd-onesmoothingisastepintherightdirectionbutperformsrelativelypoorly.
Abetterapproachisabackoffmodelinwhichwestartbyestimatingn-gramcountsbutfor
B AC KO FF MO DE L
anyparticularsequencethathasaloworzerocountwebackoffton1-grams. Linear
L IN EA R
interpolation smoothing is a backoff model that combines trigram bigram and unigram
I NT ER PO LA TI ON
S MO OT HI NG
modelsbylinearinterpolation. Itdefinestheprobability estimateas
P c ic i2:i1 λ 3 Pc ic i2:i1λ 2 Pc ic i1λ 1 Pc i
where λ λ λ 1. The parameter values λ can be fixed or they can be trained with
an expectationmaximization algorithm. It is also possible to have the values of λ depend
i
on the counts: if we have a high count of trigrams then we weigh them relatively more; if
onlyalowcountthenweputmoreweightonthebigramandunigrammodels. Onecampof
researchers has developed evermore sophisticated smoothing models while the other camp
suggestsgatheringalargercorpussothatevensimplesmoothingmodelsworkwell. Bothare
gettingatthesamegoal: reducing thevarianceinthelanguage model.
Onecomplication: note that the expression Pc ic i2:i1 asks for Pc 1c-1:0 when
i 1 but there are no characters before c . We can introduce artificial characters for
1
example defining c to be a space character or a special begin text character. Or we can
0
fall back on lower-order Markov models in effect defining c-1:0 to be the empty sequence
andthus Pc 1c-1:0 Pc 1.
With so many possible n-gram modelsunigram bigram trigram interpolated smoothing
withdifferent values of λetc.howdoweknow whatmodeltochoose? Wecanevaluate a
model with cross-validation. Split the corpus into a training corpus and a validation corpus.
Determine the parameters of the model from the training data. Then evaluate the model on
thevalidation corpus.
The evaluation can be a task-specific metric such as measuring accuracy on language
identification. Alternatively wecanhave atask-independent model oflanguage quality: cal-
culate the probability assigned to the validation corpus by the model; the higher the proba-
bility the better. This metric is inconvenient because the probability of a large corpus will
be a very small number and floating-point underflow becomes an issue. A different way of
describing theprobability ofasequenceiswithameasurecalledperplexitydefinedas
P ER PL EX IT Y
1
Perplexityc 1:N Pc 1:N N .
Perplexity canbethought ofasthereciprocal ofprobability normalized bysequence length.
Itcanalsobethoughtofastheweightedaveragebranching factorofamodel. Supposethere
are 100 characters in our language and our model says they are all equally likely. Then for
asequence ofanylength theperplexity willbe100. Ifsomecharacters aremorelikely than
others andthemodelreflectsthatthenthemodelwillhaveaperplexity lessthan100.
Now weturn to n-gram models overwords rather than characters. Allthe same mechanism
applies equally towordandcharacter models. Themaindifference isthatthevocabulary
V OC AB UL AR Y
the set of symbols that make up the corpus and the modelis larger. There are only about
more restrictive for example by treating A and a as the same symbol or by treating all
punctuation asthesamesymbol. Butwithwordmodelswehaveatleasttensofthousands of
symbolsandsometimesmillions. Thewiderangeisbecauseitisnotclearwhatconstitutesa
word. In Englishasequenceofletterssurroundedbyspacesisawordbutinsomelanguages
like Chinesewordsarenotseparatedbyspacesandevenin Englishmanydecisionsmustbe
madetohaveaclearpolicyonwordboundaries: howmanywordsareinneer-do-well? Or
in Tel:1-800-960-5660x123?
O UT OF Wordn-grammodelsneedtodealwithoutofvocabularywords. Withcharactermod-
V OC AB UL AR Y
els we didnt have to worry about someone inventing a new letter of the alphabet.1 But
withwordmodels thereisalwaysthechance ofanewwordthatwasnotseen inthetraining
corpus so we need to model that explicitly in our language model. This can be done by
adding just one new word to the vocabulary: U NK standing for the unknown word. We
can estimate n-gram counts for U NK by this trick: go through the training corpus and
the first time any individual word appears it is previously unknown so replace it with the
symbol U NK. Allsubsequent appearances of the word remain unchanged. Then compute
n-gram counts forthe corpus as usual treating U NKjust like anyother word. Then when
anunknown wordappears inatestset welook upitsprobability under U NK. Sometimes
multiple unknown-word symbols are used for different classes. For example any string of
digitsmightbereplaced with N UMoranyemailaddress with E MA IL.
To get a feeling for what word models can do we built unigram bigram and trigram
modelsoverthewordsinthisbookandthenrandomlysampledsequences ofwordsfromthe
models. Theresultsare
Unigram: logical areasareconfusion amayrighttriesagentgoalthewas...
Bigram: systemsareverysimilarcomputational approach wouldberepresented ...
Trigram: planning andscheduling areintegrated thesuccess ofnaive bayesmodelis...
Evenwiththissmallsampleitshouldbeclearthattheunigrammodelisapoorapproximation
ofeither Englishorthecontentofan A Itextbookandthatthebigramandtrigrammodelsare
Section22.2. Text Classification 865
muchbetter. Themodelsagreewiththisassessment: theperplexity was891fortheunigram
model142forthebigram modeland91forthetrigrammodel.
With the basics of n-gram modelsboth character- and word-basedestablished we
canturnnowtosomelanguage tasks.
T EX T Wenowconsiderindepththetaskoftextclassificationalsoknownascategorization: given
C LA SS IF IC AT IO N
atextofsomekinddecidewhichofapredefinedsetofclassesitbelongsto. Languageiden-
tification and genre classification areexamples oftextclassification asissentiment analysis
classifying amovieorproductreviewaspositiveornegativeandspamdetectionclassify-
S PA MD ET EC TI ON
inganemailmessageasspamornot-spam. Sincenot-spam isawkwardresearchers have
coined the term ham fornot-spam. We can treat spam detection as a problem in supervised
learning. A training set is readily available: the positive spam examples are in my spam
folder thenegativehamexamplesareinmyinbox. Hereisanexcerpt:
Spam:Wholesale Fashion Watches-57today.Designerwatchesforcheap...
Spam:Youcanbuy Viagra Fr1.85 All Medicationsatunbeatableprices! ...
Spam:W EC AN TR EA TA NY TH IN GY OU SU FF ER FR OM JU ST TR US TU S...
Spam:Sta.rtearningthesalaryyoud-eservebyobtainingthepropercredentials!
Ham:Thepracticalsignificanceofhypertreewidthinidentifyingmore...
Ham:Abstract:Wewillmotivatetheproblemofsocialidentityclustering:...
Ham:Goodtoseeyoumyfriend.Hey Peter Itwasgoodtohearfromyou....
Ham:P DSimpliesconvexityoftheresultingoptimizationproblem Kernel Ridge...
From this excerpt we can start to get an idea of what might be good features to include in
the supervised learning model. Word n-grams such as forcheap and You can buy seem
to be indicators of spam although they would have a nonzero probability in ham as well.
Character-level features also seem important: spam ismore likely tobe alluppercase and to
havepunctuationembeddedinwords. Apparentlythespammersthoughtthatthewordbigram
you deserve would be too indicative of spam and thus wrote you d-eserve instead. A
character model should detect this. We could either create a full character n-gram model
of spam and ham or we could handcraft features such as number of punctuation marks
embeddedinwords.
Note that we have two complementary ways of talking about classification. In the
language-modeling approach wedefineonen-gramlanguagemodelfor P Messagespam
bytrainingonthespamfolderandonemodelfor P Messagehambytrainingontheinbox.
Thenwecanclassify anewmessagewithanapplication of Bayesrule:
argmax Pcmessage argmax Pmessagec Pc.
cspamham cspamham
where Pcisestimated justby counting thetotal numberofspam and ham messages. This
approach workswellforspamdetection justasitdidforlanguage identification.
In the machine-learning approach we represent the message as a set of featurevalue
pairs and apply a classification algorithm h to the feature vector X. We can make the
language-modeling andmachine-learning approachescompatiblebythinkingofthen-grams
as features. This is easiest to see with a unigram model. The features are the words in the
vocabulary: a aardvark ... and the values are the number of times each word appears
inthemessage. Thatmakesthefeaturevectorlargeandsparse. Ifthereare100000wordsin
thelanguagemodelthenthefeaturevectorhaslength100000butforashortemailmessage
almostallthefeatures willhavecount zero. Thisunigram representation hasbeen called the
bagofwordsmodel. Youcanthink ofthemodelasputting thewordsofthetraining corpus
B AG OF WO RD S
in a bag and then selecting words one at a time. The notion of order of the words is lost; a
unigrammodelgivesthesameprobability toanypermutation ofatext. Higher-order n-gram
modelsmaintainsomelocalnotionofwordorder.
Withbigramsandtrigramsthenumberoffeatures issquared orcubed andwecanadd
in other non-n-gram features: the time the message was sent whether a U RL or an image
is part of the message an I D number for the sender of the message the senders number of
previousspamandhammessagesandsoon. Thechoiceoffeaturesisthemostimportantpart
ofcreatingagoodspamdetectormoreimportantthanthechoiceofalgorithmforprocessing
the features. In part this is because there is a lot of training data so if we can propose a
feature the data can accurately determine if it is good or not. It is necessary to constantly
update features because spam detection is an adversarial task; the spammers modify their
spaminresponse tothespamdetectors changes.
Itcan be expensive to run algorithms on a very large feature vector so often aprocess
offeatureselectionisusedtokeeponlythefeaturesthatbestdiscriminatebetweenspamand
F EA TU RE SE LE CT IO N
ham. Forexamplethebigramoftheisfrequentin English andmaybeequallyfrequentin
spam and ham sothere isno sense incounting it. Often the top hundred orsofeatures do a
goodjobofdiscriminating betweenclasses.
Once we have chosen a set of features we can apply any of the supervised learning
techniques we have seen; popular ones for text categorization include k-nearest-neighbors
support vector machines decision trees naive Bayes and logistic regression. All of these
have been applied to spam detection usually with accuracy in the 9899 range. With a
carefully designed featuresetaccuracy canexceed99.9.
Another way to think about classification is as a problem in data compression. A lossless
D AT AC OM PR ES SI ON
compressionalgorithmtakesasequenceofsymbolsdetectsrepeatedpatternsinitandwrites
a description of the sequence that is more compact than the original. For example the text
0.142857142857142857mightbecompressedto0.1428573. Compressionalgorithms
workbybuilding dictionaries ofsubsequences ofthetextandthenreferring toentries inthe
dictionary. Theexampleherehadonlyonedictionary entry 142857.
In effect compression algorithms are creating a language model. The L ZWalgorithm
inparticulardirectlymodelsamaximum-entropyprobabilitydistribution. Todoclassification
bycompression wefirstlumptogetherallthespamtrainingmessagesandcompressthemas
Section22.3. Information Retrieval 867
aunit. Wedothesamefortheham. Thenwhengiven anewmessage toclassify weappend
ittothespammessagesandcompresstheresult. Wealsoappend ittothehamandcompress
that. Whichever class compresses betteradds the fewernumber of additional bytes forthe
new messageis the predicted class. The idea is that a spam message will tend to share
dictionaryentrieswithotherspammessagesandthuswillcompressbetterwhenappendedto
acollection thatalready containsthespamdictionary.
Experimentswithcompression-based classificationonsomeofthestandardcorporafor
textclassificationthe 20-Newsgroups datasetthe Reuters-10 Corpora the Industry Sector
corporaindicatethatwhereasrunningoff-the-shelfcompressionalgorithmslikegzip R AR
and L ZW can be quite slow their accuracy is comparable to traditional classification algo-
rithms. This is interesting in its own right and also serves to point out that there is promise
foralgorithmsthatusecharacter n-gramsdirectlywithnopreprocessing ofthetextorfeature
selection: theyseemtobecaptiring somerealpatterns.
I NF OR MA TI ON Information retrieval is the task of finding documents that are relevant to a users need for
R ET RI EV AL
information. The best-known examples of information retrieval systems are search engines
onthe World Wide Web. A Webusercantypeaquerysuchas A Ibook2 intoasearchengine
and see a list of relevant pages. In this section we will see how such systems are built. An
information retrievalhenceforth I Rsystemcanbecharacterized by
I R
1. Acorpusofdocuments. Eachsystemmustdecidewhatitwantstotreatasadocument:
aparagraph apageoramultipage text.
2. Queries posed in a query language. A query specifies what the user wants to know.
Q UE RY LA NG UA GE
The query language can be just a list of words such as A I book; or it can specify
a phrase of words that must be adjacent as in A I book; it can contain Boolean
operatorsasin A IA NDbook;itcanincludenon-Booleanoperatorssuchas A IN EA R
bookor A Ibooksite:www.aaai.org.
3. Aresultset. Thisisthesubsetofdocumentsthatthe I Rsystemjudgestoberelevantto
R ES UL TS ET
thequery. Byrelevant wemeanlikely tobeofusetotheperson whoposed thequery
R EL EV AN T
fortheparticularinformation needexpressedinthequery.
4. A presentation of the result set. This can be as simple as a ranked list of document
P RE SE NT AT IO N
titles or as complex as a rotating color map of the result set projected onto a three-
dimensional spacerendered asatwo-dimensional display.
B OO LE AN KE YW OR D The earliest I R systems worked on a Boolean keyword model. Each word in the document
M OD EL
collection istreated as a Boolean feature that is true ofadocument ifthe word occurs in the
document and false if it does not. So the feature retrieval is true for the current chapter
but false for Chapter 15. The query language is the language of Boolean expressions over
distinguishthequerytwowordsfromtwowords.
features. A document is relevant only if the expression evaluates to true. For example the
queryinformation A NDretrievalistrueforthecurrentchapterandfalsefor Chapter15.
This model has the advantage of being simple to explain and implement. However
it has some disadvantages. First the degree of relevance of a document is a single bit so
there is no guidance as to how to order the relevant documents for presentation. Second
Boolean expressions are unfamiliar to users who are not programmers or logicians. Users
find it unintuitive that when they want to know about farming in the states of Kansas and
Nebraska they need to issue the query farming Kansas O R Nebraska. Third it can be
hard to formulate anappropriate query even foraskilled user. Suppose wetryinformation
A ND retrieval A ND models A ND optimization and get an empty result set. We could try
information O Rretrieval O Rmodels O Roptimization butifthat returns too manyresults
itisdifficulttoknowwhattotrynext.
Most I Rsystemshaveabandonedthe Booleanmodelandusemodelsbasedonthestatisticsof
B M25 SC OR IN G wordcounts. Wedescribethe B M25scoringfunctionwhichcomesfromthe Okapiproject
F UN CT IO N
of Stephen Robertson and Karen Sparck Jones at Londons City College and has been used
insearchengines suchastheopen-source Luceneproject.
Ascoringfunctiontakesadocumentandaqueryandreturnsanumericscore;themost
relevant documents have the highest scores. In the B M25 function the score is a linear
weighted combination ofscores foreach ofthe words that makeup thequery. Threefactors
affect the weight of a query term: First the frequency with which a query term appears in
a document also known as T F for term frequency. For the query farming in Kansas
documents that mention farming frequently will have higher scores. Second the inverse
document frequency oftheterm or I DF. Thewordin appears inalmostevery document
soithasahigh document frequency andthus alowinverse document frequency and thusit
isnotasimportanttothequeryasfarmingor Kansas. Thirdthelengthofthedocument.
Amillion-worddocumentwillprobablymentionallthequery wordsbutmaynotactuallybe
aboutthequery. Ashortdocument thatmentionsallthewords isamuchbettercandidate.
The B M25 function takes all three of these into account. We assume we have created
an index of the N documents in the corpus so that we can look up T Fq d the count of
i j
the number of times word q appears in document d . We also assume a table of document
i j
frequency counts D Fq that gives the number of documents that contain the word q .
i i
Thengivenadocument d andaqueryconsisting ofthewords q wehave
j 1:N
cid:12 N T Fq d k1
B M25d q I DFq i j
j 1:N i T Fq d k1bb dj
i1 i j L
where d is the length of document d in words and L is the average document length
j cid:2 j
in the corpus: L d N. We have two parameters k and b that can be tuned by
i i
cross-validation; typical values are k 2.0 and b 0.75. I DFq is the inverse document
i
Section22.3. Information Retrieval 869
frequency ofword q givenby
i
N D Fq 0.5
i
I DFq log .
i
D Fq 0.5
i
Of course it would be impractical to apply the B M25 scoring function to every document
in the corpus. Instead systems create an index ahead of time that lists for each vocabulary
I ND EX
wordthedocumentsthatcontaintheword. Thisiscalledthehitlistfortheword. Thenwhen
H IT LI ST
given a query we intersect the hit lists of the query words and only score the documents in
theintersection.
Howdoweknow whetheran I Rsystem isperforming well? Weundertake anexperiment in
whichthesystemisgivenasetofqueriesandtheresultsetsarescoredwithrespecttohuman
relevance judgments. Traditionally therehavebeentwomeasures usedinthescoring: recall
and precision. Weexplain them withthehelp ofanexample. Imagine thatan I Rsystem has
returned aresult setforasingle query forwhich weknow which documents areandare not
relevant outofacorpusof100documents. Thedocument countsineachcategory aregiven
inthefollowingtable:
Inresultset Notinresultset
Relevant 30 20
Notrelevant 10 40
Precision measures the proportion of documents in the result set that are actually relevant.
P RE CI SI ON
Inourexampletheprecision is 303010.75. Thefalsepositive rateis1.75.25.
Recall measures the proportion of all the relevant documents in the collection that are in
R EC AL L
the result set. In our example recall is 3030 20.60. The false negative rate is 1
.60.40. Inaverylargedocumentcollectionsuchasthe World Wide Webrecallisdifficult
to compute because there is no easy way to examine every page on the Web for relevance.
Allwecandoiseitherestimaterecallbysamplingorignorerecallcompletelyandjustjudge
precision. In the case of a Web search engine there may be thousands of documents in the
result set so it makes more sense to measure precision for several different sizes such as
P10 precision in the top 10 results or P50 rather than to estimate precision in the
entireresultset.
It is possible to trade off precision against recall by varying the size of the result set
returned. Intheextreme asystem thatreturns everydocument inthedocument collection is
guaranteed arecall of 100 but willhave low precision. Alternately asystem could return
a single document and have low recall but a decent chance at 100 precision. A summary
ofbothmeasuresisthe F scoreasinglenumberthatistheharmonicmeanofprecision and
1
recall 2 PR P R.
There are many possible refinements to the system described here and indeed Web search
enginesarecontinually updatingtheiralgorithmsastheydiscovernewapproachesandasthe
Webgrowsandchanges.
Onecommonrefinementisabettermodeloftheeffectofdocumentlengthonrelevance.
Singhal et al. 1996 observed that simple document length normalization schemes tend to
favor short documents too much and long documents not enough. They propose a pivoted
document length normalization scheme; the idea is that the pivot is the document length at
which the old-style normalization is correct; documents shorter than that get a boost and
longeronesgetapenalty.
The B M25 scoring function uses a word model that treats all words as completely in-
dependent but we know that some words are correlated: couch is closely related to both
couches andsofa. Many I Rsystemsattempttoaccountforthesecorrelations.
Forexample ifthequeryiscouch itwouldbeashametoexcludefromtheresultset
those documents that mention C OU CH or couches but not couch. Most I R systems
do case folding of C OU CH to couch and some use a stemming algorithm to reduce
C AS EF OL DI NG
couches to the stem form couch both in the query and the documents. This typically
S TE MM IN G
yields a small increase in recall on the order of 2 for English. However it can harm
precision. For example stemming stocking to stock will tend to decrease precision for
queries about eitherfootcoverings orfinancial instruments although itcould improverecall
for queries about warehousing. Stemming algorithms based on rules e.g. remove -ing
cannot avoid this problem but algorithms based on dictionaries dont remove -ing if the
word is already listed in the dictionary can. While stemming has a small effect in English
it is more important in other languages. In German for example it is not uncommon to
see words like Lebensversicherungsgesellschaftsangestellter life insurance company em-
ployee. Languages suchas Finnish Turkish Inuit and Yupikhaverecursive morphological
rulesthatinprinciple generate wordsofunbounded length.
Thenextstepistorecognizesynonymssuchassofaforcouch. Aswithstemming
S YN ON YM
this has the potential for small gains in recall but can hurt precision. A user who gives the
query Tim Couch wants to see results about the football player not sofas. The problem is
thatlanguagesabhorabsolutesynonymsjustasnatureabhorsavacuum Cruse1986. That
is anytime there aretwowords thatmean thesamething speakers ofthelanguage conspire
to evolve the meanings to remove the confusion. Related words that are not synonyms also
play an important role in rankingterms like leather wooden or modern can serve
to confirm that the document really is about couch. Synonyms and related words can be
found in dictionaries or by looking for correlations in documents or in queriesif we find
that many users who ask the query new sofa follow it up with the query new couch we
caninthefuturealternewsofatobenewsofa O Rnewcouch.
As a final refinement I R can be improved by considering metadatadata outside of
M ET AD AT A
thetext ofthe document. Examples include human-supplied keywords andpublication data.
Onthe Webhypertext linksbetweendocuments areacrucialsourceofinformation.
L IN KS
Page Rank3 wasoneofthetwooriginal ideas thatset Googles search apart from other Web
P AG ER AN K
search engines whenitwasintroduced in1997. Theotherinnovation wastheuseofanchor
Section22.3. Information Retrieval 871
function H IT Squeryreturnspages withhubandauthoritynumbers
pages E XP AN D-P AG ES RE LE VA NT-P AG ESquery
foreachp inpages do
p.A UT HO RI TY1
p.H UB1
repeatuntilconvergencedo
foreachp inpages dcid:2o
p.A UT HO Rcid:2 IT Y
i
I NL IN Kip.H UB
p.H UB
i
O UT LI NKip.A UT HO RI TY
N OR MA LI ZEpages
returnpages
query. R EL EV AN T-P AG ESfetchesthepagesthatmatchthequeryand E XP AN D-P AG ESadds
ineverypagethatlinkstoorislinkedfromoneoftherelevantpages. N OR MA LI ZEdivides
each pages score by the sum of the squares of all pages scores separately for both the
authorityandhubsscores.
texttheunderlinedtextinahyperlinktoindexapageeventhoughtheanchortextwason
adifferent pagethantheonebeingindexed. Page Rankwasinvented tosolvetheproblemof
thetyrannyof T F scores: ifthequeryis I BMhowdowemakesurethat I BMshomepage
ibm.comisthefirstresultevenifanotherpagementionstheterm I BMmorefrequently?
Theideaisthatibm.comhasmanyin-linkslinkstothepagesoitshouldberankedhigher:
each in-link is a vote for the quality of the linked-to page. But if we only counted in-links
thenitwouldbepossible fora Webspammertocreateanetwork ofpages andhavethemall
point to a page of his choosing increasing the score of that page. Therefore the Page Rank
algorithm is designed to weight links from high-quality sites more heavily. What is a high-
quality site? Onethatislinked tobyotherhigh-quality sites. Thedefinition isrecursive but
wewillseethattherecursion bottomsoutproperly. The Page Rankforapagepisdefinedas:
cid:12
1d P Rin
i
P Rp d
N Cin
i
i
where P Rp is the Page Rank of page p N is the total number of pages in the corpus in
i
are the pages that link in to p and Cin is the count of the total number of out-links on
i
page in . The constant d is a damping factor. It can be understood through the random
i
R AN DO MS UR FE R surfer model: imagine a Web surfer who starts at some random page and begins exploring.
M OD EL
With probability d well assume d0.85 the surfer clicks on one of the links on the page
choosing uniformly among them and with probability 1d she gets bored with the page
and restarts on a random page anywhere on the Web. The Page Rank of page p is then the
probability that the random surfer will be at page p at any point in time. Page Rank can be
computed by an iterative procedure: start with all pages having P Rp1 and iterate the
algorithm updating ranksuntiltheyconverge.
The Hyperlink-Induced Topic Search algorithm also known as Hubs and Authorities or
H IT S is another influential link-analysis algorithm see Figure 22.1. H IT S differs from
Page Rankinseveralways. Firstitisaquery-dependent measure: itratespages withrespect
to a query. That means that it must be computed anew for each querya computational
burden that most search engines have elected not totake on. Given aquery H IT S firstfinds
a set of pages that are relevant to the query. It does that by intersecting hit lists of query
words andthen adding pages inthelink neighborhood ofthese pagespages thatlink toor
arelinkedfromoneofthepagesintheoriginalrelevantset.
Each page in this set is considered an authority on the query to the degree that other
A UT HO RI TY
pages in the relevant set point to it. A page is considered a hub to the degree that it points
H UB
to other authoritative pages in the relevant set. Just as with Page Rank we dont want to
merely count the number of links; we want to give more value to the high-quality hubs and
authorities. Thus as with Page Rank weiterate a process that updates the authority score of
a page to be the sum of the hub scores of the pages that point to it and the hub score to be
thesum oftheauthority scores ofthe pages itpoints to. Ifwe then normalize thescores and
repeat k timestheprocesswillconverge.
Both Page Rank and H IT S played important roles in developing our understanding of
Webinformationretrieval. Thesealgorithmsandtheirextensionsareusedinrankingbillions
ofqueries dailyassearch engines steadily develop betterwaysofextracting yetfinersignals
ofsearchrelevance.
Information retrieval is the task of finding documents that are relevant to a query where the
Q UE ST IO N querymaybeaquestion orjustatopicareaorconcept. Questionansweringisasomewhat
A NS WE RI NG
different task in which the query really is a question and the answer is not a ranked list
of documents but rather a short responsea sentence or even just a phrase. There have
been question-answering N LP natural language processing systems since the 1960s but
onlysince2001havesuchsystemsused Webinformation retrievaltoradically increase their
breadthofcoverage.
The A SK MS R system Bankoetal. 2002 isatypical Web-based question-answering
system. It is based on the intuition that most questions will be answered many times on the
Web so question answering should be thought of as a problem in precision not recall. We
dont have to deal with all the different ways that an answer might be phrasedwe only
have to find one of them. For example consider the query Who killed Abraham Lincoln?
Suppose a system had to answer that question with access only to a single encyclopedia
whoseentryon Lincolnsaid
John Wilkes Booth altered history with a bullet. He will foreverbe known as the man
whoended Abraham Lincolnslife.
Tousethispassagetoanswerthequestion thesystem wouldhavetoknowthatendingalife
canbeakillingthat Herefersto Boothandseveralother linguistic andsemanticfacts.
Section22.4. Information Extraction 873
A SK MS Rdoesnotattemptthiskindofsophisticationitknowsnothingaboutpronoun
referenceoraboutkillingoranyotherverb. Itdoesknow15differentkindsofquestionsand
howthey canberewritten asqueries toasearch engine. Itknowsthat Who killed Abraham
Lincoln canberewritten asthequerykilled Abraham Lincolnandas Abraham Lincoln
waskilled by. Itissues theserewritten queries andexamines theresults thatcomeback
not the full Web pages just the short summaries of text that appear near the query terms.
Theresultsarebrokeninto1-2-and3-gramsandtalliedforfrequencyintheresultsetsand
for weight: an n-gram that came back from a very specific query rewrite such as the exact
phrase match query Abraham Lincoln was killed by would get more weight than one
from a general query rewrite such as Abraham O R Lincoln O R killed. We would expect
that John Wilkes Boothwouldbeamongthehighlyrankedn-gramsretrievedbutsowould
Abraham Lincolnandtheassassination ofand Fords Theatre.
Once the n-grams are scored they are filtered by expected type. If the original query
startswithwhothenwefilteronnamesofpeople;forhowmanywefilteronnumbersfor
whenonadateortime. Thereisalsoafilterthatsaystheanswershouldnotbepartofthe
question; together these should allow us to return John Wilkes Booth and not Abraham
Lincolnasthehighest-scoring response.
In some cases the answer will be longer than three words; since the components re-
sponses only go up to 3-grams a longer response would have to be pieced together from
shorter pieces. For example in a system that used only bigrams the answer John Wilkes
Boothcouldbepiecedtogetherfromhigh-scoringpieces John Wilkesand Wilkes Booth.
At the Text Retrieval Evaluation Conference T RE C A SK MS R was rated as one of
the top systems beating out competitors with the ability to do far more complex language
understanding. A SK MS R relies upon the breadth of the content on the Web rather than on
its own depth of understanding. It wont be able to handle complex inference patterns like
associating whokilled withended thelifeof. Butitknowsthatthe Webissovastthatit
canaffordtoignorepassages likethatandwaitforasimplepassage itcanhandle.
I NF OR MA TI ON Informationextractionistheprocessofacquiringknowledgebyskimmingatextandlook-
E XT RA CT IO N
ing for occurrences of a particular class of object and for relationships among objects. A
typical task is to extract instances of addresses from Web pages with database fields for
street city state and zip code; or instances of storms from weather reports with fields for
temperature wind speed and precipitation. In a limited domain this can be done with high
accuracy. Asthedomaingetsmoregeneral morecomplexlinguistic modelsandmorecom-
plex learning techniques are necessary. We will see in Chapter 23 how to define complex
language models of the phrase structure noun phrases and verb phrases of English. But so
far there are no complete models of this kind so for the limited needs of information ex-
traction we define limited models that approximate the full English model and concentrate
on just the parts that are needed for the task at hand. The models we describe in this sec-
tionareapproximations inthesamewaythatthesimple1-C NFlogical modelin Figure7.21
page271isanapproximations ofthefullwigglylogical model.
In this section we describe six different approaches to information extraction in order
ofincreasing complexity onseveraldimensions: deterministic tostochastic domain-specific
togeneral hand-crafted tolearned andsmall-scale tolarge-scale.
A TT RI BU TE-B AS ED Thesimplesttypeofinformation extraction system isan attribute-based extraction system
E XT RA CT IO N
thatassumesthattheentiretextreferstoasingleobjectandthetaskistoextractattributes of
that object. For example we mentioned in Section 12.7 the problem of extracting from the
text I BM Think Book 970. Our price: 399.00 the set of attributes Manufacturer I BM
Model Think Book970 Price399.00. We can address this problem by defining a tem-
plate also known as a pattern for each attribute we would like to extract. The template is
T EM PL AT E
R EG UL AR definedbyafinitestateautomatonthesimplestexampleofwhichistheregularexpression
E XP RE SS IO N
or regex. Regular expressions are used in Unix commands such as grep in programming
languages such as Perl and in word processors such as Microsoft Word. The details vary
slightly from one tool to another and so are best learned from the appropriate manual but
hereweshowhowtobuilduparegularexpression templatefor pricesindollars:
0-9 matchesanydigitfrom0to9
0-9 matchesoneormoredigits
.0-90-9 matchesaperiodfollowedbytwodigits
.0-90-9? matchesaperiodfollowedbytwodigitsornothing
0-9.0-90-9? matches249.99or1.23or1000000 or...
Templatesareoftendefinedwiththreeparts: aprefixregexatargetregexandapostfixregex.
Forpricesthetargetregexisasabovetheprefixwouldlook forstringssuchasprice: and
the postfix could be empty. The idea is that some clues about an attribute come from the
attribute valueitselfandsomecomefromthesurrounding text.
If a regular expression for an attribute matches the text exactly once then we can pull
outtheportion ofthetextthatisthevalueoftheattribute. Ifthereisnomatchallwecando
isgiveadefaultvalueorleavetheattributemissing;butifthereareseveralmatchesweneed
aprocesstochooseamongthem. Onestrategyistohaveseveraltemplatesforeachattribute
ordered by priority. So for example the top-priority template for price might look for the
prefixourprice:;ifthatisnotfoundwelookfortheprefix price: andifthatisnotfound
the empty prefix. Another strategy is to take all the matches and find some way to choose
among them. Forexample we could take the lowest price that is within 50 of the highest
price. Thatwillselect78.00asthetargetfromthetext Listprice99.00 specialsaleprice
78.00shipping 3.00.
R EL AT IO NA L Onestepupfromattribute-based extractionsystemsare relationalextractionsystems
E XT RA CT IO N
which deal with multiple objects and the relations among them. Thus when these systems
seethetext249.99theyneedtodeterminenotjustthatitisapricebutalsowhichobject
hasthatprice. Atypicalrelational-based extractionsystemis F AS TU Swhichhandlesnews
storiesaboutcorporate mergersandacquisitions. Itcanreadthestory
Section22.4. Information Extraction 875
Bridgestone Sports Co. said Friday it has set up a joint venture in Taiwan with a local
concernanda Japanesetradinghousetoproducegolfclubstobeshippedto Japan.
andextracttherelations:
e Joint Ventures Productegolf clubs Datee Friday
Membere Bridgestone Sports Co Memberea local concern
Memberea Japanesetrading house.
C AS CA DE D
A relational extraction system can be built asa series of cascaded finite-state transducers.
F IN IT E-S TA TE
T RA NS DU CE RS
Thatisthesystem consists ofaseries ofsmallefficientfinite-state automata F SAswhere
each automaton receives text as input transduces the text into adifferent format and passes
italongtothenextautomaton. F AS TU S consistsoffivestages:
1. Tokenization
2. Complex-wordhandling
3. Basic-group handling
4. Complex-phrase handling
5. Structuremerging
F AS TU Ss first stage is tokenization which segments the stream of characters into tokens
words numbers and punctuation. For English tokenization can be fairly simple; just sep-
aratingcharacters atwhitespaceorpunctuation doesafairlygoodjob. Sometokenizers also
dealwithmarkuplanguages suchas H TM LS GM Land X ML.
Thesecond stage handles complexwords including collocations suchassetup and
joint venture as well as proper names such as Bridgestone Sports Co. These are rec-
ognized by a combination of lexical entries and finite-state grammar rules. For example a
companynamemightberecognized bytherule
Capitalized Word Company Co Inc Ltd
The third stage handles basic groups meaning noun groups and verb groups. The idea is
to chunk these into units that will be managed by the later stages. We will see how to write
a complex description of noun and verb phrases in Chapter 23 but here we have simple
rules that only approximate the complexity of English but have the advantage of being rep-
resentable by finite state automata. The example sentence would emerge from this stage as
thefollowingsequence oftaggedgroups:
Here N Gmeansnoungroup V Gisverbgroup P Rispreposition and C Jisconjunction.
The fourth stage combines the basic groups into complex phrases. Again the aim
is to have rules that are finite-state and thus can be processed quickly and that result in
unambiguous or nearly unambiguous output phrases. One type of combination rule deals
withdomain-specific events. Forexampletherule
Company Set Up Joint Venturewith Company?
captures one way to describe the formation of a joint venture. This stage is the first one in
thecascadewheretheoutputisplacedintoadatabasetemplateaswellasbeingplacedinthe
output stream. The final stage merges structures that were built up in the previous step. If
thenextsentence says Thejointventurewillstartproduction in Januarythenthisstepwill
notice that there are two references to a joint venture and that they should be merged into
one. Thisisaninstance oftheidentityuncertaintyproblemdiscussed in Section14.6.3.
Ingeneralfinite-statetemplate-basedinformationextractionworkswellforarestricted
domaininwhichitispossible topredetermine whatsubjects willbediscussed andhowthey
will be mentioned. The cascaded transducer model helps modularize the necessary knowl-
edge easing construction of the system. These systems work especially well when they are
reverse-engineering textthathasbeengenerated byaprogram. Forexample ashopping site
onthe Webisgenerated byaprogram thattakesdatabase entriesandformatstheminto Web
pages; a template-based extractor then recovers the original database. Finite-state informa-
tionextraction islesssuccessful atrecovering information inhighly variable format suchas
textwrittenbyhumansonavarietyofsubjects.
Wheninformationextractionmustbeattemptedfromnoisyorvariedinputsimplefinite-state
approaches fare poorly. It is too hard to get all the rules and their priorities right; it is better
touseaprobabilistic modelratherthanarule-based model. Thesimplestprobabilistic model
forsequences withhidden stateisthehidden Markovmodelor H MM.
Recall from Section 15.3 that an H MM models a progression through a sequence of
hidden states x with an observation e at each step. To apply H MMs to information ex-
t t
traction we can either build one big H MM for all the attributes or build a separate H MM
for each attribute. Well do the second. The observations are the words of the text and the
hiddenstatesarewhetherweareinthetarget prefixorpostfixpartoftheattribute template
orin the background not part of a template. Forexample here is a brief text and the most
probable Viterbipathforthattextfortwo H MMsonetrainedtorecognize thespeakerina
talkannouncement andonetrainedtorecognizedates. The-indicatesabackground state:
Text: There will be a seminar by Dr. Andrew Mc Callum on Friday
Speaker: - - - - P RE P RE T AR GE T T AR GE T T AR GE T P OS T -
Date: - - - - - - - - - P RE T AR GE T
H MMshavetwobigadvantagesover F SAsforextraction. First H MMsareprobabilisticand
thus tolerant to noise. In a regular expression if a single expected character is missing the
regexfailstomatch;with H MMsthereisgracefuldegradationwithmissingcharacterswords
andwegetaprobabilityindicatingthedegreeofmatchnotjusta Booleanmatchfail. Second
Section22.4. Information Extraction 877
dr
who : professor
speaker with 0.99 robert
speak 1.0 ; michael
appointment how
will
0.99
""
has
w 0.56 is
cavalier
stevens
seminar that christel
reminder 1.0 by l
theater speakers 0.24
artist
additionally here
0.44
Prefix Target Postfix
square states are the target note the second target state has a self-loop so the target can
match a string of any length the fourcircles to the left are the prefix and the one on the
rightisthepostfix. Foreachstateonlyafewofthehigh-probabilitywordsareshown.From
Freitagand Mc Callum2000.
H MMscan be trained from data; they dont require laborious engineering of templates and
thustheycanmoreeasilybekeptuptodateastextchangesovertime.
Notethatwehaveassumed acertain levelofstructure inour H MMtemplates: theyall
consist of one or more target states and any prefix states must precede the targets postfix
states most follow the targets and other states must be background. This structure makes
it easier to learn H MMs from examples. With a partially specified structure the forward
backward algorithm can be used to learn both the transition probabilities P X t X t1 be-
tween states and the observation model P E X which says how likely each word is in
t t
each state. For example the word Friday would have high probability in one or more of
thetargetstatesofthedate H MMandlowerprobability elsewhere.
Withsufficienttrainingdatathe H MMautomaticallylearnsastructureofdatesthatwe
findintuitive: thedate H MMmighthaveonetargetstateinwhichthehigh-probability words
are Monday Tuesday etc. and which has a high-probability transition to a target state
with words Jan January Feb etc. Figure 22.2 shows the H MM for the speaker of a
talk announcement as learned from data. The prefix covers expressions such as Speaker:
and seminar by and the target has one state that covers titles and first names and another
statethatcoversinitialsandlastnames.
Once the H MMs have been learned we can apply them to a text using the Viterbi
algorithm to find the most likely path through the H MM states. One approach is to apply
each attribute H MM separately; in this case you would expect most of the H MMs to spend
most of their time in background states. This is appropriate when the extraction is sparse
whenthenumberofextracted wordsissmallcomparedtothelengthofthetext.
Theotherapproachistocombinealltheindividualattributesintoonebig H MMwhich
wouldthenfindapaththatwandersthrough different target attributes firstfindingaspeaker
target then a date target etc. Separate H MMs are better when we expect just one of each
attribute in a text and one big H MM is better when the texts are more free-form and dense
with attributes. With either approach in the end we have a collection of target attribute
observations and have to decide what to do with them. If every expected attribute has one
target filler then the decision is easy: we have an instance of the desired relation. If there
aremultiple fillersweneedtodecide whichtochoose aswediscussed withtemplate-based
systems. H MMs have the advantage of supplying probability numbers that can help make
thechoice. Ifsometargets aremissingweneedtodecideifthisisaninstance ofthedesired
relationatallorifthetargetsfoundarefalsepositives. Amachinelearningalgorithmcanbe
trainedtomakethischoice.
One issue with H MMs for the information extraction task is that they model a lot of prob-
abilities that we dont really need. An H MM is a generative model; it models the full joint
probability ofobservations andhiddenstatesandthuscanbeusedtogeneratesamples. That
is we can use the H MM model not only to parse a text and recover the speaker and date
butalsotogenerate arandom instance ofatextcontaining aspeakerandadate. Sincewere
not interested in that task it is natural to ask whether we might be better off with a model
that doesnt bother modeling that possibility. All we need in order to understand a text is a
discriminative model one that models the conditional probability of the hidden attributes
given the observations the text. Given a text e the conditional model finds the hidden
1:N
statesequence X thatmaximizes P X e .
1:N 1:N 1:N
Modeling this directly gives us some freedom. We dont need the independence as-
sumptions of the Markov modelwe can have an x that is dependent on x . A framework
t 1
C ON DI TI ON AL forthistype of modelis the conditional random fieldor C RFwhich models aconditional
R AN DO MF IE LD
probability distribution of a set of target variables given a set of observed variables. Like
Bayesiannetworks C RFscanrepresentmanydifferentstructuresofdependenciesamongthe
L IN EA R-C HA IN
variables. One common structure is the linear-chain conditional random field for repre-
C ON DI TI ON AL
R AN DO MF IE LD
senting Markovdependencies amongvariables inatemporalsequence. Thus H MMsarethe
temporal version of naive Bayes models and linear-chain C RFs are the temporal version of
logistic regression where thepredicted target isan entire state sequence ratherthan asingle
binaryvariable.
Lete be the observations e.g. words in a document and x be the sequence of
1:N 1:N
hidden states e.g. the prefix target and postfix states. A linear-chain conditional random
fielddefinesaconditional probability distribution:
P
Px e αe N i1 Fx i1x iei
1:N 1:N
whereαisanormalizationfactortomakesuretheprobabilitiessumto1and F isafeature
function definedastheweightedsumofacollection ofk component featurefunctions:
cid:12
Fx i1x iei λ kf kx i1x iei.
k
Section22.4. Information Extraction 879
The λ parameter values are learned with a M AP maximum a posteriori estimation proce-
k
durethatmaximizestheconditional likelihood ofthetraining data. Thefeaturefunctions are
thekeycomponentsofa C RF.Thefunctionf
k
hasaccesstoapairofadjacentstatesx i1and
x butalsotheentireobservationwordsequence eandthecurrentpositioninthetemporal
i
sequence i. This gives us a lot of flexibility in defining features. We can define a simple
feature function forexample one that produces avalue of1ifthe current wordis A ND RE W
andthecurrentstateis S PEcid:24 AK ER:
f 1x i1x iei 01 oif thx ei r wis S e P EA KE R ande i A ND RE W
Howarefeaturesliketheseused? Itdependsontheircorresponding weights. Ifλ 0then
1
whenever f is true it increases the probability of the hidden state sequence x . This is
another way of saying the C RFmodel should prefer the target state S PE AK ER for the word
A ND RE W. If on the other hand λ
1
0 the C RF model will try to avoid this association
andifλ 0thisfeatureisignored. Parametervaluescanbesetmanuallyorcanbelearned
1
fromdata. Nowconsiderasecondfeaturefunction:
cid:24
f 2x i1x iei 01 oif thx ei r wis S e P EA KE R ande i1 S AI D
This feature is true if the current state is S PE AK ER and the next word is said. One would
therefore expect apositive λ valuetogowiththefeature. Moreinterestingly notethatboth
2
f and f can hold at the same time for a sentence like Andrew said .... In this case the
two features overlap each other and both boost the belief in x 1 S PE AK ER. Because of the
independence assumption H MMscannot useoverlapping features; C RFscan. Furthermore
a feature in a C RFcan use any part of the sequence e . Features can also be defined over
1:N
transitionsbetweenstates. Thefeatureswedefinedherewerebinarybutingeneralafeature
functioncanbeanyreal-valuedfunction. Fordomainswherewehavesomeknowledgeabout
the types of features we would like to include the C RF formalism gives us a great deal of
flexibility in defining them. This flexibility can lead to accuracies that are higher than with
lessflexiblemodelssuchas H MMs.
So far we have thought of information extraction as finding a specific set of relations e.g.
speaker time location in a specific text e.g. a talk announcement. A different applica-
tion of extraction technology is building a large knowledge base or ontology of facts from
a corpus. This is different in three ways: First it is open-endedwe want to acquire facts
about all types of domains not just one specific domain. Second with a large corpus this
taskisdominatedbyprecision notrecalljustaswithquestionansweringonthe Web Sec-
tion 22.3.6. Third the results can be statistical aggregates gathered from multiple sources
ratherthanbeingextractedfromonespecifictext.
For example Hearst 1992 looked at the problem of learning an ontology of concept
categories and subcategories from a large corpus. In 1992 a large corpus was a 1000-page
encyclopedia; todayitwouldbea100-million-page Webcorpus. Theworkconcentrated on
templates that are very general not tied to a specific domain and have high precision are
almostalwayscorrectwhentheymatchbutlowrecalldonot alwaysmatch. Hereisoneof
themostproductive templates:
N P suchas N P N P? andor N P? .
Here the bold words and commas must appear literally in the text but the parentheses are
for grouping the asterisk means repetition of zero or more and the question mark means
optional. N P is avariable standing fora noun phrase; Chapter 23 describes how to identify
nounphrases;fornowjustassumethatweknowsomewordsarenounsandotherwordssuch
as verbs that we can reliably assume are not part of a simple noun phrase. This template
matches the texts diseases such asrabies affect your dog and supports network protocols
such as D NS concluding that rabies is a disease and D NS is a network protocol. Similar
templatescanbeconstructed withthekeywordsincludingespeciallyandorother. Of
course these templates will fail to match many relevant passages like Rabies is a disease.
Thatisintentional. The N P isa N Ptemplatedoesindeedsometimesdenoteasubcategory
relation but it often means something else as in There is a God or She is a little tired.
Withalargecorpuswecanaffordtobepicky;touseonlythehigh-precision templates. Well
miss many statements of a subcategory relationship but most likely well find a paraphrase
ofthestatementsomewhereelseinthecorpusinaformwecanuse.
Thesubcategory relationissofundamentalthatisworthwhiletohandcraftafewtemplatesto
helpidentify instances ofitoccurring innaturallanguage text. Butwhataboutthethousands
of other relations in the world? There arent enough A I grad students in the world to create
anddebug templates forallofthem. Fortunately itispossible tolearntemplates fromafew
examplesthenusethetemplatestolearnmoreexamplesfromwhichmoretemplatescanbe
learnedandsoon. Inoneofthefirstexperimentsofthiskind Brin1999startedwithadata
setofjustfiveexamples:
Isaac Asimov The Robotsof Dawn
David Brin Startide Rising
James Gleick Chaos Makinga New Science
Charles Dickens Great Expectations
William Shakespeare The Comedyof Errors
Clearlytheseareexamplesoftheauthortitle relationbutthelearningsystemhadnoknowl-
edge of authors or titles. The words in these examples were used in a search over a Web
corpus resulting in199matches. Eachmatchisdefinedasatupleofsevenstrings
Author Title Order Prefix Middle Postfix U RL
where Order is true if the author came first and false if the title came first Middle is the
characters between theauthorandtitle Prefixisthe 10characters before the match Suffix is
the10characters afterthematchand U RListhe Webaddresswherethematchwasmade.
Given a set of matches a simple template-generation scheme can find templates to
explainthematches. Thelanguage oftemplateswasdesigned tohaveaclosemappingtothe
matches themselves tobeamenable toautomated learning andtoemphasize high precision
Section22.4. Information Extraction 881
possibly at the risk of lower recall. Each template has the same seven components as a
match. The Author and Title are regexes consisting of any characters but beginning and
ending in letters and constrained to have a length from half the minimum length of the
examples to twice the maximum length. The prefix middle and postfix are restricted to
literal strings not regexes. The middle is the easiest to learn: each distinct middle string in
the set of matches is a distinct candidate template. For each such candidate the templates
Prefixisthendefinedasthelongestcommonsuffixofalltheprefixesinthematchesandthe
Postfixisdefinedasthelongestcommonprefixofallthepostfixesinthematches. Ifeitherof
these is of length zero then the template is rejected. The U RL of the template is defined as
thelongestprefixofthe U RLsinthematches.
In the experiment run by Brin the first 199 matches generated three templates. The
mostproductivetemplatewas
L IB Title B by Author
U RL:www.sff.netlocusc
Thethreetemplateswerethenusedtoretrieve4047moreauthortitleexamples. Theexam-
ples were then used to generate more templates and so on eventually yielding over 15000
titles. Givenagood setoftemplates thesystem can collect agood setofexamples. Givena
goodsetofexamplesthesystemcanbuildagoodsetoftemplates.
The biggest weakness in this approach is the sensitivity to noise. If one of the first
few templates isincorrect errors can propagate quickly. One waytolimit this problem isto
not accept a new example unless it is verified by multiple templates and not accept a new
templateunlessitdiscoversmultipleexamplesthatarealsofoundbyothertemplates.
Automatedtemplateconstructionisabigstepupfromhandcraftedtemplateconstruction but
itstill requires ahandful oflabeled examples ofeachrelation toget started. Tobuild alarge
ontology withmanythousands ofrelations eventhatamount ofworkwould beonerous; we
wouldliketohaveanextractionsystemwithnohumaninputofanykindasystemthatcould
readonitsownandbuildupitsowndatabase. Suchasystemwouldberelation-independent;
would work for any relation. In practice these systems work on all relations in parallel
becauseofthe I Odemandsoflargecorpora. Theybehaveless likeatraditionalinformation-
extraction system thatistargeted atafewrelations andmorelikeahumanreaderwholearns
fromthetextitself;becauseofthisthefieldhasbeencalled machinereading.
M AC HI NE RE AD IN G
Arepresentative machine-reading systemis T EX TR UN NE R Bankoand Etzioni2008.
T EX TR UN NE R uses cotraining toboost itsperformance but itneeds something to bootstrap
from. Inthecaseof Hearst1992specificpatternse.g.suchasprovidedthebootstrapand
for Brin 1998 itwasaset offiveauthortitle pairs. For T EX TR UN NE R the original inspi-
ration wasataxonomy ofeight verygeneral syntactic templates asshownin Figure 22.3. It
wasfeltthatasmallnumberoftemplateslikethiscouldcovermostofthewaysthatrelation-
shipsareexpressedin English. Theactualbootsrappingstartsfromasetoflabelledexamples
that are extracted from the Penn Treebank acorpus of parsed sentences. Forexample from
theparseofthesentence Einstein received the Nobel Prize in1921 T EX TR UN NE R isable
23
N AT UR AL L AN GU AG E
F OR C OM MU NI CA TI ON
Inwhichweseehowhumanscommunicatewithoneanotherinnaturallanguage
andhowcomputeragentsmightjoinintheconversation.
Communicationistheintentional exchange ofinformation broughtaboutby theproduction
C OM MU NI CA TI ON
andperceptionofsignsdrawnfromasharedsystemofconventional signs. Mostanimalsuse
S IG N
signs torepresent important messages: foodhere predator nearby approach withdraw lets
mate. Inapartially observable world communication canhelpagents besuccessful because
theycanlearninformationthatisobservedorinferredbyothers. Humansarethemostchatty
of all species and if computer agents are to be helpful theyll need to learn to speak the
language. In this chapter we look at language models for communication. Models aimed at
deep understanding of a conversation necessarily need to be more complex than the simple
models aimed at say spam classification. We start with grammatical models of the phrase
structure of sentences add semantics to the model and then apply it to machine translation
andspeechrecognition.
The n-gram language models of Chapter 22 were based on sequences of words. The big
trigram probabilities to estimate and so a corpus of even a trillion words will not be able to
supply reliable estimates for all of them. We can address the problem of sparsity through
generalization. Fromthefactthatblack dogismorefrequent thandogblackandsimilar
observations we can form the generalization that adjectives tend to come before nouns in
English whereas they tend to follow nouns in French: chien noir is more frequent. Of
coursetherearealwaysexceptions;galoreisanadjectivethatfollowsthenounitmodifies.
Despitetheexceptionsthenotionofalexicalcategoryalsoknownasapartofspeechsuch
L EX IC AL CA TE GO RY
asnounoradjective isausefulgeneralizationuseful initsownrightbutmore sowhenwe
S YN TA CT IC string together lexical categories to form syntactic categories such as noun phrase or verb
C AT EG OR IE S
phrase and combine these syntactic categories into trees representing the phrase structure
P HR AS ES TR UC TU RE
ofsentences: nestedphrases eachmarkedwithacategory.
888
Section23.1. Phrase Structure Grammars 889
G EN ER AT IV E C AP AC IT Y
Grammaticalformalismscanbeclassifiedbytheirgenerativecapacity: thesetof
languages theycanrepresent. Chomsky1957describesfourclassesofgrammat-
ical formalisms that differ only in the form of the rewrite rules. The classes can
be arranged in a hierarchy where each class can be used to describe all the lan-
guages that can be described by a less powerful class as well as some additional
languages. Herewelistthehierarchy mostpowerfulclassfirst:
Recursively enumerable grammars use unrestricted rules: both sides of the
rewrite rules can have any number of terminal and nonterminal symbols asin the
rule A B C D E. Thesegrammarsareequivalent to Turingmachinesintheir
expressivepower.
Context-sensitive grammars are restricted only in that the right-hand side
must contain at least as many symbols as the left-hand side. The name context-
sensitive comes from the fact that a rule such as A X B A Y B says that
an X can be rewritten as a Y in the context of a preceding A and a following B.
Context-sensitive grammars can represent languages such as anbncn a sequence
ofncopiesofafollowedbythesamenumberofbsandthencs.
In context-free grammars or C FGs the left-hand side consists of a sin-
gle nonterminal symbol. Thus each rule licenses rewriting the nonterminal as
the right-hand side in any context. C FGs are popular for natural-language and
programming-language grammars although itisnowwidelyaccepted thatatleast
somenaturallanguageshaveconstructionsthatarenotcontext-free Pullum1991.
Context-free grammarscanrepresent anbnbutnotanbncn.
Regulargrammars are the most restricted class. Every rule has asingle non-
terminalontheleft-handsideandaterminalsymboloptionallyfollowedbyanon-
terminalontheright-handside. Regulargrammarsareequivalentinpowertofinite-
state machines. They are poorly suited for programming languages because they
cannot represent constructs such as balanced opening and closing parentheses a
variation ofthe anbn language. Theclosest theycancomeisrepresenting a b a
sequence ofanynumberofasfollowedbyanynumberofbs.
The grammars higher up in the hierarchy have more expressive power but
the algorithms for dealing with them are less efficient. Up to the 1980s linguists
focusedoncontext-freeandcontext-sensitivelanguages. Sincethentherehasbeen
renewed interest in regular grammars brought about by the need to process and
learn from gigabytes or terabytes of online text very quickly even at the cost of
a less complete analysis. As Fernando Pereira put it The older I get the further
down the Chomsky hierarchy I go. To see what he means compare Pereira and
Warren 1980 with Mohri Pereira and Riley 2002 and note that these three
authorsallnowworkonlargetextcorporaat Google.
There have been many competing language models based on the idea of phrase struc-
P RO BA BI LI ST IC
ture; we will describe a popular model called the probabilistic context-free grammar or
C ON TE XT-F RE E
G RA MM AR P CF G.1 A grammar is a collection of rules that defines a language as a set of allowable
G RA MM AR
strings ofwords. Context-free isdescribed inthesidebar onpage 889 and probabilistic
L AN GU AG E
meansthatthegrammarassignsaprobability toeverystring. Hereisa P CF Grule:
V P Verb 0.70
V P N P 0.30.
N ON-T ER MI NA L Here V P verb phrase and N P noun phrase are non-terminal symbols. The grammar
S YM BO LS
alsorefers toactual words whicharecalled terminalsymbols. Thisruleissaying thatwith
T ER MI NA LS YM BO L
probability 0.70 averb phrase consists solely ofa verb and with probability 0.30 itis a V P
followedbyan N P. Appendix Bdescribes non-probabilistic context-free grammars.
Wenow defineagrammarforatiny fragment of English thatissuitable forcommuni-
cationbetweenagentsexploringthewumpusworld. Wecallthislanguage E . Latersections
0
improve on E to make it slightly closer to real English. We are unlikely ever to devise a
0
completegrammarfor English ifonlybecausenotwopersons wouldagreeentirelyonwhat
constitutes valid English.
0
Firstwedefinethelexiconorlistofallowablewords. Thewordsaregroupedintothelexical
L EX IC ON
categories familiar to dictionary users: nouns pronouns and names to denote things; verbs
to denote events; adjectives to modify nouns; adverbs to modify verbs; and function words:
articles such as the prepositions in and conjunctions and. Figure 23.1 shows a small
lexiconforthelanguage E .
0
Eachofthecategories endsin...toindicate thatthereareotherwordsinthecategory.
For nouns names verbs adjectives and adverbs it is infeasible even in principle to list all
the words. Not only are there tens of thousands of members in each class but new ones
like i Pod or biodieselare being added constantly. These five categories are called open
classes. Forthecategoriesofpronoun relativepronoun article preposition andconjunction
O PE NC LA SS
we could have listed all the words with a little more work. These are called closed classes;
C LO SE DC LA SS
they have a small number of words a dozen or so. Closed classes change over the course
ofcenturies notmonths. Forexample thee and thou werecommonly used pronouns in
the17thcentury wereonthedeclineinthe19thandareseen todayonlyinpoetryandsome
regionaldialects.
0
The next step is to combine the words into phrases. Figure 23.2 shows a grammar for E
0
with rules for each of the six syntactic categories and an example for each rewrite rule.2
P AR SE TR EE
thatfollowedbyaverbphrase. Anexampleofarelativeclauseisthatstinksin Thewumpusthatstinksisin
22.Anotherkindofrelativeclausehasnorelativepronoune.g.Iknowintheman Iknow.
Section23.1. Phrase Structure Grammars 891
Noun stench 0.05 breeze 0.10 wumpus0.15 pits 0.05 ...
Verb is 0.10 feel 0.10 smells 0.10 stinks 0.05 ...
Adjective right 0.10 dead 0.05 smelly 0.02 breezy 0.02...
Adverb here 0.05 ahead 0.05 nearby 0.02 ...
Pronoun me 0.10 you 0.03 I0.10 it 0.10 ...
Rel Pro that 0.40 which 0.15 who0.20 whom 0.02...
Name John 0.01 Mary 0.01 Boston 0.01 ...
Article the 0.40 a 0.30 an 0.10 every 0.05 ...
Prep to 0.20 in 0.10 on 0.05 near 0.10 ...
Conj and 0.50 or 0.10 but0.20 yet 0.02...
Digit 0 0.20 1 0.20 2 0.20 3 0.20 4 0.20 ...
Figure23.1 Thelexiconfor E .Rel Proisshortforrelativepronoun Prepforpreposition
0
and Conj forconjunction.Thesumoftheprobabilitiesforeachcategoryis1.
E : S N P V P 0.90 Ifeelabreeze
0
S Conj S 0.10 Ifeelabreezeand Itstinks
N P Pronoun 0.30 I
Name 0.10 John
Noun 0.10 pits
Article Noun 0.25 thewumpus
Article Adjs Noun 0.05 thesmellydeadwumpus
Digit Digit 0.05 34
N P P P 0.10 thewumpusin13
N P Rel Clause 0.05 thewumpusthatissmelly
V P Verb 0.40 stinks
V P N P 0.35 feelabreeze
V P Adjective 0.05 smellsdead
V P P P 0.10 isin13
V P Adverb 0.10 goahead
Adjs Adjective 0.80 smelly
Adjective Adjs 0.20 smellydead
P P Prep N P 1.00 totheeast
Rel Clause Rel Pro V P 1.00 thatissmelly
Figure23.2 Thegrammarfor E withexamplephrasesforeachrule. Thesyntacticcat-
0
egories are sentence S noun phrase N P verb phrase V P list of adjectives Adjs
prepositionalphrase P Pandrelativeclause Rel Clause.
S
0.90
N P V P
Article Noun Verb
Every wumpus smells
Figure23.3 Parsetreeforthesentence Everywumpussmellsaccordingtothegrammar
E . Eachinteriornodeofthetreeislabeledwithitsprobability. Theprobabilityofthetree
0
asawholeis0.90.250.050.150.400.100.0000675. Sincethistreeistheonly
parseofthesentencethatnumberisalsotheprobabilityof thesentence. Thetreecanalso
bewritteninlinearformas S N P Article every Noun wumpus V P Verb smells.
givesaconstructive proofthatthestring ofwordsisindeed asentence according totherules
of E . The E grammargenerates awiderangeof Englishsentences suchasthefollowing:
Johnisinthepit
Thewumpusthatstinksisin22
Maryisin Bostonandthewumpusisnear32
Unfortunately thegrammar overgenerates: thatisitgenerates sentences that arenotgram-
O VE RG EN ER AT IO N
matical such as Me go Boston and I smell pits wumpus John. It also undergenerates:
U ND ER GE NE RA TI ON
there are many sentences of English that it rejects such as I think the wumpus is smelly.
Wewillseehow to learn abetter grammarlater; fornow weconcentrate onwhat wecan do
withthegrammarwehave.
Parsingistheprocessofanalyzingastringofwordstouncoveritsphrasestructureaccording
P AR SI NG
totherulesofagrammar. Figure23.4showsthatwecanstartwiththe S symbol andsearch
topdownforatreethathasthewordsasitsleavesorwecanstartwiththewordsandsearch
bottom up for a tree that culminates in an S. Both top-down and bottom-up parsing can be
inefficienthoweverbecausetheycanenduprepeatingeffortinareasofthesearchspacethat
leadtodeadends. Considerthefollowingtwosentences:
Havethestudentsinsection2of Computer Science101taketheexam.
Havethestudentsinsection2of Computer Science101takentheexam?
Eventhoughtheysharethefirst10wordsthesesentenceshaveverydifferentparsesbecause
the first is a command and the second is a question. A left-to-right parsing algorithm would
have toguess whetherthe firstwordispart of acommand oraquestion andwillnot beable
to tell if the guess is correct until at least the eleventh word take or taken. If the algorithm
guesseswrongitwillhavetobacktrackallthewaytothefirstwordandreanalyze thewhole
sentence undertheotherinterpretation.
Section23.2. Syntactic Analysis Parsing 893
Listofitems Rule
S
N P V P S N P V P
N P V P Adjective V P V P Adjective
N P Verb Adjective V P Verb
N P Verb dead Adjective dead
N P isdead Verb is
Article Noun isdead N P Article Noun
Article wumpusisdead Noun wumpus
thewumpusisdead Article the
Figure23.4 Traceoftheprocessoffindingaparseforthestring Thewumpusisdead
asasentenceaccordingtothegrammar E . Viewedasatop-downparsewestartwiththe
0
listofitemsbeing S andoneachstepmatchanitem X witharuleoftheform X ...
andreplace X inthelistofitemswith.... Viewedasabottom-upparsewestartwiththe
listofitemsbeingthewordsofthesentenceandoneachstepmatchastringoftokens...
inthelistagainstaruleoftheform X ... andreplace... with X.
To avoid this source of inefficiency we can use dynamic programming: every time we
analyze a substring store the results so we wont have to reanalyze it later. For example
oncewediscoverthatthestudentsinsection2of Computer Science101isan N Pwecan
recordthatresultinadatastructureknownasachart. Algorithmsthatdothisarecalledchart
C HA RT
parsers. Because we are dealing with context-free grammars any phrase that was found in
thecontextofonebranchofthesearchspacecanworkjustaswellinanyotherbranchofthe
search space. Therearemanytypes ofchartparsers; wedescribe abottom-up version called
the C YKalgorithmafteritsinventors John Cocke Daniel Younger and Tadeo Kasami.
C YK AL GO RI TH M
The C YK algorithm is shown in Figure 23.5. Note that it requires a grammar with all
rulesinoneoftwoveryspecificformats: lexicalrulesofthe form X wordandsyntactic
C HO MS KY NO RM AL rules of the form X Y Z. This grammar format called Chomsky Normal Form may
F OR M
seem restrictive but it is not: any context-free grammar can be automatically transformed
into Chomsky Normal Form. Exercise23.8leadsyouthroughtheprocess.
The C YK algorithm uses space of On2m for the P table where n is the number of
wordsinthesentenceandmisthenumberofnonterminalsymbolsinthegrammarandtakes
time On3m. Sincemisconstant foraparticular grammar this iscommonly described as
On3. No algorithm can do better for general context-free grammars although there are
fasteralgorithms onmorerestricted grammars. Infact itisquite atrick forthealgorithm to
completein On3timegiventhatitispossibleforasentencetohaveanexponentialnumber
ofparsetrees. Considerthesentence
Fallleavesfallandspringleavesspring.
It is ambiguous because each word except and can be either a noun ora verb and fall
and spring can be adjectives as well. For example one meaning of Fall leaves fall is
function C YK-P AR SEwordsgrammarreturns Patableofprobabilities
N L EN GT Hwords
M thenumberofnonterminalsymbolsingrammar
Panarrayofsize M NNinitiallyall0
Insertlexicalrulesforeachword
fori 1to N do
foreachruleofform X wordsipdo
P Xi1p
Combinefirstandsecondpartsofright-handsidesofrulesfromshorttolong
forlength 2to N do
forstart 1to N length 1 do
forlen1 1to N 1do
len2 lengthlen1
foreachruleoftheform X Y Z pdo
P Xstartlength M AX PX start length
P Y start len1 P Z start len1 len2 p
return P
most probable derivation for the whole sequence and for each subsequence. It returns the
wholetable P in whichan entry P X start len is the probabilityofthe mostprobable
X of lengthlen starting atposition start. If thereis no X of thatsize atthatlocation the
probabilityis0.
equivalent to Autumnabandons autumn. With E thesentence hasfourparses:
0
S S N P Fallleavesfalland S N P springleavesspring
S S N P Fallleavesfalland S spring V P leavesspring
S S Fall V P leavesfalland S N P springleavesspring
S S Fall V P leavesfalland S spring V P leavesspring .
Ifwehadctwo-ways-ambiguous conjoined subsentences wewouldhave2c waysofchoos-
ing parses forthe subsentences.3 How does the C YK algorithm process these 2c parse trees
in Oc3 time? The answer is that it doesnt examine all the parse trees; all it has to do is
compute the probability of the most probable tree. The subtrees are all represented in the P
tableandwithalittleworkwecouldenumeratethemallinexponentialtimebutthebeauty
ofthe C YKalgorithm isthatwedonthavetoenumeratethemunlesswewantto.
Inpracticeweareusuallynotinterestedinallparses;justthebestoneorbestfew. Think
of the C YK algorithm as defining the complete state space defined by the apply grammar
""
rule operator. It is possible to search just part of this space using A search. Each state
in this space is a list of items words or categories as shown in the bottom-up parse table
Figure 23.4. The start state is a list of words and a goal state is the single item S. The
versus Xand Yand Z.Butthatisanotherstoryonetoldwellby Churchand Patil1982.
Section23.2. Syntactic Analysis Parsing 895
S NP-S BJ-2 Hereyes
V Pwere
V Pglazed
N P-2
S BA R-A DVasif
S NP-S BJshe
V Pdidnt
V PV Phear N P-1
or
V PA DV Pevensee N P-1
N P-1him
.
Figure23.6 Annotatedtree forthe sentence Hereyeswere glazedasif she didnthear
orevensee him. fromthe Penn Treebank. Notethatinthisgrammarthereisadistinction
betweenanobjectnounphrase N Pandasubjectnounphrase N P-S BJ.Notealsoagram-
maticalphenomenonwe havenotcoveredyet: the movementof a phrasefrom onepartof
thetreetoanother.Thistreeanalyzesthephrasehearorevenseehimasconsistingoftwo
constituent V Ps V P hear N P -1 and V P A DV P even see N P -1 both of which
haveamissingobjectdenoted-1whichreferstothe N P labeledelsewhereinthetreeas
N P-1him.
costofastateistheinverse ofitsprobability asdefinedbytherulesapplied sofar andthere
arevarious heuristics toestimatetheremaining distance tothegoal;thebestheuristics come
""
frommachinelearningappliedtoacorpusofsentences. Withthe A algorithmwedonthave
to search the entire state space and we are guaranteed that the first parse found will be the
mostprobable.
A P CF G has many rules with a probability for each rule. This suggests that learning the
grammarfromdatamightbebetterthanaknowledgeengineering approach. Learningiseas-
iestifwearegivenacorpusofcorrectlyparsedsentencescommonlycalledatreebank. The
T RE EB AN K
Penn Treebank Marcus etal. 1993 isthe best known; itconsists of3million words which
havebeenannotated withpartofspeech andparse-tree structure usinghumanlaborassisted
bysomeautomated tools. Figure23.6showsanannotated tree fromthe Penn Treebank.
Givenacorpusoftreeswecancreatea P CF Gjustbycountingandsmoothing. Inthe
exampleabovetherearetwonodesoftheform S NP...V P .... Wewouldcountthese
and all the other subtrees with root S in the corpus. If there are 100000 S nodes of which
60000areofthisformthenwecreatetherule:
S N P V P 0.60.
What if a treebank is not available but we have a corpus of raw unlabeled sentences? It is
still possible to learn a grammar from such a corpus but it is more difficult. First of all
weactually have twoproblems: learning the structure ofthe grammarrules and learning the
probabilitiesassociatedwitheachrule. Wehavethesamedistinctioninlearning Bayesnets.
Well assume that were given the lexical and syntactic category names. If not we can just
assume categories X ...X and use cross-validation to pick the best value of n. We can
then assume that the grammar includes every possible X Y Z or X word rule
although manyoftheseruleswillhaveprobability 0orclose to0.
Wecanthenuseanexpectationmaximization E Mapproachjustaswedidinlearning
H MMs. Theparameters wearetrying tolearn are therule probabilities; westartthem offat
random oruniform values. Thehidden variables aretheparse trees: wedont know whether
astringofwords w ...w isorisnotgenerated byarule X .... The Estepestimates
i j
the probability that each subsequence is generated by each rule. The M step then estimates
theprobability ofeachrule. Thewholecomputationcanbedoneinadynamic-programming
I NS ID EO UT SI DE fashion with an algorithm called the insideoutside algorithm in analogy to the forward
A LG OR IT HM
backwardalgorithm for H MMs.
Theinsideoutsidealgorithmseemsmagicalinthatitinducesagrammarfromunparsed
text. Butithasseveraldrawbacks. Firsttheparsesthatareassignedbytheinducedgrammars
areoften difficult tounderstand andunsatisfying tolinguists. Thismakes ithard tocombine
handcrafted knowledge with automated induction. Second itis slow: On3m3 where nis
the number of words in a sentence and m is the number of grammar categories. Third the
space of probability assignments is very large and empirically it seems that getting stuck in
localmaximaisasevereproblem. Alternativessuchassimulatedannealing cangetcloserto
the global maximum at a cost of even more computation. Lari and Young 1990 conclude
thatinsideoutside iscomputationally intractable forrealisticproblems.
Howeverprogress canbemadeifwearewillingtostepoutsidetheboundsoflearning
solelyfromunparsedtext. Oneapproachistolearnfrom prototypes: toseedtheprocesswith
adozenortworulessimilartotherulesin E . Fromtheremorecomplexrulescanbelearned
1
moreeasilyandtheresultinggrammarparses Englishwithanoverallrecallandprecisionfor
sentences of about 80 Haghighi and Klein 2006. Another approach is to use treebanks
butinadditiontolearning P CF Grulesdirectlyfromthebracketingsalsolearningdistinctions
thatarenotinthetreebank. Forexamplenotthatthetreein Figure23.6makesthedistinction
between N P and N P S BJ. The latter is used for the pronoun she the former for the
pronoun her. We will explore this issue in Section 23.6; for now let us just say that there
aremanywaysinwhich itwouldbe useful to splitacategory like N Pgrammarinduction
systems that use treebanks but automatically split categories do better than those that stick
with the original category set Petrov and Klein 2007c. The error rates for automatically
learnedgrammarsarestillabout50higherthanforhand-constructed grammarbutthegap
isdecreasing.
Theproblemwith P CF Gsisthattheyarecontext-free. Thatmeansthatthedifferencebetween
Peat abanana and Peat abandanna depends only on P Noun banana versus
P Noun bandanna and not on the relation between eat and the respective objects.
A Markov modelofordertwoormore givenasufficiently large corpus willknow that eat
Section23.3. Augmented Grammarsand Semantic Interpretation 897
a banana is more probable. We can combine a P CF G and Markov model to get the best of
both. The simplest approach is to estimate the probability of a sentence with the geometric
meanoftheprobabilitiescomputedbybothmodels. Thenwewouldknowthateatabanana
isprobable fromboththegrammaticalandlexicalpointofview. Butitstillwouldntpickup
the relation between eat and banana in eat a slightly aging but still palatable banana
because here the relation is more than two words away. Increasing the order of the Markov
model wont get at the relation precisely; to do that we can use a lexicalized P CF G as
described inthenextsection.
Anotherproblemwith P CF Gsisthattheytendtohavetoostrongapreferenceforshorter
sentences. In a corpus such as the Wall Street Journal the average length of a sentence
is about 25 words. But a P CF G will usually assign fairly high probability to many short
sentences suchas Hesleptwhereasinthe Journalweremorelikelytoseesomethinglike
Ithasbeenreportedbyareliablesourcethattheallegationthathesleptiscredible. Itseems
thatthephrases inthe Journal really arenotcontext-free; instead thewriters haveanideaof
theexpectedsentencelengthandusethatlengthasasoftglobalconstraintontheirsentences.
Thisishardtoreflectina P CF G.
In this section we see how to extend context-free grammarsto say that for example not
every N P isindependent ofcontext butrather certain N Psaremorelikely toappearinone
context andothersinanothercontext.
Togetattherelationship betweentheverbeatandthenounsbanana versusbandanna
wecanusealexicalized P CF Ginwhichtheprobabilities foraruledepend ontherelation-
L EX IC AL IZ ED PC FG
ship between words in the parse tree not just on the adjacency of words in a sentence. Of
course we cant have the probability depend on every word in the tree because we wont
have enough training data to estimate all those probabilities. Itisuseful tointroduce the no-
tion of the head of a phrasethe most important word. Thus eat is the head of the V P
H EA D
eat a banana and banana is the head of the N P a banana. Weuse the notation V Pv
to denote a phrase with category V P whose head word is v. We say that the category V P
A UG ME NT ED is augmented with the head variable v. Here is an augmented grammar that describes the
G RA MM AR
verbobject relation:
V Pv Verbv N Pn P vn
1
V Pv Verbv P v
2
N Pn Articlea Adjsj Nounn P na
3
Nounbanana banana p
n
... ...
Herethe probability P vndepends on thehead words v and n. Wewould set this proba-
1
bilitytoberelativelyhighwhenv iseatandnisbananaandlowwhennisbandanna.
Note that since we are considering only heads the distinction between eat a banana and
eat a rancid banana will not be caught by these probabilities. Another issue with this ap-
proachisthatinavocabulary withsay20000nounsand5000verbs P needs100million
1
probability estimates. Onlyafewpercentofthesecancomefromacorpus;therestwillhave
to come from smoothing see Section 22.1.2. For example we can estimate P vn for a
1
vn pair that we have not seen often or at all by backing off to a model that depends
onlyonv. Theseobjectlessprobabilities arestillveryuseful;theycancapturethedistinction
betweenatransitiveverblikeeatwhichwillhaveahighvaluefor P andalowvaluefor
1
P and an intransitive verb like sleep which will have the reverse. It is quite feasible to
2
learntheseprobabilities fromatreebank.
Augmented rules arecomplicated sowewillgive them aformaldefinition byshowing how
anaugmented rulecanbetranslated intoalogical sentence. Thesentence willhavetheform
D EF IN IT EC LA US E ofadefiniteclauseseepage256sotheresultiscalledadefiniteclausegrammaror D CG.
G RA MM AR
Well use as an example a version of a rule from the lexicalized grammar for N P with one
newpieceofnotation:
N Pn Articlea Adjsj Nounn Compatiblejn.
Thenewaspecthereisthenotationconstrainttodenotealogicalconstraintonsomeofthe
variables;theruleonlyholdswhentheconstraintistrue. Herethepredicate Compatiblejn
ismeanttotestwhetheradjectivejandnounnarecompatible;itwouldbedefinedbyaseries
ofassertions such as Compatibleblackdog. Wecanconvert thisgrammarruleintoadef-
initeclause by1reversing theorderofright-andleft-hand sides 2makingaconjunction
ofalltheconstituentsandconstraints3addingavariables tothelistofargumentsforeach
i
constituent torepresent thesequence ofwords spanned bytheconstituent 4adding aterm
for the concatenation of words Appends ... to the list of arguments for the root of the
1
tree. Thatgivesus
Articleas Adjsjs Nounns Compatiblejn
N Pn Appends s s .
Thisdefiniteclausesaysthatifthepredicate Article istrueofaheadwordaandastring s
1
and Adjs issimilarly trueofahead word j andastring s and Noun istrueofaheadword
2
n and a string s and if j and n are compatible then the predicate N P is true of the head
3
wordnandtheresultofappending strings s s ands .
The D CGtranslationleftouttheprobabilities butwecould putthembackin: justaug-
menteachconstituent withonemorevariable representing theprobability oftheconstituent
andaugment therootwithavariable thatistheproduct ofthe constituent probabilities times
theruleprobability.
The translation from grammar rule to definite clause allows us to talk about parsing
as logical inference. This makes it possible to reason about languages and strings in many
differentways. Forexampleitmeanswecandobottom-upparsingusingforwardchainingor
top-downparsingusingbackwardchaining. Infactparsing naturallanguagewith D CGswas
Section23.3. Augmented Grammarsand Semantic Interpretation 899
oneofthefirstapplications ofandmotivationsforthe Prologlogicprogramminglanguage.
L AN GU AG E Itissometimespossible toruntheprocessbackwardanddo languagegeneration aswellas
G EN ER AT IO N
parsing. Forexample skipping ahead to Figure 23.10 page 903 a logic program could be
giventhesemanticform Loves John Maryandapplythedefinite-clause rulestodeduce
S Loves John Mary Johnloves Mary.
Thisworksfortoyexamplesbutseriouslanguage-generationsystemsneedmorecontrolover
theprocessthanisaffordedbythe D CGrulesalone.
E : S N P V P ...
N P Pronoun Name Noun ...
S S
N P Pronoun Name Noun ...
O O
V P V P N P ...
O
P P Prep N P
O
Pronoun I youhe sheit ...
S
Pronoun me youhim herit ...
O
...
E : Shead N PSbjpnh V Ppnhead ...
2
N Pcpnhead Pronouncpnhead Nouncpnhead ...
V Ppnhead V Ppnhead N PObjph ...
P Phead Prephead N PObjpnh
Pronoun Sbj1 SI I
Pronoun Sbj1 Pwe we
Pronoun Obj1 Sme me
Pronoun Obj3 Pthem them
...
1
objective cases in noun phrases and thus does not overgeneratequite as badly as E . The
0
portionsthatareidenticalto E havebeenomitted. Bottom: partofanaugmentedgrammar
0
for E withthreeaugmentations: caseagreementsubjectverbagreementandheadword.
2
Sbj Obj1 S1 Pand3 Pareconstantsandlowercasenamesarevariables.
We saw in Section 23.1 that the simple grammar for E overgenerates producing nonsen-
0
tencessuchas Mesmellastench. Toavoidthisproblemourgrammarwouldhavetoknow
thatmeisnotavalid N P whenitisthesubjectofasentence. Linguistssaythatthepronoun
I is in the subjective case and me is in the objective case.4 We can account for this by
theaccusativecase.Manylanguagesalsohaveadativecaseforwordsintheindirectobjectposition.
splitting N P intotwocategories N P and N P tostand fornounphrases inthesubjective
S O
and objective case respectively. Wewould also need to split the category Pronoun into the
two categories Pronoun which includes I and Pronoun which includes me. The
S O
toppartof Figure23.7showsthegrammarforcaseagreement;wecalltheresultinglanguage
C AS EA GR EE ME NT
E . Noticethatallthe N P rulesmustbeduplicated oncefor N P andoncefor N P .
S UB JE CT VE RB Unfortunately E still overgenerates. English requires subjectverb agreement for
A GR EE ME NT 1
person and number of the subject and main verb of a sentence. For example if I is the
subject then I smell isgrammatical but I smells isnot. Ifit isthe subject weget the
reverse. In English the agreement distinctions are minimal: most verbs have one form for
third-person singular subjects he she or it and a second form for all other combinations
ofperson and number. Thereisoneexception: the verbtobe hasthree forms I amyou
are he is. So one distinction case splits N P two ways another distinction person and
numbersplits N P threewaysandasweuncoverotherdistinctionswewouldendupwithan
exponential numberofsubscripted N P formsifwetooktheapproach of E . Augmentations
1
areabetterapproach: theycanrepresent anexponential numberofformsasasinglerule.
In the bottom of Figure 23.7 wesee part of an augmented grammarfor the language
E which handles case agreement subjectverb agreement and head words. We have just
2
one N P category but N Pcpnhead has three augmentations: c is a parameter for case
pn is a parameter for person and number and head is a parameter for the head word of
the phrase. The other categories also are augmented with heads and other arguments. Lets
consideroneruleindetail:
Shead N PSbjpnh V Ppnhead.
Thisruleiseasiesttounderstand right-to-left: whenan N Panda V Pareconjoined theyform
an S but only if the N Phas the subjective Sbj case and the person and number pn of the
N P and V P are identical. If that holds then we have an S whose head is the same as the
headofthe V P.Notetheheadofthe N Pdenoted bythedummyvariable hisnotpartofthe
augmentationofthe S.Thelexicalrulesfor E fillinthevaluesoftheparametersandarealso
2
bestreadright-to-left. Forexampletherule
Pronoun Sbj1 SI I
saysthat Icanbeinterpretedasa Pronouninthesubjectivecasefirst-personsingularwith
head I. For simplicity we have omitted the probabilities for these rules but augmentation
does work with probabilities. Augmentation can also work with automated learning mecha-
nisms. Petrov and Klein 2007c show how a learning algorithm can automatically split the
N P category into N P and N P .
S O
To show how to add semantics to a grammar we start with an example that is simpler than
English: thesemanticsofarithmeticexpressions. Figure23.8showsagrammarforarithmetic
expressions whereeachruleisaugmentedwithavariableindicatingthesemanticinterpreta-
tionofthephrase. Thesemanticsofadigitsuchas3isthedigititself. Thesemanticsofan
expression suchas34istheoperatorapplied tothesemantics ofthephrase3and
Section23.3. Augmented Grammarsand Semantic Interpretation 901
Expx Expx Operatorop Expx x Applyopx x
Expx Expx
Expx Numberx
Numberx Digitx
Numberx Numberx Digitx x10x x
Digitx x 0 x 9
Operatorx xx
Figure23.8 Agrammarforarithmeticexpressionsaugmentedwithsemantics.Eachvari-
ablexirepresentsthesemanticsofaconstituent.Notetheuseofthetestnotationtodefine
logicalpredicatesthatmustbesatisfiedbutthatarenotconstituents.
Exp5
Exp2
Exp2
Exp3 Exp4 Exp2
Number3 Number4 Number2
Digit3 Operator Digit4 Operator Digit2
Figure23.9 Parsetreewithsemanticinterpretationsforthestring342.
C OM PO SI TI ON AL the phrase 4. The rules obey the principle of compositional semanticsthe semantics of
S EM AN TI CS
aphraseisafunctionofthesemanticsofthesubphrases. Figure23.9showstheparsetreefor
342 according to this grammar. The root of the parse tree is Exp5 an expression
whosesemanticinterpretation is5.
Nowletsmoveontothesemantics of English oratleast of E . Westart bydetermin-
0
ingwhatsemanticrepresentations wewanttoassociatewithwhatphrases. Weusethesimple
examplesentence Johnloves Mary. The N P Johnshouldhaveasitssemanticinterpreta-
tion the logical term John and the sentence as a whole should have as its interpretation the
logical sentence Loves John Mary. That much seems clear. The complicated part is the
V P loves Mary. The semantic interpretation of this phrase is neither a logical term nor a
complete logical sentence. Intuitively loves Mary is adescription that might ormight not
apply to aparticular person. In this case it applies to John. Thismeans that loves Mary
is a predicate that when combined with a term that represents a person the person doing
theloving yields acomplete logical sentence. Using the λ-notation see page 294 wecan
represent loves Maryasthepredicate
λx Lovesx Mary.
Nowweneed arule thatsays an N P withsemantics obj followed by a V P withsemantics
pred yieldsasentence whosesemanticsistheresultofapplying pred toobj:
Spredobj N Pobj V Ppred.
Theruletellsusthatthesemanticinterpretation of Johnloves Maryis
λx Lovesx Mary John
whichisequivalent to Loves John Mary.
The rest of the semantics follows in a straightforward way from the choices we have
madesofar. Because V Psarerepresentedaspredicatesitisagoodideatobeconsistentand
represent verbsaspredicates aswell. Theverblovesisrepresented as λy λx Lovesxy
thepredicatethatwhengiventheargument Maryreturnsthepredicateλx Lovesx Mary.
Weendupwiththegrammarshownin Figure23.10andtheparsetreeshownin Figure23.11.
We could just as easily have added semantics to E ; we chose to work with E so that the
readercanfocusononetypeofaugmentation atatime.
Adding semantic augmentations to a grammar by hand is laborious and error prone.
Therefore there have been several projects to learn semantic augmentations from examples.
C HI LL Zelle and Mooney 1996 is an inductive logic programming I LP program that
learnsagrammarandaspecializedparserforthatgrammarfromexamples. Thetargetdomain
is natural language database queries. The training examples consist of pairs of word strings
andcorresponding semanticformsforexample;
Whatisthecapitalofthestatewiththelargest population?
Answerc Capitalsc Largestp States Populationsp
C HI LLstaskistolearnapredicate Parsewordssemanticsthatisconsistent withtheex-
amples and hopefully generalizes well to other examples. Applying I LP directly to learn
this predicate results inpoor performance: the induced parserhas only about 20 accuracy.
Fortunately I LPlearners canimprovebyadding knowledge. Inthiscase mostofthe Parse
predicate was defined as a logic program and C HI LLs task was reduced to inducing the
controlrulesthatguidetheparsertoselectoneparseoveranother. Withthisadditional back-
ground knowledge C HI LL can learn to achieve 70 to 85 accuracy on various database
querytasks.
Thegrammarofreal Englishisendlessly complex. Wewillbrieflymentionsomeexamples.
Time and tense: Suppose we want to represent the difference between John loves
T IM EA ND TE NS E
Maryand Johnloved Mary. Englishusesverbtensespast presentandfuturetoindicate
Section23.3. Augmented Grammarsand Semantic Interpretation 903
Spredobj N Pobj V Ppred
V Ppredobj Verbpred N Pobj
N Pobj Nameobj
Name John John
Name Mary Mary
Verbλy λx Lovesxy loves
Figure23.10 Agrammarthatcanderiveaparsetreeandsemanticinterpretationfor John
loves Maryandthreeothersentences.Eachcategoryisaugmentedwithasingleargument
representingthesemantics.
S Loves John Mary
V Pλx Lovesx Mary
N PJohn
N PMary
Name John Verbλy λx Lovesxy Name Mary
John loves Mary
Figure23.11 Aparsetreewithsemanticinterpretationsforthestring Johnloves Mary.
the relative time of an event. One good choice to represent the time of events is the event
calculus notationof Section12.3. Ineventcalculuswehave
Johnlovesmary:E Loves John Mary During Now Extent E
Johnlovedmary:E Loves John Mary After Now Extent E .
Thissuggests thatourtwolexicalrulesforthewordslovesandlovedshould bethese:
Verbλy λx e Lovesxy During Nowe loves
Verbλy λx e Lovesxy After Nowe loved.
Other than this change everything else about the grammar remains the same which is en-
couraging news; it suggests we are on the right track if wecan so easily add a complication
like the tense of verbs although we have just scratched the surface of a complete grammar
fortimeandtense. Itisalsoencouraging thatthedistinction betweenprocesses anddiscrete
eventsthatwemadeinourdiscussion ofknowledge representation in Section12.3.1isactu-
ally reflected in language use. We can say John slept a lot last night where Sleeping is a
process category but it isodd to say John found aunicorn a lot last night where Finding
is a discrete event category. A grammar would reflect that fact by having a low probability
foraddingtheadverbialphrasealottodiscrete events.
Quantification: Consider the sentence Every agent feels abreeze. Thesentence has
Q UA NT IF IC AT IO N
only one syntactic parse under E but it is actually semantically ambiguous; the preferred
0
meaning is For every agent there exists a breeze that the agent feels but an acceptable
alternativemeaningis Thereexistsabreezethateveryagentfeels.5 Thetwointerpretations
canberepresented as
a a Agents
b b Breezes e e Feelab During Nowe;
b b Breezes a a Agents
e e Feelab During Nowe.
The standard approach to quantification is for the grammar to define not an actual logical
Q UA SI-L OG IC AL semantic sentence butrathera quasi-logical form thatisthenturned into alogical sentence
F OR M
byalgorithmsoutside oftheparsing process. Thosealgorithms canhavepreference rulesfor
preferring one quantifier scope overanotherpreferences that need not be reflected directly
inthegrammar.
Pragmatics: We have shown how an agent can perceive a string of words and use a
P RA GM AT IC S
grammar to derive a set of possible semantic interpretations. Now we address the problem
of completing the interpretation by adding context-dependent information about the current
situation. The most obvious need for pragmatic information is in resolving the meaning of
indexicals which arephrases that referdirectly tothe current situation. Forexample in the
I ND EX IC AL
sentence Iamin Bostontodayboth Iandtodayareindexicals. Theword Iwouldbe
represented bythefluent Speakeranditwouldbeuptothehearertoresolvethemeaningof
the fluentthat is not considered part of the grammar but rather an issue of pragmatics; of
usingthecontextofthecurrentsituation tointerpret fluents.
Another part of pragmatics is interpreting the speakers intent. The speakers action is
considered a speech act and it is up to the hearer to decipher what type of action it isa
S PE EC HA CT
question a statement a promise a warning a command and so on. A command such as
go to22implicitly refers tothe hearer. Sofar ourgrammarfor S covers only declarative
sentences. We can easily extend it to cover commands. A command can be formed from
a V P where the subject is implicitly the hearer. We need to distinguish commands from
statements sowealtertherulesfor S toincludethetypeofspeechact:
S Statement Speakerpredobj N Pobj V Ppred
S Command Speakerpred Hearer V Ppred.
L ON G-D IS TA NC E Long-distance dependencies: Questions introduce anew grammatical complexity. In
D EP EN DE NC IE S
Who did the agent tell you to give the gold to? the final word to should be parsed as
P P to where the denotes a gap or trace where an N P is missing; the missing N P
T RA CE
is licensed by the first word of the sentence who. A complex system of augmentations is
used to make sure that the missing N Ps match up with the licensing words in just the right
wayandprohibitgapsinthewrongplaces. Forexampleyoucanthaveagapinonebranch
of an N P conjunction: What did he play N P Dungeons and ? is ungrammatical. But
youcanhave thesamegapinboth branches ofa V P conjunction: Whatdidyou V P V P
smell and V P shootanarrowat ?
Ambiguity: Insomecaseshearersareconsciously awareofambiguityinanutterance.
A MB IG UI TY
Herearesomeexamplestakenfromnewspaperheadlines:
Section23.3. Augmented Grammarsand Semantic Interpretation 905
Squadhelpsdogbitevictim.
Policebegincampaigntorundownjaywalkers.
Helicopterpoweredbyhumanflies.
Once-sagging clothdiaperindustrysavedbyfulldumps.
Portabletoiletbombed;policehavenothingtogoon.
Teacherstrikesidlekids.
Includeyourchildren whenbakingcookies.
Hospitalsaresuedby7footdoctors.
Milkdrinkers areturningtopowder.
Safetyexpertssayschoolbuspassengers shouldbebelted.
Butmostofthetimethelanguage wehearseemsunambiguous. Thuswhenresearchers first
began to use computers to analyze language in the 1960s they were quite surprised to learn
that almost every utterance is highly ambiguous even though the alternative interpretations
mightnotbeapparenttoanativespeaker. Asystemwithalargegrammarandlexiconmight
find thousands of interpretations for a perfectly ordinary sentence. Lexical ambiguity in
L EX IC AL AM BI GU IT Y
which a word has more than one meaning is quite common; back can be an adverb go
back anadjective backdoor anounthebackoftheroom oraverbback upyourfiles.
Jackcanbeanameanounaplayingcardasix-pointedmetalgamepieceanauticalflag
afishasocketoradeviceforraisingheavyobjects oraverbtojackupacartohuntwith
S YN TA CT IC a light or to hit a baseball hard. Syntactic ambiguity refers to a phrase that has multiple
A MB IG UI TY
parses: I smelled awumpus in 22 has twoparses: one where the prepositional phrase in
22modifiesthenounandonewhereitmodifiestheverb. Thesyntacticambiguityleadstoa
S EM AN TI C semanticambiguitybecauseoneparsemeansthatthewumpusisin22andtheothermeans
A MB IG UI TY
thatastenchisin22. Inthiscasegettingthewronginterpretation couldbeadeadlymistake
fortheagent.
Finally there can be ambiguity between literal and figurative meanings. Figures of
speech are important in poetry but are surprisingly common in everyday speech as well. A
metonymy is a figure of speech in which one object is used to stand for another. When
M ET ON YM Y
we hear Chrysler announced a new model we do not interpret it as saying that compa-
nies can talk; rather we understand that a spokesperson representing the company made the
announcement. Metonymyiscommonandisofteninterpretedunconsciouslybyhumanhear-
ers. Unfortunately our grammar as it is written is not so facile. To handle the semantics of
metonymyproperlyweneedtointroduceawholenewlevelofambiguity. Wedothisbypro-
vidingtwoobjectsforthesemanticinterpretation ofeveryphraseinthesentence: oneforthe
object that the phrase literally refers to Chrysler and one for the metonymic reference the
spokesperson. We then have to say that there is a relation between the two. In our current
grammar Chryslerannounced getsinterpreted as
x Chrysler e Announcex After Now Extente.
Weneedtochange thatto
x Chrysler e Announcem After Now Extente
Metonymymx.
This says that there is one entity x that is equal to Chrysler and another entity m that did
theannouncing andthatthetwoareinametonymyrelation. Thenextstep istodefinewhat
kinds of metonymy relations can occur. The simplest case is when there is no metonymy at
alltheliteralobject xandthemetonymicobjectmareidentical:
mx m x Metonymymx.
Forthe Chrysler example a reasonable generalization is that an organization can be used to
standforaspokesperson ofthatorganization:
mx x Organizations Spokespersonmx Metonymymx.
Other metonymies include the author for the works I read Shakespeare or more generally
theproducer fortheproduct Idrivea Hondaandthepartforthewhole The Red Soxneed
a strong arm. Some examples of metonymy such as The ham sandwich on Table 4 wants
anotherbeeraremorenovelandareinterpreted withrespecttoasituation.
Ametaphor isanother figure ofspeech inwhich a phrase withone literal meaning is
M ET AP HO R
used to suggest a different meaning by way of an analogy. Thus metaphor can be seen as a
kindofmetonymywheretherelationisoneofsimilarity.
Disambiguation is the process of recovering the most probable intended meaning of
D IS AM BI GU AT IO N
an utterance. In one sense wealready have a framework for solving this problem: each rule
has a probability associated with it so the probability of an interpretation is the product of
the probabilities of the rules that led to the interpretation. Unfortunately the probabilities
reflect how common the phrases are in the corpus from which the grammar was learned
and thus reflect general knowledge not specific knowledge of the current situation. To do
disambiguation properly weneedtocombinefourmodels:
1. Theworldmodel: thelikelihoodthatapropositionoccursintheworld. Givenwhatwe
know about the world it is more likely that a speaker who says Im dead means I
aminbigtroubleratherthan Mylifeendedandyet Icanstilltalk.
2. Thementalmodel: thelikelihood thatthespeakerformstheintention ofcommunicat-
ing a certain fact to the hearer. This approach combines models of what the speaker
believes what the speaker believes the hearer believes and so on. Forexample when
a politician says I am not a crook the world model might assign a probability of
only 50 to the proposition that the politician is not a criminal and 99.999 to the
proposition thatheisnotahooked shepherds staff. Nevertheless weselecttheformer
interpretation because itisamorelikelythingtosay.
3. Thelanguagemodel: thelikelihoodthatacertainstringofwordswillbechosengiven
thatthespeakerhastheintention ofcommunicating acertainfact.
4. The acoustic model: for spoken communication the likelihood that a particular se-
quenceofsounds willbegenerated giventhatthespeaker haschosen agivenstringof
words. Section23.5coversspeechrecognition.
Section23.4. Machine Translation 907
Machinetranslationistheautomatictranslationoftextfromonenaturallanguagethesource
to another the target. It was one of the first application areas envisioned for computers
Weaver 1949 but it is only in the past decade that the technology has seen widespread
usage. Hereisapassagefrompage1ofthisbook:
A I is one of the newestfields in science and engineering. Work started in earnest soon
after World War I Iandthenameitselfwascoinedin1956. Alongwithmolecularbiol-
ogy A I is regularlycited as the field I wouldmost like to be in by scientists in other
disciplines.
Andhereitistranslated from Englishto Danishbyanonlinetool Google Translate:
A Ierenafdenyesteomraderindenforvidenskabogteknik. Arbejdestartedeforalvor
lige efter Anden Verdenskrig og navnet i sig selv var opfundet i 1956. Sammen med
molekylærbiologi er A I jævnligtnævntsom feltet Jeg ville de fleste gernevære i af
forskereiandrediscipliner.
For those who dont read Danish here is the Danish translated back to English. The words
thatcameoutdifferentareinitalics:
A Iisoneofthenewestfieldsofscienceandengineering.Workbeganinearnestjustafter
the Second World Warandthenameitselfwasinventedin1956.Togetherwithmolecular
biology A Iisfrequentlymentionedas field Iwouldmostliketobeinbyresearchers
inotherdisciplines.
The differences are all reasonable paraphrases such as frequently mentioned for regularly
cited. Theonly real erroristhe omission ofthe article the denoted bythe symbol. Thisis
typical accuracy: of the two sentences one has an error that would not be made by a native
speaker yetthemeaningisclearlyconveyed.
Historically there have been three main applications of machine translation. Rough
translation as provided by free online services gives the gist of a foreign sentence or
document but contains errors. Pre-edited translation is used by companies to publish their
documentation and sales materials inmultiple languages. The original source text iswritten
in a constrained language that is easier to translate automatically and the results are usually
edited by ahuman tocorrect anyerrors. Restricted-source translation worksfully automati-
callybutonlyonhighlystereotypical language suchasaweatherreport.
Translationisdifficultbecauseinthefullygeneralcaseitrequiresin-depthunderstand-
ing of the text. This is true even for very simple textseven texts of one word. Consider
theword Openonthedoorofastore.6 Itcommunicates theideathatthestoreisaccepting
customers at the moment. Now consider the same word Open on a large banner outside a
newlyconstructed store. Itmeansthat the store isnowindaily operation but readers ofthis
signwouldnotfeelmisledifthestoreclosed atnight withoutremoving thebanner. Thetwo
signs use the identical word to convey different meanings. In German the sign on the door
wouldbe Offenwhilethebannerwouldread Neu Eroffnet.
The problem is that different languages categorize the world differently. Forexample
the French word doux covers a wide range of meanings corresponding approximately to
the English words soft sweet and gentle. Similarly the English word hard covers
virtually all uses of the German word hart physically recalcitrant cruel and some uses
of the word schwierig difficult. Therefore representing the meaning of a sentence is
moredifficultfortranslationthanitisforsingle-language understanding. An Englishparsing
system could use predicates like Openx but for translation the representation language
wouldhavetomakemoredistinctionsperhapswith Open xrepresentingthe Offensense
1
and Open xrepresenting the Neu Eroffnet sense. Arepresentation language that makes
2
allthedistinctions necessary forasetoflanguages iscalledaninterlingua.
I NT ER LI NG UA
A translator human or machine often needs to understand the actual situation de-
scribed in the source not just the individual words. For example to translate the English
word him into Korean a choice must be made between the humble and honorific form a
choicethatdepends onthesocialrelationship betweenthespeakerandthereferent ofhim.
In Japanese the honorifics are relative so thechoice depends onthe social relationships be-
tweenthespeakerthereferentandthelistener. Translatorsbothmachineandhumansome-
timesfinditdifficult to makethis choice. Asanother example totranslate The baseball hit
the window. It broke. into French we must choose the feminine elle or the masculine
il forit so wemust decide whether it refers tothe baseball orthe window. Toget the
translation rightonemustunderstand physicsaswellaslanguage.
Sometimes there is no choice that can yield a completely satisfactory translation. For
example an Italian love poem that uses the masculine ilsole sun and feminine laluna
moon to symbolize two lovers will necessarily be altered when translated into German
wherethegendersarereversedandfurtheralteredwhentranslatedintoalanguagewherethe
gendersarethesame.7
All translation systems must model the source and target languages but systems vary in the
typeofmodelstheyuse. Somesystemsattempttoanalyzethesourcelanguagetextalltheway
into an interlingua knowledge representation and then generate sentences in the target lan-
guagefromthatrepresentation. Thisisdifficultbecause it involves threeunsolvedproblems:
creatingacompleteknowledgerepresentation ofeverything; parsingintothatrepresentation;
andgenerating sentences fromthatrepresentation.
Othersystemsarebasedonatransfermodel. Theykeepadatabaseoftranslationrules
T RA NS FE RM OD EL
orexamples and wheneverthe rule orexample matches they translate directly. Transfer
can occur at the lexical syntactic or semantic level. For example a strictly syntactic rule
maps English Adjective Noun to French Noun Adjective. A mixed syntactic and lexical
rulemaps French S etpuis S to English S andthen S . Figure23.12diagramsthe
varioustransferpoints.
thattranslationislikekissingthebridethroughaveil.
Section23.4. Machine Translation 909
Interlingua Semantics
Attraction Named John Named Mary High
English Semantics French Semantics
Loves John Mary Aime Jean Marie
English Syntax French Syntax
S NP John V Ploves N PMary S NP Jean V Paime N PMarie
English Words French Words
John loves Mary Jean aime Marie
translationsystem Vauquois1968. We startwith English textatthe top. An interlingua-
based system follows the solid lines parsing English first into a syntactic form then into
a semanticrepresentationandaninterlinguarepresentationandthenthroughgenerationto
a semantic syntactic andlexicalformin French. A transfer-basedsystem usesthe dashed
linesasa shortcut. Differentsystemsmakethetransferatdifferentpoints; somemakeitat
multiplepoints.
Now that we have seen how complex the translation task can be it should come as no sur-
prisethatthemostsuccessfulmachinetranslationsystemsarebuiltbytrainingaprobabilistic
model using statistics gathered from a large corpus of text. This approach does not need
a complex ontology of interlingua concepts nor does it need handcrafted grammars of the
sourceandtargetlanguages norahand-labeled treebank. Allitneedsisdatasampletrans-
lationsfromwhichatranslationmodelcanbelearned. Totranslateasentenceinsay English
""
einto Frenchfwefindthestringofwords f thatmaximizes
f argmax Pf e argmax Pef Pf.
f
Here the factor Pfis the target language model for French; it says how probable a given
L AN GU AG EM OD EL
T RA NS LA TI ON sentence is in French. Pef is the translation model; it says how probable an English
M OD EL
sentence is as a translation for a given French sentence. Similarly Pf e is a translation
modelfrom Englishto French.
Shouldweworkdirectly on Pfeorapply Bayesruleandworkon Pef Pf?
In diagnostic applications like medicine it is easier to model the domain in the causal di-
rection: Psymptomsdiseaseratherthan Pdiseasesymptoms. Butintranslation both
directions are equally easy. The earliest work in statistical machine translation did apply
Bayes rulein part because the researchers had agood language model Pf andwanted
to make use of it and in part because they came from a background in speech recognition
which is a diagnostic problem. We follow their lead in this chapter but we note that re-
cent work in statistical machine translation often optimizes Pf e directly using a more
sophisticated modelthattakesintoaccount manyofthefeatures fromthelanguage model.
The language model Pf could address any levels on the right-hand side of Fig-
ure 23.12 but the easiest and most common approach is to build an n-gram model from a
French corpus as we have seen before. This captures only a partial local idea of French
sentences; howeverthatisoftensufficientforroughtranslation.8
Thetranslationmodelislearnedfromabilingualcorpusacollectionofparalleltexts
B IL IN GU AL CO RP US
each an English French pair. Now if we had an infinitely large corpus then translating a
sentence wouldjustbealookuptask: wewouldhaveseenthe Englishsentence beforeinthe
corpus so we could just return the paired French sentence. But of course our resources are
finite and most of the sentences we will be asked to translate will be novel. However they
will be composed of phrases that we have seen before even if some phrases are as short as
one word. For example in this book common phrases include in this exercise we will
sizeofthestatespaceasafunction oftheandnotesattheendofthechapter. Ifasked
totranslatethenovelsentence Inthisexercisewewillcomputethesizeofthestatespaceasa
function ofthenumberofactions. into Frenchweshouldbeabletobreakthesentenceinto
phrases find the phrases in the English corpus this book find the corresponding French
phrases from the French translation of the book and then reassemble the French phrases
intoanorderthatmakessensein French. Inotherwordsgivenasource Englishsentence e
findinga Frenchtranslation f isamatterofthreesteps:
1. Breakthe Englishsentenceintophrases e ...e .
2. For each phrase e choose a corresponding French phrase f . We use the notation
i i
Pf e forthephrasal probability that f isatranslation ofe .
i i i i
3. Choose a permutation of the phrases f ...f . Wewill specify this permutation in a
way that seems a little complicated but is designed to have a simple probability dis-
tribution: For each f we choose a distortion d which is the number of words that
D IS TO RT IO N i i
phrase f
i
hasmovedwithrespectto f i1;positiveformovingtotheright negativefor
movingtotheleftandzeroiff
i
immediatelyfollowsf i1.
wumpus sleeping in 2 2 is broken into five phrases e ...e . Each of them is translated
into a corresponding phrase f and then these are permuted into the order f f f f f .
i 1 3 4 2 5
Wespecifythepermutation intermsofthedistortions d ofeach Frenchphrase definedas
i
d
i
S TA RTf i E NDf i11
where S TA RTf iistheordinal numberofthefirstwordofphrase f
i
inthe Frenchsentence
and E NDf i1istheordinalnumberofthelastwordofphrase f i1. In Figure23.13wesee
thatf a 22immediatelyfollowsf quidortandthusd 0. Phrasef howeverhas
movedonewords totheright of f sod 1. Asaspecial case wehave d 0 because f
startsatposition 1and E NDf 0isdefinedtobe0eventhough f
0
doesnotexist.
Now that we have defined the distortion d we can define the probability distribution
i
for distortion Pd . Note that for sentences bounded by length n we have d n and
i i
recherchedutempsperdubeginsandendswiththesamewordlongtempssosometranslatorshavedecidedto
dothesamethusbasingthetranslationofthefinalwordononethatappearedroughly2millionwordsearlier.
Section23.4. Machine Translation 911
so the full probability distribution Pd has only 2n 1 elements far fewer numbers to
i
learn than the number of permutations n!. That is why we defined the permutation in this
circuitous way. Of course this is a rather impoverished model of distortion. It doesnt say
that adjectives are usually distorted to appear after the noun when we are translating from
Englishto Frenchthat factisrepresented inthe Frenchlanguage model Pf. Thedistor-
tion probability is completely independent of the words in the phrasesit depends only on
the integer value d . Theprobability distribution provides a summary of the volatility of the
i
permutations; howlikelyadistortion of Pd2iscomparedto Pd0forexample.
Were ready now to put it all together: we can define Pfde the probability that
thesequenceofphrases f withdistortions disatranslation ofthesequenceofphrases e. We
make the assumption that each phrase translation and each distortion is independent of the
others andthuswecanfactortheexpression as
cid:25
Pfde Pf e Pd
i i i
i
e e e e e
There is a smelly wumpus sleeping in 2 2
f f f f f
Il y a un wumpus malodorant qui dort à 2 2
d 0 d -2 d 1 d 1 d 0
Figure23.13 Candidate Frenchphrasesforeachphraseofan Englishsentencewithdis-
tortiondvaluesforeach Frenchphrase.
Thatgivesusawaytocomputetheprobability Pfdeforacandidate translation f
anddistortion d. Buttofindthebestf anddwecant justenumerate sentences; withmaybe
translations and5!reorderings foreachofthose. Wewillhavetosearchforagoodsolution.
A local beam search see page 125 with a heuristic that estimates probability has proven
effectiveatfindinganearly-most-probable translation.
Allthat remains is to learn the phrasal and distortion probabilities. Wesketch the pro-
cedure;seethenotesattheendofthechapterfordetails.
1. Findparalleltexts: Firstgatheraparallelbilingual corpus. Forexamplea Hansard9
H AN SA RD
is a record of parliamentary debate. Canada Hong Kong and other countries pro-
duce bilingual Hansards the European Union publishes its official documents in 11
languages and the United Nations publishes multilingual documents. Bilingual text is
also available online; some Web sites publish parallel content with parallel U RLs for
exampleenforthe Englishpageandfrforthecorresponding Frenchpage. The
leadingstatistical translation systemstrainonhundreds ofmillionsofwordsofparallel
textandbillions ofwordsofmonolingual text.
2. Segmentintosentences: Theunitoftranslation isasentencesowewillhavetobreak
the corpus into sentences. Periods are strong indicators of the end of a sentence but
consider Dr. J. R. Smith of Rodeo Dr. paid 29.99 on 9.9.09.; only the final period
ends a sentence. One way to decide if a period ends a sentence is to train a model
that takes as features the surrounding words and their parts of speech. This approach
achievesabout98accuracy.
3. Alignsentences: Foreachsentence inthe Englishversiondetermine whatsentences
it corresponds to in the French version. Usually the next sentence of English corre-
sponds tothe nextsentence of French ina1:1 match but sometimes there isvariation:
onesentenceinonelanguagewillbesplitintoa2:1matchortheorderoftwosentences
willbeswappedresultingina2:2match. Bylookingatthesentencelengthsalonei.e.
shortsentences shouldalignwithshortsentences itispossibletoalignthem1:11:2
or 2:2 etc. with accuracy in the 90 to 99 range using a variation on the Viterbi
algorithm. Evenbetteralignmentcanbeachievedbyusinglandmarksthatarecommon
toboth languages such asnumbers dates proper names orwords that weknow from
abilingualdictionaryhaveanunambiguoustranslation. Forexampleifthe3rd English
and 4th French sentences contain the string 1989 and neighboring sentences do not
thatisgoodevidencethatthesentences shouldbealignedtogether.
4. Alignphrases: Withinasentencephrasescanbealignedbyaprocessthatissimilarto
that used for sentence alignment but requiring iterative improvement. When we start
wehavenowayofknowingthatquidortalignswithsleeping butwecanarriveat
thatalignmentbyaprocessofaggregationofevidence. Overalltheexamplesentences
we have seen we notice that qui dort and sleeping co-occur with high frequency
and that in the pair of aligned sentences no phrase other than qui dort co-occurs so
frequently in other sentences with sleeping. A complete phrase alignment over our
corpusgivesusthephrasalprobabilities afterappropriate smoothing.
5. Extract distortions: Once we have an alignment of phrases we can define distortion
probabilities. Simplycount howoften distortion occurs in thecorpus foreachdistance
d 012...andapplysmoothing.
6. Improveestimateswith E M:Useexpectationmaximization toimprovetheestimates
of Pf e and Pd values. We compute the best alignments with the current values
oftheseparametersinthe Estepthenupdatetheestimatesinthe Mstepanditeratethe
processuntilconvergence.
S PE EC H Speechrecognitionisthetaskofidentifyingasequenceofwordsutteredbyaspeaker given
R EC OG NI TI ON
the acoustic signal. It has become one of the mainstream applications of A Imillions of
Section23.5. Speech Recognition 913
people interact with speech recognition systems every day to navigate voice mail systems
search the Web from mobile phones and other applications. Speech is an attractive option
whenhands-free operation isnecessary aswhenoperating machinery.
Speech recognition is difficult because the sounds made by a speaker are ambiguous
and well noisy. As a well-known example the phrase recognize speech sounds almost
the same as wreck a nice beach when spoken quickly. Even this short example shows
several of the issues that make speech problematic. First segmentation: written words in
S EG ME NT AT IO N
English have spaces between them but in fast speech there are no pauses in wreck a nice
that would distinguish it as a multiword phrase as opposed to the single word recognize.
Second coarticulation: when speaking quickly the s sound at the end of nice merges
C OA RT IC UL AT IO N
with the b sound at the beginning of beach yielding something that is close to a sp.
Another problem that does not show up in this example is homophoneswords like to
H OM OP HO NE S
tooandtwothatsoundthesamebutdifferinmeaning.
Wecanviewspeech recognition asaproblem inmost-likely-sequence explanation. As
we saw in Section 15.2 this is the problem of computing the most likely sequence of state
variables x given a sequence of observations e . In this case the state variables are the
1:t 1:t
wordsandtheobservationsaresounds. Morepreciselyanobservationisavectoroffeatures
extractedfromtheaudiosignal. Asusualthemostlikelysequencecanbecomputedwiththe
helpof Bayesruletobe:
argmax Pword sound argmax Psound word Pword .
1:t 1:t 1:t 1:t 1:t
word1:t word1:t
Here Psound word is the acoustic model. It describes the sounds of wordsthat
A CO US TI CM OD EL 1:t 1:t
ceiling begins with a soft c and sounds the same as sealing. Pword is known as
1:t
the language model. It specifies the prior probability of each utterancefor example that
L AN GU AG EM OD EL
ceiling fanisabout500timesmorelikelyasawordsequence thansealingfan.
N OI SY CH AN NE L This approach was named the noisy channel model by Claude Shannon 1948. He
M OD EL
described asituation inwhichanoriginal message the wordsinourexample istransmitted
over a noisy channel such as a telephone line such that a corrupted message the sounds
in our example are received at the other end. Shannon showed that no matter how noisy
the channel it is possible to recover the original message with arbitrarily small error if we
encode the original message in a redundant enough way. The noisy channel approach has
beenapplied tospeechrecognition machinetranslation spellingcorrection andothertasks.
Once we define the acoustic and language models we can solve for the most likely
sequence of words using the Viterbi algorithm Section 15.2.3 on page 576. Most speech
recognition systems usealanguage modelthatmakesthe Markovassumptionthat thecur-
rentstate Word dependsonlyonafixednumbernofpreviousstatesandrepresent Word
t t
asasinglerandom variable takingonafinitesetofvalues whichmakesita Hidden Markov
Model H MM.Thusspeechrecognitionbecomesasimpleapplicationofthe H MMmethod-
ologyasdescribedin Section15.3simplethatisoncewedefinetheacousticandlanguage
models. Wecoverthemnext.
Vowels Consonants B N Consonants P Z
Phone Example Phone Example Phone Example
iy beat b bet p pet
ih bit ch Chet r rat
eh bet d debt s set
æ bat f fat sh shoe
ah but g get t ten
ao bought hh hat th thick
ow boat hv high dh that
uh book jh jet dx butter
ey bait k kick v vet
er Bert l let w wet
ay buy el bottle wh which
oy boy m met y yet
axr diner em bottom z zoo
aw down n net zh measure
ax about en button
ix roses ng sing
aa cot eng washing - silence
American English. There are several alternative notations including an International Pho-
netic Alphabet I PAwhichcontainsthephonesinallknownlanguages.
Sound waves are periodic changes in pressure that propagate through the air. When these
waves strike the diaphragm of a microphone the back-and-forth movement generates an
electric current. An analog-to-digital converter measures the size of the currentwhich ap-
proximates theamplitude ofthesound waveatdiscrete intervals called the samplingrate.
S AM PL IN GR AT E
Speechsounds whicharemostlyintherangeof100 Hz100cyclespersecondto1000 Hz
are typically sampled at arate of 8 k Hz. C Ds and mp3 files are sampled at 44.1 k Hz. The
Q UA NT IZ AT IO N precisionofeachmeasurementisdeterminedbythequantizationfactor;speechrecognizers
F AC TO R
typically keep 8 to 12 bits. That means that a low-end system sampling at 8 k Hzwith 8-bit
quantization wouldrequirenearlyhalfamegabyteperminuteofspeech.
Since we only want to know what words were spoken not exactly what they sounded
like wedont need tokeep allthatinformation. Weonly need todistinguish between differ-
entspeechsounds. Linguistshaveidentifiedabout100speechsoundsorphonesthatcanbe
P HO NE
composed to form all the words in all known human languages. Roughly speaking a phone
is the sound that corresponds to a single vowel or consonant but there are some complica-
tions: combinations ofletterssuchasthandngproducesinglephones andsomeletters
producedifferentphonesindifferentcontextse.g.theainratandrate. Figure23.14lists
Section23.5. Speech Recognition 915
allthephones thatareused in English withanexampleofeach. Aphonemeisthesmallest
P HO NE ME
unit of sound that has a distinct meaning to speakers of a particular language. Forexample
thetinsticksounds similarenough tothetintickthatspeakers of Englishconsider
them thesamephoneme. Butthedifference issignificant inthe Thailanguage sothere they
aretwophonemes. Torepresentspoken Englishwewantarepresentationthatcandistinguish
betweendifferentphonemesbutonethatneednotdistinguishthenonphonemicvariationsin
sound: loudorsoftfastorslowmaleorfemalevoiceetc.
First we observe that although the sound frequencies in speech may be several k Hz
the changes in the content of the signal occur much less often perhaps at no more than 100
Hz. Therefore speechsystemssummarizetheproperties ofthesignalovertimeslicescalled
frames. Aframelength ofabout 10milliseconds i.e.80samples at 8k Hzisshort enough
F RA ME
toensurethatfewshort-duration phenomena willbemissed. Overlapping framesareusedto
makesurethatwedontmissasignalbecause ithappens tofallonaframeboundary.
Eachframe issummarized byavectorof features. Picking outfeatures from aspeech
F EA TU RE
signal is like listening to an orchestra and saying here the French horns are playing loudly
and the violins are playing softly. Well give a brief overview of the features in a typical
system. First a Fourier transform is used to determine the amount of acoustic energy at
about a dozen frequencies. Then we compute a measure called the mel frequency cepstral
M EL FR EQ UE NC Y
C EP ST RA L coefficient M FC C or M FC C for each frequency. We also compute the total energy in
C OE FF IC IE NT MF CC
the frame. That gives thirteen features; for each one we compute the difference between
this frame and the previous frame and the difference between differences for a total of 39
features. These arecontinuous-valued; theeasiest waytofitthem intothe H MMframework
istodiscretizethevalues. Itisalsopossibletoextendthe H MMmodeltohandlecontinuous
mixtures of Gaussians. Figure 23.15 shows the sequence of transformations from the raw
soundtoasequence offrameswithdiscrete features.
We have seen how to go from the raw acoustic signal to a series of observations e .
t
Now we have to describe the unobservable states of the H MM and define the transition
model P X t X t1 and the sensor model P E t X t. The transition model can be broken
into two levels: word and phone. Well start from the bottom: the phone model describes
P HO NE MO DE L
Analog acoustic signal:
Sampled quantized
digital signal:
Frames with features:
Figure23.15 Translatingthe acoustic signal into a sequence of frames. In this diagram
each frame is described by the discretized values of three acoustic features; a real system
wouldhavedozensoffeatures.
Phone H MM for m:
Onset Mid End F IN AL
Output probabilities for the phone H MM:
Onset: Mid: End:
C : 0.5 C : 0.2 C : 0.1
C : 0.2 C : 0.7 C : 0.5
C : 0.3 C : 0.1 C : 0.4
outputseachwithitsownprobability.The M FC Cfeaturelabels C through C arearbitrary
standingforsomecombinationoffeaturevalues.
a Word model with dialect variation:
t ow m t ow
aa
b Word model with coarticulation and dialect var:iations
1.0
t m t ow
ah aa
Figure23.17 Twopronunciationmodelsofthewordtomato. Eachmodelis shownas
atransitiondiagramwithstatesascirclesandarrowsshowingallowedtransitionswiththeir
associatedprobabilities. a Amodelallowingfordialect differences. The0.5numbersare
estimatesbasedonthetwoauthorspreferredpronunciations.b Amodelwithacoarticula-
tioneffectonthefirstvowelallowingeithertheowortheahphone.
Section23.5. Speech Recognition 917
a phone as three states the onset middle and end. For example the t phone has a silent
beginning asmallexplosive burst ofsound inthemiddle andusually ahissing attheend.
phone has a duration of 50100 milliseconds or 510 frames. The self-loops in each state
allowsforvariation inthis duration. Bytaking manyself-loops especially inthe midstate
we can represent a long mmmmmmmmmmm sound. Bypassing the self-loops yields a
shortmsound.
P RO NU NC IA TI ON In Figure 23.17 the phone models are strung together to form a pronunciation model
M OD EL
fora word. According to Gershwin 1937 you say t ow m ey t ow and I say t ow m aa t
ow. Figure 23.17a shows a transition model that provides for this dialect variation. Each
ofthecirclesinthisdiagramrepresents aphonemodelliketheonein Figure23.16.
Inaddition todialect variation wordscan have coarticulation variation. Forexample
the t phone is produced with the tongue at the top of the mouth whereas the ow has the
tongue near the bottom. When speaking quickly the tongue doesnt have time to get into
position for the ow and we end up with t ah rather than t ow. Figure 23.17b gives
a model for tomato that takes this coarticulation effect into account. More sophisticated
phonemodelstakeintoaccountthecontextofthesurrounding phones.
There can be substantial variation in pronunciation for a word. The most common
pronunciation of because is b iy k ah z but that only accounts for about a quarter of
uses. Anotherquarterapproximately substitutes ixihoraxforthefirstvowelandthe
remainder substitute ax or aa for the second vowel zh or s for the final z or drop
beentirely leavingcuz.
For general-purpose speech recognition the language model can be an n-gram model of
text learned from a corpus of written sentences. However spoken language has different
characteristics than written language so it is better to get a corpus of transcripts of spoken
language. For task-specific speech recognition the corpus should be task-specific: to build
yourairlinereservationsystemgettranscriptsofpriorcalls. Italsohelpstohavetask-specific
vocabulary suchasalistofalltheairportsandcitiesservedandalltheflightnumbers.
Partofthedesignofavoiceuserinterfaceistocoercetheuserintosayingthingsfroma
limitedsetofoptionssothatthespeechrecognizerwillhaveatighterprobabilitydistribution
todeal with. Forexample asking What city do you wantto go to? elicits aresponse with
ahighlyconstrained language modelwhileasking Howcan Ihelpyou? doesnot.
Thequalityofaspeechrecognition systemdependsonthequalityofallofitscomponents
the language model the word-pronunciation models the phone models and the signal-
processing algorithms used to extract spectral features from the acoustic signal. We have
discussed how the language model can be constructed from a corpus of written text and we
leave the details of signal processing to other textbooks. We are left with the pronunciation
andphonemodels. Thestructureofthepronunciationmodelssuchasthetomatomodelsin
Figure23.17isusuallydevelopedbyhand. Largepronunciation dictionaries arenowavail-
able for English and other languages although their accuracy varies greatly. The structure
of the three-state phone models is the same for all phones as shown in Figure 23.16. That
leavestheprobabilities themselves.
Asusual wewillacquire theprobabilities from acorpus thistimeacorpus ofspeech.
The most common type of corpus to obtain is one that includes the speech signal for each
sentence paired with a transcript of the words. Building a model from this corpus is more
difficult than building an n-gram model of text because we have to build a hidden Markov
modelthephonesequenceforeachwordandthephonestateforeachtimeframearehidden
variables. In the early days of speech recognition the hidden variables were provided by
laborious hand-labeling of spectrograms. Recent systems use expectationmaximization to
automaticallysupplythemissingdata. Theideaissimple: givenan H MMandanobservation
sequence wecan use thesmoothing algorithms from Sections 15.2 and15.3 tocompute the
probability ofeachstateateachtimestepandbyasimpleextension theprobability ofeach
statestate pair at consecutive time steps. These probabilities can be viewed as uncertain
labels. From the uncertain labels we can estimate new transition and sensor probabilities
and the E M procedure repeats. The method is guaranteed to increase the fit between model
anddataoneachiterationanditgenerallyconvergestoamuchbettersetofparametervalues
thanthoseprovidedbytheinitial hand-labeled estimates.
The systems with the highest accuracy work by training a different model for each
speaker thereby capturing differences indialect aswellasmalefemale andothervariations.
This training can require several hours of interaction with the speaker so the systems with
themostwidespread adoption donotcreatespeaker-specific models.
Theaccuracyofasystemdependsonanumberoffactors. Firstthequalityofthesignal
matters: ahigh-qualitydirectionalmicrophoneaimedatastationarymouthinapaddedroom
will do much better than a cheap microphone transmitting a signal over phone lines from a
car in traffic with the radio playing. The vocabulary size matters: when recognizing digit
strings withavocabulary of11words1-9plusohandzero theworderrorratewillbe
below 0.5 whereas it rises to about 10 on news stories with a 20000-word vocabulary
and20onacorpuswitha64000-wordvocabulary. Thetaskmatterstoo: whenthesystem
is trying to accomplish a specific taskbook a flight or give directions to a restaurantthe
taskcanoftenbeaccomplished perfectly evenwithaworderrorrateof10ormore.
Natural language understanding is one of the most important subfields of A I. Unlike most
otherareasof A Inaturallanguageunderstandingrequires anempiricalinvestigationofactual
humanbehaviorwhich turnsouttobecomplexandinteresting.
Formal language theory and phrase structure grammars and in particular context-
free grammar are useful tools fordealing withsome aspects of natural language. The
probabilistic context-free grammar P CF Gformalismiswidelyused.
24
P ER CE PT IO N
Inwhichweconnect thecomputertotherawunwashedworld.
Perceptionprovidesagentswithinformationabouttheworldtheyinhabitbyinterpreting the
P ER CE PT IO N
response of sensors. A sensor measures some aspect of the environment in a form that can
S EN SO R
beusedasinputbyanagentprogram. Thesensorcouldbeassimpleasaswitchwhichgives
onebittellingwhetheritisonorofforascomplexastheeye. Avarietyofsensorymodalities
are available to artificial agents. Those they share with humans include vision hearing and
touch. Modalities that are not available to the unaided human include radio infrared G PS
andwirelesssignals. Somerobotsdoactivesensingmeaningtheysendoutasignalsuchas
radarorultrasound andsensethereflectionofthissignaloffoftheenvironment. Ratherthan
tryingtocoverallofthese thischapterwillcoveronemodalityindepth: vision.
We saw in our description of P OM DPs Section 17.4 page 658 that a model-based
decision-theoretic agent in a partially observable environment has a sensor modela prob-
ability distribution P ES over the evidence that its sensors provide given a state of the
world. Bayesrulecanthenbeusedtoupdatetheestimationofthestate.
For vision the sensor model can be broken into two components: An object model
O BJ EC TM OD EL
describes the objects that inhabit the visual worldpeople buildings trees cars etc. The
objectmodelcouldincludeaprecise3 Dgeometricmodeltakenfromacomputer-aideddesign
C ADsystemoritcouldbevagueconstraintssuchasthefactthathumaneyesareusually5
to7cmapart. Arenderingmodeldescribesthephysicalgeometricandstatisticalprocesses
R EN DE RI NG MO DE L
that produce thestimulus from theworld. Rendering models are quite accurate but they are
ambiguous. For example a white object under low light may appear as the same color as a
black object underintense light. Asmall nearby object maylook thesame asa large distant
object. Without additional evidence we cannot tell if the image that fills the frame is a toy
Godzillaorarealmonster.
Ambiguity can be managed with prior knowledgewe know Godzilla is not real so
the image must be a toyor by selectively choosing to ignore the ambiguity. For example
the vision system for an autonomous car may not be able to interpret objects that are far in
the distance but the agent can choose to ignore the problem because it is unlikely to crash
intoanobjectthatismilesaway.
928
Section24.1. Image Formation 929
Adecision-theoretic agent isnot theonlyarchitecture that canmakeuseofvision sen-
sors. Forexample fruit flies Drosophila are in part reflex agents: they have cervical giant
fibersthatformadirectpathwayfromtheirvisualsystemtothewingmusclesthatinitiatean
escape responsean immediate reaction without deliberation. Flies and many other flying
animals make used of a closed-loop control architecture to land on an object. The visual
system extracts an estimate of the distance to the object and the control system adjusts the
wingmusclesaccordinglyallowingveryfastchangesofdirectionwithnoneedforadetailed
modeloftheobject.
Compared to the data from other sensors such as the single bit that tells the vacuum
robot that it has bumped into a wall visual observations are extraordinarily rich both in
the detail they can reveal and in the sheer amount of data they produce. A video camera
for robotic applications might produce a million 24-bit pixels at 60 Hz; a rate of 10 G B per
minute. The problem for a vision-capable agent then is: Which aspects of the rich visual
stimulusshouldbeconsideredtohelptheagentmakegoodactionchoicesandwhichaspects
should be ignored? Visionand all perceptionserves to further the agents goals not as
anendtoitself.
F EA TU RE We can characterize three broad approaches to the problem. The feature extraction
E XT RA CT IO N
approach as exhibited by Drosophila emphasizes simple computations applied directly to
the sensor observations. Inthe recognition approach an agent drawsdistinctions among the
R EC OG NI TI ON
objectsitencountersbasedonvisualandotherinformation. Recognitioncouldmeanlabeling
eachimagewithayesornoastowhetheritcontainsfoodthatweshouldforageorcontains
Grandmas face. Finally in the reconstruction approach an agent builds ageometric model
R EC ON ST RU CT IO N
oftheworldfromanimageorasetofimages.
The last thirty years of research have produced powerful tools and methods for ad-
dressing these approaches. Understanding these methods requires an understanding of the
processes bywhich imagesareformed. Therefore wenowcoverthephysical andstatistical
phenomena thatoccurintheproduction ofanimage.
Imaging distorts the appearance of objects. For example a picture taken looking down a
long straight set of railway tracks will suggest that the rails converge and meet. As another
example if you hold your hand in front of your eye you can block out the moon which is
not smaller than your hand. Asyou moveyour hand back and forth ortilt it your hand will
seem toshrink and grow in the image but itis not doing so inreality Figure 24.1. Models
oftheseeffectsareessential forbothrecognition andreconstruction.
Image sensors gather light scattered from objects in a scene and create a two-dimensional
S CE NE
image. In the eye the image is formed on the retina which consists of two types of cells:
I MA GE
about 100 million rods which are sensitive to light at a wide range of wavelengths and 5
Figure24.1 Imagingdistortsgeometry. Parallel lines appearto meet in the distance as
intheimageoftherailwaytracksontheleft. Inthecentera smallhandblocksoutmostof
a largemoon. Onthe rightisa foreshorteningeffect: thehandistilted awayfromthe eye
makingitappearshorterthaninthecenterfigure.
million cones. Cones which are essential for color vision are of three main types each of
which is sensitive to a different set of wavelengths. In cameras the image is formed on an
image plane which can be a piece of film coated with silver halides or a rectangular grid
of a few million photosensitive pixels each a complementary metal-oxide semiconductor
P IX EL
C MO S or charge-coupled device C CD. Each photon arriving at the sensor produces an
effect whose strength depends on the wavelength of the photon. The output of the sensor
is the sum of all effects due to photons observed in some time window meaning that image
sensorsreportaweightedaverageoftheintensity oflightarrivingatthesensor.
To see a focused image we must ensure that all the photons from approximately the
samespotinthescenearriveatapproximatelythesamepointintheimageplane. Thesimplest
way to form a focused image is to view stationary objects with a pinhole camera which
P IN HO LE CA ME RA
consists ofapinhole opening Oatthefrontofabox and animageplane atthebackofthe
box Figure 24.2. Photons from the scene must pass through the pinhole so if it is small
enough then nearby photons in the scene will be nearby in the image plane and the image
willbeinfocus.
Thegeometryofsceneandimageiseasiesttounderstand withthepinholecamera. We
useathree-dimensional coordinatesystemwiththeoriginatthepinholeandconsiderapoint
cid:2
P in the scene with coordinates X YZ. P gets projected to the point P in the image
planewithcoordinates xyz. Iff isthedistancefromthepinholetotheimageplanethen
bysimilartriangles wecanderivethefollowingequations:
x X y Y f X f Y
x y .
f Z f Z Z Z
P ER SP EC TI VE These equations define animage-formation process known as perspective projection. Note
P RO JE CT IO N
that the Z inthe denominator means that the farther awayan object is the smallerits image
Section24.1. Image Formation 931
Image
plane Y P
X
Z
P
Pinhole
f
Figure24.2 Eachlight-sensitiveelementintheimageplaneatthebackofapinholecam-
erareceiveslightfromathesmallrangeofdirectionsthatpassesthroughthepinhole. Ifthe
pinholeissmallenoughtheresultisafocusedimageatthebackofthepinhole.Theprocess
ofprojectionmeansthatlargedistantobjectslookthesameassmallernearbyobjects.Note
thattheimageisprojectedupsidedown.
will be. Also note that the minus signs mean that the image is inverted both leftright and
updown comparedwiththescene.
Under perspective projection distant objects look small. This is what allows you to
coverthemoonwithyourhand Figure24.1. Animportantresultofthiseffectisthatparallel
linesconverge toapointonthehorizon. Thinkofrailwaytracks Figure24.1. Alineinthe
sceneinthedirection U VWandpassingthroughthepoint X Y Z canbedescribed
as the set of points X λ UY λ VZ λ W with λvarying between and .
Differentchoices of X Y Z yielddifferent linesparallel tooneanother. Theprojection
ofapoint P fromthislineontotheimageplaneisgivenby
cid:13 λ cid:14
X λ U Y λ V
f f .
Z λ W Z λ W
As λ or λ this becomes p f U Wf V W if W cid:7 0. This means that
two parallel lines leaving different points in space will converge in the imagefor large λ
theimagepointsarenearlythesamewhateverthevalueof X Y Z againthinkrailway
V AN IS HI NG PO IN T
tracks Figure 24.1. We call p the vanishing point associated with the family of straight
lineswithdirection U VW. Lineswiththesamedirectionsharethesamevanishingpoint.
The drawback of the pinhole camera is that we need a small pinhole to keep the image in
focus. Butthesmallerthepinholethefewerphotonsgetthroughmeaningtheimagewillbe
dark. Wecan gather more photons by keeping the pinhole open longer but then wewill get
motion blurobjects inthe scene thatmovewillappearblurred because theysend photons
M OT IO NB LU R
to multiple locations on the image plane. If we cant keep the pinhole open longer we can
trytomakeitbigger. Morelightwillenterbutlightfromasmallpatchofobjectinthescene
willnowbespreadoverapatchontheimageplane causingablurred image.
Image plane
Light Source
Iris
Cornea Fovea
Visual Axis
Optic Nerve
Lens
Optical Axis
Lens
System Retina
Figure24.3 Lensescollectthelightleavingascenepointinarangeofdirectionsandsteer
italltoarriveatasinglepointontheimageplane. Focusingworksforpointslyingcloseto
a focal plane in space; other points will not be focused properly. In cameras elements of
thelenssystemmovetochangethefocalplanewhereasintheeyetheshapeofthelensis
changedbyspecializedmuscles.
Vertebrate eyes and modern cameras use a lens system to gather sufficient light while
L EN S
keeping the image in focus. A large opening is covered with a lens that focuses light from
nearby object locations downtonearby locations intheimageplane. However lenssystems
have a limited depth of field: they can focus light only from points that lie within a range
D EP TH OF FI EL D
of depths centered around a focal plane. Objects outside this range willbe out of focus in
F OC AL PL AN E
the image. Tomovethe focal plane thelens inthe eyecan change shape Figure 24.3; in a
camerathelensesmovebackandforth.
Perspective effects arent always pronounced. For example spots on a distant leopard may
looksmallbecausetheleopardisfarawaybuttwospotsthatarenexttoeachotherwillhave
aboutthesamesize. Thisisbecausethedifferenceindistancetothespotsissmallcompared
tothedistance tothemandsowecansimplify theprojection model. Theappropriate model
S CA LE D is scaled orthographic projection. The idea is as follows: If the depth Z of points on the
O RT HO GR AP HI C
P RO JE CT IO N object varies within some range Z Δ Z with Δ Z Z then the perspective scaling
factor f Z canbeapproximated byaconstant s f Z . Theequations forprojection from
0
the scene coordinates X YZ to the image plane become x s X and y s Y. Scaled
orthographicprojectionisanapproximationthatisvalidonlyforthosepartsofthescenewith
notmuchinternaldepthvariation. Forexamplescaledorthographic projectioncanbeagood
modelforthefeaturesonthefrontofadistant building.
The brightness of a pixel in the image is a function of the brightness of the surface patch in
thescenethatprojectstothepixel. Wewillassumealinearmodelcurrentcamerashavenon-
linearities attheextremesoflightanddarkbutarelinear inthemiddle. Imagebrightness is
Section24.1. Image Formation 933
Diffuse reflection bright
Specularities
Diffuse reflection dark Cast shadow
Figure24.4 Avarietyofilluminationeffects. Therearespecularities onthemetalspoon
andonthemilk. Thebrightdiffusesurfaceisbrightbecauseitfacesthelightdirection. The
darkdiffusesurfaceisdarkbecauseitistangentialtotheilluminationdirection.Theshadows
appearatsurfacepointsthatcannotseethelightsource.Photoby Mike Linksvayermlinksva
onflickr.
astrongifambiguouscuetotheshapeofanobjectandfromtheretoitsidentity. Peopleare
usually able to distinguish the three main causes of varying brightness and reverse-engineer
the objects properties. Thefirstcause is overall intensity ofthelight. Eventhough awhite
O VE RA LL IN TE NS IT Y
object inshadow maybelessbright thanablackobject indirectsunlight theeyecandistin-
guishrelativebrightnesswellandperceivethewhiteobjectaswhite. Seconddifferentpoints
in the scene may reflect more or less of the light. Usually the result is that people perceive
R EF LE CT
these points aslighterordarker andsoseetexture ormarkings ontheobject. Third surface
patches facing thelightarebrighter thansurface patches tilted awayfrom thelight aneffect
known as shading. Typically people can tell that this shading comes from the geometry of
S HA DI NG
theobject butsometimesgetshading andmarkingsmixedup. Forexample astreakofdark
makeupunderacheekbonewilloftenlooklikeashadingeffectmakingthefacelookthinner.
D IF FU SE Most surfaces reflect light by a process of diffuse reflection. Diffuse reflection scat-
R EF LE CT IO N
terslightevenlyacross thedirections leaving asurface sothebrightness ofadiffusesurface
doesnt depend ontheviewingdirection. Mostcloth paints roughwoodensurfaces vegeta-
tion and rough stone are diffuse. Mirrors are not diffuse because what you see depends on
the direction in which you look at the mirror. The behavior of a perfect mirror is known as
S PE CU LA R specularreflection. Somesurfacessuch asbrushed metal plastic orawetfloordisplay
R EF LE CT IO N
smallpatcheswherespecularreflection hasoccurred calledspecularities. Theseareeasyto
S PE CU LA RI TI ES
identifybecausetheyaresmallandbright Figure24.4. Foralmostallpurposesitisenough
tomodelallsurfaces asbeingdiffusewithspecularities.
θ
θ
A B
Figure24.5 Twosurfacepatchesareilluminatedbyadistantpointsourcewhoseraysare
shown as gray arrowheads. Patch A is tilted away from the source θ is close to 900 and
collectslessenergybecauseitcutsfewerlightraysperunitsurfacearea. Patch Bfacingthe
sourceθiscloseto00collectsmoreenergy.
Themainsourceofillumination outsideisthesunwhoseraysalltravelparalleltoone
D IS TA NT PO IN T another. Wemodel this behavior asa distantpointlight source. Thisisthemostimportant
L IG HT SO UR CE
model of lighting and is quite effective for indoor scenes as well as outdoor scenes. The
amountoflightcollectedbyasurfacepatchinthismodeldependsontheangleθbetweenthe
illumination direction andthenormaltothesurface.
A diffuse surface patch illuminated by a distant point light source will reflect some
fraction of the light it collects; this fraction is called the diffuse albedo. White paper and
D IF FU SE AL BE DO
snowhaveahighalbedoabout0.90whereasflatblackvelvetandcharcoalhavealowalbedo
of about 0.05 which means that 95 of the incoming light is absorbed within the fibers of
L AM BE RT SC OS IN E thevelvetortheporesofthecharcoal. Lambertscosinelawstatesthatthebrightness ofa
L AW
diffusepatchisgivenby
I ρ I cosθ
0
whereρisthediffusealbedo I istheintensityofthelightsourceandθistheanglebetween
0
the light source direction and the surface normal see Figure 24.5. Lamperts law predicts
bright image pixels come from surface patches that face the light directly and dark pixels
come from patches that see the light only tangentially so that the shading on a surface pro-
vides some shape information. We explore this cue in Section 24.4.5. If the surface is not
reached bythelight source thenitisin shadow. Shadowsareveryseldom auniform black
S HA DO W
because the shadowed surface receives some light from other sources. Outdoors the most
important such source is the sky which is quite bright. Indoors light reflected from other
surfaces illuminates shadowed patches. These interreflections can have a significant effect
I NT ER RE FL EC TI ON S
on the brightness of other surfaces too. These effects are sometimes modeled by adding a
A MB IE NT constant ambientilluminationtermtothepredicted intensity.
I LL UM IN AT IO N
Section24.2. Early Image-Processing Operations 935
Fruit is a bribe that a tree offers to animals to carry its seeds around. Trees have evolved to
have fruit that turns red oryellow when ripe and animals have evolved todetect these color
changes. Light arriving at the eye has different amounts of energy at different wavelengths;
thiscanberepresented byaspectralenergydensityfunction. Humaneyesrespondtolightin
the 380750nm wavelength region with three different types of color receptor cells which
have peakreceptiveness at420mm blue 540nm green and 570nm red. Thehumaneye
cancaptureonlyasmallfractionofthefullspectralenergy densityfunctionbutitisenough
totellwhenthefruitisripe.
P RI NC IP LE OF Theprincipleoftrichromacystatesthatforanyspectralenergydensitynomatterhow
T RI CH RO MA CY
complicateditispossibletoconstructanotherspectralenergydensityconsistingofamixture
ofjustthreecolorsusually redgreenandbluesuchthat ahumancanttellthedifference
between the two. That means that our T Vs and computer displays can get by with just the
three redgreenblue or R GB color elements. It makes our computer vision algorithms
easier too. Each surface can be modeled with three different albedos for R GB. Similarly
each light source can be modeled with three R GB intensities. We then apply Lamberts
cosine law to each to get three R GB pixel values. This model predicts correctly that the
same surface will produce different colored image patches under different-colored lights. In
facthumanobserversarequitegoodatignoringtheeffects ofdifferentcoloredlightsandare
abletoestimatethecolorofthesurfaceunderwhitelightaneffectknownascolorconstancy.
C OL OR CO NS TA NC Y
Quiteaccurate colorconstancy algorithms arenowavailable; simpleversions showupinthe
auto white balance function of your camera. Note that if we wanted to build a camera for
mantis shrimp we would need 12 different pixel colors corresponding to the 12 types of
colorreceptorsofthecrustacean.
Wehave seen how light reflects off objects in the scene to form an image consisting of say
fivemillion 3-byte pixels. With all sensors there will be noise in the image and in any case
thereisalotofdatatodealwith. Sohowdowegetstartedonanalyzing thisdata?
Inthis section wewillstudy threeuseful image-processing operations: edge detection
texture analysis and computation of optical flow. These are called early or low-level
operations because they are the first in a pipeline of operations. Early vision operations are
characterized by their local nature they can be carried out in one part of the image without
regard for anything more than a few pixels away and by their lack of knowledge: we can
perform these operations without consideration of the objects that might be present in the
scene. This makes the low-level operations good candidates for implementation in parallel
hardwareeither in a graphics processor unit G PU or an eye. We will then look at one
mid-leveloperation: segmenting theimageintoregions.
A
B
1
1
1
3
4
discontinuities;3reflectancediscontinuities;4illuminationdiscontinuitiesshadows.
Edges are straight lines or curves in the image plane across which there is a significant
E DG E
change in image brightness. The goal of edge detection is to abstract away from the messy
multimegabyte imageandtowardamorecompact abstract representation asin Figure24.6.
The motivation is that edge contours in the image correspond to important scene contours.
In the figure we have three examples of depth discontinuity labeled 1; two surface-normal
discontinuities labeled 2; a reflectance discontinuity labeled 3; and an illumination discon-
tinuity shadow labeled 4. Edgedetection isconcerned only withthe image and thus does
notdistinguish betweenthesedifferenttypesofscenediscontinuities; laterprocessing will.
b shows the output of an edge-detection algorithm on this image. As you can see there
is a difference between the output and an ideal line drawing. There are gaps where no edge
appears andtherearenoiseedgesthatdonotcorrespond toanythingofsignificance inthe
scene. Laterstagesofprocessing willhavetocorrectfortheseerrors.
Howdowedetect edgesinanimage? Considertheprofileofimagebrightness along a
one-dimensional cross-section perpendicular to an edgefor example the one between the
leftedgeofthedeskandthewall. Itlookssomethinglikewhatisshownin Figure24.8top.
Edgescorrespondtolocationsinimageswherethebrightnessundergoesasharpchange
so anaive idea would be todifferentiate the image and look forplaces where the magnitude
cid:2
ofthederivative I xislarge. Thatalmostworks. In Figure24.8middleweseethatthere
isindeedapeakatx50buttherearealsosubsidiarypeaksatotherlocationse.g.x75.
These arise because of the presence of noise in the image. If wesmooth the image first the
spurious peaksarediminished asweseeinthebottomofthefigure.
Section24.2. Early Image-Processing Operations 937
a b
Figure24.7 a Photographofastapler. b Edgescomputedfroma.
2
1
0
1
1
0
1
1
0
1
Figure24.8 Top:Intensityprofile Ixalongaone-dimensionalsectionacrossanedgeat
x50. Middle: Thederivativeofintensity Icid:5x. Largevaluesofthisfunctioncorrespond
to edges but the function is noisy. Bottom: The derivative of a smoothed version of the
intensity I Gσcid:5whichcanbecomputedinonestepastheconvolution I Gcid:5 σ. Thenoisy
candidateedgeatx75hasdisappeared.
The measurement of brightness at a pixel in a C CD camera is based on a physical
process involving the absorption of photons and the release of electrons; inevitably there
will be statistical fluctuations of the measurementnoise. The noise can be modeled with
a Gaussian probability distribution with each pixel independent of the others. One way to
smooth animage istoassign toeach pixel theaverage ofits neighbors. Thistends tocancel
outextremevalues. Buthowmanyneighborsshouldweconsideronepixelawayortwoor
more? Onegoodanswerisaweighted average thatweights thenearest pixelsthemostthen
gradually decreases the weight for more distant pixels. The Gaussian filter does just that.
G AU SS IA NF IL TE R
Usersof Photoshoprecognize thisasthe Gaussianbluroperation. Recallthatthe Gaussian
function withstandard deviation σ andmean0is
N x 1 ex22σ2 inonedimension or
σ
2πσ
N xy 1 ex2y22σ2 intwodimensions.
σ 2πσ2
The application of the Gaussian filter replaces the intensity Ix y with the sum over all
xy pixels of Ixy N d where d is the distance from x y to xy. This kind of
σ 0 0
weighted sum isso common that there is aspecial name and notation forit. Wesay that the
function histheconvolution oftwofunctions f andg denoted f gifwehave
C ON VO LU TI ON
cid:12
hx f gx fugxu inonedimension or
u
cid:12 cid:12
hxy f gxy fuvgxuyv intwo.
uv
Sothesmoothingfunctionisachievedbyconvolving theimagewiththe Gaussian I N . A
σ
σof1pixelisenoughtosmoothoverasmallamountofnoisewhereas2pixelswillsmootha
largeramountbutatthelossofsomedetail. Becausethe Gaussians influencefadesquickly
atadistance wecanreplace theinthesumswith3σ.
Wecanoptimizethecomputation bycombiningsmoothingandedgefindingintoasin-
gleoperation. Itisatheoremthatforanyfunctions f andgthederivativeoftheconvolution
f
gcid:2
is equal to the convolution with the derivative f
gcid:2
. So rather than smoothing
theimage and then differentiating wecan just convolve the imagewith thederivative ofthe
cid:2
smoothing function N . We then mark as edges those peaks in the response that are above
σ
somethreshold.
Thereisanatural generalization ofthisalgorithm fromone-dimensional crosssections
togeneraltwo-dimensional images. Intwodimensionsedgesmaybeatanyangleθ. Consid-
eringtheimagebrightness asascalarfunction ofthevariables xyitsgradient isavector
cid:31
cid:13 cid:14
I
I
I x x .
I I
y
y
Edgescorrespond tolocationsinimageswherethebrightnessundergoes asharpchange and
so the magnitude of the gradient I should be large at an edge point. Of independent
interestisthedirectionofthegradient
cid:13 cid:14
I cosθ
.
I sinθ
Thisgivesusaθ θxyateverypixelwhichdefinestheedgeorientation atthatpixel.
O RI EN TA TI ON
Section24.2. Early Image-Processing Operations 939
Asinonedimension toformthegradientwedontcompute Ibutrather I N
σ
the gradient after smoothing the image by convolving it with a Gaussian. And again the
shortcut is that this is equivalent to convolving the image with the partial derivatives of a
Gaussian. Oncewehavecomputed thegradient wecanobtain edges byfinding edge points
and linking them together. To tell whether a point is an edge point we must look at other
points a small distance forward and back along the direction of the gradient. If the gradient
magnitude at one of these points is larger then we could get a better edge point by shifting
the edge curve very slightly. Furthermore if the gradient magnitude is too small the point
cannot be an edge point. So at an edge point the gradient magnitude is a local maximum
alongthedirectionofthegradient andthegradient magnitudeisaboveasuitablethreshold.
Oncewehavemarkededgepixelsbythisalgorithmthenextstageistolinkthosepixels
thatbelongtothesameedgecurves. Thiscanbedonebyassumingthatanytwoneighboring
edgepixelswithconsistent orientations mustbelongtothesameedgecurve.
In everyday language texture is the visual feel of a surfacewhat you see evokes what
T EX TU RE
the surface might feel like if you touched it texture has the same root as textile. In
computational vision texture refers to a spatially repeating pattern on a surface that can be
sensedvisually. Examplesincludethepatternofwindowsonabuildingstitchesonasweater
spots on a leopard blades of grass on a lawn pebbles on a beach and people in a stadium.
Sometimes the arrangement is quite periodic as in the stitches on a sweater; in other cases
suchaspebblesonabeachtheregularity isonlystatistical.
Whereasbrightnessisapropertyofindividualpixelstheconceptoftexturemakessense
only for a multipixel patch. Given such a patch we could compute the orientation at each
pixelandthencharacterize thepatchbyahistogram oforientations. Thetextureofbricksin
awallwould havetwopeaks inthe histogram one vertical and onehorizontal whereas the
textureofspotsonaleopards skinwouldhaveamoreuniform distribution oforientations.
Figure24.9showsthatorientationsarelargelyinvarianttochangesinillumination. This
makes texture an important clue for object recognition because other clues such as edges
canyielddifferent resultsindifferent lighting conditions.
Inimagesoftexturedobjectsedgedetectiondoesnotworkaswellasitdoesforsmooth
objects. This is because the most important edges can be lost among the texture elements.
Quiteliterallywemaymissthetigerforthestripes. Thesolutionistolookfordifferencesin
texture properties just thewaywelook fordifferences in brightness. Apatch on atigerand
apatchonthegrassybackgroundwillhaveverydifferentorientationhistogramsallowingus
tofindtheboundary curvebetweenthem.
Next let us consider what happens when we have a video sequence instead of just a single
static image. When an object in the video is moving orwhen the camera is moving relative
to an object the resulting apparent motion in the image is called optical flow. Optical flow
O PT IC AL FL OW
describes the direction and speed of motion of features in the imagethe optical flow of a
a b
Figure24.9 Twoimagesofthesametextureofcrumpledricepaperwithdifferentillumi-
nationlevels. Thegradientvectorfieldateveryeighthpixelisplottedontopofeachone.
Noticethat asthe lightgetsdarkerallthegradientvectorsgetshorter. Thevectorsdonot
rotatesothegradientorientationsdonotchange.
video of a race car would be measured in pixels per second not miles per hour. Theoptical
flow encodes useful information about scene structure. For example in a video of scenery
taken from a moving train distant objects have slower apparent motion than close objects;
thus the rate of apparent motion can tell us something about distance. Optical flow also
enablesustorecognizeactions. In Figure24.10aandbweshowtwoframesfromavideo
of a tennis player. In c we display the optical flow vectors computed from these images
showingthattheracketandfrontlegaremovingfastest.
The optical flow vector field can be represented at any point xy by its components
v xyinthexdirectionandv xyintheydirection. Tomeasureopticalflowweneedto
x y
findcorresponding pointsbetween onetimeframeandthenext. Asimple-minded technique
is based on the fact that image patches around corresponding points have similar intensity
patterns. Consider a block of pixels centered at pixel p x y at time t . This block
of pixels is to be compared with pixel blocks centered at various candidate pixels at x
0
D y D at time t D . One possible measure of similarity is the sum of squared
x 0 y 0 t
S UM OF SQ UA RE D differences S SD:
D IF FE RE NC ES cid:12
S SD D D Ixyt Ix D y D t D 2 .
x y x y t
xy
Here xy ranges over pixels in the block centered at x y . We find the D D that
minimizes the S SD.The optical flow at x y is then v v D D D D . Note
thatforthistoworkthereneedstobesometextureorvariationinthescene. Ifoneislooking
at a uniform white wall then the S SD is going to be nearly the same for the different can-
Section24.2. Early Image-Processing Operations 941
Figure24.10 Twoframesofavideosequence. Ontherightistheopticalflowfieldcor-
responding to the displacement from one frame to the other. Note how the movement of
thetennisracketandthe frontleg iscapturedbythe directionsof thearrows. Courtesyof
Thomas Brox.
didate matches and the algorithm is reduced to making a blind guess. The best-performing
algorithms for measuring optical flow rely on a variety of additional constraints when the
sceneisonlypartially textured.
Segmentationistheprocessofbreakinganimageintoregionsofsimilarpixels. Eachimage
S EG ME NT AT IO N
pixel can be associated with certain visual properties such as brightness color and texture.
R EG IO NS
Within an object or a single part of an object these attributes vary relatively little whereas
across an inter-object boundary there is typically a large change in one or more of these at-
tributes. Therearetwoapproachestosegmentationonefocusingondetectingtheboundaries
oftheseregions andtheotherondetecting theregionsthemselves Figure24.11.
Aboundary curvepassing throughapixel xywillhaveanorientation θsooneway
toformalizetheproblem ofdetecting boundarycurvesisasa machinelearningclassification
problem. Based on features from a local neighborhood wewant to compute the probability
P xyθthatindeedthereisaboundarycurveatthatpixelalongthatorientation. Consider
b
a circular disk centered at xy subdivided into two half disks by adiameter oriented at θ.
If there isa boundary at xyθ the two half disks might be expected to differsignificantly
intheirbrightnesscolorandtexture. Martin Fowlkesand Malik2004usedfeaturesbased
on differences in histograms of brightness color and texture values measured in these two
halfdisks andthen trained aclassifier. Forthistheyused a datasetofnatural images where
humanshadmarkedthegroundtruthboundaries andthegoaloftheclassifierwastomark
exactlythoseboundaries markedbyhumansandnoothers.
Boundariesdetectedbythistechniqueturnouttobesignificantlybetterthanthosefound
usingthesimpleedge-detectiontechniquedescribedpreviously. Butstilltherearetwolimita-
tions. 1 Theboundarypixelsformedbythresholding P xyθarenotguaranteed toform
b
closed curves sothisapproach doesnt deliverregions and 2thedecision making exploits
onlylocalcontextanddoesnotuseglobalconsistency constraints.
a b c d
Figure24.11 a Originalimage. b Boundarycontourswherethehigher the Pb value
the darker the contour. c Segmentation into regions correspondingto a fine partition of
theimage. Regionsarerenderedintheirmeancolors. d Segmentationintoregionscorre-
spondingtoa coarserpartitionoftheimageresultinginfewerregions. Courtesyof Pablo
Arbelaez Michael Maire Charles Fowlkesand Jitendra Malik
Thealternativeapproachisbasedontryingtoclusterthe pixelsintoregionsbasedon
their brightness color and texture. Shi and Malik 2000 set this up as a graph partitioning
problem. The nodes of the graph correspond to pixels and edges to connections between
pixels. Theweight W ontheedgeconnectingapairofpixelsiandjisbasedonhowsimilar
ij
thetwopixelsareinbrightness color texture etc. Partitionsthatminimizeanormalized cut
criterion are then found. Roughly speaking the criterion for partitioning the graph is to
minimize the sum of weights of connections across the groups of pixels and maximize the
sumofweightsofconnections withinthegroups.
Segmentation based purely on low-level local attributes such as brightness and color
cannot be expected to deliver the final correct boundaries of all the objects in the scene. To
reliably find object boundaries we need high-level knowledge of the likely kinds of objects
inthescene. Representing thisknowledge isatopicofactiveresearch. Apopularstrategy is
to produce an over-segmentation of animage containing hundreds of homogeneous regions
known as superpixels. From there knowledge-based algorithms can take over; they will
S UP ER PI XE LS
finditeasier todeal withhundreds ofsuperpixels ratherthan millions ofrawpixels. Howto
exploithigh-level knowledgeofobjects isthesubjectofthenextsection.
Appearanceisshorthand forwhatanobject tendstolooklike. Someobject categoriesfor
A PP EA RA NC E
example baseballsvary rather little in appearance; all of the objects in the category look
about the same under most circumstances. In this case we can compute a set of features
describing eachclassofimageslikelytocontain theobject thentestitwithaclassifier.
Section24.3. Object Recognition by Appearance 943
Otherobjectcategoriesfor examplehouses orballetdancersvary greatly. Ahouse
canhavedifferentsizecolorandshapeandcanlookdifferentfromdifferentangles. Adancer
looksdifferentineachposeorwhenthestagelightschange colors. Ausefulabstractionisto
saythatsomeobjectsaremadeupoflocalpatternswhichtendtomovearoundwithrespectto
oneanother. Wecanthenfindtheobjectbylookingatlocalhistogramsofdetectorresponses
whichexposewhethersomepartispresent butsuppress thedetailsofwhereitis.
Testing each class of images with a learned classifier is an important general recipe.
It works extremely well for faces looking directly at the camera because at low resolution
and underreasonable lighting allsuch faces look quite similar. Theface isround and quite
brightcomparedtotheeyesockets;thesearedarkbecausetheyaresunkenandthemouthis
adarkslashasaretheeyebrows. Majorchangesofilluminationcancausesomevariationsin
this pattern but the range of variation is quite manageable. That makes it possible to detect
face positions in an image that contains faces. Once a computational challenge this feature
isnowcommonplace ineveninexpensive digitalcameras.
Forthe moment we will consider only faces where the nose is oriented vertically; we
will deal with rotated faces below. We sweep a round window of fixed size over the image
compute features for it and present the features to a classifier. This strategy is sometimes
calledtheslidingwindow. Featuresneedtoberobusttoshadowsandtochangesinbrightness
S LI DI NG WI ND OW
causedbyilluminationchanges. Onestrategyistobuildfeaturesoutofgradientorientations.
Another is to estimate and correct the illumination in each image window. To find faces of
different sizes repeat the sweep over larger or smaller versions of the image. Finally we
postprocess theresponses acrossscalesandlocations toproduce thefinalsetofdetections.
Postprocessing is important because it is unlikely that we have chosen a window size
that is exactly the right size for a face even if we use multiple sizes. Thus we will likely
have several overlapping windows that each report a match for a face. However if we use
aclassifierthat can report strength ofresponse forexample logistic regression orasupport
vector machine we can combine these partial overlapping matches at nearby locations to
yieldasinglehigh-quality match. Thatgivesusafacedetectorthatcansearchoverlocations
and scales. To search rotations as well we use two steps. We train a regression procedure
to estimate the best orientation of any face present in a window. Now for each window we
estimatetheorientation reorientthewindowthentestwhetheraverticalfaceispresentwith
ourclassifier. Allthisyieldsasystemwhosearchitecture issketched in Figure24.12.
Training data is quite easily obtained. There are several data sets of marked-up face
images and rotated face windows are easy to build just rotate a window from a training
data set. One trick that is widely used is to take each example window then produce new
examples by changing the orientation of the window the center of the window or the scale
very slightly. This is an easy way of getting a bigger data set that reflects real images fairly
well; the trick usually improves performance significantly. Face detectors built along these
linesnowperform verywellforfrontalfacessideviewsare harder.
Non-maximal
suppresion
Image Responses Detections
Estimate
orientation
Correct Rotate
Features Classifier
illumination window
two parts here. On the top we go from images to responses then apply non-maximum
suppressionto findthestrongestlocalresponse. Theresponsesareobtainedbytheprocess
illustratedonthebottom. Wesweepawindowoffixedsizeoverlargerandsmallerversions
of the image so as to find smaller or larger faces respectively. The illumination in the
window is corrected and then a regression engine quite often a neural net predicts the
orientationofthe face. Thewindowiscorrectedto thisorientationandthen presentedto a
classifier. Classifieroutputsarethenpostprocessedtoensurethatonlyonefaceisplacedat
eachlocationintheimage.
Many objects produce much more complex patterns than faces do. This is because several
effectscanmovefeaturesaroundinanimageoftheobject. Effectsinclude Figure24.13
Foreshorteningwhichcausesapatternviewedataslanttobesignificantly distorted.
Aspect which causes objects to look different when seen from different directions.
Evenassimpleanobjectasadoughnut hasseveralaspects;seenfromthesideitlooks
likeaflattenedovalbutfromaboveitisanannulus.
Occlusion where some parts are hidden from some viewing directions. Objects can
occlude one another or parts of an object can occlude other parts an effect known as
self-occlusion.
Deformation where internal degrees of freedom of the object change its appearance.
Forexamplepeoplecanmovetheirarmsandlegsaroundgeneratingaverywiderange
ofdifferentbodyconfigurations.
However our recipe of searching across location and scale can still work. This is because
some structure will be present in the images produced by the object. Forexample apicture
of a car is likely to show some of headlights doors wheels windows and hubcaps though
theymaybeinsomewhatdifferentarrangementsindifferentpictures. Thissuggestsmodeling
objectswithpatternelementscollectionsofparts. Thesepatternelementsmaymovearound
Section24.3. Object Recognition by Appearance 945
Foreshortening Aspect
Occlusion Deformation
circular patch on the top left. This patch is viewed at a slant and so is elliptical in the
image.Secondobjectsviewedfromdifferentdirectionscanchangeshapequitedramatically
aphenomenonknownasaspect. On thetoprightarethreedifferentaspectsofadoughnut.
Occlusion causes the handle of the mug on the bottom left to disappear when the mug is
rotated. In this case because the body and handle belong to the same mug we have self-
occlusion.Finallyonthebottomrightsomeobjectscandeformdramatically.
withrespect toone another but ifmostofthe pattern elements arepresent inabout theright
placethentheobjectispresent. Anobjectrecognizeristhenacollectionoffeaturesthatcan
tellwhetherthepatternelementsarepresent andwhethertheyareinabouttherightplace.
The most obvious approach is to represent the image window with a histogram of the
pattern elements that appear there. This approach does not work particularly well because
too many patterns get confused with one another. For example if the pattern elements are
color pixels the French U K and Netherlands flags will get confused because they have
approximately the same color histograms though the colors are arranged in very different
ways. Quite simple modifications of histograms yield very useful features. The trick is to
preserve some spatial detail in the representation; for example headlights tend to be at the
front of a car and wheels tend to be at the bottom. Histogram-based features have been
successful inawidevarietyofrecognition applications; wewillsurveypedestrian detection.
The World Bankestimatesthateachyearcaraccidentskillabout1.2millionpeopleofwhom
abouttwothirdsarepedestrians. Thismeansthatdetecting pedestrians isanimportantappli-
cation problem because cars that can automatically detect and avoid pedestrians might save
many lives. Pedestrians wear many different kinds of clothing and appear in many different
configurations but at relatively low resolution pedestrians can have a fairly characteristic
appearance. The most usual cases are lateral or frontal views of a walk. In these cases
Image Orientation Positive Negative
histograms components components
quitecomplexobjects. Ontheleftanimageofapedestrian. Onthecenterleftlocalorien-
tation histogramsforpatches. We then applya classifier such as a supportvectormachine
tofindtheweightsforeachhistogramthatbestseparatethepositiveexamplesofpedestrians
fromnon-pedestrians. Weseethatthepositivelyweightedcomponentslookliketheoutline
ofa person. Thenegativecomponentsarelessclear;theyrepresentallthepatternsthatare
notpedestrians.Figurefrom Dalaland Triggs2005cid:2c I EE E.
we see either a lollipop shape the torso is wider than the legs which are together in
the stance phase of the walk or a scissor shape where the legs are swinging in the
walk. Weexpect to see someevidence ofarms and legs and the curve around theshoulders
and head also tends to visible and quite distinctive. This means that with a careful feature
construction wecanbuildausefulmoving-window pedestrian detector.
There isnt always a strong contrast between the pedestrian and the background so it
isbettertouseorientations thanedgestorepresent theimagewindow. Pedestrians canmove
their arms and legs around so we should use a histogram to suppress some spatial detail in
thefeature. Webreakupthewindowintocellswhichcouldoverlapandbuildanorientation
histogram in each cell. Doing so will produce a feature that can tell whether the head-and-
shoulders curve is at the top of the window orat the bottom but will not change if the head
movesslightly.
One further trick is required to make a good feature. Because orientation features are
not affected by illumination brightness we cannot treat high-contrast edges specially. This
meansthatthedistinctive curvesontheboundary ofapedestrian aretreatedinthesameway
as fine texture detail in clothing or in the background and so the signal may be submerged
innoise. Wecanrecovercontrastinformationbycountinggradientorientations withweights
that reflect how significant a gradient is compared to other gradients in the same cell. We
will write Ix for the gradient magnitude at point x in the image write C for the cell
whosehistogram wewishtocompute and write wx C fortheweight thatwewilluseforthe
Section24.4. Reconstructing the3 DWorld 947
Scale Invariant Feature Transformanearlierversionof the H OGfeature. Ontheleftim-
agesofashoeandatelephonethatserveasobjectmodels.Inthecenteratestimage.Onthe
righttheshoeandthetelephonehavebeendetectedby: findingpointsintheimagewhose
S IF Tfeaturedescriptionsmatchamodel;computinganestimateofposeofthemodel;and
verifyingthatestimate. Astrongmatchisusuallyverifiedwithrarefalsepositives. Images
from Lowe1999cid:2c I EE E.
orientation atxforthiscell. Anaturalchoiceofweightis
wx C cid:2 u
C
I x
Iu
.
This compares the gradient magnitude to others in the cell so gradients that are large com-
pared to their neighbors get a large weight. The resulting feature is usually called a H OG
featurefor Histogram Of Gradientorientations.
H OG FE AT UR E
This feature construction is the main way in which pedestrian detection differs from
facedetection. Otherwisebuildingapedestrian detector isverylikebuilding afacedetector.
The detector sweeps a window across the image computes features for that window then
presents it to a classifier. Non-maximum suppression needs to be applied to the output. In
mostapplications thescale andorientation oftypical pedestrians isknown. Forexample in
driving applications in which a camera is fixedto the car we expect to view mainly vertical
pedestrians and we are interested only in nearby pedestrians. Several pedestrian data sets
havebeenpublished andthesecanbeusedfortrainingtheclassifier.
Pedestrians are not the only type of object we can detect. In Figure 24.15 we see that
similartechniques canbeusedtofindavarietyofobjectsindifferent contexts.
In this section we show how to go from the two-dimensional image to a three-dimensional
representation of the scene. The fundamental question is this: Given that all points in the
scenethatfallalongaraytothepinholeareprojected tothe samepointintheimagehowdo
werecoverthree-dimensional information? Twoideascometoourrescue:
Ifwehavetwoormoreimagesfrom different camerapositions thenwecantriangu-
latetofindtheposition ofapointinthescene.
We can exploit background knowledge about the physical scene that gave rise to the
image. Givenanobject model P Sceneandarendering model P Image Scenewe
cancomputeaposteriordistribution P Scene Image.
Thereisasyetnosingle unifiedtheoryforscene reconstruction. Wesurveyeightcommonly
usedvisualcues: motionbinocularstereopsis multipleviewstextureshadingcontour
andfamiliarobjects.
Ifthecameramovesrelativetothethree-dimensional scene theresulting apparent motionin
theimage optical flowcanbeasource ofinformation forboth themovementofthecamera
and depth in the scene. To understand this we state without proof an equation that relates
theopticalflowtotheviewerstranslational velocity Tandthedepthinthescene.
Thecomponents oftheopticalflowfieldare
T x T T y T
x z y z
v xy v xy
x y
Zxy Zxy
where Zxy is the z-coordinate of the point in the scene corresponding to the point in the
imageatxy.
Note that both components of the optical flow v xy and v xy are zero at the
x y
F OC US OF point x T T y T T . This point is called the focus of expansion of the flow
E XP AN SI ON x z y z
field. Suppose we change the origin in the xy plane to lie at the focus of expansion; then
cid:2 cid:2
the expressions for optical flow take on a particularly simple form. Let xy be the new
coordinates definedbyxcid:2 x T T ycid:2 y T T . Then
x z y z
cid:2 cid:2
x T y T
cid:2 cid:2 z cid:2 cid:2 z
v xy v xy .
x Zxcid:2ycid:2 y Zxcid:2ycid:2
Notethatthere isascale-factor ambiguity here. Ifthecamerawasmoving twiceasfast and
everyobjectinthescenewastwiceasbigandattwicethedistance tothecameratheoptical
flowfieldwouldbeexactlythesame. Butwecanstillextractquiteusefulinformation.
1. Suppose you are a fly trying to land on a wall and you want to know the time-to-
contact at the current velocity. This time is given by Z T . Note that although the
z
instantaneous optical flow field cannot provide either the distance Z or the velocity
component T it can provide the ratio of the two and can therefore be used to control
z
thelanding approach. Thereisconsiderable experimental evidence thatmanydifferent
animalspeciesexploitthiscue.
2. Consider two points at depths Z Z respectively. We may not know the absolute
value of either of these but by considering the inverse of the ratio of the optical flow
magnitudes atthese points wecandetermine thedepthratio Z Z . Thisisthecueof
motion parallax one we use when we look out of the side window of a moving car or
trainandinferthattheslowermovingpartsofthelandscape arefartheraway.
Section24.4. Reconstructing the3 DWorld 949
Perceived object
Left image Right image
Right Left
Disparity
a b
move in the camera plane. The disparity in positions that results is a cue to depth. If we
superimposeleftandrightimageasinbweseethedisparity.
Most vertebrates have two eyes. This is useful for redundancy in case of a lost eye but it
helps in other ways too. Most prey have eyes on the side of the head to enable a widerfield
B IN OC UL AR of vision. Predators have the eyes in the front enabling them to use binocular stereopsis.
S TE RE OP SI S
Theidea issimilartomotion parallax except thatinstead ofusing images overtime weuse
two or more images separated in space. Because a given feature in the scene will be in a
different place relative to the z-axis of each image plane if we superpose the two images
there willbeadisparity inthe location oftheimage feature inthe twoimages. Youcan see
D IS PA RI TY
this in Figure 24.16 where the nearest point of the pyramid is shifted to the left in the right
imageandtotherightintheleftimage.
Note that to measure disparity we need to solve the correspondence problem that is
determine for a point in the left image the point in the right image that results from the
projection of the same scene point. This is analogous to what one has to do in measuring
optical flow and the most simple-minded approaches are somewhat similar and based on
comparingblocksofpixelsaroundcorrespondingpointsusingthesumofsquareddifferences.
Inpracticeweusemuchmoresophisticatedalgorithmswhichexploitadditionalconstraints.
Assuming that we can measure disparity how does this yield information about depth
in the scene? We will need to work out the geometrical relationship between disparity and
depth. First wewill consider the case when both the eyes or cameras are looking forward
withtheiropticalaxesparallel. Therelationship oftherightcameratotheleftcameraisthen
just a displacement along the x-axis by an amount b the baseline. We can use the optical
flow equations from the previous section if we think of this as resulting from a translation
Left
eye δθ2
P
L
θ
b P P
0
P
R
Right
eye Z δ Z
Figure24.17 Therelationbetweendisparityanddepthinstereopsis. Thecentersofpro-
jectionofthetwoeyesarebapartandtheopticalaxesintersectatthefixationpoint P . The
0
point P in the scene projects to points P L and P R in the two eyes. In angular terms the
disparitybetweentheseisδθ. Seetext.
vector T acting for time δt with T bδt and T T 0. The horizontal and vertical
x y z
disparityaregivenbytheopticalflowcomponentsmultipliedbythetimestepδt H v δt
x
V v δt. Carryingoutthesubstitutions wegettheresultthat H b Z V 0. Inwords
y
the horizontal disparity is equal to the ratio of the baseline to the depth and the vertical
disparity iszero. Giventhatweknow bwecanmeasure H andrecoverthedepth Z.
Under normal viewing conditions humans fixate; that is there is some point in the
F IX AT E
sceneatwhichtheopticalaxesofthetwoeyesintersect. Figure24.17showstwoeyesfixated
at a point P which is at a distance Z from the midpoint of the eyes. For convenience
0
we will compute the angular disparity measured in radians. The disparity at the point of
fixation P is zero. For some other point P in the scene that is δ Z farther away we can
0
compute the angular displacements of the left and right images of P which wewill call P
L
and P respectively. If each of these is displaced by an angle δθ2 relative to P then the
R 0
displacement between P and P whichisthedisparityof Pisjustδθ. From Figure24.17
L R
tanθ b2 andtanθδθ2 b2 butforsmallangles tanθ θso
Z Zδ Z
b2 b2 bδ Z
δθ2
Z Z δ Z 2 Z2
andsincetheactualdisparity isδθwehave
bδ Z
disparity .
Z2
Inhumansbthebaselinedistancebetweentheeyesisabout6cm. Supposethat Z isabout
B AS EL IN E
of arc this gives a δ Z of 0.4 mm. For Z 30 cm we get the impressively small value
δ Z 0.036 mm. Thatis atadistance of30cmhumans candiscriminate depths thatdiffer
byaslittleas0.036mmenabling ustothreadneedlesandthelike.
Section24.4. Reconstructing the3 DWorld 951
Figure24.18 a Fourframesfroma videosequencein whichthe camerais movedand
rotatedrelativetotheobject.b Thefirstframeofthesequenceannotatedwithsmallboxes
highlightingthefeaturesfoundbythefeaturedetector.Courtesyof Carlo Tomasi.
Shapefromopticalfloworbinoculardisparityaretwoinstancesofamoregeneralframework
thatofexploiting multipleviewsforrecovering depth. Incomputervision thereisnoreason
forustoberestrictedtodifferentialmotionortoonlyusetwocamerasconvergingatafixation
point. Therefore techniques have been developed that exploit the information available in
multiple views even from hundreds or thousands of cameras. Algorithmically there are
threesubproblems thatneedtobesolved:
The correspondence problem i.e. identifying features in the different images that are
projections ofthesamefeatureinthethree-dimensional world.
The relative orientation problem i.e. determining the transformation rotation and
translation betweenthecoordinate systemsfixedtothedifferent cameras.
Thedepthestimationproblemi.e.determiningthedepthsofvariouspointsintheworld
forwhichimageplaneprojections wereavailable inatleast twoviews
The development of robust matching procedures for the correspondence problem accompa-
niedbynumericallystablealgorithmsforsolvingforrelativeorientations andscenedepthis
oneofthesuccessstoriesofcomputervision. Resultsfromonesuchapproachdueto Tomasi
and Kanade1992areshownin Figures24.18and24.19.
Earlierwesaw how texture wasused forsegmenting objects. Itcan also beused toestimate
distances. In Figure 24.20weseethatahomogeneous texture inthescene results invarying
textureelements ortexels intheimage. Allthepaving tilesinaareidentical inthescene.
T EX EL
Theyappeardifferent intheimagefortworeasons:
a b
Figure24.19 a Three-dimensionalreconstructionofthelocationsoftheimagefeatures
in Figure24.18shownfromabove.b Therealhousetakenfromthesameposition.
1. Differencesinthedistancesofthetexelsfromthecamera. Distantobjectsappearsmaller
byascaling factorof1 Z.
2. Differences in the foreshortening of the texels. If all the texels are in the ground plane
then distance ones are viewed at an angle that is farther off the perpendicular and so
are more foreshortened. The magnitude of the foreshortening effect is proportional to
cosσ where σ is the slant the angle between the Z-axis and n the surface normal to
thetexel.
Researchers have developed various algorithms that try to exploit the variation in the
appearance of the projected texels as a basis for determining surface normals. However the
accuracy and applicability of these algorithms is not anywhere as general as those based on
usingmultipleviews.
Shadingvariation inthe intensity oflight received from different portions ofasurface ina
sceneis determined by the geometry of the scene and by the reflectance properties of the
surfaces. In computer graphics the objective is to compute the image brightness Ixy
given the scene geometry and reflectance properties of the objects in the scene. Computer
visionaimstoinverttheprocessthat istorecoverthegeometryandreflectance properties
given the image brightness Ixy. This has proved to be difficult to do in anything but the
simplestcases.
From the physical model of section 24.1.4 we know that if a surface normal points
toward the light source the surface is brighter and if it points away the surface is darker.
We cannot conclude that a dark patch has its normal pointing away from the light; instead
it could have low albedo. Generally albedo changes quite quickly in images and shading
Section24.4. Reconstructing the3 DWorld 953
a b
Figure24.20 a Atexturedscene.Assumingthattherealtextureisuniformallowsrecov-
eryofthesurfaceorientation. Thecomputedsurfaceorientationisindicatedbyoverlayinga
blackcircleandpointertransformedasifthecirclewerepaintedonthesurfaceatthatpoint.
b Recoveryofshapefromtextureforacurvedsurfacewhitecircleandpointerthistime.
Imagescourtesyof Jitendra Malikand Ruth Rosenholtz1994.
changes rather slowly and humans seem to be quite good at using this observation to tell
whether low illumination surface orientation or albedo caused a surface patch to be dark.
To simplify the problem let us assume that the albedo is known at every surface point. It
is still difficult to recover the normal because the image brightness is one measurement but
thenormalhastwounknownparameters sowecannot simplysolveforthenormal. Thekey
to this situation seems to be that nearby normals will be similar because most surfaces are
smooththey donothavesharpchanges.
Therealdifficultycomesindealingwithinterreflections. Ifweconsideratypicalindoor
scene such as the objects inside an office surfaces are illuminated not only by the light
sources but also by the light reflected from other surfaces in the scene that effectively serve
assecondary light sources. These mutual illumination effects are quite significant and make
itquitedifficulttopredicttherelationshipbetweenthenormalandtheimagebrightness. Two
surface patches with the same normal might have quite different brightnesses because one
receives light reflected from a large white wall and the other faces only a dark bookcase.
Despite these difficulties the problem is important. Humans seem to be able to ignore the
effects of interreflections and get a useful perception of shape from shading but we know
frustratingly littleaboutalgorithms todothis.
When we look at a line drawing such as Figure 24.21 we get a vivid perception of three-
dimensional shapeandlayout. How? Itisacombination ofrecognition offamiliarobjectsin
thesceneandtheapplication ofgeneric constraints suchasthefollowing:
Occluding contours such as the outlines of the hills. Oneside ofthe contour is nearer
totheviewer theotherside isfarther away. Features suchaslocal convexity andsym-
Figure24.21 Anevocativelinedrawing.Courtesyof Isha Malik.
metryprovidecuestosolvingthefigure-groundproblemassigning whichsideofthe
F IG UR E-G RO UN D
contour is figure nearer and which is ground farther. At an occluding contour the
lineofsightistangential tothesurfaceinthescene.
T-junctions. When one object occludes another the contour of the farther object is
interruptedassumingthatthenearerobjectisopaque. A T-junctionresultsintheimage.
Position on the ground plane. Humans like many other terrestrial animals are very
ofteninascenethatcontainsagroundplanewithvariousobjectsatdifferentlocations
G RO UN DP LA NE
onthis plane. Because ofgravity typical objects dont floatinairbut aresupported by
thisgroundplaneandwecanexploittheveryspecialgeometryofthisviewingscenario.
Let us work out the projection of objects of different heights and at different loca-
tions on the ground plane. Suppose that the eye or camera is at a height h above
c
theground plane. Consider an object of height δ Y resting on the ground plane whose
bottom is at Xh Z and top is at Xδ Y h Z. The bottom projects to the
c c
imagepointf X Zfh Zandthetoptof X Zfδ Y h Z. Thebottomsof
c c
nearerobjectssmall Zprojecttopointslowerintheimageplane;fartherobjects have
bottomsclosertothehorizon.
A typical adult human head is about 9 inches long. This means that for someone who is 43
feetawaytheanglesubtendedbytheheadatthecamerais1degree. Ifweseeapersonwhose
head appears to subtend just half a degree Bayesian inference suggests we are looking at a
normal person who is 86 feet away rather than someone with a half-size head. This line of
reasoning suppliesuswithamethodtochecktheresultsofapedestrian detector aswellasa
methodtoestimatethedistancetoanobject. Forexampleallpedestrians areaboutthesame
heightandtheytendtostandonagroundplane. Ifweknowwherethehorizonisinanimage
wecanrankpedestrians bydistancetothecamera. Thisworks becauseweknowwheretheir
Section24.4. Reconstructing the3 DWorld 955
Image plane
Horizon
Ground plane
C
C B
B
A
A
areclosertothehorizonintheimagemustbefartherawaytopdrawing. Thismeansthey
mustlooksmallerintheimageleftlowerdrawing.Thismeansthatthesizeandlocationof
realpedestriansinanimagedependupononeanotherandonthelocationofthehorizon.To
exploitthis we need to identify the groundplane which is done using shape-from-texture
methods.Fromthisinformationandfromsomelikelypedestrianswecanrecoverahorizon
asshowninthecenterimage.Ontherightacceptablepedestrianboxesgiventhisgeometric
context. Noticethatpedestrianswhoarehigherinthescenemustbesmaller. Iftheyarenot
thentheyarefalsepositives.Imagesfrom Hoiemetal.2008cid:2c I EE E.
feet are and pedestrians whose feet are closer to the horizon in the image are farther away
fromthecamera Figure24.22. Pedestrianswhoarefarther awayfromthecameramustalso
besmallerintheimage. Thismeanswecanruleoutsomedetectorresponsesifadetector
finds a pedestrian who is large in the image and whose feet are close to the horizon it has
found an enormous pedestrian; these dont exist so the detector is wrong. In fact many or
mostimagewindowsarenotacceptable pedestrian windowsandneednotevenbepresented
tothedetector.
There are several strategies for finding the horizon including searching for a roughly
horizontal line with a lot of blue above it and using surface orientation estimates obtained
from texture deformation. A more elegant strategy exploits the reverse of our geometric
constraints. Areasonablyreliablepedestriandetectoris capableofproducingestimatesofthe
horizon if there are several pedestrians in the scene at different distances from the camera.
Thisisbecause therelativescaling ofthepedestrians isacuetowherethehorizon is. Sowe
canextractahorizonestimatefromthedetectorthenusethisestimatetoprunethepedestrian
detectors mistakes.
Iftheobjectisfamiliarwecanestimatemorethanjustthedistancetoitbecausewhatit
lookslikeintheimagedependsverystronglyonitsposei.e.itspositionandorientationwith
respecttotheviewer. Thishasmanyapplications. Forinstanceinanindustrialmanipulation
task the robot arm cannot pick up an object until the pose is known. In the case of rigid
objects whether three-dimensional ortwo-dimensional this problem hasasimple and well-
definedsolution basedonthealignmentmethodwhichwenowdevelop.
A LI GN ME NT ME TH OD
The object is represented by M features or distinguished points m m ...m in
three-dimensional spaceperhaps the vertices of a polyhedral object. These are measured
in some coordinate system that is natural for the object. The points are then subjected to
an unknown three-dimensional rotation R followed by translation by an unknown amount t
and then projection to give rise to image feature points p p ...p on the image plane.
In general N cid:7 M because some model points may be occluded and the feature detector
couldmisssomefeaturesorinventfalseonesduetonoise. Wecanexpressthisas
p Π Rm t Qm
i i i
for a three-dimensional model point m and the corresponding image point p . Here R
i i
is a rotation matrix t is a translation and Π denotes perspective projection or one of its
approximations suchasscaled orthographic projection. Thenetresultisatransformation Q
that will bring the model point m into alignment with the image point p . Although we do
i i
notknow Qinitiallywedoknowforrigidobjectsthat Qmustbethesameforallthemodel
points.
Wecansolvefor Qgiventhethree-dimensional coordinates ofthreemodelpoints and
theirtwo-dimensional projections. Theintuition is asfollows: wecanwrite downequations
relating the coordinates of p to those of m . In these equations the unknown quantities
i i
correspond totheparametersoftherotationmatrix Randthetranslation vectort. Ifwehave
enough equations we ought to be able to solve for Q. We will not give a proof here; we
merelystatethefollowingresult:
Given three noncollinear points m m and m in the model and their scaled
orthographic projections p p and p on the image plane there exist exactly
twotransformationsfromthethree-dimensionalmodelcoordinateframetoatwo-
dimensional imagecoordinate frame.
Thesetransformationsarerelatedbyareflectionaroundthe imageplaneandcanbecomputed
by asimple closed-form solution. If wecould identify the corresponding model features for
threefeatures intheimagewecouldcompute Qtheposeoftheobject.
Letusspecifypositionandorientationinmathematicalterms. Thepositionofapoint P
inthesceneischaracterizedbythreenumbersthe X YZcoordinatesof P inacoordinate
frame with its origin at the pinhole and the Z-axis along the optical axis Figure 24.2 on
image. This specifies the ray from the pinhole along which P lies; what we do not know is
thedistance. Thetermorientation couldbeusedintwosenses:
1. The orientation of the object as a whole. This can be specified in terms of a three-
dimensional rotationrelating itscoordinate frametothat ofthecamera.
Section24.5. Object Recognition from Structural Information 957
2. Theorientation of thesurface oftheobject at P. Thiscan be specified by anormal
vectornwhichisavectorspecifyingthedirectionthatisperpendiculartothesurface.
Often weexpress the surface orientation using the variables slant and tilt. Slant is the
S LA NT
anglebetweenthe Z-axisandn. Tiltistheanglebetweenthe X-axisandtheprojection
T IL T
ofnontheimageplane.
When the camera moves relative to an object both the objects distance and its orientation
change. What is preserved is the shape of the object. If the object is a cube that fact is
S HA PE
notchanged whentheobjectmoves. Geometershavebeenattempting toformalizeshapefor
centuriesthebasicconceptbeingthatshapeiswhatremainsunchangedundersomegroupof
transformationsfor examplecombinations ofrotations andtranslations. Thedifficultylies
infindingarepresentationofglobalshapethatisgeneralenoughtodealwiththewidevariety
ofobjectsintherealworldnotjustsimpleformslikecylindersconesandspheresandyet
canberecovered easily fromthevisual input. Theproblem of characterizing the localshape
of a surface is much better understood. Essentially this can be done in terms of curvature:
howdoesthesurface normalchangeasonemovesindifferentdirections onthesurface? For
a plane there is no change at all. For a cylinder if one moves parallel to the axis there is
no change but in the perpendicular direction the surface normal rotates at a rate inversely
proportional to the radius of the cylinder and so on. All this is studied in the subject called
differential geometry.
Theshape ofanobject isrelevantforsomemanipulation taskse.g.deciding whereto
graspanobject butitsmostsignificant roleisinobjectrecognition wheregeometric shape
alongwithcolorandtextureprovidethemostsignificantcuestoenableustoidentifyobjects
classifywhatisintheimageasanexampleofsomeclassonehasseenbeforeandsoon.
Puttingaboxaroundpedestriansinanimagemaywellbeenoughtoavoiddrivingintothem.
Wehaveseenthatwecanfindaboxbypooling theevidence provided byorientations using
histogram methodstosuppress potentially confusing spatial detail. Ifwewanttoknowmore
aboutwhatsomeoneisdoingwewillneedtoknowwheretheirarmslegsbodyandheadlie
inthepicture. Individual bodyparts arequitedifficulttodetect ontheirownusingamoving
windowmethodbecausetheircolorandtexturecanvarywidelyandbecausetheyareusually
small in images. Often forearms and shins are as small as two to three pixels wide. Body
parts do not usually appear on their own and representing what is connected to what could
bequitepowerfulbecausepartsthatareeasytofindmighttelluswheretolookforpartsthat
aresmallandhardtodetect.
Inferring thelayoutofhumanbodies inpictures isanimportant taskinvision because
the layout of the body often reveals what people are doing. A model called a deformable
D EF OR MA BL E templatecantelluswhichconfigurations areacceptable: theelbowcanbendbuttheheadis
T EM PL AT E
neverjoinedtothefoot. Thesimplestdeformabletemplatemodelofapersonconnectslower
armstoupperarmsupperarmstothetorsoandsoon. Therearerichermodels: forexample
we could represent the fact that left and right upper arms tend to have the same color and
textureasdoleftandrightlegs. Theserichermodelsremaindifficulttoworkwithhowever.
For the moment we assume that we know what the persons body parts look like e.g. we
know the color and texture of the persons clothing. We can model the geometry of the
body asatreeofeleven segments upperandlowerleftandright armsandlegsrespectively
a torso a face and hair on top of the face each of which is rectangular. We assume that
theposition andorientation poseoftheleftlowerarmisindependent ofallothersegments
P OS E
given the pose of the left upper arm; that the pose of the left upper arm is independent of
all segments given the pose of the torso; and extend these assumptions in the obvious way
to include the right arm and the legs the face and the hair. Such models are often called
cardboardpeoplemodels. Themodelformsatreewhichisusuallyrootedatthetorso. We
willsearchtheimageforthebestmatchtothiscardboardpersonusinginferencemethodsfor
atree-structured Bayesnetsee Chapter14.
There are two criteria for evaluating a configuration. First an image rectangle should
looklikeitssegment. Forthemomentwewillremainvagueaboutpreciselywhatthatmeans
butweassumewehaveafunctionφ thatscoreshowwellanimagerectanglematchesabody
i
segment. For each pair of related segments we have another function ψ that scores how
well relations between a pair of image rectangles match those to be expected from the body
segments. The dependencies between segments form a tree so each segment has only one
parent and we could write ψ ipai. All the functions will be larger if the match is better
so we can think of them as being like a log probability. The cost of a particular match that
allocates imagerectangle m tobodysegmentiisthen
cid:12 i cid:12
φ im i ψ ipaim impai.
isegments isegments
Dynamicprogramming canfindthebestmatchbecause therelational modelisatree.
Itisinconvenienttosearchacontinuousspaceandwewilldiscretizethespaceofimage
rectangles. We do so by discretizing the location and orientation of rectangles of fixed size
the sizes may be different for different segments. Because ankles and knees are different
""
we need to distinguish between a rectangle and the same rectangle rotated by 180 . One
couldvisualize theresultasasetofverylargestacks ofsmallrectangles ofimage cutoutat
different locations and orientations. There is one stack per segment. We must now find the
best allocation of rectangles to segments. This will be slow because there are many image
rectangles andforthemodelwehavegivenchoosing therighttorsowillbe O M6ifthere
are M image rectangles. However various speedups are available foran appropriate choice
of ψ and the method is practical Figure 24.23. Themodel is usually known asa pictorial
P IC TO RI AL structuremodel.
S TR UC TU RE MO DE L
Recallourassumptionthatweknowwhatweneedtoknowaboutwhatthepersonlooks
like. If weare matching a person in a single image the most useful feature forscoring seg-
ment matches turns out tobe color. Texture features dont work wellin mostcases because
foldsonlooseclothingproducestrongshadingpatternsthatoverlaytheimagetexture. These
Section24.5. Object Recognition from Structural Information 959
Figure24.23 Apictorialstructuremodelevaluatesamatchbetweenasetofimagerect-
angles and a cardboard person shown on the left by scoring the similarity in appearance
betweenbodysegmentsandimagesegmentsandthespatialrelationsbetweentheimageseg-
ments.Generallyamatchisbetteriftheimagesegmentshaveabouttherightappearanceand
areinabouttherightplacewithrespecttooneanother. Theappearancemodelusesaverage
colors forhair head torso and upperand lower arms and legs. The relevantrelations are
shownasarrows.Ontherightthebestmatchforaparticularimageobtainedusingdynamic
programming. The match is a fair estimate of the configuration of the body. Figure from
Felzenszwalband Huttenlocher2000cid:2c I EE E.
patternsarestrongenoughtodisruptthetruetextureofthe cloth. Incurrentworkψtypically
reflects the need for the ends of the segments to be reasonably close together but there are
usually no constraints on the angles. Generally we dont know what a person looks like
and must build a model of segment appearances. We call the description of what a person
A PP EA RA NC E looksliketheappearancemodel. Ifwemustreporttheconfiguration ofapersoninasingle
M OD EL
image wecan start with apoorly tuned appearance model estimate configuration with this
then re-estimate appearance and so on. In video wehave many frames of the same person
andthiswillrevealtheirappearance.
Tracking people in video is an important practical problem. If we could reliably report the
location of arms legs torso and head in video sequences we could build much improved
game interfaces and surveillance systems. Filtering methods have not had much success
withthisproblem because people canproduce large accelerations andmovequite fast. This
means that for 30 Hz video the configuration of the body in frame i doesnt constrain the
configurationofthebodyinframei1allthatstrongly. Currentlythemosteffectivemethods
exploit thefactthatappearance changes veryslowlyfromframetoframe. Ifwecaninferan
appearance model of an individual from the video then we can use this information in a
pictorial structure model to detect that person in each frame of the video. We can then link
theselocations acrosstimetomakeatrack.
torso
arm
Lateral walking Appearance Body part Detected figure
detector model maps
motion blur
interlacing
taininganappearancemodelthenapplyingit. Toobtaintheappearancemodelwescanthe
image to find a lateral walking pose. The detector does not need to be very accurate but
shouldproducefewfalsepositives. Fromthedetectorresponsewe canreadoffpixelsthat
lieoneachbodysegmentandothersthatdonotlieonthatsegment.Thismakesitpossibleto
buildadiscriminativemodeloftheappearanceofeachbodypartandthesearetiedtogether
intoapictorialstructuremodelofthepersonbeingtracked. Finallywecanreliablytrackby
detectingthismodelineachframe.Astheframesinthelowerpartoftheimagesuggestthis
procedurecantrackcomplicatedfast-changingbodyconfigurationsdespitedegradationof
thevideosignalduetomotionblur. Figurefrom Ramananetal.2007cid:2c I EE E.
There are several ways to infer a good appearance model. We regard the video as a
large stack of pictures of the person we wish to track. We can exploit this stack by looking
for appearance models that explain many of the pictures. This would work by detecting
bodysegmentsineachframeusingthefactthatsegmentshaveroughly paralleledges. Such
detectors are not particularly reliable but the segments we want to find are special. They
will appear at least once in most of the frames of video; such segments can be found by
clustering the detector responses. It is best to start with the torso because it is big and
because torso detectors tend to be reliable. Once we have a torso appearance model upper
leg segments should appear near the torso and so on. This reasoning yields an appearance
model but it can be unreliable if people appear against a near-fixed background where the
segment detector generates lots of false positives. An alternative is to estimate appearance
formanyoftheframesofvideobyrepeatedlyreestimatingconfigurationandappearance; we
then see if one appearance model explains manyframes. Another alternative which isquite
Section24.6. Using Vision 961
andmotion.Forexampledrinkinginvolvesmovementsofthehandinfrontoftheface. The
firstthreeimagesarecorrectdetectionsofdrinking;thefourthisafalse-positivethecookis
lookingintothecoffeepotbutnotdrinkingfromit. Figurefrom Laptevand Perez2007
cid:2c I EE E.
reliableinpracticeistoapplyadetectorforafixedbodyconfigurationtoalloftheframes. A
goodchoiceofconfiguration isonethatiseasytodetectreliably andwherethereisastrong
chance theperson willappearinthat configuration even inashort sequence lateral walking
isagoodchoice. Wetunethedetector tohavealowfalsepositive ratesoweknowwhenit
responds that we have found a real person; and because wehave localized their torso arms
legsandheadweknowwhatthesesegmentslooklike.
If vision systems could analyze video and understood what people are doing we would be
able to: design buildings and public places better by collecting and using data about what
peopledoinpublic;buildmoreaccuratemoresecureandlessintrusivesurveillancesystems;
buildcomputersportscommentators;andbuildhuman-computerinterfacesthatwatchpeople
and react to their behavior. Applications for reactive interfaces range from computer games
thatmakeaplayergetupandmovearoundtosystemsthatsaveenergybymanagingheatand
lightinabuilding tomatchwheretheoccupants areandwhattheyaredoing.
Some problems are well understood. If people are relatively small in the video frame
andthebackgroundisstableitiseasytodetectthepeoplebysubtractingabackgroundimage
from the current frame. If the absolute value of the difference is large this background
B AC KG RO UN D subtraction declares the pixel to be a foreground pixel; by linking foreground blobs over
S UB TR AC TI ON
timeweobtainatrack.
Structuredbehaviors likeballetgymnastics ortaichihavespecificvocabularies ofac-
tions. Whenperformed againstasimplebackground videos oftheseactionsareeasytodeal
with. Background subtraction identifies the major moving regions and we can build H OG
featureskeepingtrackofflowratherthanorientationtopresenttoaclassifier. Wecandetect
consistent patterns of action with a variant of our pedestrian detector where the orientation
featuresarecollected intohistogram bucketsovertimeaswellasspace Figure24.25.
Moregeneral problems remain open. Thebig research question isto link observations
of the body and the objects nearby to the goals and intentions of the moving people. One
source ofdifficulty isthat welack asimple vocabulary ofhuman behavior. Behavior isalot
like color in that people tend to think they know a lot of behavior names but cant produce
longlistsofsuchwordsondemand. Thereisquitealotofevidencethatbehaviorscombine
you can for example drink a milkshake while visiting an A TMbut we dont yet know
what the pieces are how the composition works or how many composites there might be.
Asecond source ofdifficulty isthatwedont know whatfeatures expose whatishappening.
Forexampleknowingsomeoneisclosetoan A TMmaybeenoughtotellthattheyrevisiting
the A TM.Athirddifficultyisthattheusualreasoningabouttherelationshipbetweentraining
and test data is untrustworthy. For example we cannot argue that a pedestrian detector is
safesimply because itperforms wellonalarge dataset because thatdatasetmaywellomit
important butrarephenomena forexample people mounting bicycles. Wewouldnt want
ourautomateddrivertorunoverapedestrian whohappened to dosomething unusual.
Many Web sites offer collections of images for viewing. How can we find the images we
want? Letssuppose theuserenters atextquery suchasbicycle race. Someoftheimages
willhavekeywordsorcaptionsattached orwillcomefrom Webpagesthatcontain textnear
the image. Forthese image retrieval can be like text retrieval: ignore the images and match
theimagestextagainstthequerysee Section22.3onpage867.
However keywords are usually incomplete. Forexample a picture of a cat playing in
thestreetmightbetaggedwithwordslikecatandstreetbutitiseasytoforgettomention
thegarbagecanorthefishbones. Thusaninterestingtaskistoannotateanimagewhich
mayalready haveafewkeywordswithadditional appropriate keywords.
In the most straightforward version of this task we have a set of correctly tagged ex-
ample images and we wish to tag some test images. This problem is sometimes known as
auto-annotation. Themostaccurate solutions areobtained using nearest-neighbors methods.
One finds the training images that are closest tothe test image in a feature space metric that
istrained usingexamples thenreportstheirtags.
Another version of the problem involves predicting which tags to attach to which re-
gionsinatestimage. Herewedonotknowwhichregionsproducedwhichtagsforthetrain-
ingdata. Wecanuseaversionofexpectationmaximizationtoguessaninitialcorrespondence
between textandregions andfrom thatestimate abetterdecomposition into regions andso
on.
Binocular stereopsis works because for each point we have four measurements constraining
three unknown degrees of freedom. The four measurements are the xy positions of the
pointineachviewandtheunknowndegreesoffreedomarethexyzcoordinatevaluesof
thepointinthescene. Thisrathercrudeargumentsuggests correctlythattherearegeometric
constraints thatpreventmostpairsofpointsfrombeingacceptablematches. Manyimagesof
asetofpointsshouldrevealtheirpositions unambiguously.
We dont always need a second picture to get a second view of a set of points. If we
believe the original set of points comes from a familiar rigid 3 D object then wemight have
Section24.6. Using Vision 963
anobject modelavailable asasource ofinformation. Ifthis objectmodelconsists ofasetof
3 Dpointsorofasetofpictures oftheobject andifwecanestablish pointcorrespondences
wecandeterminetheparametersofthecamerathatproducedthepointsintheoriginalimage.
This is very powerful information. We could use it to evaluate our original hypothesis that
the points come from an object model. We do this by using some points to determine the
parameters of the camera then projecting model points in this camera and checking to see
whetherthereareimagepointsnearby.
Wehavesketchedhereatechnologythatisnowveryhighlydeveloped. Thetechnology
can be generalized to deal with views that are not orthographic; to deal with points that are
observed in only some views; to deal with unknown camera properties like focal length; to
exploitvarioussophisticated searchesforappropriatecorrespondences; andtodoreconstruc-
tion from verylarge numbers ofpoints andofviews. Ifthelocations ofpoints intheimages
areknownwithsomeaccuracyandtheviewingdirectionsarereasonable veryhighaccuracy
cameraandpointinformation canbeobtained. Someapplications are
Model-building: For example one might build a modeling system that takes a video
sequence depicting an object and produces a very detailed three-dimensional mesh of
texturedpolygonsforuseincomputergraphicsandvirtualrealityapplications. Models
like this can now be built from apparently quite unpromising sets of pictures. For ex-
ample Figure 24.26 shows a model of the Statue of Liberty built from pictures found
onthe Internet.
Matching moves: To place computer graphics characters into real video we need to
know how the camera moved for the real video so that we can render the character
correctly.
Path reconstruction: Mobile robots need to know where they have been. If they are
moving in a world of rigid objects then performing a reconstruction and keeping the
camerainformation isonewaytoobtainapath.
Oneoftheprincipal usesofvisionistoprovideinformation bothformanipulating objects
pickingthemupgraspingthemtwirlingthemandsoonandfornavigatingwhileavoiding
obstacles. The ability to use vision for these purposes is present in the most primitive of
animal visual systems. In many cases the visual system is minimal in the sense that it
extracts from the available light field just the information the animal needs to inform its
behavior. Quite probably modern vision systems evolved from early primitive organisms
that used a photosensitive spot at one end to orient themselves toward or away from the
light. We saw in Section 24.4 that flies use a very simple optical flow detection system to
land on walls. A classic study What the Frogs Eye Tells the Frogs Brain Lettvin et al.
1959observesofafrogthat Hewillstarvetodeathsurroundedbyfoodifitisnotmoving.
Hischoiceoffoodisdetermined onlybysizeandmovement.
Let us consider a vision system for an automated vehicle driving on a freeway. The
tasksfacedbythedriverincludethefollowing:
a b c
a b c
Figure24.26 Thestateoftheartinmultiple-viewreconstructionisnowhighlyadvanced.
This figureoutlines a system builtby Michael Goesele and colleaguesfrom the University
of Washington T U Darmstadt and Microsoft Research. Froma collectionof picturesof a
monumenttakenbyalargecommunityofusersandpostedonthe Internetatheirsystem
candeterminetheviewingdirectionsforthosepicturesshownbythesmallblackpyramids
inbandacomprehensive3 Dreconstructionshowninc.
1. Lateral controlensure that the vehicle remains securely within its lane or changes
lanessmoothlywhenrequired.
2. Longitudinal controlensure thatthereisasafedistancetothevehicleinfront.
3. Obstacleavoidancemonitorvehiclesinneighboringlanesandbepreparedforevasive
maneuversifoneofthemdecidestochangelanes.
The problem for the driver is to generate appropriate steering acceleration and braking ac-
tionstobestaccomplish thesetasks.
Forlateralcontroloneneedstomaintainarepresentation ofthepositionandorientation
ofthecarrelativetothelane. Wecanuseedge-detectionalgorithmstofindedgescorrespond-
ingtothelane-marker segments. Wecanthen fitsmooth curves tothese edgeelements. The
parameters of these curves carry information about the lateral position of the car the direc-
tion it is pointing relative to the lane and the curvature of the lane. This information along
with information about the dynamics of the car is all that is needed by the steering-control
system. Ifwehave good detailed mapsofthe road then the vision system serves toconfirm
ourposition andtowatchforobstacles thatarenotonthemap.
Forlongitudinal control oneneeds toknowdistances tothe vehicles infront. Thiscan
be accomplished with binocular stereopsis or optical flow. Using these techniques vision-
controlled carscannowdrivereliably athighwayspeeds.
Themoregeneralcaseofmobilerobotsnavigating invarious indoorandoutdoorenvi-
ronments has been studied too. One particular problem localizing the robot in its environ-
ment now has pretty good solutions. A group at Sarnoff has developed a system based on
two cameras looking forward that track feature points in 3 D and use that to reconstruct the
25
R OB OT IC S
Inwhichagentsareendowedwithphysicaleffectors withwhichtodomischief.
Robotsarephysical agentsthatperformtasksbymanipulating thephysicalworld. Todoso
R OB OT
they are equipped with effectors such as legs wheels joints and grippers. Effectors have
E FF EC TO R
a single purpose: to assert physical forces on the environment.1 Robots are also equipped
with sensors which allow them to perceive their environment. Present day robotics em-
S EN SO R
ploys adiverse setofsensors including camerasandlasers tomeasure theenvironment and
gyroscopes andaccelerometers tomeasuretherobotsownmotion.
Mostoftodaysrobotsfallintooneofthreeprimarycategories. Manipulatorsorrobot
M AN IP UL AT OR
arms Figure 25.1a are physically anchored to their workplace for example in a factory
assembly line or on the International Space Station. Manipulator motion usually involves
a chain of controllable joints enabling such robots to place their effectors in any position
within the workplace. Manipulators are by far the most common type of industrial robots
with approximately one million units installed worldwide. Some mobile manipulators are
used in hospitals to assist surgeons. Few car manufacturers could survive without robotic
manipulators andsomemanipulators haveevenbeenusedtogenerate original artwork.
Thesecondcategoryisthemobilerobot. Mobilerobotsmoveabouttheirenvironment
M OB IL ER OB OT
using wheels legs or similar mechanisms. They have been put to use delivering food in
hospitals moving containers at loading docks and similar tasks. Unmanned ground vehi-
clesor U GVsdriveautonomously onstreets highways andoff-road. Theplanetaryrover
U GV
shownin Figure25.2bexplored Marsforaperiodof3months in1997. Subsequent N AS A
P LA NE TA RY RO VE R
robots include thetwin Mars Exploration Roversoneisdepicted onthecoverofthisbook
which landed in 2003 and were still operating six years later. Other types of mobile robots
includeunmannedairvehicles U AVscommonlyusedforsurveillancecrop-spraying and
U AV
fromtheactuatorthecontrollinethatcommunicatesacommandtotheeffector.
971
a b
courtesyof Nachi Robotic Systems. b Hondas P3and Asimohumanoidrobots.
a b
Imagecourtesyof General Atomics Aeronautical Systems. b N AS As Sojourneramobile
robotthatexploredthesurfaceof Marsin July1997.
military operations. Figure 25.2a shows a U AV commonly used by the U.S. military. Au-
tonomous underwater vehicles A UVs are used in deep sea exploration. Mobile robots
A UV
deliverpackagesintheworkplaceandvacuumthefloorsathome.
The third type of robot combines mobility with manipulation and is often called a
M OB IL E mobilemanipulator. Humanoidrobotsmimicthehuman torso. Figure25.1b showstwo
M AN IP UL AT OR
early humanoid robots both manufactured by Honda Corp. in Japan. Mobile manipulators
H UM AN OI DR OB OT
Section25.2. Robot Hardware 973
canapplytheireffectorsfurtherafieldthananchoredmanipulators canbuttheirtaskismade
harderbecausetheydonthavetherigidity thattheanchorprovides.
The field of robotics also includes prosthetic devices artificial limbs ears and eyes
forhumans intelligent environments such as an entire house that is equipped withsensors
andeffectors andmultibodysystemswhereinroboticactionisachievedthroughswarmsof
smallcooperating robots.
Real robots must cope with environments that are partially observable stochastic dy-
namic and continuous. Many robot environments are sequential and multiagent as well.
Partial observability and stochasticity are the result of dealing with a large complex world.
Robot cameras cannot see around corners and motion commands are subject to uncertainty
due to gears slipping friction etc. Also the real world stubbornly refuses to operate faster
thanrealtime. Inasimulatedenvironmentitispossibletousesimplealgorithmssuchasthe
Q-learning algorithm described in Chapter21 tolearn inafew C PUhours from millions of
trials. Inarealenvironment itmighttakeyearstorunthesetrials. Furthermore realcrashes
reallyhurtunlikesimulatedones. Practicalroboticsystemsneedtoembodypriorknowledge
abouttherobotitsphysicalenvironment andthetasksthattherobotwillperformsothatthe
robotcanlearnquicklyandperform safely.
Robotics brings together many of the concepts we have seen earlier in the book in-
cluding probabilistic state estimation perception planning unsupervised learning and re-
inforcement learning. For some of these concepts robotics serves as a challenging example
application. Forotherconceptsthischapterbreaksnewgroundinintroducing thecontinuous
versionoftechniques thatwepreviously sawonlyinthediscrete case.
Sofarinthisbookwehavetakentheagentarchitecturesensors effectorsandprocessors
asgivenandwehaveconcentratedontheagentprogram. Thesuccessofrealrobotsdepends
atleastasmuchonthedesignofsensorsandeffectors thatareappropriate forthetask.
Sensors are the perceptual interface between robot and environment. Passive sensors such
P AS SI VE SE NS OR
ascameras aretrueobservers oftheenvironment: theycapture signals thataregenerated by
other sources in the environment. Active sensors such as sonar send energy into the envi-
A CT IV ES EN SO R
ronment. Theyrelyonthefactthatthisenergy isreflectedbacktothesensor. Activesensors
tendtoprovidemoreinformationthanpassivesensorsbutattheexpenseofincreasedpower
consumption and with a danger of interference when multiple active sensors are used at the
sametime. Whetheractive orpassive sensors canbe divided into three types depending on
whethertheysensetheenvironmenttherobotslocationortherobotsinternalconfiguration.
Range finders are sensors that measure the distance to nearby objects. In the early
R AN GE FI ND ER
days of robotics robots were commonly equipped with sonar sensors. Sonar sensors emit
S ON AR SE NS OR S
directional sound waves which are reflected by objects with some of the sound making it
a b
Figure25.3 a Time offlightcamera; image courtesyof Mesa Imaging Gmb H. b3 D
rangeimageobtainedwiththiscamera.Therangeimagemakesitpossibletodetectobstacles
andobjectsinarobotsvicinity.
back into the sensor. The time and intensity of the returning signal indicates the distance
to nearby objects. Sonar is the technology of choice for autonomous underwater vehicles.
Stereovisionsee Section24.4.2reliesonmultiplecamerastoimagethe environment from
S TE RE OV IS IO N
slightly different viewpoints analyzing theresulting parallax intheseimagestocomputethe
range of surrounding objects. For mobile ground robots sonar and stereo vision are now
rarelyusedbecausetheyarenotreliablyaccurate.
Mostgroundrobotsarenowequippedwithopticalrangefinders. Justlikesonarsensors
opticalrangesensorsemitactivesignalslightandmeasurethetimeuntilareflectionofthis
T IM EO FF LI GH T signalarrivesbackatthesensor. Figure25.3ashowsa timeofflightcamera. Thiscamera
C AM ER A
acquires range images like the one shown in Figure 25.3b at up to 60 frames per second.
Other range sensors use laser beams and special 1-pixel cameras that can be directed using
complex arrangements of mirrors or rotating elements. These sensors are called scanning
lidars short for light detection and ranging. Scanning lidars tend to provide longer ranges
S CA NN IN GL ID AR S
thantimeofflightcamerasandtendtoperformbetterinbrightdaylight.
Other common range sensors include radar which is often the sensor of choice for
U AVs. Radar sensors can measure distances of multiple kilometers. On the other extreme
end of range sensing are tactile sensors such as whiskers bump panels and touch-sensitive
T AC TI LE SE NS OR S
skin. These sensors measure range based on physical contact and can be deployed only for
sensingobjects veryclosetotherobot.
A second important class of sensors is location sensors. Most location sensors use
L OC AT IO NS EN SO RS
rangesensingasaprimarycomponenttodeterminelocation. Outdoorsthe Global Position-
G LO BA L
ing System G PSis the most common solution to the localization problem. G PS measures
P OS IT IO NI NG
S YS TE M
the distance to satellites that emit pulsed signals. At present there are 31 satellites in orbit
transmitting signalsonmultiple frequencies. G PSreceiverscanrecoverthedistance tothese
satellites by analyzing phase shifts. By triangulating signals from multiple satellites G PS
Section25.2. Robot Hardware 975
receivers candetermine theirabsolute location on Earthto withinafewmeters. Differential
G PSinvolves asecond ground receiver with known location providing millimeteraccuracy
D IF FE RE NT IA LG PS
under ideal conditions. Unfortunately G PS does not work indoors or underwater. Indoors
localization is often achieved by attaching beacons in the environment at known locations.
Many indoor environments are full of wireless base stations which can help robots localize
through the analysis of the wireless signal. Underwater active sonar beacons can provide a
senseoflocation usingsoundtoinform A UVsoftheirrelativedistances tothosebeacons.
P RO PR IO CE PT IV E Thethirdimportantclassisproprioceptive sensorswhichinformtherobotofitsown
S EN SO R
motion. To measure the exact configuration of a robotic joint motors are often equipped
withshaftdecodersthatcounttherevolution ofmotorsinsmallincrements. Onrobotarms
S HA FT DE CO DE R
shaft decoders can provide accurate information overany period of time. Onmobile robots
shaftdecoders thatreportwheelrevolutions canbeusedfor odometrythemeasurementof
O DO ME TR Y
distance traveled. Unfortunately wheels tend to drift and slip so odometry is accurate only
over short distances. External forces such as the current for A UVs and the wind for U AVs
increase positional uncertainty. Inertial sensors such as gyroscopes rely on the resistance
I NE RT IA LS EN SO R
ofmasstothechangeofvelocity. Theycanhelpreduceuncertainty.
Otherimportant aspects of robot state are measured by force sensors and torque sen-
F OR CE SE NS OR
sors. Theseareindispensablewhenrobotshandlefragileobjectsorobjectswhoseexactshape
T OR QU ES EN SO R
and location is unknown. Imagine aone-ton robotic manipulator screwing in alight bulb. It
would be all too easy to apply too much force and break the bulb. Force sensors allow the
robottosensehowharditisgripping thebulb andtorquesensors allowittosensehowhard
it is turning. Good sensors can measure forces in all three translational and three rotational
directions. Theydothisatafrequency ofseveralhundred timesasecond sothatarobotcan
quicklydetectunexpected forcesandcorrect itsactionsbeforeitbreaksalightbulb.
Effectors are the means by which robots move and change the shape of their bodies. To
understand thedesign ofeffectors itwillhelptotalkaboutmotionandshapeintheabstract
D EG RE EO F using the concept of a degree of freedom D OF We count one degree of freedom for each
F RE ED OM
independentdirectioninwhicharobotoroneofitseffectorscanmove. Forexamplearigid
mobile robot such as an A UV has six degrees of freedom three for its xyz location in
space and three for its angular orientation known as yaw roll and pitch. These six degrees
definethekinematicstate2 orposeoftherobot. Thedynamicstateofarobotincludesthese
K IN EM AT IC ST AT E
sixplusanadditionalsixdimensionsfortherateofchangeofeachkinematicdimensionthat
P OS E
istheirvelocities.
D YN AM IC ST AT E
Fornonrigidbodiesthereareadditional degreesoffreedomwithintherobotitself. For
example the elbow of a human arm possesses two degree of freedom. It can flex the upper
arm towards oraway and can rotate right orleft. Thewrist has three degrees offreedom. It
can move up and down side to side and can also rotate. Robot joints also have one two
or three degrees of freedom each. Six degrees of freedom are required to place an object
such as a hand at a particular point in a particular orientation. The arm in Figure 25.4a
has exactly six degrees of freedom created by five revolute joints that generate rotational
R EV OL UT EJ OI NT
motionandoneprismaticjointthatgeneratesslidingmotion. Youcanverifythatthehuman
P RI SM AT IC JO IN T
armasawholehasmorethansixdegreesoffreedom byasimpleexperiment: putyourhand
onthetableandnoticethatyoustillhavethefreedom torotateyourelbowwithoutchanging
theconfigurationofyourhand. Manipulators thathaveextra degreesoffreedomareeasierto
control than robots with only the minimum number of D OFs. Many industrial manipulators
therefore haveseven D OFsnotsix.
P
R R
R R
θ
R
x y
a b
Figure25.4 a The Stanford Manipulatoranearlyrobotarmwithfiverevolutejoints R
andoneprismaticjoint Pforatotalofsix degreesoffreedom. b Motionofa nonholo-
nomicfour-wheeledvehiclewithfront-wheelsteering.
Formobilerobotsthe D OFsarenotnecessarilythesameasthenumberofactuatedele-
ments. Considerforexampleyouraveragecar: itcanmoveforwardorbackwardanditcan
turn giving it two D OFs. In contrast a cars kinematic configuration is three-dimensional:
onanopenflatsurface onecaneasily maneuveracartoany xypoint inanyorientation.
See Figure 25.4b. Thus the carhasthree effective degrees of freedom but twocontrol-
E FF EC TI VE DO F
lable degrees of freedom. We say a robot is nonholonomic if it has more effective D OFs
C ON TR OL LA BL ED OF
than controllable D OFsand holonomic if the two numbers are the same. Holonomic robots
N ON HO LO NO MI C
areeasiertocontrolitwouldbemucheasiertoparkacarthatcouldmovesidewaysaswell
asforwardandbackwardbut holonomicrobotsarealsomechanically morecomplex. Most
robotarmsareholonomic andmostmobilerobotsarenonholonomic.
Mobile robots have a range of mechanisms for locomotion including wheels tracks
and legs. Differential drive robots possess two independently actuated wheels or tracks
D IF FE RE NT IA LD RI VE
one on each side as on a military tank. If both wheels move at the same velocity the robot
movesonastraight line. Iftheymoveinopposite directions therobot turnsonthespot. An
alternativeisthesynchrodriveinwhicheachwheelcanmoveandturnarounditsownaxis.
S YN CH RO DR IV E
To avoid chaos the wheels are tightly coordinated. When moving straight for example all
wheelspointinthesamedirectionandmoveatthesamespeed. Bothdifferentialandsynchro
drives are nonholonomic. Some more expensive robots use holonomic drives which have
threeormorewheelsthatcanbeoriented andmovedindependently.
Some mobile robots possess arms. Figure 25.5a displays a two-armed robot. This
robots arms use springs to compensate for gravity and they provide minimal resistance to
Section25.2. Robot Hardware 977
a b
courtesyof Willow Garagecid:2c 2009.b Oneof Marc Raibertsleggedrobotsinmotion.
external forces. Such a design minimizes the physical danger to people who might stumble
intosucharobot. Thisisakeyconsideration indeploying robotsindomesticenvironments.
Legs unlike wheels can handle rough terrain. However legs are notoriously slow on
flatsurfaces andtheyaremechanically difficulttobuild. Roboticsresearchers havetriedde-
signsrangingfromoneleguptodozensoflegs. Leggedrobots havebeenmadetowalkrun
andevenhopasweseewiththeleggedrobotin Figure25.5b. Thisrobotisdynamically
D YN AM IC AL LY stable meaning that it can remain upright while hopping around. A robot that can remain
S TA BL E
upright without moving its legs is called statically stable. A robot is statically stable if its
S TA TI CA LL YS TA BL E
centerofgravityisabovethepolygonspannedbyitslegs. Thequadrupedfour-leggedrobot
shown in Figure 25.6a may appear statically stable. However it walks by lifting multiple
legs atthesame time which renders itdynamically stable. Therobot can walkon snow and
ice and it will not fall over even if you kick it as demonstrated in videos available online.
Two-leggedrobotssuchasthosein Figure25.6baredynamically stable.
Other methods of movement are possible: air vehicles use propellers or turbines; un-
derwater vehicles use propellers or thrusters similar to those used on submarines. Robotic
blimpsrelyonthermaleffectstokeepthemselvesaloft.
Sensorsandeffectorsalonedonotmakearobot. Acomplete robotalsoneedsasource
of power to drive its effectors. The electric motor is the most popular mechanism for both
E LE CT RI CM OT OR
P NE UM AT IC manipulator actuation and locomotion but pneumatic actuation using compressed gas and
A CT UA TI ON
H YD RA UL IC hydraulicactuation usingpressurized fluidsalsohavetheirapplication niches.
A CT UA TI ON
a b
Figure25.6 a Four-leggeddynamically-stablerobot Big Dog. Imagecourtesy Boston
Dynamicscid:2c 2009.b2009 Robo Cup Standard Platform Leaguecompetitionshowingthe
winningteam B-Humanfromthe D FK Icenteratthe Universityof Bremen.Throughoutthe
match B-Human outscored their opponents64:1. Their success was built on probabilistic
stateestimationusingparticlefiltersand Kalmanfilters;onmachine-learningmodelsforgait
optimization;andondynamickickingmoves.Imagecourtesy D FK Icid:2c 2009.
Perceptionistheprocessbywhichrobotsmapsensormeasurementsintointernalrepresenta-
tions of the environment. Perception is difficult because sensors are noisy and the environ-
ment is partially observable unpredictable and often dynamic. In other words robots have
all the problems of state estimation or filtering that we discussed in Section 15.2. As a
rule of thumb good internal representations for robots have three properties: they contain
enoughinformationfortherobottomakegooddecisions theyarestructuredsothattheycan
be updated efficiently and they are natural in the sense that internal variables correspond to
naturalstatevariablesinthephysical world.
In Chapter15 wesawthat Kalmanfilters H MMsanddynamic Bayesnets canrepre-
sentthetransitionandsensormodelsofapartiallyobservableenvironment andwedescribed
bothexactandapproximate algorithms forupdating the beliefstatethe posteriorprobabil-
ity distribution over the environment state variables. Several dynamic Bayes net models for
this process were shown in Chapter 15. For robotics problems we include the robots own
past actions as observed variables in the model. Figure 25.7 shows the notation used in this
chapter: X isthestateoftheenvironmentincludingtherobotattimet Z istheobservation
t t
receivedattimetand A istheactiontakenaftertheobservation isreceived.
t
Section25.3. Robotic Perception 979
A t2 A t1 A t
X t1 X t X t1
Z t1 Z t Z t1
actionsandmeasurementsasillustratedbythisdynamic Bayesnetwork.
Wewould liketocompute thenew belief state P X z a fromthecurrent
t1 1:t1 1:t
belief state P X
t
z 1:ta 1:t1 and the new observation z t1. We did this in Section 15.2
but here there are two differences: we condition explicitly on the actions as well as the ob-
servations and wedeal with continuous rather than discrete variables. Thus we modify the
recursivefilteringequation 15.5onpage572touseintegration ratherthansummation:
P X z a
t1 1:t1 1:t cid:26
α Pz
t1
X t1 P X
t1
x ta t Px
t
z 1:ta 1:t1 dx
t
. 25.1
This equation states that the posterior over the state variables X at time t 1 is calculated
recursively from the corresponding estimate one time step earlier. This calculation involves
the previous action a and the current sensor measurement z . For example if our goal
t t1
is to develop a soccer-playing robot X might be the location of the soccer ball relative
t1
to the robot. The posterior P X tz 1:ta 1:t1 isa probability distribution overall states that
captureswhatweknowfrompastsensormeasurementsandcontrols. Equation25.1tellsus
how to recursively estimate this location by incrementally folding in sensor measurements
e.g.cameraimagesandrobotmotioncommands. Theprobability P X x a iscalled
t1 t t
thetransitionmodelormotionmodeland Pz X isthesensormodel.
M OT IO NM OD EL t1 t1
Localization is the problem of finding out where things areincluding the robot itself.
L OC AL IZ AT IO N
Knowledge about where things are is at the core of any successful physical interaction with
the environment. For example robot manipulators must know the location of objects they
seektomanipulate; navigating robotsmustknowwheretheyaretofindtheirwayaround.
To keep things simple let us consider a mobile robot that moves slowly in a flat 2 D
world. Letusalso assume therobot isgiven anexact mapofthe environment. Anexample
of such a map appears in Figure 25.10. The pose of such a mobile robot is defined by its
two Cartesian coordinates with values x and y and its heading with value θ as illustrated in
Figure25.8a. Ifwearrange those threevalues inavector thenanyparticular stateisgiven
cid:12
by X x y θ . Sofarsogood.
t t t t
ω Δt
t
x y
i i
θ
t1
hx v Δt
t t
Z Z Z Z
x 1 2 3 4
t1
θ
t
x
t
a b
Figure25.8 a Asimplifiedkinematicmodelofamobilerobot. Therobotisshownasa
circlewithaninteriorlinemarkingtheforwarddirection. Thestatextconsistsofthextyt
position shown implicitly and the orientation θt. The new state xt1 is obtained by an
updateinpositionofvtΔt andinorientationofωtΔt. Alsoshownisalandmarkatxiyi
observedattimet. b Therange-scansensormodel.Twopossiblerobotposesareshownfor
agivenrangescanz z z z . Itismuchmorelikelythattheposeontheleftgenerated
therangescanthantheposeontheright.
In the kinematic approximation each action consists of the instantaneous specifica-
tionoftwovelocitiesa translational velocity v andarotationalvelocity ω . Forsmalltime
t t
intervals Δtacrudedeterministic modelofthemotionofsuchrobotsisgivenby
""
v Δtcosθ
t t
Xˆ f X v ω X v Δtsinθ .
t1 t cid:27tcid:28cid:29cid:30t t t t
ω Δt
at t
The notation
Xˆ
refers to a deterministic state prediction. Of course physical robots are
somewhat unpredictable. This is commonly modeled by a Gaussian distribution with mean
f X v ω andcovariance Σ . See Appendix Aforamathematicaldefinition.
t t t x
P X X v ω N Xˆ Σ .
t1 t t t t1 x
Thisprobability distribution istherobots motionmodel. Itmodels theeffects ofthemotion
a onthelocation oftherobot.
t
Next we need a sensor model. We will consider two kinds of sensor model. The
first assumes that the sensors detect stable recognizable features of the environment called
landmarks. Foreachlandmarktherangeandbearingarereported. Supposetherobotsstate
L AN DM AR K
cid:12 cid:12
isx x y θ anditsensesalandmarkwhoselocationisknowntobex y . Without
t t t t i i
noisetherangeandbearingcanbecalculatedbysimplegeometry. See Figure25.8a. The
exactprediction oftheobservedrangeandbearingwouldbe
cid:13 cid:10 cid:14
x x 2y y 2
ˆz t hx t at rctani yiyt t θ i .
xixt t
Section25.3. Robotic Perception 981
Again noise distorts ourmeasurements. Tokeep things simple onemight assume Gaussian
noisewithcovariance Σ givingusthesensormodel
z
Pz x Nˆz Σ .
t t t z
A somewhat different sensor model is used for an array of range sensors each of which
has a fixed bearing relative to the robot. Such sensors produce a vector of range values
cid:12
z z ...z . Given apose x let zˆ be the exact range along the jth beam direction
t 1 M t j
fromx tothenearestobstacle. Asbeforethiswillbecorruptedby Gaussiannoise. Typically
t
we assume that the errors for the different beam directions are independent and identically
distributed sowehave
cid:25 M
Pz x α ezjzˆj2σ2 .
t t
j1
oneofwhichisreasonablylikelytohaveproducedtheobservedscanandoneofwhichisnot.
Comparing the range-scan model to the landmark model we see that the range-scan model
has the advantage that there is no need to identify a landmark before the range scan can be
interpreted; indeed in Figure 25.8b the robot faces a featureless wall. On the other hand
ifthere arevisible identifiable landmarks theymayprovideinstant localization.
multivariate Gaussian andtheparticle filterwhichrepresents thebeliefstatebyacollection
of particles that correspond to states. Most modern localization algorithms use one of two
representations oftherobots belief P X
t
z 1:ta 1:t1.
M ON TE CA RL O Localization using particle filtering is called Monte Carlo localization or M CL. The
L OC AL IZ AT IO N
M CLalfgorithm isaninstance oftheparticle-filtering algorithm of Figure15.17page598.
All we need to do is supply the appropriate motion model and sensor model. Figure 25.9
showsoneversionusingtherange-scanmodel. Theoperationofthealgorithmisillustratedin
Figure25.10astherobotfindsoutwhereitisinsideanofficebuilding. Inthefirstimagethe
particles areuniformly distributed based ontheprior indicating globaluncertainty aboutthe
robots position. In the second image the first set of measurements arrives and the particles
form clusters in the areas of high posterior belief. In the third enough measurements are
available topushalltheparticles toasinglelocation.
The Kalman filter is the other major way to localize. A Kalman filter represents the
posterior P X
t
z 1:ta 1:t1bya Gaussian. Themeanofthis Gaussianwillbedenotedμ tand
itscovariance Σ . Themainproblemwith Gaussianbeliefsisthattheyareonly closedunder
t
linearmotionmodelsf andlinearmeasurementmodels h. Fornonlinear f orhtheresultof
updating a filteris in general not Gaussian. Thus localization algorithms using the Kalman
filter linearize the motion and sensor models. Linearization is a local approximation of a
L IN EA RI ZA TI ON
nonlinear function by a linear function. Figure 25.11 illustrates the concept of linearization
foraone-dimensional robotmotionmodel. Ontheleftitdepictsanonlinearmotionmodel
fx a the control a is omitted in this graph since it plays no role in the linearization.
t t t
Ontherightthisfunctionisapproximatedbyalinearfunctionfx a . Thislinearfunction
t t
is tangent to f at the point μ the mean of ourstate estimate at time t. Such alinearization
t
function M ON TE-C AR LO-L OC AL IZ AT IO Naz N PXcid:5 X v ω Pzzmreturns
asetofsamplesforthenexttimestep
inputs:arobotvelocitiesv andω
zrangescanz 1...z M
P Xcid:5 X v ωmotionmodel
Pzzrangesensornoisemodel
m2 Dmapoftheenvironment
persistent: Savectorofsamplesofsize N
localvariables: Wavectorofweightsofsize N
Scid:5atemporaryvectorofparticlesofsize N
Wcid:5avectorofweightsofsize N
if S isemptythen initializationphase
fori1to N do
Sisamplefrom P X
0
fori1to N do updatecycle
Scid:5isamplefrom P Xcid:5 X Sivω
Wcid:5i1
forj 1to M do
z R AY CA STj X Scid:5im
Wcid:5i Wcid:5i Pzjz
S WE IG HT ED-S AM PL E-W IT H-R EP LA CE ME NT NScid:5 Wcid:5
return S
Figure25.9 A Monte Carlolocalizationalgorithmusingarange-scansensormodelwith
independentnoise.
is called firstdegree Taylor expansion. A Kalman filterthat linearizes f and h via Taylor
T AY LO RE XP AN SI ON
expansion is called an extended Kalman filter or E KF. Figure 25.12 shows a sequence
of estimates of a robot running an extended Kalman filter localization algorithm. As the
robotmovestheuncertainty initslocationestimateincreases asshownbytheerrorellipses.
Its error decreases as it senses the range and bearing to a landmark with known location
and increases again as the robot loses sight of the landmark. E KF algorithms work well if
landmarks are easily identified. Otherwise the posterior distribution may be multimodal as
in Figure25.10b. Theproblem ofneeding toknow theidentity oflandmarks isaninstance
ofthedataassociation problemdiscussed in Figure15.6.
Insomesituations nomapoftheenvironment isavailable. Thentherobotwillhaveto
acquire amap. Thisis abit ofachicken-and-egg problem: the navigating robot willhave to
determine its location relative to a map it doesnt quite know at the same time building this
mapwhileitdoesntquiteknowitsactuallocation. Thisproblemisimportantformanyrobot
applications and it has been studied extensively under the name simultaneous localization
S IM UL TA NE OU S
andmappingabbreviated as S LA M.
L OC AL IZ AT IO NA ND
M AP PI NG
S LA M problems are solved using many different probabilistic techniques including
theextended Kalmanfilterdiscussed above. Usingthe E KFisstraightforward: justaugment
Section25.3. Robotic Perception 983
Robot position
a
Robot position
b
Robot position
c
Figure25.10 Monte Carlolocalizationaparticlefilteringalgorithmformobilerobotlo-
calization. a Initialglobaluncertainty. b Approximatelybimodaluncertaintyafternavi-
gatinginthesymmetriccorridor.c Unimodaluncertaintyafterenteringaroomandfinding
ittobedistinctive.
""
f X a fμ a F X μ
X X t t t t t t t
t1 f X a t1 f X a
t t t t
""
Σ fμ a Σ fμ a
t1 t t t1 t t Σ
t1
μ X μ X
t t t t
Σ Σ
t t
a b
Figure25.11 One-dimensionalillustrationofalinearizedmotionmodel:a Thefunction
fandtheprojectionofameanμ
t
andacovarianceintervalbasedonΣ tintotimet1.
b Thelinearizedversionisthetangentoff atμ . Theprojectionofthemeanμ iscorrect.
t t
HowevertheprojectedcovarianceΣ t1differsfromΣ t1.
robot
landmark
Figure25.12 Exampleoflocalizationusingtheextended Kalmanfilter. Therobotmoves
ona straightline. As it progressesits uncertaintyincreasesgraduallyasillustrated bythe
errorellipses. Whenitobservesalandmarkwithknownpositiontheuncertaintyisreduced.
the state vector to include the locations of the landmarks in the environment. Luckily the
E KFupdatescalesquadratically soforsmallmapse.g.afewhundredlandmarksthecom-
putation is quite feasible. Richer maps are often obtained using graph relaxation methods
similartothe Bayesian network inference techniques discussed in Chapter14. Expectation
maximization isalsousedfor S LA M.
Not all of robot perception is about localization or mapping. Robots also perceive the tem-
perature odors acoustic signals andsoon. Manyofthese quantities canbeestimated using
variants of dynamic Bayes networks. Allthat is required for such estimators are conditional
probability distributions that characterize the evolution ofstate variables overtime andsen-
sormodelsthatdescribe therelationofmeasurementstostatevariables.
It is also possible to program a robot as a reactive agent without explicitly reasoning
aboutprobability distributions overstates. Wecoverthat approach in Section25.6.3.
The trend in robotics is clearly towards representations with well-defined semantics.
Section25.3. Robotic Perception 985
a b c
aonlytheroadisclassifiedasdrivablestripedarea. The V-shapeddarklineshowswhere
thevehicleisheading. Inbthevehicleiscommandedtodriveofftheroadontoa grassy
surface and the classifier is beginningto classify some of the grass as drivable. In c the
vehiclehasupdateditsmodelofdrivablesurfacetocorrespondtograssaswellasroad.
Probabilistictechniquesoutperformotherapproachesinmanyhardperceptualproblemssuch
aslocalizationandmapping. Howeverstatisticaltechniquesaresometimestoocumbersome
and simplersolutions maybejust as effective inpractice. Tohelp decide whichapproach to
takeexperience workingwithrealphysical robotsisyourbestteacher.
Machine learning plays an important role in robot perception. This is particularly the case
when the best internal representation is not known. One common approach is to map high-
dimensionalsensorstreamsintolower-dimensionalspacesusingunsupervisedmachinelearn-
L OW-D IM EN SI ON AL ing methods see Chapter 18. Such an approach is called low-dimensional embedding.
E MB ED DI NG
Machine learning makes it possible to learn sensor and motion models from data while si-
multaneously discovering asuitableinternal representations.
Another machine learning technique enables robots to continuously adapt to broad
changes in sensor measurements. Picture yourself walking from a sun-lit space into a dark
neon-litroom. Clearlythingsaredarkerinside. Butthechangeoflightsourcealsoaffectsall
the colors: Neon light has a stronger component of green light than sunlight. Yet somehow
weseem not to notice the change. If wewalk together with people into a neon-lit room we
dont think that suddenly theirfaces turned green. Ourperception quickly adapts tothe new
lighting conditions andourbrainignores thedifferences.
Adaptive perception techniques enable robots to adjust to such changes. One example
is shown in Figure 25.13 taken from the autonomous driving domain. Here an unmanned
ground vehicle adapts its classifier of the concept drivable surface. How does this work?
The robot uses a laser to provide classification for a small area right in front of the robot.
When this area is found to be flat in the laser range scan it is used as a positive training
examplefortheconcept drivable surface. Amixture-of-Gaussians technique similartothe
E M algorithm discussed in Chapter 20 is then trained to recognize the specific color and
texture coefficients of the small sample patch. The images in Figure 25.13 are the result of
applying thisclassifiertothefullimage.
Methods that make robots collect theirowntraining data with labels! are called self-
S EL F-S UP ER VI SE D supervised. Inthisinstancetherobotusesmachinelearningtoleverageashort-rangesensor
L EA RN IN G
that works wellforterrain classification into asensor that cansee much farther. Thatallows
therobot todrive faster slowingdownonly whenthesensor modelsaysthere isachange in
theterrainthatneedstobeexaminedmorecarefully bytheshort-range sensors.
Allof arobots deliberations ultimately comedown to deciding how tomoveeffectors. The
P OI NT-T O-P OI NT point-to-pointmotionproblemistodelivertherobotoritsendeffectortoadesignatedtarget
M OT IO N
location. A greater challenge is the compliant motion problem in which a robot moves
C OM PL IA NT MO TI ON
whilebeinginphysicalcontact withanobstacle. Anexampleofcompliantmotionisarobot
manipulatorthatscrewsinalightbulborarobotthatpushesaboxacrossatabletop.
We begin by finding a suitable representation in which motion-planning problems can
bedescribed andsolved. Itturnsoutthatthe configuration spacethespaceofrobotstates
defined by location orientation and joint anglesis abetterplace towork than the original
3 Dspace. Thepathplanningproblem istofindapathfromone configuration toanotherin
P AT HP LA NN IN G
configurationspace. Wehavealreadyencounteredvariousversionsofthepath-planningprob-
lem throughout this book; the complication added by robotics is that path planning involves
continuousspaces. Therearetwomainapproaches: celldecompositionandskeletonization.
Each reduces the continuous path-planning problem to a discrete graph-search problem. In
thissectionweassumethatmotionisdeterministicandthatlocalizationoftherobotisexact.
Subsequent sections willrelaxtheseassumptions.
We will start with a simple representation for a simple robot motion problem. Consider the
robot arm shown in Figure 25.14a. It has two joints that move independently. Moving
the joints alters the xy coordinates of the elbow and the gripper. The arm cannot move
in the z direction. This suggests that the robots configuration can be described by a four-
dimensionalcoordinate: x y forthelocationoftheelbowrelativetotheenvironmentand
e e
x y for the location of the gripper. Clearly these four coordinates characterize the full
g g
W OR KS PA CE state of the robot. They constitute what is known as workspace representation since the
R EP RE SE NT AT IO N
coordinates oftherobotarespecifiedinthesamecoordinate system astheobjects itseeksto
manipulate or to avoid. Workspace representations are well-suited for collision checking
especially iftherobotandallobjectsarerepresented bysimplepolygonal models.
The problem with the workspace representation is that not all workspace coordinates
areactually attainable eveninthe absence ofobstacles. Thisisbecause ofthe linkagecon-
L IN KA GE straints on the space of attainable workspace coordinates. Forexample the elbow position
C ON ST RA IN TS
x y and the gripper position x y are always a fixed distance apart because they are
e e g g
joined byarigid forearm. Arobot motion plannerdefined over workspace coordinates faces
the challenge of generating paths that adhere to these constraints. This is particularly tricky
Section25.4. Planningto Move 987
table
eelb
table
sshou
ee vertical
obstacle
left wall
ss
a b
Figure25.14 a Workspacerepresentationofarobotarmwith2 DO Fs. Theworkspace
is a box with a flat obstacle hangingfromthe ceiling. b Configurationspace of the same
robot. Onlywhiteregionsinthespaceareconfigurationsthatarefreeofcollisions. Thedot
inthisdiagramcorrespondstotheconfigurationoftherobotshownontheleft.
because thestatespaceiscontinuous andtheconstraints arenonlinear. Itturnsouttobeeas-
C ON FI GU RA TI ON iertoplanwithaconfigurationspacerepresentation. Insteadofrepresenting thestateofthe
S PA CE
robot by the Cartesian coordinates of its elements we represent the state by a configuration
of the robots joints. Our example robot possesses two joints. Hence we can represent its
state with the two angles ϕ and ϕ for the shoulder joint and elbow joint respectively. In
s e
theabsenceofanyobstaclesarobotcouldfreelytakeonanyvalueinconfigurationspace. In
particular whenplanning apath one could simply connect the present configuration and the
target configuration by a straight line. In following this path the robot would then move its
jointsataconstant velocity untilatargetlocationisreached.
Unfortunatelyconfigurationspaceshavetheirownproblems. Thetaskofarobotisusu-
ally expressed in workspace coordinates not in configuration space coordinates. This raises
the question of how tomapbetween workspace coordinates and configuration space. Trans-
forming configuration space coordinates into workspace coordinates is simple: it involves
a series of straightforward coordinate transformations. These transformations are linear for
prismaticjointsandtrigonometricforrevolutejoints. Thischainofcoordinatetransformation
isknownaskinematics.
K IN EM AT IC S
Theinverseproblem ofcalculating theconfiguration ofarobotwhoseeffectorlocation
I NV ER SE isspecifiedinworkspacecoordinatesisknownasinversekinematics. Calculatingtheinverse
K IN EM AT IC S
kinematicsishardespeciallyforrobotswithmany D OFs. Inparticularthesolutionisseldom
unique. Figure 25.14a shows one of twopossible configurations that put the gripper in the
samelocation. Theotherconfiguration wouldhastheelbowbelowtheshoulder.
conf-2
conf-1
conf-3
conf-3
conf-1
conf-2
ee
ss
a b
Figure25.15 Threerobotconfigurationsshowninworkspaceandconfigurationspace.
In general this two-link robot arm has between zero and two inverse kinematic solu-
tions for any set of workspace coordinates. Most industrial robots have sufficient degrees
of freedom to find infinitely many solutions to motion problems. To see how this is possi-
ble simply imagine that we added a third revolute joint to our example robot one whose
rotational axis is parallel to the ones of the existing joints. In such a case we can keep the
locationbutnottheorientation! ofthegripperfixedandstillfreelyrotateitsinternaljoints
formostconfigurationsoftherobot. Withafewmorejointshowmany? wecanachievethe
same effect while keeping the orientation of the gripper constant as well. We have already
seen an example of this in the experiment of placing your hand on the desk and moving
your elbow. The kinematic constraint of your hand position is insufficient to determine the
configuration of your elbow. In other words the inverse kinematics of your shoulderarm
assemblypossesses aninfinitenumberofsolutions.
The second problem with configuration space representations arises from the obsta-
cles that may exist in the robots workspace. Ourexample in Figure 25.14a shows several
suchobstacles including afree-hanging obstacle thatprotrudes intothecenteroftherobots
workspace. In workspace such obstacles take on simple geometric formsespecially in
most robotics textbooks which tend to focus on polygonal obstacles. But how do they look
inconfiguration space?
Figure25.14bshowstheconfigurationspaceforourexamplerobotunderthespecific
obstacleconfigurationshownin Figure25.14a. Theconfigurationspacecanbedecomposed
into twosubspaces: thespace ofallconfigurations that arobot mayattain commonly called
free space and the space of unattainable configurations called occupied space. The white
F RE ES PA CE
area in Figure 25.14b corresponds to the free space. All other regions correspond to occu-
O CC UP IE DS PA CE
Section25.4. Planningto Move 989
piedspace. Thedifferent shadings oftheoccupied spacecorresponds tothedifferent objects
in the robots workspace; the black region surrounding the entire free space corresponds to
configurations inwhich the robot collides with itself. Itis easy tosee that extreme values of
the shoulder or elbow angles cause such a violation. The two oval-shaped regions on both
sidesoftherobotcorrespondtothetableonwhichtherobotismounted. Thethirdovalregion
corresponds tothe leftwall. Finally themostinteresting object inconfiguration space isthe
verticalobstaclethathangsfromtheceilingandimpedestherobotsmotions. Thisobjecthas
afunnyshapeinconfiguration space: itishighlynonlinearandatplacesevenconcave. With
a little bit of imagination the reader will recognize the shape of the gripper at the upper left
end. We encourage the reader to pause for a moment and study this diagram. The shape of
thisobstacle isnot atallobvious! Thedotinside Figure25.14b markstheconfiguration of
the robot as shown in Figure 25.14a. Figure 25.15 depicts three additional configurations
both in workspace and in configuration space. In configuration conf-1 the gripper encloses
theverticalobstacle.
Eveniftherobotsworkspaceisrepresentedbyflatpolygonstheshapeofthefreespace
can be very complicated. In practice therefore one usually probes a configuration space
instead ofconstructing it explicitly. Aplanner may generate aconfiguration and then test to
see ifit isin free space by applying the robot kinematics and then checking forcollisions in
workspace coordinates.
C EL L The first approach to path planning uses cell decompositionthat is it decomposes the
D EC OM PO SI TI ON
free space into a finite number of contiguous regions called cells. These regions have the
important property that the path-planning problem within a single region can be solved by
simple means e.g. movingalong astraight line. Thepath-planning problem then becomes
adiscretegraph-searchproblemverymuchlikethesearchproblemsintroducedin Chapter3.
The simplest cell decomposition consists of a regularly spaced grid. Figure 25.16a
shows a square grid decomposition of the space and a solution path that is optimal for this
gridsize. Grayscale shadingindicates the valueofeachfree-space gridcelli.e.thecostof
theshortestpathfromthatcelltothegoal. Thesevaluescanbecomputedbyadeterministic
formofthe V AL UE-I TE RA TI ON algorithmgivenin Figure17.4onpage653. Figure25.16b
""
showsthecorresponding workspacetrajectoryforthearm. Ofcoursewecanalsousethe A
algorithm tofindashortest path.
Such a decomposition has the advantage that it is extremely simple to implement but
it also suffers from three limitations. First itis workable only forlow-dimensional configu-
rationspacesbecausethenumberofgridcellsincreasesexponentially withdthenumberof
dimensions. Sounds familiar? This is the curse!dimensionalityof dimensionality. Second
thereistheproblemofwhattodowithcellsthataremixedthatisneitherentirelywithin
free space norentirely within occupied space. Asolution path that includes such a cell may
not be areal solution because there may be no wayto cross the cell in the desired direction
inastraight line. Thiswouldmakethepathplanner unsound. Ontheotherhand ifweinsist
thatonlycompletely freecellsmaybeused theplannerwillbeincomplete becauseitmight
goal
start
goal
start
a b
Figure25.16 a Valuefunctionandpathfoundforadiscretegridcellapproximationof
theconfigurationspace. b Thesamepathvisualizedinworkspacecoordinates.Noticehow
therobotbendsitselbowtoavoidacollisionwiththeverticalobstacle.
bethe casethat theonly paths tothegoal gothrough mixedcellsespecially ifthecell size
is comparable to that of the passageways and clearances in the space. And third any path
through adiscretized statespacewillnotbesmooth. Itisgenerally difficulttoguarantee that
a smooth solution exists near the discrete path. So a robot may not be able to execute the
solution foundthroughthisdecomposition.
Cell decomposition methods can be improved in a number of ways to alleviate some
oftheseproblems. Thefirstapproachallows furthersubdivision ofthemixedcellsperhaps
using cells of half the original size. This can be continued recursively until a path is found
that lies entirely within free cells. Of course the method only works if there is a way to
decideifagivencellisamixedcellwhichiseasyonlyiftheconfigurationspaceboundaries
haverelativelysimplemathematicaldescriptions. Thismethodiscompleteprovidedthereis
aboundonthesmallestpassagewaythroughwhichasolutionmustpass. Althoughitfocuses
most of the computational effort on the tricky areas within the configuration space it still
fails to scale well to high-dimensional problems because each recursive splitting of a cell
creates2d smallercells. Asecondwaytoobtainacompletealgorithmistoinsistonanexact
E XA CT CE LL cell decomposition ofthe free space. Thismethod must allow cells tobe irregularly shaped
D EC OM PO SI TI ON
where they meet the boundaries of free space but the shapes must still be simple in the
sense that it should be easy to compute a traversal of any free cell. This technique requires
somequiteadvanced geometricideassoweshallnotpursueitfurtherhere.
Examining the solution path shown in Figure 25.16a we can see an additional diffi-
cultythatwillhavetoberesolved. Thepathcontainsarbitrarilysharpcorners;arobotmoving
at any finite speed could not execute such a path. This problem is solved by storing certain
continuous values foreach grid cell. Consider an algorithm which stores foreach grid cell
Section25.4. Planningto Move 991
the exact continuous state that was attained with the cell was first expanded in the search.
Assumefurther thatwhenpropagating information tonearbygridcellsweusethiscontinu-
ousstateasabasisandapplythecontinuousrobotmotionmodelforjumpingtonearbycells.
In doing so we can now guarantee that the resulting trajectory is smooth and can indeed be
executedbytherobot. Onealgorithm thatimplementsthisishybrid A.
H YB RI DA
Noticethatin Figure25.16 thepathgoesveryclosetotheobstacle. Anyonewhohasdriven
acarknowsthataparkingspacewithonemillimeterofclearanceoneithersideisnotreallya
parking spaceatall;forthesamereason wewouldprefersolution pathsthatarerobustwith
respecttosmallmotionerrors.
This problem can be solved by introducing a potential field. A potential field is a
P OT EN TI AL FI EL D
functiondefinedoverstatespacewhosevaluegrowswiththedistancetotheclosestobstacle.
Figure25.17ashowssuchapotential fieldthedarkeraconfiguration statethecloseritis
toanobstacle.
Thepotentialfieldcanbeusedasanadditionalcosttermintheshortest-pathcalculation.
Thisinducesaninterestingtradeoff. Ontheonehandtherobotseekstominimizepathlength
tothegoal. Ontheotherhandittriestostayawayfromobstaclesbyvirtueofminimizingthe
potentialfunction. Withtheappropriateweightbalancing thetwoobjectives aresultingpath
maylook liketheone shownin Figure 25.17b. Thisfigurealsodisplays thevalue function
derived from the combined cost function again calculated by value iteration. Clearly the
resulting pathislonger butitisalsosafer.
There exist many other ways to modify the cost function. For example it may be
desirable to smooth the control parameters over time. For example when driving a car a
smooth pathisbetterthan ajerky one. Ingeneral suchhigher-order constraints arenoteasy
toaccommodate intheplanning process unless wemakethemostrecent steering command
apartofthestate. Howeveritisofteneasytosmooththeresulting trajectory afterplanning
using conjugate gradient methods. Such post-planning smoothing is essential in many real-
worldapplications.
Thesecondmajorfamilyofpath-planningalgorithmsisbasedontheideaofskeletonization.
S KE LE TO NI ZA TI ON
Thesealgorithmsreducetherobotsfreespacetoaone-dimensionalrepresentation forwhich
theplanning problemiseasier. Thislower-dimensional representation iscalledaskeletonof
theconfiguration space.
V OR ON OI GR AP H
spacethe set of all points that are equidistant to two or more obstacles. To do path plan-
ning with a Voronoi graph the robot first changes its present configuration to a point on the
Voronoi graph. It is easy to show that this can always be achieved by a straight-line motion
inconfiguration space. Secondtherobotfollowsthe Voronoigraphuntilitreachesthepoint
nearest to the target configuration. Finally the robot leaves the Voronoi graph and moves to
thetarget. Againthisfinalstepinvolves straight-line motioninconfiguration space.
start goal
a b
Pathfoundbysimultaneouslyminimizingpathlengthandthepotential.
a b
Figure25.18 a The Voronoigraphisthesetofpointsequidistanttotwoormoreobsta-
clesinconfigurationspace. b Aprobabilisticroadmapcomposedof400randomlychosen
pointsinfreespace.
In this way the original path-planning problem is reduced to finding a path on the
Voronoi graph which is generally one-dimensional except in certain nongeneric cases and
hasfinitelymanypointswherethreeormoreone-dimensional curvesintersect. Thusfinding
Section25.5. Planning Uncertain Movements 993
the shortest path along the Voronoi graph is a discrete graph-search problem of the kind
discussed in Chapters 3 and 4. Following the Voronoi graph may not give us the shortest
path but the resulting paths tend to maximize clearance. Disadvantages of Voronoi graph
techniquesarethattheyaredifficulttoapplytohigher-dimensional configurationspacesand
that they tend to induce unnecessarily large detours when the configuration space is wide
open. Furthermorecomputingthe Voronoigraphcanbedifficultespecially inconfiguration
spacewheretheshapesofobstacles canbecomplex.
P RO BA BI LI ST IC An alternative to the Voronoi graphs is the probabilistic roadmap a skeletonization
R OA DM AP
approach that offers morepossible routes andthus deals betterwithwide-open spaces. Fig-
ure25.18bshowsanexampleofaprobabilisticroadmap. Thegraphiscreatedbyrandomly
generating a large number of configurations and discarding those that do not fall into free
space. Two nodes are joined by an arc if it is easy to reach one node from the otherfor
example by a straight line in free space. The result of all this is a randomized graph in the
robots free space. If we add the robots start and goal configurations to this graph path
planning amounts to adiscrete graph search. Theoretically this approach is incomplete be-
cause a bad choice of random points may leave us without any paths from start to goal. It
is possible to bound the probability of failure in terms of the number of points generated
and certain geometric properties of the configuration space. It is also possible to direct the
generation of sample points towards the areas where a partial search suggests that a good
path may be found working bidirectionally from both the start and the goal positions. With
theseimprovementsprobabilisticroadmapplanningtendstoscalebettertohigh-dimensional
configuration spacesthanmostalternative path-planning techniques.
Noneoftherobotmotion-planning algorithmsdiscussedthusfaraddressesakeycharacteris-
ticofroboticsproblems: uncertainty. Inroboticsuncertaintyarisesfrompartialobservability
oftheenvironment andfromthestochastic orunmodeled effectsoftherobots actions. Er-
rorscanalsoarisefrom theuseofapproximation algorithms suchasparticle filtering which
doesnotprovide therobotwithanexactbelief stateevenifthestochastic nature oftheenvi-
ronmentismodeledperfectly.
Most of todays robots use deterministic algorithms for decision making such as the
path-planning algorithms of the previous section. To do so it is common practice to extract
the most likely state from the probability distribution produced by the state estimation al-
M OS TL IK EL YS TA TE
gorithm. The advantage of this approach is purely computational. Planning paths through
configuration space is already a challenging problem; it would be worse if we had to work
with a full probability distribution overstates. Ignoring uncertainty in this way works when
theuncertainty issmall. Infactwhentheenvironment modelchangesovertimeastheresult
ofincorporating sensormeasurements manyrobotsplanpathsonlineduring planexecution.
Thisistheonlinereplanningtechnique of Section11.3.3.
O NL IN ER EP LA NN IN G
Unfortunately ignoring the uncertainty does not always work. In some problems the
robots uncertainty is simply too massive: How can we use a deterministic path planner to
control amobile robot that has no clue where itis? Ingeneral if the robots true state is not
the one identified by the maximum likelihood rule the resulting control will be suboptimal.
Depending on the magnitude of the error this can lead to all sorts of unwanted effects such
ascollisions withobstacles.
Thefieldofroboticshasadoptedarangeoftechniquesforaccommodatinguncertainty.
Somearederived from thealgorithms given in Chapter17fordecision making underuncer-
tainty. Iftherobotfacesuncertaintyonlyinitsstatetransitionbutitsstateisfullyobservable
theproblemisbestmodeledasa Markovdecisionprocess M DP.Thesolutionofan M DPis
anoptimalpolicywhichtellstherobotwhattodoineverypossible state. Inthiswayitcan
handleallsortsofmotionerrorswhereasasingle-path solution fromadeterministic planner
N AV IG AT IO N would be muchless robust. Inrobotics policies arecalled navigation functions. Thevalue
F UN CT IO N
function shown in Figure 25.16a can be converted into such a navigation function simply
byfollowingthegradient.
Justasin Chapter17partialobservability makestheproblemmuchharder. Theresult-
ing robot control problem is a partially observable M DPor P OM DP.In such situations the
robot maintains aninternal beliefstate liketheonesdiscussed in Section25.3. Thesolution
to a P OM DP is a policy defined over the robots belief state. Put differently the input to
thepolicyisanentire probability distribution. Thisenables therobottobase itsdecision not
only on what it knows but also on what it does not know. For example if it is uncertain
I NF OR MA TI ON aboutacriticalstatevariableitcanrationallyinvokeaninformationgatheringaction. This
G AT HE RI NG AC TI ON
is impossible inthe M DPframework since M DPsassume full observability. Unfortunately
techniquesthatsolve P OM DPsexactlyareinapplicabletoroboticstherearenoknowntech-
niquesforhigh-dimensionalcontinuousspaces. Discretizationproduces P OM DPsthatarefar
too large to handle. Oneremedy isto makethe minimization of uncertainty acontrol objec-
C OA ST AL tive. For example the coastal navigation heuristic requires the robot to stay near known
N AV IG AT IO N
landmarks to decrease its uncertainty. Another approach applies variants of the probabilis-
tic roadmap planning method to the belief space representation. Such methods tend to scale
bettertolargediscrete P OM DPs.
Uncertaintycanalsobehandledusingso-calledrobustcontrolmethodsseepage836rather
R OB US TC ON TR OL
than probabilistic methods. A robust method is one that assumes a bounded amount of un-
certainty in each aspect of a problem but does not assign probabilities to values within the
allowed interval. A robust solution is one that works no matter what actual values occur
providedtheyarewithintheassumedinterval. Anextremeformofrobustmethodisthecon-
formant planningapproach given in Chapter11it produces plans that work with nostate
information atall.
F IN E-M OT IO N Here we look at a robust method that is used for fine-motion planning or F MP in
P LA NN IN G
robotic assembly tasks. Fine-motion planning involves moving a robot arm in very close
proximity to a static environment object. The main difficulty with fine-motion planning is
Section25.5. Planning Uncertain Movements 995
initial Cv
configuration
motion v
envelope
Figure25.19 Atwo-dimensionalenvironmentvelocityuncertaintyconeandenvelopeof
possiblerobotmotions. Theintendedvelocityis v butwithuncertaintytheactualvelocity
couldbeanywherein Cvresultinginafinalconfigurationsomewhereinthemotionenvelope
whichmeanswewouldntknowifwehittheholeornot.
initial
Cv
configuration
v
motion
envelope
Figure25.20 Thefirstmotioncommandandtheresultingenvelopeofpossiblerobotmo-
tions. No matter what the error we know the final configuration will be to the left of the
hole.
thattherequiredmotionsandtherelevantfeaturesoftheenvironmentareverysmall. Atsuch
smallscalestherobotisunabletomeasureorcontrolitspositionaccuratelyandmayalsobe
uncertain of the shape of the environment itself; we will assume that these uncertainties are
all bounded. The solutions to F MP problems will typically be conditional plans or policies
thatmakeuseofsensorfeedbackduringexecutionandareguaranteedtoworkinallsituations
consistent withtheassumeduncertainty bounds.
A fine-motion plan consists of a series of guarded motions. Each guarded motion
G UA RD ED MO TI ON
consistsof1amotioncommandand2aterminationconditionwhichisapredicateonthe
robots sensor values and returns true to indicate the end of the guarded move. The motion
commands are typically compliant motions that allow the effector to slide if the motion
C OM PL IA NT MO TI ON
commandwouldcausecollisionwithanobstacle. Asanexample Figure25.19showsatwo-
dimensional configuration space with a narrow vertical hole. It could be the configuration
space forinsertion ofarectangular pegintoaholeoracarkeyintotheignition. Themotion
commandsareconstant velocities. Thetermination conditions arecontact withasurface. To
modeluncertainty incontrol weassumethatinsteadofmovinginthecommandeddirection
the robots actual motion lies in the cone C about it. The figure shows what would happen
v
Cv
v
motion
envelope
Figure25.21 Thesecondmotioncommandandtheenvelopeofpossiblemotions. Even
witherrorwewilleventuallygetintothehole.
if we commanded a velocity straight down from the initial configuration. Because of the
uncertainty in velocity the robot could move anywhere in the conical envelope possibly
going into the hole but more likely landing to one side of it. Because the robot would not
thenknowwhichsideoftheholeitwasonitwouldnotknowwhichwaytomove.
A more sensible strategy is shown in Figures 25.20 and 25.21. In Figure 25.20 the
robotdeliberatelymovestoonesideofthehole. Themotioncommandisshowninthefigure
and the termination test is contact with any surface. In Figure 25.21 a motion command is
given that causes the robot to slide along the surface and into the hole. Because all possible
velocities inthemotionenvelope aretotheright therobot willslidetotherightwheneverit
is in contact with a horizontal surface. It will slide down the right-hand vertical edge of the
holewhenittouches itbecauseallpossiblevelocities aredownrelativetoaverticalsurface.
It will keep moving until it reaches the bottom of the hole because that is its termination
condition. In spite of the control uncertainty all possible trajectories of the robot terminate
incontactwiththebottomoftheholethatisunlesssurfaceirregularitiescausetherobotto
stickinoneplace.
As one might imagine the problem of constructing fine-motion plans is not trivial; in
fact it is a good deal harder than planning with exact motions. One can either choose a
fixed number of discrete values foreach motion oruse the environment geometry to choose
directions that givequalitatively different behavior. Afine-motion planner takes asinput the
configuration-spacedescriptiontheangleofthevelocityuncertaintyconeandaspecification
ofwhatsensing ispossible fortermination surface contact inthiscase. Itshould produce a
multistepconditional planorpolicythatisguaranteed tosucceed ifsuchaplanexists.
Ourexample assumes that the planner has an exact model ofthe environment but itis
possible to allow forbounded errorinthis modelas follows. Iftheerror canbedescribed in
termsofparametersthoseparameterscanbeaddedasdegreesoffreedomtotheconfiguration
space. In the last example if the depth and width of the hole were uncertain we could add
them as two degrees of freedom to the configuration space. It is impossible to move the
robot in these directions in the configuration space or to sense its position directly. But
boththoserestrictionscanbeincorporated whendescribingthisproblemasan F MPproblem
by appropriately specifying control and sensor uncertainties. This gives a complex four-
dimensional planning problem but exactly the same planning techniques can be applied.
Section25.6. Moving 997
Noticethatunlikethedecision-theoretic methodsin Chapter17thiskindofrobustapproach
results in plans designed for the worst-case outcome rather than maximizing the expected
qualityoftheplan. Worst-caseplansareoptimalinthedecision-theoretic senseonlyiffailure
duringexecution ismuchworsethananyoftheothercostsinvolved inexecution.
So far we have talked about how to plan motions but not about how to move. Ourplans
particularlythoseproducedbydeterministicpathplannersassumethattherobotcansimply
followanypaththatthealgorithm produces. Intherealworldofcourse thisisnotthecase.
Robots have inertia and cannot execute arbitrary paths except at arbitrarily slow speeds. In
mostcasestherobotgetstoexertforcesratherthanspecifypositions. Thissectiondiscusses
methodsforcalculating theseforces.
Section 25.2introduced thenotion of dynamicstate whichextends thekinematic state ofa
robotbyitsvelocity. Forexampleinaddition totheangleofarobotjoint thedynamicstate
also captures the rate of change of the angle and possibly even its momentary acceleration.
The transition model for a dynamic state representation includes the effect of forces on this
D IF FE RE NT IA L rate of change. Such models are typically expressed via differential equations which are
E QU AT IO N
equations that relate a quantity e.g. a kinematic state to the change of the quantity over
timee.g.velocity. Inprinciple wecould havechosen toplanrobot motionusing dynamic
models instead of ourkinematic models. Such a methodology would lead to superior robot
performance if wecould generate the plans. However the dynamic state has higher dimen-
sion than the kinematic space and the curse of dimensionality would render many motion
planningalgorithmsinapplicable forallbutthemostsimplerobots. Forthisreasonpractical
robotsystemoftenrelyonsimplerkinematicpathplanners.
A common technique to compensate for the limitations of kinematic plans is to use a
separate mechanism acontroller forkeeping therobotontrack. Controllers aretechniques
C ON TR OL LE R
for generating robot controls in real time using feedback from the environment so as to
achieve a control objective. If the objective is to keep the robot on a preplanned path it is
R EF ER EN CE oftenreferredtoasareferencecontrollerandthepathiscalledareferencepath. Controllers
C ON TR OL LE R
that optimize aglobal cost function are known as optimal controllers. Optimal policies for
R EF ER EN CE PA TH
O PT IM AL continuous M DPsareineffectoptimalcontrollers.
C ON TR OL LE RS
On the surface the problem of keeping a robot on a prespecified path appears to be
relatively straightforward. In practice however even this seemingly simple problem has its
pitfalls. Figure 25.22a illustrates what can go wrong; it shows the path of a robot that
attempts to follow a kinematic path. Whenever a deviation occurswhether due to noise or
toconstraints ontheforcestherobotcanapplytherobotprovidesanopposingforcewhose
magnitude is proportional to this deviation. Intuitively this might appear plausible since
deviations should be compensated by a counterforce to keep the robot on track. However
a b c
proportionalcontrolwith gain factor 0.1 and c P D proportionalderivativecontrolwith
gainfactors0.3fortheproportionalcomponentand0.8forthedifferentialcomponent.Inall
casestherobotarmtriestofollowthepathshowningray.
as Figure 25.22a illustrates our controller causes the robot to vibrate rather violently. The
vibration is the result of a natural inertia of the robot arm: once driven back to its reference
positiontherobotthenovershootswhichinducesasymmetricerrorwithoppositesign. Such
overshooting may continue along an entire trajectory and the resulting robot motion is far
fromdesirable.
Before we can define a better controller let us formally describe what went wrong.
Controllers that provide force in negative proportion to the observed error are known as P
controllers. The letter P stands for proportional indicating that the actual control is pro-
P CO NT RO LL ER
portional totheerroroftherobotmanipulator. Moreformally letytbethereference path
parameterized bytimeindex t. Thecontrol a generated bya Pcontrollerhastheform:
t
a K ytx .
t P t
Herex isthestateoftherobotattimetand K isaconstantknownasthegainparameterof
G AI NP AR AM ET ER t P
thecontrolleranditsvalueiscalledthegainfactor; K regulateshowstronglythecontroller
p
corrects fordeviations between the actual state x and the desired one yt. Inour example
t
K 1. At first glance one might think that choosing a smaller value for K would
P P
remedy the problem. Unfortunately this is not the case. Figure 25.22b shows a trajectory
for K .1 still exhibiting oscillatory behavior. Lower values of the gain parameter may
P
simply slow down the oscillation but do not solve the problem. In fact in the absence of
friction the P controller is essentially a spring law; so it will oscillate indefinitely around a
fixedtargetlocation.
Traditionally problems of this type fall into the realm of control theory a field of
increasingimportancetoresearchersin A I.Decadesofresearchinthisfieldhaveledtoalarge
numberofcontrollers thataresuperiortothesimplecontrollawgivenabove. Inparticular a
referencecontrollerissaidtobestableifsmallperturbationsleadtoaboundederrorbetween
S TA BL E
the robot and the reference signal. It is said to be strictly stable if it is able to return to and
S TR IC TL YS TA BL E
Section25.6. Moving 999
thenstayonitsreference pathuponsuchperturbations. Our Pcontrollerappears tobestable
butnotstrictlystable sinceitfailstostayanywherenear itsreference trajectory.
The simplest controller that achieves strict stability in our domain is a P D controller.
P DC ON TR OL LE R
Theletter Pstandsagainforproportional and Dstandsforderivative. P Dcontrollers are
described bythefollowingequation:
ytx
a K ytx K t . 25.2
t P t D
t
As this equation suggests P D controllers extend P controllers by a differential component
which adds to the value of a a term that is proportional to the first derivative of the error
t
ytx overtime. Whatistheeffectofsuchaterm? Ingeneral aderivativetermdampens
t
thesystemthatisbeingcontrolled. Toseethisconsiderasituationwheretheerrorytx
t
ischanging rapidlyovertimeasisthecaseforour Pcontroller above. Thederivativeofthis
error will then counteract the proportional term which will reduce the overall response to
theperturbation. Howeverifthesameerrorpersists anddoesnotchange thederivativewill
vanishandtheproportional termdominates thechoiceofcontrol.
Figure25.22c showstheresult ofapplying this P Dcontroller toourrobot arm using
asgainparameters K .3and K .8. Clearlytheresulting pathismuchsmoother and
P D
doesnotexhibitanyobviousoscillations.
P D controllers do have failure modes however. In particular P D controllers may fail
to regulate an error down to zero even in the absence of external perturbations. Often such
a situation is the result of a systematic external force that is not part of the model. An au-
tonomouscardrivingonabanked surface forexample mayfinditselfsystematically pulled
to one side. Wearand tear in robot arms cause similar systematic errors. In such situations
anover-proportionalfeedbackisrequiredtodrivetheerrorclosertozero. Thesolutiontothis
problemliesinaddingathirdtermtothecontrollawbasedontheintegratederrorovertime:
cid:26
ytx
a K ytx K ytx dt K t . 25.3
t P t I t D
t
""
Here K isyetanothergainparameter. Theterm ytx dtcalculatestheintegralofthe
I t
error over time. The effect of this term is that long-lasting deviations between the reference
signal and the actual state are corrected. If for example x is smaller than yt for a long
t
periodoftimethisintegralwillgrowuntiltheresulting control a forcesthiserrortoshrink.
t
Integral terms then ensure thatacontroller doesnotexhibitsystematic error attheexpense
ofincreased danger ofoscillatory behavior. Acontroller withallthree terms iscalled a P ID
controller forproportional integral derivative. P IDcontrollers arewidelyused inindustry
P ID CO NT RO LL ER
foravarietyofcontrolproblems.
We introduced potential fields as an additional cost function in robot motion planning but
theycanalsobeusedforgenerating robotmotiondirectly dispensing withthepathplanning
phase altogether. To achieve this we have to define an attractive force that pulls the robot
towardsitsgoalconfiguration andarepellent potential fieldthat pushes therobot awayfrom
obstacles. Such a potential field is shown in Figure 25.23. Its single global minimum is
goal start goal
start
a b
repelling forces asserted from the obstacles and an attracting force that correspondsto the
goalconfiguration.a Successfulpath. b Localoptimum.
the goal configuration and the value is the sum of the distance to this goal configuration
and the proximity to obstacles. No planning was involved in generating the potential field
shown in the figure. Because of this potential fields are well suited to real-time control.
field. In many applications the potential field can be calculated efficiently for any given
configuration. Moreover optimizing the potential amounts to calculating the gradient of the
potential for the present robot configuration. These calculations can be extremely efficient
especially when compared to path-planning algorithms all of which are exponential in the
dimensionality oftheconfiguration spacethe D OFsintheworstcase.
The fact that the potential field approach manages to find a path to the goal in such
an efficient manner even over long distances in configuration space raises the question as
to whether there is a need for planning in robotics at all. Are potential field techniques
sufficient or were we just lucky in our example? The answer is that we were indeed lucky.
Potentialfieldshavemanylocalminimathatcantraptherobot. In Figure25.23btherobot
approaches theobstacle bysimplyrotating itsshoulderjoint untilitgetsstuckonthewrong
side of the obstacle. The potential field is not rich enough to make the robot bend its elbow
sothatthearmfitsundertheobstacle. Inotherwordspotentialfieldcontrolisgreatforlocal
robotmotionbutsometimeswestillneedglobalplanning. Anotherimportantdrawbackwith
potentialfieldsisthattheforcestheygeneratedependonlyontheobstacleandrobotpositions
notontherobotsvelocity. Thuspotentialfieldcontrolisreallyakinematicmethodandmay
failiftherobotismovingquickly.
retract lift higher
yes
move
forward no
S 3 stuck? S 4
lift up set down
push backward
S S
a b
A FS M for the control of a single leg. Notice that this A FS M reacts to sensor feedback:
ifalegisstuckduringtheforwardswingingphaseitwillbeliftedincreasinglyhigher.
Sofarwehave considered control decisions that require some modelof theenvironment for
constructing either areference path orapotential field. There are some difficulties with this
approach. First models that are sufficiently accurate are often difficult to obtain especially
in complex or remote environments such as the surface of Mars or for robots that have
few sensors. Second even in cases where we can devise a model with sufficient accuracy
computational difficulties and localization error might render these techniques impractical.
Insomecasesareflexagentarchitecture using reactive controlismoreappropriate.
R EA CT IV EC ON TR OL
Forexamplepicturealeggedrobotthatattemptstoliftalegoveranobstacle. Wecould
givethisrobotarulethatsaysliftthelegasmallheight handmoveitforwardandiftheleg
encounters anobstacle moveitbackandstart againatahigher height. Youcould saythat h
ismodeling anaspect oftheworld butwecan alsothink of hasanauxiliary variable ofthe
robotcontroller devoidofdirectphysicalmeaning.
One such example is the six-legged hexapod robot shown in Figure 25.24a de-
signed forwalking through rough terrain. Therobots sensors areinadequate toobtain mod-
elsoftheterrainforpathplanning. Moreoverevenifweaddedsufficientlyaccuratesensors
the twelve degrees of freedom two for each leg would render the resulting path planning
problem computationally intractable.
It is possible nonetheless to specify a controller directly without an explicit environ-
mental model. We have already seen this with the P D controller which was able to keep a
complexrobotarmontarget withoutanexplicitmodeloftherobotdynamics;itdidhowever
require a reference path generated from a kinematic model. Forthe hexapod robot we first
choose agait orpattern ofmovementofthelimbs. Onestatically stable gaitistofirstmove
G AI T
the right front right rear and left center legs forward keeping the other three fixed and
then move the other three. This gait works well on flat terrain. On rugged terrain obstacles
may prevent a leg from swinging forward. This problem can be overcome by a remarkably
simple control rule: when a legs forward motion is blocked simply retract it lift it higher
Figure25.25 Multiple exposuresofan R C helicopterexecutinga flip based ona policy
learnedwithreinforcementlearning.Imagescourtesyof Andrew Ng Stanford University.
andtry again. Theresulting controller isshownin Figure25.24b asafinitestate machine;
itconstitutes areflex agent with state where the internal state isrepresented by the index of
thecurrentmachinestates through s .
Variantsofthissimplefeedback-driven controllerhavebeenfoundtogenerate remark-
ably robust walking patterns capable ofmaneuvering therobot overrugged terrain. Clearly
such a controller is model-free and it does not deliberate or use search for generating con-
trols. Environmental feedbackplaysacrucialroleinthecontrollersexecution. Thesoftware
alonedoesnotspecifywhatwillactuallyhappenwhentherobotisplacedinanenvironment.
Behavior that emerges through the interplay of a simple controller and a complex envi-
E ME RG EN T ronment is often referred to as emergent behavior. Strictly speaking all robots discussed
B EH AV IO R
in this chapter exhibit emergent behavior due to the fact that no model is perfect. Histori-
cally however the term has been reserved for control techniques that do not utilize explicit
environmental models. Emergentbehaviorisalsocharacteristic ofbiological organisms.
Oneparticularlyexcitingformofcontrolisbasedonthepolicysearchformofreinforcement
learning see Section 21.5. This work has been enormously influential in recent years at
is has solved challenging robotics problems for which previously no solution existed. An
example is acrobatic autonomous helicopter flight. Figure 25.25 shows an autonomous flip
of a small R C radio-controlled helicopter. This maneuver is challenging due to the highly
nonlinear nature of the aerodynamics involved. Only the most experienced of human pilots
areable toperform it. Yetapolicy search methodasdescribed in Chapter21 using only a
fewminutesofcomputation learnedapolicythatcansafely executeaflipeverytime.
Policy search needs an accurate model of the domain before it can find a policy. The
input to this model is the state of the helicopter at time t the controls at time t and the
resultingstateattimetΔt. Thestateofahelicoptercanbedescribedbythe3 Dcoordinates
of the vehicle its yaw pitch and roll angles and the rate of change of these six variables.
The controls are the manual controls of of the helicopter: throttle pitch elevator aileron
and rudder. Allthat remains is the resulting statehow are wegoing to define a model that
accurately says how the helicopter responds to each control? The answer is simple: Let an
expert human pilot fly the helicopter and record the controls that the expert transmits over
the radio and the state variables of the helicopter. About four minutes of human-controlled
flightsufficestobuildapredictive modelthatissufficiently accuratetosimulatethevehicle.
What is remarkable about this example is the ease with which this learning approach
solvesachallengingroboticsproblem. Thisisoneofthemanysuccessesofmachinelearning
inscientificfieldspreviously dominated bycarefulmathematicalanalysis andmodeling.
S OF TW AR E Amethodologyforstructuring algorithmsiscalleda softwarearchitecture. Anarchitecture
A RC HI TE CT UR E
includes languages and tools forwriting programs as wellas an overall philosophy forhow
programscanbebroughttogether.
Modern-day software architectures for robotics must decide how to combine reactive
control and model-based deliberative planning. In many ways reactive and deliberate tech-
niques have orthogonal strengths and weaknesses. Reactive control is sensor-driven and ap-
propriate for making low-level decisions in real time. However it rarely yields a plausible
solutionatthegloballevelbecauseglobalcontroldecisionsdependoninformationthatcan-
not be sensed at the time of decision making. For such problems deliberate planning is a
moreappropriate choice.
Consequently most robot architectures use reactive techniques at the lower levels of
control anddeliberative techniques atthe higherlevels. Weencountered such acombination
in our discussion of P D controllers where we combined a reactive P D controller with a
deliberate path planner. Architectures that combine reactive and deliberate techniques are
H YB RI D calledhybridarchitectures.
A RC HI TE CT UR E
S UB SU MP TI ON Thesubsumptionarchitecture Brooks 1986 isaframework forassembling reactive con-
A RC HI TE CT UR E
trollers out of finite state machines. Nodes in these machines may contain tests for certain
sensorvariables inwhichcasetheexecutiontraceofafinitestatemachineisconditioned on
the outcome of such a test. Arcs can be tagged with messages that will be generated when
traversingthemandthataresenttotherobotsmotorsorto otherfinitestatemachines. Addi-
tionally finitestatemachines possess internal timersclocks thatcontrol thetimeittakesto
A UG ME NT ED FI NI TE traverse anarc. Theresulting machinesarerefereed toas augmentedfinitestatemachines
S TA TE MA CH IN E
or A FS Mswheretheaugmentation referstotheuseofclocks.
An example of a simple A FS M is the four-state machine shown in Figure 25.24b
which generates cyclic leg motion for a hexapod walker. This A FS M implements a cyclic
controller whose execution mostly does not rely on environmental feedback. The forward
swing phase however does rely on sensor feedback. If the leg is stuck meaning that it has
failed to execute the forward swing the robot retracts the leg lifts it up a little higher and
attempts to execute the forward swing once again. Thus the controller is able to react to
contingencies arisingfromtheinterplay oftherobotanditsenvironment.
The subsumption architecture offers additional primitives for synchronizing A FS Ms
and for combining output values of multiple possibly conflicting A FS Ms. In this way it
enablestheprogrammertocomposeincreasinglycomplexcontrollersinabottom-upfashion.
In our example wemight begin with A FS Msforindividual legs followed by an A FS Mfor
coordinating multiple legs. On top of this wemight implement higher-level behaviors such
ascollision avoidance whichmightinvolvebacking upandturning.
The idea of composing robot controllers from A FS Ms is quite intriguing. Imagine
how difficult it would be to generate the same behavior with any of the configuration-space
path-planning algorithms described in the previous section. First we would need an accu-
rate model of the terrain. The configuration space of a robot with six legs each of which
is driven by two independent motors totals eighteen dimensions twelve dimensions for the
configuration of the legs and six for the location and orientation of the robot relative to its
environment. Evenifourcomputerswerefastenoughtofindpathsinsuchhigh-dimensional
spaces we would have to worry about nasty effects such as the robot sliding down a slope.
Because of such stochastic effects a single path through configuration space would almost
certainly be too brittle and even a P ID controller might not be able to cope with such con-
tingencies. Inother words generating motion behavior deliberately issimply too complex a
problem forpresent-day robotmotionplanning algorithms.
Unfortunately the subsumption architecture has its own problems. First the A FS Ms
are driven by raw sensor input an arrangement that works if the sensor data is reliable and
containsallnecessary informationfordecisionmakingbutfailsifsensordatahastobeinte-
gratedinnontrivialwaysovertime. Subsumption-stylecontrollershavethereforemostlybeen
appliedtosimpletaskssuchasfollowingawallormovingtowardsvisiblelightsources. Sec-
ondthelackofdeliberationmakesitdifficulttochangethetaskoftherobot. Asubsumption-
style robot usually does just one task and it has no notion of how to modify its controls to
accommodate different goals just like the dung beetle on page 39. Finally subsumption-
stylecontrollers tendtobedifficulttounderstand. Inpractice theintricateinterplaybetween
dozens of interacting A FS Ms and the environment is beyond what most human program-
mers can comprehend. For all these reasons the subsumption architecture is rarely used in
robotics despite its great historical importance. However it has had an influence on other
architectures andonindividual components ofsomearchitectures.
Hybrid architectures combine reaction with deliberation. The most popular hybrid architec-
T HR EE-L AY ER ture is the three-layer architecture which consists of a reactive layer an executive layer
A RC HI TE CT UR E
andadeliberative layer.
Thereactivelayerprovideslow-levelcontroltotherobot. Itischaracterized byatight
R EA CT IV EL AY ER
sensoraction loop. Itsdecision cycleisoftenontheorder ofmilliseconds.
Theexecutivelayerorsequencing layerservesasthegluebetweenthereactivelayer
E XE CU TI VE LA YE R
andthedeliberative layer. Itaccepts directives bythedeliberative layer andsequences them
for the reactive layer. For example the executive layer might handle a set of via-points
generated by a deliberative path planner and make decisions as to which reactive behavior
to invoke. Decision cycles at the executive layer are usually in the order of a second. The
executive layer is also responsible for integrating sensor information into an internal state
representation. Forexampleitmayhosttherobotslocalizationandonlinemappingroutines.
S EN SO R I NT ER FA CE P ER CE PT IO N P LA NN IN GC ON TR OL U SE R I NT ER FA CE
R DD F database corridor Top level control Touch screen U I
pausedisable command Wireless E-Stop
Laser 1 interface
R DD F corridor smoothed and original driving mode
Laser 2 interface
Laser 3 interface Road finder road center Path planner
Laser 4 interface laser map
Laser 5 interface Laser mapper map trajectory V EH IC LE
Camera interface Vision mapper vision map I NT ER FA CE
Radar interface Radar mapper obstacle list Steering control
vehicle state pose velocity Touareg interface
vehicle
G PS position U KF Pose estimation state Throttlebrake control
Power server interface
G PS compass vehicle state pose velocity
I MU interface Surface assessment velocity limit
Wheel velocity
Brakesteering
heart beats Linux processes startstop emergency stop
health status
Process controller Health monitor
power onoff
data
G LO BA L
Data logger File system
S ER VI CE S
Communication requests Communication channels clocks
Inter-process communication I PC server Time server
pipelineinwhichallmodulesprocessdatasimultaneously.
The deliberative layer generates global solutions to complex tasks using planning.
D EL IB ER AT IV EL AY ER
Because of the computational complexity involved in generating such solutions its decision
cycleisoftenintheorderofminutes. Thedeliberative layerorplanning layer usesmodels
for decision making. Those models might be either learned from data or supplied and may
utilizestateinformation gathered attheexecutivelayer.
Variantsofthethree-layerarchitecturecanbefoundinmostmodern-dayrobotsoftware
systems. Thedecomposition intothreelayersisnotverystrict. Somerobotsoftwaresystems
possessadditionallayerssuchasuserinterfacelayersthatcontroltheinteractionwithpeople
oramultiagent level forcoordinating arobots actions withthat of otherrobots operating in
thesameenvironment.
P IP EL IN E Anotherarchitectureforrobotsisknownasthe pipelinearchitecture. Justlikethesubsump-
A RC HI TE CT UR E
tionarchitecturethepipelinearchitectureexecutesmultipleprocessinparallel. Howeverthe
specificmodulesinthisarchitecture resemblethoseinthethree-layer architecture.
S EN SO RI NT ER FA CE tonomous car. Dataenters thispipeline atthe sensor interface layer. Theperception layer
L AY ER
P ER CE PT IO NL AY ER
a b
Figure25.27 a The Helpmaterobottransportsfoodandothermedicalitemsindozens
of hospitals worldwide. b Kiva robotsare part of a material-handlingsystem formoving
shelvesinfulfillmentcenters. Imagecourtesyof Kiva Systems.
then updates the robots internal models of the environment based on this data. Next these
P LA NN IN GA ND models are handed to the planning and control layer which adjusts the robots internal
C ON TR OL LA YE R
plansturnsthemintoactualcontrols fortherobot. Thosearethencommunicated backtothe
V EH IC LE IN TE RF AC E vehiclethroughthevehicleinterfacelayer.
L AY ER
The key to the pipeline architecture is that this all happens in parallel. While the per-
ception layer processes the most recent sensor data the control layer bases its choices on
slightly older data. In this way the pipeline architecture is similar to the human brain. We
dontswitchoffourmotioncontrollerswhenwedigestnewsensordata. Insteadweperceive
plan andactallatthesametime. Processes inthepipeline architecture runasynchronously
andallcomputation isdata-driven. Theresulting systemisrobust anditisfast.
Thearchitecturein Figure25.26alsocontainsothercross-cuttingmodulesresponsible
forestablishing communication betweenthedifferentelementsofthepipeline.
Herearesomeoftheprimeapplication domainsforrobotictechnology.
Industryand Agriculture. Traditionally robotshavebeenfieldedinareasthatrequire
difficult human labor yet are structured enough to be amenable to robotic automation. The
best example is the assembly line where manipulators routinely perform tasks such as as-
sembly part placement material handling welding and painting. In many of these tasks
robots have become more cost-effective than human workers. Outdoors many of the heavy
machines that we use to harvest mine or excavate earth have been turned into robots. For
a b
Figure25.28 a Roboticcar B OS Swhichwonthe D AR PA Urban Challenge. Courtesy
of Carnegie Mellon University.b Surgicalrobotsintheoperatingroom.Imagecourtesyof
da Vinci Surgical Systems.
exampleaprojectat Carnegie Mellon Universityhasdemonstratedthatrobotscanstrippaint
offlargeshipsabout50timesfasterthanpeoplecanandwithamuchreducedenvironmental
impact. Prototypes ofautonomous miningrobotshavebeenfoundtobefasterandmorepre-
cisethanpeopleintransportingoreinundergroundmines. Robotshavebeenusedtogenerate
high-precision maps of abandoned mines and sewer systems. While many of these systems
are still in their prototype stages it is only a matter of time until robots will take overmuch
ofthesemimechanical workthatispresently performedbypeople.
Transportation. Robotictransportationhasmanyfacets: fromautonomoushelicopters
thatdeliverpayloads tohard-to-reach locations toautomatic wheelchairs thattransport peo-
plewhoareunabletocontrolwheelchairsbythemselvestoautonomousstraddlecarriersthat
outperform skilledhumandriverswhentransporting containers fromshipstotrucksonload-
ingdocks. Aprimeexampleofindoortransportation robots orgofers isthe Helpmaterobot
shown in Figure 25.27a. This robot has been deployed in dozens of hospitals to transport
food and other items. In factory settings autonomous vehicles are now routinely deployed
to transport goods in warehouses and between production lines. The Kiva system shown in
Figure25.27bhelpsworkersatfulfillmentcenterspackagegoodsintoshippingcontainers.
Manyoftheserobotsrequireenvironmentalmodificationsfortheiroperation. Themost
common modifications are localization aids such as inductive loops in the floor active bea-
cons or barcode tags. An open challenge in robotics is the design of robots that can use
naturalcuesinsteadofartificialdevicestonavigateparticularlyinenvironmentssuchasthe
deepoceanwhere G PSisunavailable.
Robotic cars. Most of use cars every day. Many of us make cell phone calls while
driving. Some of us even text. The sad result: more than a million people die every year in
trafficaccidents. Robotic carslike B OS S and S TA NL EY offerhope: Notonlywilltheymake
drivingmuchsaferbuttheywillalsofreeusfromtheneedtopayattentiontotheroadduring
ourdailycommute.
Progress in robotic cars was stimulated by the D AR PA Grand Challenge a race over
100milesofunrehearseddesertterrainwhichrepresented amuchmorechallengingtaskthan
a b
Figure25.29 a A robotmappingan abandonedcoalmine. b A 3 D mapof the mine
acquiredbytherobot.
hadeverbeenaccomplishedbefore. Stanfords S TA NL EYvehiclecompletedthecourseinless
thansevenhours in2005 winninga2million prizeandaplaceinthe National Museum of
American History. Figure 25.28a depicts B OS S which in 2007 won the D AR PA Urban
Challengeacomplicatedroadraceoncitystreetswhererobotsfacedotherrobotsandhadto
obeytrafficrules.
Healthcare. Robotsareincreasinglyusedtoassistsurgeonswithinstrumentplacement
whenoperatingonorgansasintricateasbrainseyesandhearts. Figure25.28bshowssuch
asystem. Robotshavebecomeindispensable toolsinarangeofsurgical procedures suchas
hip replacements thanks to their high precision. In pilot studies robotic devices have been
found to reduce the danger of lesions when performing colonoscopy. Outside the operating
room researchers have begun to develop robotic aides for elderly and handicapped people
suchasintelligentroboticwalkersandintelligenttoysthatprovidereminderstotakemedica-
tion andprovide comfort. Researchers arealso working onrobotic devices forrehabilitation
thataidpeopleinperforming certainexercises.
Hazardous environments. Robots have assisted people in cleaning up nuclear waste
most notably in Chernobyl and Three Mile Island. Robots were present after the collapse
of the World Trade Center where they entered structures deemed too dangerous for human
searchandrescuecrews.
Somecountries haveused robots totransport ammunition and todefuse bombsa no-
toriously dangerous task. A number of research projects are presently developing prototype
robots for clearing minefields on land and at sea. Most existing robots for these tasks are
teleoperateda human operates them by remote control. Providing such robots with auton-
omyisanimportant nextstep.
Exploration. Robots have gone where no one has gone before including the surface
of Mars see Figure 25.2b and the cover. Robotic arms assist astronauts in deploying
and retrieving satellites and in building the International Space Station. Robots also help
exploreunderthesea. Theyareroutinelyusedtoacquiremapsofsunkenships. Figure25.29
showsarobotmappinganabandonedcoalminealongwitha3 Dmodelofthemineacquired
a b
Figure25.30 a Roombatheworldsbest-sellingmobilerobotvacuumsfloors. Image
courtesyofi Robotcid:2c 2009. b Robotichandmodeledafterhumanhand. Imagecourtesy
of Universityof Washingtonand Carnegie Mellon University.
using range sensors. In 1996 a team of researches released a legged robot into the crater
of an active volcano to acquire data for climate research. Unmanned air vehicles known as
dronesareusedinmilitaryoperations. Robotsarebecomingveryeffectivetoolsforgathering
D RO NE
information indomainsthataredifficultordangerous for peopletoaccess.
Personal Services. Service is an up-and-coming application domain of robotics. Ser-
vice robots assist individuals in performing daily tasks. Commercially available domestic
service robots include autonomous vacuum cleaners lawn mowers and golf caddies. The
worlds most popular mobile robot is a personal service robot: the robotic vacuum cleaner
Roomba shown in Figure 25.30a. More than three million Roombas have been sold.
R OO MB A
Roombacannavigateautonomously andperform itstaskswithouthumanhelp.
Other service robots operate in public places such as robotic information kiosks that
have been deployed in shopping malls and trade fairs or in museums as tour guides. Ser-
vice tasks require human interaction and theability tocoperobustly withunpredictable and
dynamicenvironments.
Entertainment. Robots have begun to conquer the entertainment and toy industry.
In Figure 25.6b we see robotic soccer a competitive game very much like human soc-
R OB OT IC SO CC ER
cer but played with autonomous mobile robots. Robot soccer provides great opportunities
for research in A I since it raises a range of problems relevant to many other more serious
robot applications. Annual robotic soccer competitions have attracted large numbers of A I
researchers andaddedalotofexcitementtothefieldofrobotics.
Human augmentation. A final application domain of robotic technology is that of
human augmentation. Researchers have developed legged walking machines that can carry
people around very much like a wheelchair. Several research efforts presently focus on the
developmentofdevicesthatmakeiteasierforpeopletowalkormovetheirarmsbyproviding
additionalforcesthroughextraskeletalattachments. Ifsuchdevicesareattachedpermanently
they can be thought of as artificial robotic limbs. Figure 25.30b shows a robotic hand that
mayserveasaprosthetic deviceinthefuture.
Robotic teleoperation or telepresence is another form of human augmentation. Tele-
operation involves carrying out tasks over long distances with the aid of robotic devices.
A popular configuration for robotic teleoperation is the masterslave configuration where
a robot manipulator emulates the motion of a remote human operator measured through a
haptic interface. Underwater vehicles are often teleoperated; the vehicles can go to a depth
thatwouldbedangerous forhumansbutcanstillbeguided bythehumanoperator. Allthese
systemsaugmentpeoplesabilitytointeractwiththeirenvironments. Someprojectsgoasfar
asreplicating humansatleastataverysuperficiallevel. Humanoidrobotsarenowavailable
commercially throughseveralcompaniesin Japan.
Robotics concerns itself with intelligent agents that manipulate the physical world. In this
chapter wehavelearned thefollowingbasicsofrobothardwareandsoftware.
Robots are equipped with sensors for perceiving their environment and effectors with
which they can assert physical forces on their environment. Most robots are either
manipulators anchored atfixedlocations ormobilerobotsthatcanmove.
Robotic perception concerns itself with estimating decision-relevant quantities from
sensor data. To do so we need an internal representation and a method for updating
this internal representation overtime. Commonexamples of hard perceptual problems
includelocalization mappingandobjectrecognition.
Probabilisticfilteringalgorithmssuchas Kalmanfiltersandparticle filtersareuseful
forrobotperception. Thesetechniquesmaintainthebelief stateaposteriordistribution
overstatevariables.
Theplanningofrobotmotionisusuallydoneinconfigurationspacewhereeachpoint
specifiesthelocation andorientation oftherobotanditsjointangles.
Configuration space search algorithms include cell decomposition techniques which
decomposethespaceofallconfigurations intofinitelymanycellsandskeletonization
techniques whichprojectconfiguration spacesontolower-dimensional manifolds. The
motionplanning problem isthensolvedusingsearchinthese simplerstructures.
Apath found byasearch algorithm can be executed byusing the path asthe reference
trajectory for a P ID controller. Controllers are necessary in robotics to accommodate
smallperturbations; pathplanning aloneisusually insufficient.
Potential field techniques navigate robots by potential functions defined overthe dis-
tance to obstacles and the goal location. Potential field techniques may get stuck in
localminimabuttheycangeneratemotiondirectlywithouttheneedforpathplanning.
Sometimes itis easier to specify a robot controller directly rather than deriving apath
from an explicit model of the environment. Such controllers can often be written as
simplefinitestatemachines.
26
P HI LO SO PH IC AL
F OU ND AT IO NS
In which we consider what it means to think and whether artifacts could and
shouldeverdoso.
Philosophers have been around far longer than computers and have been trying to resolve
some questions that relate to A I: How do minds work? Is it possible for machines to act
intelligently in the way that people do and if they did would they have real conscious
minds? Whataretheethicalimplications ofintelligent machines?
Firstsometerminology: theassertionthatmachinescouldactasiftheywereintelligent
iscalled theweak A Ihypothesis byphilosophers andtheassertion thatmachines thatdoso
W EA KA I
areactuallythinking notjustsimulating thinking iscalledthestrong A Ihypothesis.
S TR ON GA I
Most A Iresearchers take the weak A Ihypothesis forgranted and dont careabout the
strong A I hypothesisas long as their program works they dont care whether you call it a
simulation of intelligence or real intelligence. All A I researchers should be concerned with
theethicalimplications oftheirwork.
The proposal for the 1956 summer workshop that defined the field of Artificial Intelligence
Mc Carthyetal.1955madetheassertionthat Everyaspectoflearningoranyotherfeature
ofintelligencecanbesopreciselydescribedthatamachinecanbemadetosimulateit. Thus
A I wasfounded on theassumption that weak A I ispossible. Others have asserted that weak
A I is impossible: Artificial intelligence pursued within the cult of computationalism stands
notevenaghostofachanceofproducing durableresults Sayre1993.
Clearly whether A I isimpossible depends on how itisdefined. In Section 1.1 wede-
fined A Iasthequestforthebestagentprogramonagivenarchitecture. Withthisformulation
A I isby definition possible: forany digital architecture with k bits ofprogram storage there
areexactly2k agentprogramsandallwehavetodotofindthebestoneisenumerateandtest
them all. This might not be feasible for large k but philosophers deal with the theoretical
notthepractical.
Our definition of A I works well for the engineering problem of finding a good agent
given an architecture. Therefore were tempted to end this section right now answering the
title question in the affirmative. But philosophers are interested in the problem of compar-
ing two architectureshuman and machine. Furthermore they have traditionally posed the
C AN MA CH IN ES question notintermsofmaximizingexpected utilitybutratheras Canmachinesthink?
T HI NK?
Thecomputerscientist Edsger Dijkstra 1984 said that Thequestion ofwhether Ma-
C AN SU BM AR IN ES chines Can Think ...isabout asrelevant asthequestion ofwhether Submarines Can Swim.
S WI M?
The American Heritage Dictionarys first definition of swim is To move through water by
means of the limbs fins or tail and most people agree that submarines being limbless
cannot swim. Thedictionary alsodefines flyas Tomovethrough theairbymeansofwings
orwinglikepartsandmostpeopleagreethatairplaneshavingwinglikepartscanfly. How-
everneitherthequestionsnortheanswershaveanyrelevance tothedesignorcapabilities of
airplanes andsubmarines; rathertheyareabout theusage of wordsin English. Thefactthat
ships do swim in Russian only amplifies this point.. The practical possibility of thinking
machineshasbeenwithusforonly50yearsorsonotlongenoughforspeakersof Englishto
settleonameaningforthewordthinkdoes itrequireabrainorjustbrain-like parts.
Alan Turinginhisfamouspaper Computing Machineryand Intelligence1950sug-
gested that instead of asking whether machines can think we should ask whether machines
canpassabehavioral intelligencetestwhichhascometobecalledthe Turing Test. Thetest
T UR IN GT ES T
is for a program to have a conversation via online typed messages with an interrogator for
five minutes. The interrogator then has to guess if the conversation is with a program or a
person; the program passes the test if it fools the interrogator 30 of the time. Turing con-
jectured thatbytheyear2000 acomputerwithastorage of 109 unitscouldbeprogrammed
wellenoughtopassthetest. Hewaswrongprogramshaveyettofoolasophisticatedjudge.
On the other hand many people have been fooled when they didnt know they might
be chatting with a computer. The E LI ZA program and Internet chatbots such as M GO NZ
Humphrys 2008 and N AT AC HA TA have fooled their correspondents repeatedly and the
chatbot C YB ER LO VE R hasattractedtheattentionoflawenforcementbecauseofitspenchant
fortrickingfellowchattersintodivulging enoughpersonalinformation thattheiridentitycan
be stolen. The Loebner Prize competition held annually since 1991 is the longest-running
Turing Test-likecontest. Thecompetitions haveledtobettermodelsofhumantypingerrors.
Turing himself examined a wide variety of possible objections to the possibility of in-
telligent machines including virtually all of those that have been raised in the half-century
sincehispaperappeared. Wewilllookatsomeofthem.
Theargument from disability makesthe claim thatamachine can neverdo X.Asexam-
plesof X Turingliststhefollowing:
Bekindresourcefulbeautifulfriendlyhaveinitiativehaveasenseofhumortellright
from wrong make mistakes fall in love enjoy strawberries and cream make someone
fallin lovewith it learnfromexperienceuse wordsproperlybethe subjectofitsown
thoughthaveasmuchdiversityofbehaviorasmandosomethingreallynew.
In retrospect some of these are rather easywere all familiar with computers that make
mistakes. We are also familiar with a century-old technology that has had a proven ability
to make someone fall in love with itthe teddy bear. Computer chess expert David Levy
predicts that by 2050 people will routinely fall in love with humanoid robots Levy 2007.
Asforarobotfallinginlovethatisacommonthemeinfiction1 buttherehasbeenonlylim-
itedspeculation aboutwhetheritisinfactlikely Kimetal.2007. Programsdoplaychess
checkers and other games; inspect parts on assembly lines steer cars and helicopters; diag-
nose diseases; and do hundreds of other tasks as well as or better than humans. Computers
have made small but significant discoveries in astronomy mathematics chemistry mineral-
ogy biology computer science and other fields. Each of these required performance at the
levelofahumanexpert.
Given what we now know about computers it is not surprising that they do well at
combinatorial problems such as playing chess. Butalgorithms also perform athuman levels
on tasks that seemingly involve human judgment oras Turing put it learning from experi-
ence and the ability to tell right from wrong. As far back as 1955 Paul Meehl see also
Groveand Meehl 1996 studied the decision-making processes of trained experts atsubjec-
tive tasks such as predicting the success of a student in a training program orthe recidivism
of a criminal. In 19 out of the 20 studies he looked at Meehl found that simple statistical
learning algorithmssuchaslinearregression ornaive Bayespredictbetterthantheexperts.
The Educational Testing Service has used an automated program to grade millions of essay
questions on the G MA T exam since 1999. The program agrees with human graders 97 of
thetimeaboutthesamelevelthattwohumangraders agree Burstein etal.2001.
Itisclearthatcomputerscandomanythingsaswellasorbetterthanhumansincluding
thingsthatpeoplebelieverequiregreathumaninsightandunderstanding. Thisdoesnotmean
ofcoursethatcomputersuseinsightandunderstanding inperformingthesetasksthoseare
not part of behavior and we address such questions elsewherebut the point is that ones
firstguessaboutthementalprocesses requiredtoproduceagivenbehaviorisoftenwrong. It
is also true of course that there are many tasks at which computers do not yet excel to put
itmildlyincluding Turingstaskofcarrying onanopen-ended conversation.
It is well known through the work of Turing 1936 and Godel 1931 that certain math-
ematical questions are in principle unanswerable by particular formal systems. Godels in-
completeness theorem see Section 9.5 isthe most famous example of this. Briefly forany
formal axiomatic system F powerful enough to do arithmetic it is possible to construct a
so-called Godelsentence G Fwiththefollowingproperties:
G Fisasentence of Fbutcannotbeprovedwithin F.
If F isconsistent then G Fistrue.
A I2001and Wall-E2008andinsong Noel Cowards1955versionof Lets Do It:Lets Fallin Lovepredicted
probablywelllivetoseemachinesdoit.Hedidnt.
Philosophers suchas J.R.Lucas1961 haveclaimedthatthistheorem showsthatmachines
arementallyinferiortohumansbecausemachinesareformalsystemsthatarelimitedbythe
incompletenesstheoremtheycannotestablishthetruthoftheirown Godelsentencewhile
humans have no such limitation. This claim has caused decades of controversy spawning a
vast literature including two books by the mathematician Sir Roger Penrose 1989 1994
thatrepeattheclaimwithsomefreshtwistssuchasthehypothesisthathumansaredifferent
becausetheirbrainsoperatebyquantumgravity. Wewillexamineonlythreeoftheproblems
withtheclaim.
First Godelsincompletenesstheoremappliesonlytoformalsystemsthatarepowerful
enough to do arithmetic. This includes Turing machines and Lucass claim is in part based
ontheassertionthatcomputersare Turingmachines. Thisisagoodapproximation butisnot
quitetrue. Turingmachines areinfinite whereascomputers arefiniteandanycomputercan
therefore bedescribed asaverylarge system inpropositional logic whichisnotsubject to
Godels incompleteness theorem. Second anagent should notbetooashamed thatitcannot
establish thetruthofsomesentence whileotheragentscan. Considerthesentence
J.R.Lucascannotconsistentlyassertthatthissentenceistrue.
If Lucas asserted this sentence then he would be contradicting himself so therefore Lucas
cannotconsistently assertitandhenceitmustbetrue. Wehavethusdemonstrated thatthere
isasentencethat Lucascannotconsistentlyassertwhileotherpeopleandmachinescan. But
thatdoesnotmakeusthinklessof Lucas. Totakeanotherexamplenohumancouldcompute
the sum of a billion 10 digit numbers in his or her lifetime but a computer could do it in
seconds. Still wedonotseethisasafundamental limitation inthehumans ability tothink.
Humanswerebehavingintelligentlyforthousandsofyearsbeforetheyinventedmathematics
soitisunlikelythatformalmathematicalreasoningplaysmorethanaperipheralroleinwhat
itmeanstobeintelligent.
Third and most important even if we grant that computers have limitations on what
they can prove there is no evidence that humans are immune from those limitations. It is
all too easy to show rigorously that a formal system cannot do X and then claim that hu-
manscando X usingtheirowninformalmethodwithoutgivinganyevidenceforthisclaim.
Indeed itisimpossible toprovethathumansarenotsubjectto Godelsincompleteness theo-
rembecauseanyrigorousproofwouldrequireaformalizationoftheclaimedunformalizable
human talent and hence refute itself. So weare left with an appeal to intuition that humans
can somehow perform superhuman feats of mathematical insight. This appeal is expressed
witharguments suchaswemustassumeourownconsistency ifthought istobepossible at
all Lucas 1976. But if anything humans are known to be inconsistent. This is certainly
true for everyday reasoning but it is also true for careful mathematical thought. A famous
example is the four-color map problem. Alfred Kempe published a proof in 1879 that was
widely accepted and contributed to his election as a Fellow of the Royal Society. In 1890
however Percy Heawoodpointed outaflawandthetheorem remainedunproveduntil1977.
Oneofthemostinfluentialandpersistentcriticismsof A Iasanenterprise wasraisedby Tur-
ingastheargument from informality ofbehavior. Essentially this istheclaim thathuman
behavior is fartoo complex to be captured by any simple set of rules and that because com-
puters candonomorethanfollowasetofrules theycannot generate behavior asintelligent
as that of humans. The inability to capture everything in a set of logical rules is called the
Q UA LI FI CA TI ON qualificationproblemin A I.
P RO BL EM
The principal proponent of this view has been the philosopher Hubert Dreyfus who
hasproduced aseriesofinfluentialcritiques ofartificialintelligence: What Computers Cant
Do 1972 the sequel What Computers Still Cant Do 1992 and with his brother Stuart
Mind Over Machine1986.
The position they criticize came to be called Good Old-Fashioned A I or G OF AI a
term coined by philosopher John Haugeland 1985. G OF AI is supposed to claim that all
intelligent behaviorcanbecapturedbyasystemthatreasonslogicallyfromasetoffactsand
rules describing thedomain. Ittherefore corresponds tothe simplest logical agent described
in Chapter7. Dreyfusiscorrectinsayingthatlogicalagentsarevulnerabletothequalification
problem. Aswesawin Chapter13probabilistic reasoning systemsaremoreappropriate for
open-ended domains. The Dreyfus critique therefore isnot addressed against computers per
se but rather against one particular way of programming them. It is reasonable to suppose
howeverthatabookcalled What First-Order Logical Rule-Based Systems Without Learning
Cant Domighthavehadlessimpact.
Under Dreyfussviewhumanexpertisedoesincludeknowledgeofsomerulesbutonly
asaholistic context orbackground withinwhichhumans operate. Hegivestheexample
ofappropriate social behavior in giving and receiving gifts: Normally one simply responds
intheappropriate circumstances bygiving anappropriate gift. Oneapparently hasadirect
sense ofhow things are done andwhattoexpect. Thesameclaim ismadeinthe context of
chess playing: Amere chess mastermight need tofigure out whatto do but a grandmaster
justseestheboardasdemandingacertainmove...therightresponsejustpopsintohisorher
head. Itiscertainlytruethatmuchofthethoughtprocessesofapresent-giverorgrandmaster
is done at a level that is not open to introspection by the conscious mind. But that does not
mean that the thought processes do not exist. The important question that Dreyfus does not
answer is how the right move gets into the grandmasters head. One is reminded of Daniel
Dennetts1984comment
Itisratherasifphilosophersweretoproclaimthemselvesexpertexplainersofthemeth-
ods of stage magicians and then when we ask how the magician does the sawing-the-
lady-in-halftricktheyexplainthatitisreallyquiteobvious: themagiciandoesntreally
sawherinhalf;hesimplymakesitappearthathedoes. Buthowdoeshedothat? we
ask. Notourdepartmentsaythephilosophers.
Dreyfus and Dreyfus 1986 propose a five-stage process of acquiring expertise beginning
with rule-based processing of the sort proposed in G OF AI and ending with the ability to
select correct responses instantaneously. In making this proposal Dreyfus and Dreyfus in
effectmovefrombeing A Icriticsto A Itheoriststheyproposeaneuralnetworkarchitecture
organized into a vast case library but point out several problems. Fortunately all of their
problems havebeenaddressed somewithpartialsuccess and somewithtotalsuccess. Their
problemsincludethefollowing:
1. Good generalization from examples cannot be achieved without background knowl-
edge. They claim no one has any idea how to incorporate background knowledge into
the neural network learning process. In fact we saw in Chapters 19 and 20 that there
are techniques for using prior knowledge in learning algorithms. Those techniques
howeverrelyontheavailability ofknowledgeinexplicitformsomethingthat Dreyfus
and Dreyfus strenuously deny. Inourview thisisagood reason foraserious redesign
of current models of neural processing so that they can take advantage of previously
learnedknowledgeinthewaythatotherlearning algorithms do.
2. Neural network learning is a form of supervised learning see Chapter 18 requiring
the prior identification of relevant inputs and correct outputs. Therefore they claim
it cannot operate autonomously without the help of a human trainer. In fact learning
without a teacher can be accomplished by unsupervised learning Chapter 20 and
reinforcementlearning Chapter21.
3. Learning algorithms do not perform well with many features and if we pick a subset
offeaturesthereisnoknownwayofaddingnewfeaturesshouldthecurrentsetprove
inadequate to account for the learned facts. In fact new methods such as support
vector machines handle large feature sets very well. With the introduction of large
Web-based data sets manyapplications inareas such aslanguage processing Shaand
Pereira 2003 and computervision Violaand Jones 2002a routinely handle millions
of features. We saw in Chapter 19 that there are also principled ways to generate new
features although muchmoreworkisneeded.
4. The brain is able to direct its sensors to seek relevant information and to process it
to extract aspects relevant to the current situation. But Dreyfus and Dreyfus claim
Currently nodetails ofthismechanism areunderstood orevenhypothesized inaway
that could guide A I research. In fact the field of active vision underpinned by the
theory of information value Chapter 16 is concerned with exactly the problem of
directing sensors and already some robots have incorporated the theoretical results
obtained. S TA NL EYs132-mile trip through thedesert page 28 wasmadepossible in
largepartbyanactivesensingsystem ofthiskind.
Insum manyofthe issues Dreyfus has focused onbackground commonsense knowledge
the qualification problem uncertainty learning compiled forms of decision makingare
indeed important issues and have by now been incorporated into standard intelligent agent
design. Inourviewthisisevidence of A Isprogress notof itsimpossibility.
One of Dreyfus strongest arguments is for situated agents rather than disembodied
logicalinferenceengines. Anagentwhoseunderstandingofdogcomesonlyfromalimited
set of logical sentences such as Dogx Mammalx is at a disadvantage compared
to an agent that has watched dogs run has played fetch with them and has been licked by
one. As philosopher Andy Clark 1998 says Biological brains are first and foremost the
control systems for biological bodies. Biological bodies move and act in rich real-world
surroundings. Tounderstandhowhumanorotheranimalagentsworkwehavetoconsider
E MB OD IE D thewholeagentnotjusttheagentprogram. Indeedtheembodiedcognitionapproachclaims
C OG NI TI ON
that it makes no sense to consider the brain separately: cognition takes place within a body
which is embedded in an environment. We need to study the system as a whole; the brain
augmentsitsreasoning byreferringtotheenvironment asthereaderdoesinperceiving and
creating marks on paper to transfer knowledge. Under the embodied cognition program
robotics vision andothersensorsbecomecentral notperipheral.
Many philosophers have claimed that a machine that passes the Turing Test would still not
be actually thinking but would be only a simulation of thinking. Again the objection was
foreseen by Turing. Hecitesaspeechby Professor Geoffrey Jefferson1949:
Notuntilamachinecouldwriteasonnetorcomposeaconcertobecauseofthoughtsand
emotionsfeltandnotbythechancefallofsymbolscouldweagreethatmachineequals
brainthatisnotonlywriteitbutknowthatithadwrittenit.
Turing calls this theargument from consciousnessthe machine has to beaware ofits own
mentalstatesandactions. Whileconsciousness isanimportant subject Jeffersons keypoint
actually relates to phenomenology or the study of direct experience: the machine has to
actually feel emotions. Others focus on intentionalitythat is the question of whether the
machines purported beliefs desires and other representations are actually about some-
thingintherealworld.
Turings response to the objection is interesting. He could have presented reasons that
machines can in fact be conscious or have phenomenology or have intentions. Instead he
maintains that the question is just as ill-defined as asking Can machines think? Besides
why should we insist on a higher standard for machines than we do for humans? After all
in ordinary life we never have any direct evidence about the internal mental states of other
humans. Nevertheless Turingsays Insteadofarguingcontinuallyoverthispointitisusual
tohavethepoliteconvention thateveryonethinks.
Turing argues that Jefferson would be willing to extend the polite convention to ma-
chinesifonlyhehadexperiencewithonesthatactintelligently. Hecitesthefollowingdialog
whichhasbecomesuchapartof A Isoraltraditionthatwesimplyhavetoinclude it:
H UM AN: Inthefirstlineofyoursonnetwhichreadsshall Icomparetheetoasummers
daywouldnotaspringdaydoaswellorbetter?
M AC HI NE: Itwouldntscan.
H UM AN: Howaboutawintersday.Thatwouldscanallright.
M AC HI NE: Yesbutnobodywantstobecomparedtoawintersday.
H UM AN: Wouldyousay Mr.Pickwickremindedyouof Christmas?
M AC HI NE: Inaway.
H UM AN: Yet Christmasisawintersdayand Idonotthink Mr.Pickwickwouldmind
thecomparison.
M AC HI NE: Idontthinkyoureserious. Byawintersdayonemeansatypicalwinters
dayratherthanaspecialonelike Christmas.
One can easily imagine some future time in which such conversations with machines are
commonplace and it becomes customary to make no linguistic distinction between real
andartificialthinking. Asimilartransition occurred in theyearsafter1848whenartificial
urea wassynthesized forthe first timeby Frederick Wohler. Priorto this event organic and
inorganic chemistry were essentially disjoint enterprises and many thought that no process
could existthatwouldconvert inorganic chemicals intoorganic material. Oncethesynthesis
was accomplished chemists agreed that artificial urea was urea because it had all the right
physical properties. Those who had posited an intrinsic property possessed by organic ma-
terial that inorganic material could never have were faced with the impossibility of devising
anytestthatcouldrevealthesupposed deficiencyofartificialurea.
For thinking we have not yet reached our 1848 and there are those who believe that
artificialthinkingnomatterhowimpressivewillneverbereal. Forexamplethephilosopher
John Searle1980argues asfollows:
Noonesupposesthatacomputersimulationofastormwillleaveusallwet...Whyon
earthwouldanyoneinhisrightmindsupposeacomputersimulationofmentalprocesses
actuallyhadmentalprocesses?pp.3738
While it is easy to agree that computer simulations of storms do not make us wet it is not
clear how to carry this analogy over to computer simulations of mental processes. After
all a Hollywood simulation of a storm using sprinklers and wind machines does make the
actors wet andavideo gamesimulation ofastorm does makethe simulated characters wet.
Most people are comfortable saying that a computer simulation of addition is addition and
ofchessischess. Infactwetypically speakofanimplementation ofaddition orchessnota
simulation. Arementalprocesses morelikestormsormorelikeaddition?
Turings answerthe polite conventionsuggests that the issue will eventually go
away by itself once machines reach a certain level of sophistication. This would have the
effect ofdissolving the difference between weakand strong A I. Against this one may insist
that there is a factual issue at stake: humans do have real minds and machines might or
might not. To address this factual issue we need to understand how it is that humans have
real minds not just bodies that generate neurophysiological processes. Philosophical efforts
M IN DB OD Y to solve this mindbodyproblem are directly relevant to the question ofwhether machines
P RO BL EM
couldhaverealminds.
Themindbodyproblemwasconsideredbytheancient Greekphilosophers andbyvar-
ious schools of Hindu thought but was first analyzed in depth by the 17th-century French
philosopher andmathematician Rene Descartes. His Meditations on First Philosophy1641
considered theminds activity ofthinking aprocess withnospatial extent ormaterial prop-
erties andthephysical processes ofthebody concluding that thetwomustexist inseparate
realmswhat we would now call a dualist theory. The mindbody problem faced by du-
D UA LI SM
alists is the question of how the mind can control the body if the two are really separate.
Descartesspeculated thatthetwomightinteractthroughthepinealglandwhichsimplybegs
thequestion ofhowthemindcontrolsthepinealgland.
Themonisttheoryofmindoftencalled physicalismavoids thisproblem byasserting
M ON IS M
themindisnotseparate fromthebodythat mentalstates arephysical states. Mostmodern
P HY SI CA LI SM
philosophers ofmindarephysicalistsofoneformoranother andphysicalismallowsatleast
in principle for the possibility of strong A I. The problem for physicalists is to explain how
physicalstatesin particular themolecularconfigurations andelectrochemical processes of
thebraincansimultaneouslybementalstatessuchasbeinginpainenjoyingahamburger
M EN TA LS TA TE S
knowingthatoneisridingahorseorbelieving that Viennaisthecapitalof Austria.
Physicalistphilosophershaveattemptedtoexplicatewhatitmeanstosaythatapersonand
byextension acomputeris inaparticularmentalstate. Theyhavefocused inparticularon
intentionalstates. Thesearestatessuchasbelieving knowingdesiring fearingandsoon
I NT EN TI ON AL ST AT E
thatrefertosomeaspectoftheexternalworld. Forexample theknowledgethatoneiseating
ahamburgerisabelief aboutthehamburgerandwhatishappening toit.
If physicalism is correct it must be the case that the proper description of a persons
mental state is determined by that persons brain state. Thus if I am currently focused on
eatingahamburgerinamindfulwaymyinstantaneousbrainstateisaninstanceoftheclassof
mentalstatesknowingthatoneiseatingahamburger. Ofcoursethespecificconfigurations
of all the atoms ofmy brain are not essential: there are manyconfigurations of mybrain or
ofotherpeoplesbrainthatwouldbelongtothesameclassofmentalstates. Thekeypointis
that the same brain state could not correspond toa fundamentally distinct mental state such
astheknowledge thatoneiseatingabanana.
The simplicity of this view is challenged by some simple thought experiments. Imag-
ine if you will that your brain was removed from your body at birth and placed in a mar-
velouslyengineered vat. Thevatsustainsyourbrainallowingittogrowanddevelop. Atthe
sametimeelectronic signalsarefedtoyourbrainfromacomputersimulation ofanentirely
fictitious world and motor signals from your brain are intercepted and used to modify the
simulation as appropriate.2 In fact the simulated life you live replicates exactly the life you
would have lived had your brain not been placed in the vat including simulated eating of
simulatedhamburgers. Thusyoucouldhaveabrainstateidenticaltothatofsomeonewhois
really eating areal hamburger but it would be literally false to say that you have the mental
state knowing that one is eating a hamburger. You arent eating a hamburger you have
neverevenexperienced ahamburger andyoucouldnottherefore havesuchamentalstate.
Thisexampleseemstocontradicttheviewthatbrainstatesdeterminementalstates. One
waytoresolvethedilemmaistosaythatthecontentofmentalstatescanbeinterpreted from
two different points of view. The wide content view interprets it from the point of view
W ID EC ON TE NT
of an omniscient outside observer with access to the whole situation who can distinguish
differencesintheworld. Underthisviewthecontentofmentalstatesinvolvesboththebrain
state and the environment history. Narrow content on the other hand considers only the
N AR RO WC ON TE NT
brainstate. Thenarrowcontentofthebrainstatesofarealhamburger-eater andabrain-in-a-
vathamburger-eater isthesameinbothcases.
Widecontent isentirelyappropriate ifonesgoalsaretoascribe mentalstatestoothers
who share ones world to predict their likely behavior and its effects and so on. This is the
settinginwhichourordinarylanguage aboutmentalcontent hasevolved. Ontheotherhand
if one is concerned with the question of whether A I systems are really thinking and really
do have mental states then narrow content is appropriate; it simply doesnt make sense to
say that whether or not an A I system is really thinking depends on conditions outside that
system. Narrow content is also relevant if we are thinking about designing A I systems or
understandingtheiroperationbecauseitisthenarrowcontentofabrainstatethatdetermines
whatwillbethenarrow content ofthenextbrainstate. Thisleads naturally totheideathat
what matters about a brain statewhat makes it have one kind of mental content and not
anotheris itsfunctional rolewithinthementaloperation oftheentityinvolved.
The theory of functionalism says that a mental state is any intermediate causal condition
F UN CT IO NA LI SM
between input and output. Under functionalist theory any two systems with isomorphic
causal processes would have the same mental states. Therefore a computer program could
have thesamemental states as aperson. Ofcourse wehavenot yetsaid whatisomorphic
really means but the assumption is that there is some level of abstraction below which the
specificimplementation doesnotmatter.
The claims of functionalism are illustrated most clearly by the brain replacement ex-
periment. This thought experiment was introduced by the philosopher Clark Glymour and
wastouchedonby John Searle1980butismostcommonlyassociatedwithroboticist Hans
Moravec1988. Itgoeslikethis: Supposeneurophysiology hasdevelopedtothepointwhere
theinputoutputbehaviorandconnectivityofalltheneuronsinthehumanbrainareperfectly
understood. Supposefurtherthatwecanbuildmicroscopicelectronicdevicesthatmimicthis
behavior and can be smoothly interfaced to neural tissue. Lastly suppose that some mirac-
ulous surgical technique can replace individual neurons with the corresponding electronic
devices without interrupting the operation of the brain as a whole. The experiment consists
ofgradually replacing alltheneurons insomeones headwithelectronic devices.
We are concerned with both the external behavior and the internal experience of the
subject during and after the operation. By the definition of the experiment the subjects
external behavior must remain unchanged compared with what would be observed if the
operation were not carried out.3 Now although the presence or absence of consciousness
cannot easily be ascertained by a third party the subject of the experiment ought at least to
be able to record any changes in his or her own conscious experience. Apparently there is
a direct clash of intuitions as to what would happen. Moravec a robotics researcher and
functionalist isconvinced hisconsciousness wouldremainunaffected. Searleaphilosopher
andbiological naturalist isequallyconvinced hisconsciousness wouldvanish:
You find to your total amazement that you are indeed losing control of your external
behavior. You find forexample thatwhen doctorstest yourvision youhearthem say
Weareholdinguparedobjectinfrontofyou;pleasetelluswhatyousee. Youwant
tocryout Icantseeanything. Imgoingtotallyblind. Butyouhearyourvoicesaying
in a way thatis completelyout of yourcontrol I see a red object in frontof me. ...
your conscious experience slowly shrinks to nothing while your externally observable
behaviorremainsthesame.Searle1992
One can do more than argue from intuition. First note that for the external behavior to re-
mainthesamewhilethesubject gradually becomesunconscious itmustbethecasethatthe
subjects volition is removed instantaneously and totally; otherwise the shrinking of aware-
nesswouldbereflectedinexternalbehavior Help Imshrinking! orwordstothateffect.
This instantaneous removal of volition as a result of gradual neuron-at-a-time replacement
seemsanunlikely claimtohavetomake.
Second consider what happens if we do ask the subject questions concerning his or
herconscious experience during the period when no real neurons remain. Bythe conditions
of the experiment we will get responses such as I feel fine. I must say Im a bit surprised
because Ibelieved Searlesargument. Orwemightpokethesubjectwithapointedstickand
observetheresponse Ouchthathurt. Nowinthenormalcourseofaffairstheskepticcan
dismiss suchoutputs from A Iprograms asmerecontrivances. Certainly itiseasy enough to
usearule suchas Ifsensor12reads Highthenoutput Ouch. Butthepoint hereisthat
because we have replicated the functional properties of a normal human brain we assume
thattheelectronic braincontainsnosuchcontrivances. Thenwemusthaveanexplanation of
themanifestations ofconsciousness produced bytheelectronic brainthatappealsonlytothe
functional properties of the neurons. Andthis explanation must also apply to the real brain
whichhasthesamefunctional properties. Therearethreepossible conclusions:
1. Thecausalmechanismsofconsciousnessthatgeneratethesekindsofoutputsinnormal
brainsarestilloperating intheelectronic version which istherefore conscious.
2. Theconsciousmentaleventsinthenormalbrainhavenocausalconnectiontobehavior
andaremissingfromtheelectronic brain whichistherefore notconscious.
3. Theexperimentisimpossible andtherefore speculation aboutitismeaningless.
Althoughwecannotruleoutthesecondpossibility itreducesconsciousnesstowhatphiloso-
pherscallanepiphenomenalrolesomething thathappens butcastsnoshadowasitwere
E PI PH EN OM EN ON
on the observable world. Furthermore if consciousness is indeed epiphenomenal then it
cannotbethecasethatthesubjectsays Ouchbecauseithurtsthatisbecauseofthecon-
sciousexperience ofpain. Instead thebrainmustcontain a second unconscious mechanism
thatisresponsible forthe Ouch.
Patricia Churchland 1986 points out that the functionalist arguments that operate at
the level of the neuron can also operate at the level of any larger functional unita clump
of neurons a mental module a lobe a hemisphere or the whole brain. That means that if
youacceptthenotionthatthebrainreplacementexperiment showsthatthereplacementbrain
is conscious then you should also believe that consciousness is maintained when the entire
brainisreplacedbyacircuitthatupdatesitsstateandmaps frominputstooutputsviaahuge
lookup table. This is disconcerting to many people including Turing himself who have
theintuition thatlookup tablesarenotconsciousor atleast thattheconscious experiences
generated during table lookup are not the same as those generated during the operation of a
system thatmightbedescribed eveninasimple-minded computational senseasaccessing
andgenerating beliefs introspections goalsandsoon.
A strong challenge to functionalism has been mounted by John Searles 1980 biological
B IO LO GI CA L naturalismaccordingtowhichmentalstatesarehigh-levelemergentfeaturesthatarecaused
N AT UR AL IS M
by low-level physical processes in the neurons and it is the unspecified properties of the
neurons that matter. Thus mental states cannot be duplicated just on the basis of some pro-
gram having the same functional structure with the same inputoutput behavior; we would
requirethattheprogramberunningonanarchitecturewiththesamecausalpowerasneurons.
Tosupporthisview Searledescribes ahypothetical system thatisclearlyrunning aprogram
andpassesthe Turing Testbutthatequallyclearlyaccording to Searledoesnotunderstand
anything of its inputs and outputs. His conclusion is that running the appropriate program
i.e.havingtherightoutputs isnotasufficient condition forbeingamind.
The system consists of a human who understands only English equipped with a rule
book writtenin English and various stacks ofpaper someblank somewithindecipherable
inscriptions. The human therefore plays the role of the C PU the rule book is the program
and the stacks of paper are the storage device. The system is inside a room with a small
opening totheoutside. Through theopening appear slips ofpaperwithindecipherable sym-
bols. Thehuman findsmatching symbols intherule book andfollows theinstructions. The
instructionsmayincludewritingsymbolsonnewslipsofpaperfindingsymbolsinthestacks
rearrangingthestacksandsoon. Eventuallytheinstructionswillcauseoneormoresymbols
tobetranscribed ontoapieceofpaperthatispassedbacktotheoutsideworld.
Sofar so good. But from the outside wesee a system that is taking input in the form
of Chinese sentences and generating answers in Chinese that are as intelligent as those
in the conversation imagined by Turing.4 Searle then argues: the person in the room does
not understand Chinese given. The rule book and the stacks of paper being just pieces of
paper do not understand Chinese. Therefore there is no understanding of Chinese. Hence
accordingto Searlerunningtherightprogramdoesnotnecessarilygenerateunderstanding.
Like Turing Searle considered and attempted to rebuff a number of replies to his ar-
gument. Several commentators including John Mc Carthy and Robert Wilensky proposed
what Searle calls the systems reply. The objection is that asking if the human in the room
understands Chinese is analogous to asking if the C PU can take cube roots. In both cases
the answer is no and in both cases according to the systems reply the entire system does
havethecapacityinquestion. Certainlyifoneasksthe Chinese Roomwhetheritunderstands
Chinese theanswerwouldbeaffirmativeinfluent Chinese. By Turingspoliteconvention
thisshouldbeenough. Searlesresponse istoreiteratethe pointthattheunderstanding isnot
inthehumanand cannot beinthepaper sothere cannot beanyunderstanding. Heseemsto
be relying on the argument that a property of the whole must reside in one of the parts. Yet
millionsofyearshasnobearingonthelogicalstructureoftheargument. Oneaimofphilosophicaltrainingisto
developafinelyhonedsenseofwhichobjectionsaregermaneandwhicharenot.
wateriswet even though neither Hnor O is. Therealclaim madeby Searle rests upon the
2
followingfouraxioms Searle1990:
1. Computerprogramsareformalsyntactic.
2. Humanmindshavementalcontents semantics.
3. Syntaxbyitselfisneitherconstitutive ofnorsufficient forsemantics.
4. Brainscauseminds.
From the first three axioms Searle concludes that programs are not sufficient for minds. In
otherwordsanagentrunningaprogram mightbeamindbutitisnotnecessarilyamindjust
by virtue of running the program. From the fourth axiom he concludes Any other system
capable of causing minds would have to have causal powers at least equivalent to those
of brains. From there he infers that any artificial brain would have to duplicate the causal
powers of brains not just run a particular program and that human brains do not produce
mentalphenomena solelybyvirtueofrunningaprogram.
The axioms are controversial. For example axioms 1 and 2 rely on an unspecified
distinction between syntax and semantics that seems to be closely related to the distinction
betweennarrowandwidecontent. Ontheonehandwecanviewcomputersasmanipulating
syntactic symbols; on the other we can view them as manipulating electric current which
happens to be what brains mostly do according to our current understanding. So it seems
wecouldequally saythatbrainsaresyntactic.
Assuming we are generous in interpreting the axioms then the conclusionthat pro-
grams are not sufficient for mindsdoes follow. But the conclusion is unsatisfactoryall
Searlehasshownisthatifyouexplicitly denyfunctionalism thatiswhathisaxiom3does
then you cant necessarily conclude that non-brains are minds. This is reasonable enough
almost tautologicalso the whole argument comes down to whether axiom 3 can be ac-
cepted. Accordingto Searlethepointofthe Chinese Roomargumentistoprovideintuitions
for axiom 3. The public reaction shows that the argument is acting as what Daniel Dennett
1991 calls an intuition pump: it amplifies ones prior intuitions so biological naturalists
I NT UI TI ON PU MP
are more convinced of their positions and functionalists are convinced only that axiom 3 is
unsupported or that in general Searles argument is unconvincing. The argument stirs up
combatants but has done little to change anyones opinion. Searle remains undeterred and
has recently started calling the Chinese Room arefutation ofstrong A I rather than just an
argument Snell2008.
Eventhosewhoaccept axiom 3andthus accept Searles argument haveonly theirin-
tuitionstofallbackonwhendecidingwhatentitiesareminds. Theargumentpurportstoshow
thatthe Chinese Roomisnotamindbyvirtueofrunningtheprogrambuttheargumentsays
nothing about how to decide whether the room ora computer some other type of machine
oranalienisamindbyvirtueofsomeotherreason. Searlehimselfsaysthatsomemachines
do have minds: humans are biological machines with minds. According to Searle human
brains may or may not be running something like an A I program but if they are that is not
the reason they are minds. It takes more to make a mindaccording to Searle something
equivalent tothecausal powersof individual neurons. What these powersare isleftunspec-
ified. Itshould benoted however that neurons evolved to fulfill functional rolescreatures
withneuronswerelearninganddecidinglongbeforeconsciousness appearedonthescene. It
wouldbearemarkable coincidence ifsuchneurons justhappened togenerate consciousness
because of some causal powers that are irrelevant to their functional capabilities; after all it
isthefunctional capabilities thatdictate survivaloftheorganism.
In the case of the Chinese Room Searle relies on intuition not proof: just look at the
room; whats there to be a mind? But one could make the same argument about the brain:
just look at this collection of cells or of atoms blindly operating according to the laws of
biochemistryorofphysicswhatstheretobeamind? Whycanahunkofbrainbeamind
whileahunkoflivercannot? Thatremainsthegreatmystery.
Running through all the debates about strong A Ithe elephant in the debating room so
to speakis the issue of consciousness. Consciousness is often broken down into aspects
C ON SC IO US NE SS
such as understanding and self-awareness. The aspect we will focus on is that of subjective
experience: whyitisthatitfeelslikesomethingtohavecertainbrainstatese.g.whileeating
ahamburger whereasitpresumablydoesnotfeellikeanythingtohaveotherphysicalstates
e.g.whilebeingarock. Thetechnicaltermfortheintrinsicnatureofexperiences isqualia
Q UA LI A
fromthe Latinwordmeaningroughly suchthings.
Qualia present a challenge for functionalist accounts of the mind because different
qualia could be involved in what are otherwise isomorphic causal processes. Consider for
I NV ER TE D exampletheinvertedspectrumthoughtexperimentwhichthesubjectiveexperienceofper-
S PE CT RU M
son X when seeing red objects is the same experience that the rest of us experience when
seeinggreenobjectsandviceversa. X stillcallsredobjectsredstopsforredtrafficlights
and agrees that the redness of red traffic lights is a more intense red than the redness of the
settingsun. Yet Xssubjective experience isjustdifferent.
Qualiaarechallengingnotjustforfunctionalism butforallofscience. Supposeforthe
sakeofargumentthatwehavecompletedtheprocessofscientificresearchonthebrainwe
have found that neural process P in neuron N transforms molecule Ainto molecule B
and so on and on. There is simply no currently accepted form of reasoning that would lead
from such findings to the conclusion that the entity owning those neurons has any particular
subjective experience. This explanatory gap has led some philosophers to conclude that
E XP LA NA TO RY GA P
humansaresimplyincapable offorming aproperunderstanding oftheirownconsciousness.
Others notably Daniel Dennett 1991 avoid the gap by denying the existence of qualia
attributing themtoaphilosophical confusion.
Turinghimselfconcedesthatthequestionofconsciousness isadifficultonebutdenies
that it has much relevance to the practice of A I: I do not wish to give the impression that I
think there is no mystery about consciousness ... But I do not think these mysteries neces-
sarily need to be solved before we can answer the question with which we are concerned in
thispaper. Weagreewith Turingweareinterested increating programs thatbehave intel-
ligently. Theadditional project ofmaking themconscious isnotonethatweareequipped to
takeonnoronewhosesuccesswewouldbeabletodetermine.
So far we have concentrated on whether we can develop A I but we must also consider
whetherweshould. Iftheeffectsof A Itechnologyaremorelikelytobenegativethanpositive
then it would be the moral responsibility of workers in the field to redirect their research.
Many new technologies have had unintended negative side effects: nuclear fission brought
Chernobyl and the threat of global destruction; the internal combustion engine brought air
pollution global warming and the paving-over of paradise. In a sense automobiles are
robotsthathaveconquered theworldbymakingthemselves indispensable.
All scientists and engineers face ethical considerations of how they should act on the
job what projects should or should not be done and how they should be handled. See the
handbook on the Ethics of Computing Berleur and Brunnstein 2001. A I however seems
toposesomefreshproblemsbeyondthatofsaybuilding bridgesthatdontfalldown:
Peoplemightlosetheirjobstoautomation.
Peoplemighthavetoomuchortoolittleleisuretime.
Peoplemightlosetheirsenseofbeingunique.
A Isystemsmightbeusedtowardundesirable ends.
Theuseof A Isystemsmightresultinalossofaccountability.
Thesuccessof A Imightmeantheendofthehumanrace.
Wewilllookateachissueinturn.
Peoplemightlose theirjobsto automation. Themodern industrial economy has be-
comedependent oncomputers ingeneralandselect A Iprogramsinparticular. Forexample
much of the economy especially in the United States depends on the availability of con-
sumer credit. Credit card applications charge approvals and fraud detection are now done
by A I programs. One could say that thousands of workers have been displaced by these A I
programs but in fact if you took away the A I programs these jobs would not exist because
human laborwould addanunacceptable costtothetransactions. Sofar automation through
information technology in general and A I in particular has created more jobs than it has
eliminated and has created more interesting higher-paying jobs. Nowthat thecanonical A I
program isanintelligent agent designed toassistahumanlossofjobsislessofaconcern
than it was when A I focused on expert systems designed to replace humans. But some
researchersthinkthatdoingthecompletejobistherightgoalfor A I.Inreflectingonthe25th
Anniversaryofthe A AA INils Nilsson2005setasachallenge thecreation ofhuman-level
A Ithat could pass theemployment testrather than the Turing Testarobot that could learn
todoanyoneofarangeofjobs. Wemayendupinafuturewhereunemploymentishighbut
eventheunemployed serveasmanagers oftheirowncadreofrobotworkers.
Peoplemighthavetoomuchortoolittleleisuretime. Alvin Tofflerwrotein Future
Shock 1970 The work week has been cut by 50 percent since the turn of the century. It
is not out of the way to predict that it will be slashed in half again by 2000. Arthur C.
Clarke 1968b wrote that people in 2001 might be faced with a future of utter boredom
wherethemainproblem inlifeisdeciding whichofseveral hundred T Vchannels toselect.
The only one of these predictions that has come close to panning out is the number of T V
channels. Instead people working inknowledge-intensive industries have found themselves
partofanintegratedcomputerized systemthatoperates24hoursaday;tokeepuptheyhave
beenforcedtoworklongerhours. Inanindustrialeconomyrewardsareroughlyproportional
to the time invested; working 10 more would tend to mean a 10 increase in income. In
an information economy marked by high-bandwidth communication and easy replication of
intellectualpropertywhat Frankand Cook1996callthe Winner-Take-All Societythere
isalargerewardforbeingslightlybetterthanthecompetition;working10morecouldmean
a 100 increase in income. So there is increasing pressure on everyone to work harder. A I
increases the pace of technological innovation and thus contributes to this overall trend but
A Ialso holds the promise ofallowing ustotake some timeoffand let ourautomated agents
handlethingsforawhile. Tim Ferriss2007recommendsusingautomationandoutsourcing
toachieveafour-hour workweek.
Peoplemightlosetheirsenseofbeingunique. In Computer Powerand Human Rea-
son Weizenbaum 1976 theauthorofthe E LI ZA program points outsome ofthe potential
threatsthat A Iposestosociety. Oneof Weizenbaumsprincipalargumentsisthat A Iresearch
makespossibletheideathathumansareautomataanideathatresultsinalossofautonomy
orevenofhumanity. Wenotethattheideahasbeenaroundmuchlongerthan A Igoingback
at least to L Homme Machine La Mettrie 1748. Humanity has survived other setbacks to
our sense of uniqueness: De Revolutionibus Orbium Coelestium Copernicus 1543 moved
the Earth away from the center of the solar system and Descent of Man Darwin 1871 put
Homosapiens atthesamelevelasotherspecies. A Iifwidelysuccessful maybeatleast as
threatening tothemoralassumptions of21st-century societyas Darwinstheoryofevolution
wastothoseofthe19thcentury.
A I systems might be used toward undesirable ends. Advanced technologies have
oftenbeenusedbythepowerfultosuppress theirrivals. Asthenumbertheorist G.H.Hardy
wrote Hardy1940 Ascienceissaidtobeusefulifitsdevelopmenttendstoaccentuatethe
existing inequalities in the distribution of wealth or more directly promotes the destruction
of human life. Thisholds forall sciences A Ibeing no exception. Autonomous A I systems
are now commonplace onthe battlefield; the U.S.military deployed over5000 autonomous
aircraft and 12000 autonomous ground vehicles in Iraq Singer 2009. One moral theory
holds thatmilitary robots arelikemedieval armortaken toitslogical extreme: noonewould
have moral objections to a soldier wanting to wear a helmet when being attacked by large
angry axe-wielding enemies and a teleoperated robot is like a very safe form of armor. On
the other hand robotic weapons pose additional risks. To the extent that human decision
making is taken out of the firing loop robots may end up making decisions that lead to the
killing of innocent civilians. At a larger scale the possession of powerful robots like the
possession ofsturdyhelmetsmaygiveanationoverconfidence causingittogotowarmore
recklessly than necessary. In most wars at least one party is overconfident in its military
abilitiesotherwise theconflictwouldhavebeenresolvedpeacefully.
Weizenbaum 1976 also pointed out that speech recognition technology could lead to
widespread wiretapping andhencetoalossofcivilliberties. Hedidntforesee aworldwith
terroristthreatsthatwouldchangethebalanceofhowmuchsurveillancepeoplearewillingto
accept buthedidcorrectly recognize that A Ihasthepotential tomass-produce surveillance.
Hisprediction has inpart come true: the U.K.now has anextensive network ofsurveillance
cameras andothercountries routinely monitor Webtrafficandtelephone calls. Someaccept
thatcomputerization leads toaloss ofprivacy Sun Microsystems C EO Scott Mc Nealyhas
said You have zero privacy anyway. Get over it. David Brin 1998 argues that loss of
privacy is inevitable and the way to combat the asymmetry of power of the state over the
individual is to make the surveillance accessible to all citizens. Etzioni 2004 argues for a
balancing ofprivacyandsecurity; individual rightsandcommunity.
Theuseof A Isystemsmightresultinalossofaccountability. Inthelitigious atmo-
sphere that prevails in the United States legal liability becomes an important issue. When a
physicianreliesonthejudgmentofamedicalexpertsystemforadiagnosis whoisatfaultif
thediagnosisiswrong? Fortunatelydueinparttothegrowinginfluenceofdecision-theoretic
methods in medicine it is now accepted that negligence cannot be shown if the physician
performsmedicalproceduresthathavehigh expectedutilityeveniftheactualresultiscatas-
trophic for the patient. The question should therefore be Who is at fault if the diagnosis is
unreasonable? So far courts have held that medical expert systems play the same role as
medicaltextbooksandreferencebooks;physiciansareresponsibleforunderstanding therea-
soning behind any decision and for using their own judgment in deciding whether to accept
the systems recommendations. In designing medical expert systems as agents therefore
the actions should be thought of not as directly affecting the patient but as influencing the
physiciansbehavior. Ifexpertsystemsbecomereliablymoreaccuratethanhumandiagnosti-
ciansdoctorsmightbecomelegallyliableiftheydontusetherecommendationsofanexpert
system. Atul Gawande2002exploresthispremise.
Similarissuesarebeginningtoariseregardingtheuseofintelligentagentsonthe Inter-
net. Someprogress hasbeen madeinincorporating constraints intointelligent agents sothat
theycannotforexampledamagethefilesofotherusers Weldand Etzioni1994. Theprob-
lem is magnified when money changes hands. If monetary transactions are made on ones
behalf byan intelligent agent isone liable forthedebts incurred? Would itbepossible for
an intelligent agent to have assets itself and to perform electronic trades on its own behalf?
So far these questions do not seem to be well understood. To our knowledge no program
has been granted legal status as an individual for the purposes of financial transactions; at
present it seems unreasonable to do so. Programs are also not considered to be drivers
forthepurposes ofenforcing trafficregulations onrealhighways. In California lawatleast
there do not seem to be any legal sanctions to prevent an automated vehicle from exceeding
thespeedlimitsalthoughthedesignerofthevehicles controlmechanismwouldbeliablein
thecase ofanaccident. Aswithhuman reproductive technology the lawhasyet tocatch up
withthenewdevelopments.
The success of A I might mean the end of the human race. Almost any technology
hasthepotentialtocauseharminthewronghandsbutwith A Iandroboticswehavethenew
problemthatthewronghandsmightbelongtothetechnologyitself. Countlesssciencefiction
stories have warned about robots or robothuman cyborgs running amok. Early examples
include Mary Shelleys Frankenstein orthe Modern Prometheus18185 and Karel Capeks
play R.U.R.1921 in which robots conquer the world. In movies we have The Terminator
1984 which combines the cliches of robots-conquer-the-world with time travel and The
Matrix1999 whichcombinesrobots-conquer-the-world withbrain-in-a-vat.
Itseemsthat robots are theprotagonists ofso manyconquer-the-world stories because
theyrepresent theunknown justlikethewitches andghosts oftales fromearliereras orthe
Martians from The War of the Worlds Wells 1898. The question is whether an A I system
posesabiggerriskthantraditional software. Wewilllookatthreesourcesofrisk.
First the A I systems state estimation may be incorrect causing it to do the wrong
thing. Forexampleanautonomous carmightincorrectly estimatethepositionofacarinthe
adjacent lane leading toan accident that might kill the occupants. Moreseriously amissile
defense system might erroneously detect an attack and launch a counterattack leading to
the death of billions. These risks are not really risks of A I systemsin both cases the same
mistakecouldjustaseasilybemadebyahumanasbyacomputer. Thecorrectwaytomitigate
these risks is to design a system with checks and balances so that a single state-estimation
errordoesnotpropagatethrough thesystemunchecked.
Second specifying the right utility function for an A I system to maximize is not so
easy. Forexamplewemightproposeautilityfunctiondesignedtominimizehumansuffering
expressed as anadditive reward function overtimeasin Chapter17. Giventhewayhumans
are however well always find a way to suffer even in paradise; so the optimal decision for
the A I system is toterminate thehuman race as soon as possibleno humans no suffering.
With A I systems then we need to be very careful what we ask for whereas humans would
have no trouble realizing that the proposed utility function cannot be taken literally. On the
otherhandcomputersneednotbetaintedbytheirrationalbehaviorsdescribedin Chapter16.
Humans sometimes use their intelligence in aggressive ways because humans have some
innately aggressive tendencies due tonatural selection. Themachines webuild need not be
innately aggressive unless we decide to build them that way or unless they emerge as the
end product ofamechanism design that encourages aggressive behavior. Fortunately there
aretechniques suchasapprenticeship learning thatallowsustospecifyautilityfunctionby
example. One can hope that a robot that is smart enough to figure out how to terminate the
humanraceisalsosmartenoughtofigureoutthatthatwasnottheintended utilityfunction.
Third the A I systems learning function may cause it to evolve into a system with
unintended behavior. This scenario is the most serious and is unique to A I systems so we
willcoveritinmoredepth. I.J.Goodwrote1965
U LT RA IN TE LL IG EN T Let an ultraintelligent machine be defined as a machine that can far surpass all the
M AC HI NE
intellectualactivitiesofanymanhoweverclever. Sincethedesignofmachinesisoneof
theseintellectualactivitiesanultraintelligentmachinecoulddesignevenbettermachines;
there would then unquestionablybe an intelligence explosion and the intelligence of
manwouldbeleftfarbehind. Thusthefirstultraintelligentmachineisthelastinvention
that man need evermake providedthat the machine is docile enoughto tell us how to
keepitundercontrol.
T EC HN OL OG IC AL The intelligence explosion has also been called the technological singularity by mathe-
S IN GU LA RI TY
matics professor and science fiction author Vernor Vinge who writes 1993 Within thirty
yearswewillhavethetechnological meanstocreatesuperhuman intelligence. Shortlyafter
thehumanerawillbeended. Goodand Vingeandmanyotherscorrectlynotethatthecurve
of technological progress on many measures is growing exponentially at present consider
Moores Law. Howeveritisaleaptoextrapolatethatthecurvewillcontinuetoasingularity
ofnear-infinitegrowth. Sofareveryothertechnologyhasfollowedan S-shapedcurvewhere
the exponential growth eventually tapers off. Sometimes new technologies step in when the
old ones plateau; sometimes wehit hard limits. With less than acentury ofhigh-technology
historytogoonitisdifficulttoextrapolate hundreds ofyearsahead.
Note that the concept of ultraintelligent machines assumes that intelligence is an es-
pecially important attribute and if you have enough of it all problems can be solved. But
we know there are limits on computability and computational complexity. If the problem
ofdefining ultraintelligent machines orevenapproximations tothem happens to fallin the
class of say N EX PT IM E-complete problems and if there are no heuristic shortcuts then
even exponential progress in technology wont helpthe speed of light puts a strict upper
bound on how much computing can bedone; problems beyond that limitwill not besolved.
Westilldontknowwherethoseupperbounds are.
Vinge is concerned about the coming singularity but some computer scientists and
futurists relish it. Hans Moravec 2000 encourages ustogiveeveryadvantage toourmind
children the robots we create which may surpass us in intelligence. There is even a new
wordtranshumanismfortheactivesocialmovementthatlooksforwardtothisfuturein
T RA NS HU MA NI SM
which humans are merged withor replaced byrobotic and biotech inventions. Suffice it
tosaythatsuchissuespresentachallengeformostmoraltheoristswhotakethepreservation
ofhuman lifeand thehuman species tobe agood thing. Ray Kurzweiliscurrently themost
visibleadvocate forthesingularity viewwritingin The Singularity is Near2005:
The Singularitywillallowustotranscendtheselimitationsofourbiologicalbodiesand
brain. We will gainpoweroverourfates. Ourmortality will be in ourown hands. We
willbeabletoliveaslongaswewantasubtlydifferentstatementfromsayingwewill
liveforever.Wewillfullyunderstandhumanthinkingandwillvastlyextendandexpand
itsreach. Bytheendofthiscenturythenonbiologicalportionofourintelligencewillbe
trillionsoftrillionsoftimesmorepowerfulthanunaidedhumanintelligence.
Kurzweil also notes the potential dangers writing Butthe Singularity will also amplify the
abilitytoactonourdestructive inclinations soitsfullstoryhasnotyetbeenwritten.
If ultraintelligent machines are a possibility we humans would do well to make sure
that wedesign their predecessors in such a way that they design themselves to treat us well.
Science fiction writer Isaac Asimov 1942 was the first to address this issue with his three
lawsofrobotics:
1. A robot may not injure a human being or through inaction allow a human being to
cometoharm.
2. Arobotmustobeyordersgiventoitbyhumanbeingsexcept wheresuchorderswould
conflictwiththe First Law.
3. Arobotmustprotectitsownexistenceaslongassuchprotection doesnotconflictwith
the Firstor Second Law.
Theselawsseemreasonable atleast toushumans.6 Butthetrickishowtoimplement these
laws. In the Asimov story Roundabout a robot is sent to fetch some selenium. Later the
robot isfound wandering inacircle around theselenium source. Everytimeitheads toward
the source it senses a danger and the third law causes it to veer away. But every time it
veers away the danger recedes and the power of the second law takes over causing it to
veer back towards the selenium. The set of points that define the balancing point between
thetwolawsdefinesacircle. Thissuggests thatthelawsarenotlogical absolutes butrather
are weighed against each other with a higher weighting for the earlier laws. Asimov was
probably thinking of an architecture based on control theoryperhaps a linear combination
offactorswhile todaythemostlikelyarchitecture wouldbeaprobabilistic reasoning agent
that reasons over probability distributions of outcomes and maximizes utility as defined by
the three laws. But presumably we dont want our robots to prevent a human from crossing
the street because of the nonzero chance of harm. That means that the negative utility for
harm to a human must be much greater than for disobeying but that each of the utilities is
finitenotinfinite.
Yudkowsky2008goesintomoredetailabouthowtodesigna Friendly A I.Heasserts
F RI EN DL YA I
that friendliness adesire notto harm humans should bedesigned infrom the start but that
thedesigners shouldrecognize boththattheirowndesigns maybeflawedandthattherobot
willlearnandevolveovertime. Thusthechallenge isoneofmechanism designto definea
mechanism for evolving A I systems under a system of checks and balances and to give the
systemsutilityfunctions thatwillremainfriendly inthefaceofsuchchanges.
Wecantjustgiveaprogramastaticutilityfunctionbecausecircumstancesandourde-
sired responses tocircumstances change overtime. Forexample iftechnology hadallowed
us to design a super-powerful A I agent in 1800 and endow it with the prevailing morals of
thetime itwouldbefighting today toreestablish slavery andabolish womens right tovote.
Ontheotherhandifwebuild an A Iagenttodayandtellittoevolveitsutility function how
canweassure that itwontreason that Humans think itismoraltokill annoying insects in
part because insect brains are so primitive. But human brains are primitive compared to my
powerssoitmustbemoralformetokillhumans.
Omohundro 2008 hypothesizes that even an innocuous chess program could pose a
risk to society. Similarly Marvin Minsky once suggested that an A I program designed to
solve the Riemann Hypothesis might end up taking over all the resources of Earth to build
more powerful supercomputers to help achieve its goal. The moral is that even if you only
want your program to play chess or prove theorems if you give it the capability to learn
and alter itself you need safeguards. Omohundro concludes that Social structures which
cause individuals to bearthecost oftheirnegative externalities would go along way toward
ensuringastableandpositivefuture Thisseemstobeanexcellentideaforsocietyingeneral
regardlessofthepossibility ofultraintelligent machines.
tosacrificeitsownlifetosaveahuman.
